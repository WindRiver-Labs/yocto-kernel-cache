From a0a4e3d5ee025f84e28de544382747ebed7bffc6 Mon Sep 17 00:00:00 2001
From: Subbaraya Sundeep <sbhatta@marvell.com>
Date: Tue, 3 Nov 2020 15:06:13 +0530
Subject: [PATCH 0932/1921] octeontx2-pf: Use multi segments in NIX CQE_RX

To receive frame sizes upto 64K a single receive
packet buffer is not sufficient because maximum
receive buffer size which can be set is 32K
(configured in NIX_RQ_CTX_S[LPB_SIZEM1]<35:24>).
Hardware transfers bigger frames using RX scatter
gather and writes the CQE RX descriptor with
addresses of all the segments of the packet.
This patch modifies current code to read
all the segments in CQE_RX and to use fixed
size packet receive buffers of 4K.

Change-Id: I59b6a2bc3fbac035dd3ba3e679066baa4646c37f
Signed-off-by: Subbaraya Sundeep <sbhatta@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/39964
Tested-by: sa_ip-sw-jenkins <sa_ip-sw-jenkins@marvell.com>
Reviewed-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 .../marvell/octeontx2/nic/otx2_common.c       | 11 ---
 .../ethernet/marvell/octeontx2/nic/otx2_pf.c  | 38 ++++++++---
 .../marvell/octeontx2/nic/otx2_txrx.c         | 67 ++++++++++++++-----
 3 files changed, 77 insertions(+), 39 deletions(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
index d135e3a1c292..9595a11dcbea 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
@@ -234,8 +234,6 @@ int otx2_hw_set_mtu(struct otx2_nic *pfvf, int mtu)
 		return -ENOMEM;
 	}
 
-	/* Add EDSA/HIGIG2 header len to maxlen */
-	pfvf->max_frs = mtu +  OTX2_ETH_HLEN + pfvf->addl_mtu;
 	req->maxlen = pfvf->max_frs;
 
 	err = otx2_sync_mbox_msg(&pfvf->mbox);
@@ -1626,7 +1624,6 @@ void otx2_set_cints_affinity(struct otx2_nic *pfvf)
 	}
 }
 
-#define OTX2_MAX_MTU_SUPPORTED	16380
 u16 otx2_get_max_mtu(struct otx2_nic *pfvf)
 {
 	struct nix_hw_info *rsp;
@@ -1647,14 +1644,6 @@ u16 otx2_get_max_mtu(struct otx2_nic *pfvf)
 		rsp = (struct nix_hw_info *)
 		       otx2_mbox_get_rsp(&pfvf->mbox.mbox, 0, &req->hdr);
 
-		/* Currently driver doesn't support multi-segment ingress pkts
-		 * and the maximum buffer size we can configure in HW is 32K,
-		 * so limit the MTU for now to 16K which is same as that of
-		 * max MTU on RPM interfaces
-		 */
-		if (rsp->max_mtu > OTX2_MAX_MTU_SUPPORTED)
-			rsp->max_mtu = OTX2_MAX_MTU_SUPPORTED;
-
 		/* HW counts VLAN insertion bytes (8 for double tag)
 		 * irrespective of whether SQE is requesting to insert VLAN
 		 * in the packet or not. Hence these 8 bytes have to be
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
index fc36e61d5f50..4693189b691d 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
@@ -1350,6 +1350,7 @@ static int otx2_init_hw_resources(struct otx2_nic *pf)
 	struct nix_lf_free_req *free_req;
 	struct mbox *mbox = &pf->mbox;
 	struct otx2_hw *hw = &pf->hw;
+	size_t max_pkt_bytes;
 	struct msg_req *req;
 	int err = 0, lvl;
 
@@ -1361,18 +1362,33 @@ static int otx2_init_hw_resources(struct otx2_nic *pf)
 	hw->sqpool_cnt = hw->tot_tx_queues;
 	hw->pool_cnt = hw->rqpool_cnt + hw->sqpool_cnt;
 
-	/* Get the size of receive buffers to allocate.
-	 *
-	 * Depending interface mode ie timestamp and/or Higig2 enabled
-	 * and presence of DSA tags in L2 header, the packet size would
-	 * vary, hence consider all while calculating receive buffer size.
+	/* Add EDSA/HIGIG2 header length and timestamp length to maxlen */
+	pf->max_frs = pf->netdev->mtu + OTX2_ETH_HLEN + pf->addl_mtu +
+		      OTX2_HW_TIMESTAMP_LEN + pf->xtra_hdr;
+
+	/* The data transferred by NIX to memory consists of actual packet
+	 * plus additional data which has timestamp and/or EDSA/HIGIG2
+	 * headers if interface is configured in corresponding modes.
+	 * NIX transfers entire data using 6 segments/buffers and writes
+	 * a CQE_RX descriptor with those segment addresses. First segment
+	 * has additional data prepended to packet. Also software omits a
+	 * headroom of 128 bytes in each receive buffer. Hence the maximum
+	 * number of actual packet bytes per one CQE_RX descriptor with a
+	 * receive buffer of size 4K is:
+	 * ((4k - 128) * 6) - additional bytes in segment 1.
 	 */
-	pf->rbsize = pf->netdev->mtu + OTX2_ETH_HLEN;
-	pf->rbsize += OTX2_HW_TIMESTAMP_LEN + pf->addl_mtu + pf->xtra_hdr;
-	if (!pf->xdp_prog)
-		pf->rbsize = RCV_FRAG_LEN(pf->rbsize);
-	else
-		pf->rbsize = RCV_FRAG_LEN(XDP_PACKET_HEADROOM + pf->rbsize);
+	max_pkt_bytes = DMA_BUFFER_LEN(0x1000) * 6;
+	max_pkt_bytes -= OTX2_ETH_HLEN + OTX2_HW_TIMESTAMP_LEN +
+			 pf->addl_mtu + pf->xtra_hdr;
+
+	/* Use packet receive buffers of size 4K since hardware can support
+	 * MTU of size 16K for RPM and 64K for LBK. If MTU is more than
+	 * maximum pkt_bytes then use receive buffers of size 12K so that
+	 * it works for 64K MTU.
+	 */
+	pf->rbsize = 0x1000;
+	if (pf->netdev->mtu > max_pkt_bytes)
+		pf->rbsize = 0x3000;
 
 	mutex_lock(&mbox->lock);
 	/* NPA init */
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
index 47739a9dea66..11b3ea83d956 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
@@ -307,12 +307,11 @@ static bool otx2_check_rcv_errors(struct otx2_nic *pfvf,
 		/* For now ignore all the NPC parser errors and
 		 * pass the packets to stack.
 		 */
-		if (cqe->sg.segs == 1)
-			return false;
+		return false;
 	}
 
 	/* If RXALL is enabled pass on packets to stack. */
-	if (cqe->sg.segs == 1 && (pfvf->netdev->features & NETIF_F_RXALL))
+	if (pfvf->netdev->features & NETIF_F_RXALL)
 		return false;
 
 	/* Free buffer back to pool */
@@ -321,7 +320,7 @@ static bool otx2_check_rcv_errors(struct otx2_nic *pfvf,
 	return true;
 }
 
-static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
+static bool otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 				 struct napi_struct *napi,
 				 struct otx2_cq_queue *cq,
 				 struct nix_cqe_rx_s *cqe)
@@ -329,22 +328,18 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 	struct nix_rx_parse_s *parse = &cqe->parse;
 	struct sk_buff *skb = NULL;
 
-	if (unlikely(parse->errlev || parse->errcode || cqe->sg.segs > 1)) {
+	if (unlikely(parse->errlev || parse->errcode)) {
 		if (otx2_check_rcv_errors(pfvf, cqe, cq->cq_idx))
-			return;
+			return false;
 	}
 
-	if (pfvf->xdp_prog)
-		if (otx2_xdp_rcv_pkt_handler(pfvf, cqe, cq))
-			return;
-
 	skb = napi_get_frags(napi);
 	if (unlikely(!skb))
-		return;
+		return false;
 
-	otx2_skb_add_frag(pfvf, skb, cqe->sg.seg_addr,
-			  cqe->sg.seg_size, parse);
-	cq->pool_ptrs++;
+	if (pfvf->xdp_prog)
+		if (otx2_xdp_rcv_pkt_handler(pfvf, cqe, cq))
+			return false;
 
 	otx2_set_rxhash(pfvf, cqe, skb);
 
@@ -354,7 +349,21 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 
 	otx2_set_taginfo(parse, skb);
 
-	napi_gro_frags(napi);
+	return true;
+}
+
+static void otx2_rcv_add_frags(struct otx2_nic *pfvf,
+			       struct napi_struct *napi,
+			       struct otx2_cq_queue *cq,
+			       struct nix_cqe_rx_s *cqe,
+			       u64 iova, int len)
+{
+	struct nix_rx_parse_s *parse = &cqe->parse;
+	/* skb is checked for NULL before calling this function */
+	struct sk_buff *skb = napi_get_frags(napi);
+
+	otx2_skb_add_frag(pfvf, skb, iova, len, parse);
+	cq->pool_ptrs++;
 }
 
 static int otx2_rx_napi_handler(struct otx2_nic *pfvf,
@@ -362,7 +371,9 @@ static int otx2_rx_napi_handler(struct otx2_nic *pfvf,
 				struct otx2_cq_queue *cq, int budget)
 {
 	struct nix_cqe_rx_s *cqe;
+	struct nix_rx_sg_s *sg;
 	int processed_cqe = 0;
+	u8 segs;
 
 	/* Make sure HW writes to CQ are done */
 	dma_rmb();
@@ -377,8 +388,30 @@ static int otx2_rx_napi_handler(struct otx2_nic *pfvf,
 		cq->cq_head++;
 		cq->cq_head &= (cq->cqe_cnt - 1);
 
-		otx2_rcv_pkt_handler(pfvf, napi, cq, cqe);
-
+		if (!otx2_rcv_pkt_handler(pfvf, napi, cq, cqe))
+			goto next;
+
+		sg = &cqe->sg;
+		while (sg->subdc == NIX_SUBDC_SG && sg->segs) {
+			/* Hardware supports three segements per SG */
+			for (segs = 0; segs < sg->segs; segs++) {
+				if (segs == 0)
+					otx2_rcv_add_frags(pfvf, napi, cq, cqe,
+							   sg->seg_addr,
+							   sg->seg_size);
+				if (segs == 1)
+					otx2_rcv_add_frags(pfvf, napi, cq, cqe,
+							   sg->seg2_addr,
+							   sg->seg2_size);
+				if (segs == 2)
+					otx2_rcv_add_frags(pfvf, napi, cq, cqe,
+							   sg->seg3_addr,
+							   sg->seg3_size);
+			}
+			sg++;
+		}
+		napi_gro_frags(napi);
+next:
 		cqe->hdr.cqe_type = NIX_XQE_TYPE_INVALID;
 		cqe->sg.seg_addr = 0x00;
 		processed_cqe++;
-- 
2.31.1

