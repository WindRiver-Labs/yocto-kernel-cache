From 8d0e3a1b76a26131d400b0bb8cdce9f137309d85 Mon Sep 17 00:00:00 2001
From: Srujana Challa <schalla@marvell.com>
Date: Fri, 23 Apr 2021 09:53:28 +0530
Subject: [PATCH 1556/1921] crypto: octeontx2: sync with upstream driver

This patch syncs CPT PF driver with upstreamed CPT PF
driver.

Signed-off-by: Srujana Challa <schalla@marvell.com>
Change-Id: If19a9cd94da158658bfd5accac8fc51e9a376afd
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/50776
Reviewed-by: Jerin Jacob Kollanukkaran <jerinj@marvell.com>
Reviewed-by: Sunil Kovvuri Goutham <sgoutham@marvell.com>
Tested-by: Sunil Kovvuri Goutham <sgoutham@marvell.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 drivers/crypto/marvell/octeontx2/Makefile     |   18 +-
 .../marvell/octeontx2/otx2_cpt_common.h       |  122 +-
 .../marvell/octeontx2/otx2_cpt_hw_types.h     |  405 ++--
 .../marvell/octeontx2/otx2_cpt_mbox_common.c  |  299 +--
 .../marvell/octeontx2/otx2_cpt_mbox_common.h  |  106 --
 .../marvell/octeontx2/otx2_cpt_reqmgr.h       |   86 +-
 drivers/crypto/marvell/octeontx2/otx2_cptlf.c |  429 +++++
 drivers/crypto/marvell/octeontx2/otx2_cptlf.h |  186 +-
 .../marvell/octeontx2/otx2_cptlf_main.c       |  967 ----------
 drivers/crypto/marvell/octeontx2/otx2_cptpf.h |   46 +-
 .../marvell/octeontx2/otx2_cptpf_main.c       |  633 +++----
 .../marvell/octeontx2/otx2_cptpf_mbox.c       |  711 +++----
 .../marvell/octeontx2/otx2_cptpf_ucode.c      | 1685 +++++------------
 .../marvell/octeontx2/otx2_cptpf_ucode.h      |   46 +-
 drivers/crypto/marvell/octeontx2/otx2_cptvf.h |   14 +-
 .../marvell/octeontx2/otx2_cptvf_algs.c       |  500 ++---
 .../marvell/octeontx2/otx2_cptvf_algs.h       |   55 +-
 .../marvell/octeontx2/otx2_cptvf_main.c       |  384 ++--
 .../marvell/octeontx2/otx2_cptvf_mbox.c       |  222 +--
 .../marvell/octeontx2/otx2_cptvf_reqmgr.c     |  231 ++-
 20 files changed, 2699 insertions(+), 4446 deletions(-)
 delete mode 100644 drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.h
 create mode 100644 drivers/crypto/marvell/octeontx2/otx2_cptlf.c
 delete mode 100644 drivers/crypto/marvell/octeontx2/otx2_cptlf_main.c

diff --git a/drivers/crypto/marvell/octeontx2/Makefile b/drivers/crypto/marvell/octeontx2/Makefile
index df5df622bf51..b9c6201019e0 100644
--- a/drivers/crypto/marvell/octeontx2/Makefile
+++ b/drivers/crypto/marvell/octeontx2/Makefile
@@ -1,14 +1,10 @@
-# SPDX-License-Identifier: GPL-2.0
+# SPDX-License-Identifier: GPL-2.0-only
 obj-$(CONFIG_CRYPTO_DEV_OCTEONTX2_CPT) += octeontx2-cpt.o octeontx2-cptvf.o
 
-common-objs := otx2_cpt_mbox_common.o
-octeontx2-cpt-objs := otx2_cptpf_main.o otx2_cptpf_mbox.o otx2_cptpf_ucode.o \
-		      ${common-objs}
-octeontx2-cptvf-objs := otx2_cptvf_main.o otx2_cptvf_mbox.o otx2_cptlf_main.o \
-			otx2_cptvf_reqmgr.o otx2_cptvf_algs.o
+octeontx2-cpt-objs := otx2_cptpf_main.o otx2_cptpf_mbox.o \
+		      otx2_cpt_mbox_common.o otx2_cptpf_ucode.o otx2_cptlf.o
+octeontx2-cptvf-objs := otx2_cptvf_main.o otx2_cptvf_mbox.o otx2_cptlf.o \
+			otx2_cpt_mbox_common.o otx2_cptvf_reqmgr.o \
+			otx2_cptvf_algs.o
 
-ifeq ($(CONFIG_CRYPTO_DEV_OCTEONTX2_CPT), m)
-	octeontx2-cptvf-objs += ${common-objs}
-endif
-
-ccflags-y += -I$(src)/../../../net/ethernet/marvell/octeontx2/af/
+ccflags-y += -I$(srctree)/drivers/net/ethernet/marvell/octeontx2/af
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cpt_common.h b/drivers/crypto/marvell/octeontx2/otx2_cpt_common.h
index 861a184cafca..b847548a156a 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cpt_common.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cpt_common.h
@@ -1,11 +1,5 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell CPT OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
 
 #ifndef __OTX2_CPT_COMMON_H
@@ -18,21 +12,18 @@
 #include <linux/crypto.h>
 #include "otx2_cpt_hw_types.h"
 #include "rvu.h"
+#include "mbox.h"
 
 #define OTX2_CPT_MAX_VFS_NUM 128
-#define OTX2_CPT_MAX_LFS_NUM 64
-
-#define OTX2_CPT_RVU_PFFUNC(pf, func)	\
-	((((pf) & RVU_PFVF_PF_MASK) << RVU_PFVF_PF_SHIFT) | \
-	(((func) & RVU_PFVF_FUNC_MASK) << RVU_PFVF_FUNC_SHIFT))
-
 #define OTX2_CPT_RVU_FUNC_ADDR_S(blk, slot, offs) \
-		((blk << 20) | (slot << 12) | offs)
+		(((blk) << 20) | ((slot) << 12) | (offs))
+#define OTX2_CPT_RVU_PFFUNC(pf, func)	\
+		((((pf) & RVU_PFVF_PF_MASK) << RVU_PFVF_PF_SHIFT) | \
+		(((func) & RVU_PFVF_FUNC_MASK) << RVU_PFVF_FUNC_SHIFT))
 
-#define OTX2_CPT_DMA_MINALIGN 128
 #define OTX2_CPT_INVALID_CRYPTO_ENG_GRP 0xFF
-
 #define OTX2_CPT_NAME_LENGTH 64
+#define OTX2_CPT_DMA_MINALIGN 128
 
 #define BAD_OTX2_CPT_ENG_TYPE OTX2_CPT_MAX_ENG_TYPES
 
@@ -43,6 +34,84 @@ enum otx2_cpt_eng_type {
 	OTX2_CPT_MAX_ENG_TYPES,
 };
 
+/* Take mbox id from end of CPT mbox range in AF (range 0xA00 - 0xBFF) */
+#define MBOX_MSG_RX_INLINE_IPSEC_LF_CFG 0xBFE
+#define MBOX_MSG_GET_ENG_GRP_NUM        0xBFF
+#define MBOX_MSG_GET_CAPS               0xBFD
+#define MBOX_MSG_GET_KVF_LIMITS         0xBFC
+
+/*
+ * Message request to config cpt lf for inline inbound ipsec.
+ * This message is only used between CPT PF <-> CPT VF
+ */
+struct otx2_cpt_rx_inline_lf_cfg {
+	struct mbox_msghdr hdr;
+	u16 sso_pf_func;
+};
+
+/*
+ * Message request and response to get engine group number
+ * which has attached a given type of engines (SE, AE, IE)
+ * This messages are only used between CPT PF <=> CPT VF
+ */
+struct otx2_cpt_egrp_num_msg {
+	struct mbox_msghdr hdr;
+	u8 eng_type;
+};
+
+struct otx2_cpt_egrp_num_rsp {
+	struct mbox_msghdr hdr;
+	u8 eng_type;
+	u8 eng_grp_num;
+};
+
+/*
+ * Message request and response to get kernel crypto limits
+ * This messages are only used between CPT PF <-> CPT VF
+ */
+struct otx2_cpt_kvf_limits_msg {
+	struct mbox_msghdr hdr;
+};
+
+struct otx2_cpt_kvf_limits_rsp {
+	struct mbox_msghdr hdr;
+	u8 kvf_limits;
+};
+
+/* CPT HW capabilities */
+union otx2_cpt_eng_caps {
+	u64 u;
+	struct {
+		u64 reserved_0_4:5;
+		u64 mul:1;
+		u64 sha1_sha2:1;
+		u64 chacha20:1;
+		u64 zuc_snow3g:1;
+		u64 sha3:1;
+		u64 aes:1;
+		u64 kasumi:1;
+		u64 des:1;
+		u64 crc:1;
+		u64 reserved_14_63:50;
+	};
+};
+
+/*
+ * Message request and response to get HW capabilities for each
+ * engine type (SE, IE, AE).
+ * This messages are only used between CPT PF <=> CPT VF
+ */
+struct otx2_cpt_caps_msg {
+	struct mbox_msghdr hdr;
+};
+
+struct otx2_cpt_caps_rsp {
+	struct mbox_msghdr hdr;
+	u16 cpt_pf_drv_version;
+	u8 cpt_revision;
+	union otx2_cpt_eng_caps eng_caps[OTX2_CPT_MAX_ENG_TYPES];
+};
+
 static inline void otx2_cpt_write64(void __iomem *reg_base, u64 blk, u64 slot,
 				    u64 offs, u64 val)
 {
@@ -56,4 +125,23 @@ static inline u64 otx2_cpt_read64(void __iomem *reg_base, u64 blk, u64 slot,
 	return readq_relaxed(reg_base +
 			     OTX2_CPT_RVU_FUNC_ADDR_S(blk, slot, offs));
 }
+
+int otx2_cpt_send_ready_msg(struct otx2_mbox *mbox, struct pci_dev *pdev);
+int otx2_cpt_send_mbox_msg(struct otx2_mbox *mbox, struct pci_dev *pdev);
+
+int otx2_cpt_send_af_reg_requests(struct otx2_mbox *mbox,
+				  struct pci_dev *pdev);
+int otx2_cpt_add_read_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			     u64 reg, u64 *val, int blkaddr);
+int otx2_cpt_add_write_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			      u64 reg, u64 val, int blkaddr);
+int otx2_cpt_read_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			 u64 reg, u64 *val, int blkaddr);
+int otx2_cpt_write_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			  u64 reg, u64 val, int blkaddr);
+struct otx2_cptlfs_info;
+int otx2_cpt_attach_rscrs_msg(struct otx2_cptlfs_info *lfs);
+int otx2_cpt_detach_rsrcs_msg(struct otx2_cptlfs_info *lfs);
+int otx2_cpt_msix_offset_msg(struct otx2_cptlfs_info *lfs);
+
 #endif /* __OTX2_CPT_COMMON_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cpt_hw_types.h b/drivers/crypto/marvell/octeontx2/otx2_cpt_hw_types.h
index 436bea359da6..f25bb8cf3275 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cpt_hw_types.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cpt_hw_types.h
@@ -1,11 +1,5 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
 
 #ifndef __OTX2_CPT_HW_TYPES_H
@@ -14,16 +8,13 @@
 #include <linux/types.h>
 
 /* Device IDs */
-#define OTX2_CPT_PCI_PF_DEVICE_ID 0xa0FD
-#define OTX2_CPT_PCI_VF_DEVICE_ID 0xa0FE
+#define OTX2_CPT_PCI_PF_DEVICE_ID 0xA0FD
+#define OTX2_CPT_PCI_VF_DEVICE_ID 0xA0FE
 
 /* Mailbox interrupts offset */
 #define OTX2_CPT_PF_MBOX_INT	6
 #define OTX2_CPT_PF_INT_VEC_E_MBOXX(x, a) ((x) + (a))
 
-/* Number of MSIX supported in PF */
-#define	OTX2_CPT_PF_MSIX_VECTORS 7
-
 /* Maximum supported microcode groups */
 #define OTX2_CPT_MAX_ENGINE_GROUPS 8
 
@@ -32,83 +23,83 @@
 /*
  * CPT VF MSIX vectors and their offsets
  */
-#define	OTX2_CPT_VF_MSIX_VECTORS 1
+#define OTX2_CPT_VF_MSIX_VECTORS 1
 #define OTX2_CPT_VF_INTR_MBOX_MASK BIT(0)
 
 /* CPT LF MSIX vectors */
-#define	OTX2_CPT_LF_MSIX_VECTORS 2
+#define OTX2_CPT_LF_MSIX_VECTORS 2
 
 /* OcteonTX2 CPT PF registers */
-#define OTX2_CPT_PF_CONSTANTS		(0x0ll)
-#define OTX2_CPT_PF_RESET		(0x100ll)
-#define OTX2_CPT_PF_DIAG		(0x120ll)
-#define OTX2_CPT_PF_BIST_STATUS		(0x160ll)
-#define OTX2_CPT_PF_ECC0_CTL		(0x200ll)
-#define OTX2_CPT_PF_ECC0_FLIP		(0x210ll)
-#define OTX2_CPT_PF_ECC0_INT		(0x220ll)
-#define OTX2_CPT_PF_ECC0_INT_W1S	(0x230ll)
-#define OTX2_CPT_PF_ECC0_ENA_W1S	(0x240ll)
-#define OTX2_CPT_PF_ECC0_ENA_W1C	(0x250ll)
-#define OTX2_CPT_PF_MBOX_INTX(b)	(0x400ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_MBOX_INT_W1SX(b)	(0x420ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_MBOX_ENA_W1CX(b)	(0x440ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_MBOX_ENA_W1SX(b)	(0x460ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_EXEC_INT		(0x500ll)
-#define OTX2_CPT_PF_EXEC_INT_W1S	(0x520ll)
-#define OTX2_CPT_PF_EXEC_ENA_W1C	(0x540ll)
-#define OTX2_CPT_PF_EXEC_ENA_W1S	(0x560ll)
-#define OTX2_CPT_PF_GX_EN(b)		(0x600ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_EXEC_INFO		(0x700ll)
-#define OTX2_CPT_PF_EXEC_BUSY		(0x800ll)
-#define OTX2_CPT_PF_EXEC_INFO0		(0x900ll)
-#define OTX2_CPT_PF_EXEC_INFO1		(0x910ll)
-#define OTX2_CPT_PF_INST_REQ_PC		(0x10000ll)
-#define OTX2_CPT_PF_INST_LATENCY_PC	(0x10020ll)
-#define OTX2_CPT_PF_RD_REQ_PC		(0x10040ll)
-#define OTX2_CPT_PF_RD_LATENCY_PC	(0x10060ll)
-#define OTX2_CPT_PF_RD_UC_PC		(0x10080ll)
-#define OTX2_CPT_PF_ACTIVE_CYCLES_PC	(0x10100ll)
-#define OTX2_CPT_PF_EXE_CTL		(0x4000000ll)
-#define OTX2_CPT_PF_EXE_STATUS		(0x4000008ll)
-#define OTX2_CPT_PF_EXE_CLK		(0x4000010ll)
-#define OTX2_CPT_PF_EXE_DBG_CTL		(0x4000018ll)
-#define OTX2_CPT_PF_EXE_DBG_DATA	(0x4000020ll)
-#define OTX2_CPT_PF_EXE_BIST_STATUS	(0x4000028ll)
-#define OTX2_CPT_PF_EXE_REQ_TIMER	(0x4000030ll)
-#define OTX2_CPT_PF_EXE_MEM_CTL		(0x4000038ll)
-#define OTX2_CPT_PF_EXE_PERF_CTL	(0x4001000ll)
-#define OTX2_CPT_PF_EXE_DBG_CNTX(b)	(0x4001100ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_EXE_PERF_EVENT_CNT	(0x4001180ll)
-#define OTX2_CPT_PF_EXE_EPCI_INBX_CNT(b) (0x4001200ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_EXE_EPCI_OUTBX_CNT(b) (0x4001240ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_ENGX_UCODE_BASE(b)	(0x4002000ll | (u64)(b) << 3)
-#define OTX2_CPT_PF_QX_CTL(b)		(0x8000000ll | (u64)(b) << 20)
-#define OTX2_CPT_PF_QX_GMCTL(b)		(0x8000020ll | (u64)(b) << 20)
-#define OTX2_CPT_PF_QX_CTL2(b)		(0x8000100ll | (u64)(b) << 20)
-#define OTX2_CPT_PF_VFX_MBOXX(b, c)	(0x8001000ll | (u64)(b) << 20 | \
-					 (u64)(c) << 8)
+#define OTX2_CPT_PF_CONSTANTS           (0x0)
+#define OTX2_CPT_PF_RESET               (0x100)
+#define OTX2_CPT_PF_DIAG                (0x120)
+#define OTX2_CPT_PF_BIST_STATUS         (0x160)
+#define OTX2_CPT_PF_ECC0_CTL            (0x200)
+#define OTX2_CPT_PF_ECC0_FLIP           (0x210)
+#define OTX2_CPT_PF_ECC0_INT            (0x220)
+#define OTX2_CPT_PF_ECC0_INT_W1S        (0x230)
+#define OTX2_CPT_PF_ECC0_ENA_W1S        (0x240)
+#define OTX2_CPT_PF_ECC0_ENA_W1C        (0x250)
+#define OTX2_CPT_PF_MBOX_INTX(b)        (0x400 | (b) << 3)
+#define OTX2_CPT_PF_MBOX_INT_W1SX(b)    (0x420 | (b) << 3)
+#define OTX2_CPT_PF_MBOX_ENA_W1CX(b)    (0x440 | (b) << 3)
+#define OTX2_CPT_PF_MBOX_ENA_W1SX(b)    (0x460 | (b) << 3)
+#define OTX2_CPT_PF_EXEC_INT            (0x500)
+#define OTX2_CPT_PF_EXEC_INT_W1S        (0x520)
+#define OTX2_CPT_PF_EXEC_ENA_W1C        (0x540)
+#define OTX2_CPT_PF_EXEC_ENA_W1S        (0x560)
+#define OTX2_CPT_PF_GX_EN(b)            (0x600 | (b) << 3)
+#define OTX2_CPT_PF_EXEC_INFO           (0x700)
+#define OTX2_CPT_PF_EXEC_BUSY           (0x800)
+#define OTX2_CPT_PF_EXEC_INFO0          (0x900)
+#define OTX2_CPT_PF_EXEC_INFO1          (0x910)
+#define OTX2_CPT_PF_INST_REQ_PC         (0x10000)
+#define OTX2_CPT_PF_INST_LATENCY_PC     (0x10020)
+#define OTX2_CPT_PF_RD_REQ_PC           (0x10040)
+#define OTX2_CPT_PF_RD_LATENCY_PC       (0x10060)
+#define OTX2_CPT_PF_RD_UC_PC            (0x10080)
+#define OTX2_CPT_PF_ACTIVE_CYCLES_PC    (0x10100)
+#define OTX2_CPT_PF_EXE_CTL             (0x4000000)
+#define OTX2_CPT_PF_EXE_STATUS          (0x4000008)
+#define OTX2_CPT_PF_EXE_CLK             (0x4000010)
+#define OTX2_CPT_PF_EXE_DBG_CTL         (0x4000018)
+#define OTX2_CPT_PF_EXE_DBG_DATA        (0x4000020)
+#define OTX2_CPT_PF_EXE_BIST_STATUS     (0x4000028)
+#define OTX2_CPT_PF_EXE_REQ_TIMER       (0x4000030)
+#define OTX2_CPT_PF_EXE_MEM_CTL         (0x4000038)
+#define OTX2_CPT_PF_EXE_PERF_CTL        (0x4001000)
+#define OTX2_CPT_PF_EXE_DBG_CNTX(b)     (0x4001100 | (b) << 3)
+#define OTX2_CPT_PF_EXE_PERF_EVENT_CNT  (0x4001180)
+#define OTX2_CPT_PF_EXE_EPCI_INBX_CNT(b)  (0x4001200 | (b) << 3)
+#define OTX2_CPT_PF_EXE_EPCI_OUTBX_CNT(b) (0x4001240 | (b) << 3)
+#define OTX2_CPT_PF_ENGX_UCODE_BASE(b)  (0x4002000 | (b) << 3)
+#define OTX2_CPT_PF_QX_CTL(b)           (0x8000000 | (b) << 20)
+#define OTX2_CPT_PF_QX_GMCTL(b)         (0x8000020 | (b) << 20)
+#define OTX2_CPT_PF_QX_CTL2(b)          (0x8000100 | (b) << 20)
+#define OTX2_CPT_PF_VFX_MBOXX(b, c)     (0x8001000 | (b) << 20 | \
+					 (c) << 8)
 
 /* OcteonTX2 CPT LF registers */
-#define OTX2_CPT_LF_CTL                 (0x10ull)
-#define OTX2_CPT_LF_DONE_WAIT           (0x30ull)
-#define OTX2_CPT_LF_INPROG              (0x40ull)
-#define OTX2_CPT_LF_DONE                (0x50ull)
-#define OTX2_CPT_LF_DONE_ACK            (0x60ull)
-#define OTX2_CPT_LF_DONE_INT_ENA_W1S    (0x90ull)
-#define OTX2_CPT_LF_DONE_INT_ENA_W1C    (0xa0ull)
-#define OTX2_CPT_LF_MISC_INT            (0xb0ull)
-#define OTX2_CPT_LF_MISC_INT_W1S        (0xc0ull)
-#define OTX2_CPT_LF_MISC_INT_ENA_W1S    (0xd0ull)
-#define OTX2_CPT_LF_MISC_INT_ENA_W1C    (0xe0ull)
-#define OTX2_CPT_LF_Q_BASE              (0xf0ull)
-#define OTX2_CPT_LF_Q_SIZE              (0x100ull)
-#define OTX2_CPT_LF_Q_INST_PTR          (0x110ull)
-#define OTX2_CPT_LF_Q_GRP_PTR           (0x120ull)
-#define OTX2_CPT_LF_NQX(a)              (0x400ull | (u64)(a) << 3)
-#define OTX2_CPT_RVU_FUNC_BLKADDR_SHIFT	20
+#define OTX2_CPT_LF_CTL                 (0x10)
+#define OTX2_CPT_LF_DONE_WAIT           (0x30)
+#define OTX2_CPT_LF_INPROG              (0x40)
+#define OTX2_CPT_LF_DONE                (0x50)
+#define OTX2_CPT_LF_DONE_ACK            (0x60)
+#define OTX2_CPT_LF_DONE_INT_ENA_W1S    (0x90)
+#define OTX2_CPT_LF_DONE_INT_ENA_W1C    (0xa0)
+#define OTX2_CPT_LF_MISC_INT            (0xb0)
+#define OTX2_CPT_LF_MISC_INT_W1S        (0xc0)
+#define OTX2_CPT_LF_MISC_INT_ENA_W1S    (0xd0)
+#define OTX2_CPT_LF_MISC_INT_ENA_W1C    (0xe0)
+#define OTX2_CPT_LF_Q_BASE              (0xf0)
+#define OTX2_CPT_LF_Q_SIZE              (0x100)
+#define OTX2_CPT_LF_Q_INST_PTR          (0x110)
+#define OTX2_CPT_LF_Q_GRP_PTR           (0x120)
+#define OTX2_CPT_LF_NQX(a)              (0x400 | (a) << 3)
+#define OTX2_CPT_RVU_FUNC_BLKADDR_SHIFT 20
 /* LMT LF registers */
-#define OTX2_CPT_LMT_LFBASE		BIT_ULL(OTX2_CPT_RVU_FUNC_BLKADDR_SHIFT)
-#define OTX2_CPT_LMT_LF_LMTLINEX(a)	(OTX2_CPT_LMT_LFBASE | 0x000 | \
+#define OTX2_CPT_LMT_LFBASE             BIT_ULL(OTX2_CPT_RVU_FUNC_BLKADDR_SHIFT)
+#define OTX2_CPT_LMT_LF_LMTLINEX(a)     (OTX2_CPT_LMT_LFBASE | 0x000 | \
 					 (a) << 12)
 /* RVU VF registers */
 #define OTX2_RVU_VF_INT                 (0x20)
@@ -121,14 +112,14 @@
  *
  * Enumerates ucode errors
  */
-enum otx2_cpt_ucode_error_code_e {
-	CPT_NO_UCODE_ERROR = 0x00,
-	ERR_OPCODE_UNSUPPORTED = 0x01,
+enum otx2_cpt_ucode_comp_code_e {
+	OTX2_CPT_UCC_SUCCESS = 0x00,
+	OTX2_CPT_UCC_INVALID_OPCODE = 0x01,
 
 	/* Scatter gather */
-	ERR_SCATTER_GATHER_WRITE_LENGTH = 0x02,
-	ERR_SCATTER_GATHER_LIST = 0x03,
-	ERR_SCATTER_GATHER_NOT_SUPPORTED = 0x04,
+	OTX2_CPT_UCC_SG_WRITE_LENGTH = 0x02,
+	OTX2_CPT_UCC_SG_LIST = 0x03,
+	OTX2_CPT_UCC_SG_NOT_SUPPORTED = 0x04,
 
 };
 
@@ -142,7 +133,6 @@ enum otx2_cpt_comp_e {
 	OTX2_CPT_COMP_E_NOTDONE = 0x00,
 	OTX2_CPT_COMP_E_GOOD = 0x01,
 	OTX2_CPT_COMP_E_FAULT = 0x02,
-	OTX2_CPT_RESERVED = 0x03,
 	OTX2_CPT_COMP_E_HWERR = 0x04,
 	OTX2_CPT_COMP_E_INSTERR = 0x05,
 	OTX2_CPT_COMP_E_LAST_ENTRY = 0x06
@@ -221,41 +211,29 @@ union otx2_cpt_inst_s {
 	u64 u[8];
 
 	struct {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		u64 nixtx_addr:60;
-		u64 doneint:1;
-		u64 nixtxl:3;
-#else /* Word 0 - Little Endian */
+		/* Word 0 */
 		u64 nixtxl:3;
 		u64 doneint:1;
 		u64 nixtx_addr:60;
-#endif /* Word 0 - End */
+		/* Word 1 */
 		u64 res_addr;
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 2 - Big Endian */
-		u64 rvu_pf_func:16;
-		u64 reserved_172_175:4;
-		u64 grp:10;
-		u64 tt:2;
-		u64 tag:32;
-#else /* Word 2 - Little Endian */
+		/* Word 2 */
 		u64 tag:32;
 		u64 tt:2;
 		u64 grp:10;
 		u64 reserved_172_175:4;
 		u64 rvu_pf_func:16;
-#endif /* Word 2 - End */
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 3 - Big Endian */
-		u64 wq_ptr:61;
-		u64 reserved_194_193:2;
-		u64 qord:1;
-#else /* Word 3 - Little Endian */
+		/* Word 3 */
 		u64 qord:1;
 		u64 reserved_194_193:2;
 		u64 wq_ptr:61;
-#endif /* Word 3 - End */
+		/* Word 4 */
 		u64 ei0;
+		/* Word 5 */
 		u64 ei1;
+		/* Word 6 */
 		u64 ei2;
+		/* Word 7 */
 		u64 ei3;
 	} s;
 };
@@ -289,17 +267,10 @@ union otx2_cpt_res_s {
 	u64 u[2];
 
 	struct {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		u64 reserved_17_63:47;
-		u64 doneint:1;
-		u64 uc_compcode:8;
-		u64 compcode:8;
-#else /* Word 0 - Little Endian */
 		u64 compcode:8;
 		u64 uc_compcode:8;
 		u64 doneint:1;
 		u64 reserved_17_63:47;
-#endif /* Word 0 - End */
 		u64 reserved_64_127;
 	} s;
 };
@@ -311,19 +282,12 @@ union otx2_cpt_res_s {
  * This register contains implementation-related parameters of CPT.
  */
 union otx2_cptx_af_constants1 {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_af_constants1_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		uint64_t reserved_48_63        : 16;
-		uint64_t ae                    : 16;
-		uint64_t ie                    : 16;
-		uint64_t se                    : 16;
-#else /* Word 0 - Little Endian */
-		uint64_t se                    : 16;
-		uint64_t ie                    : 16;
-		uint64_t ae                    : 16;
-		uint64_t reserved_48_63        : 16;
-#endif /* Word 0 - End */
+		u64 se:16;
+		u64 ie:16;
+		u64 ae:16;
+		u64 reserved_48_63:16;
 	} s;
 };
 
@@ -334,27 +298,16 @@ union otx2_cptx_af_constants1 {
  *
  */
 union otx2_cptx_lf_misc_int {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_lf_misc_int_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		uint64_t reserved_7_63               : 57;
-		uint64_t fault                       :  1;
-		uint64_t hwerr                       :  1;
-		uint64_t reserved_4_4                :  1;
-		uint64_t nwrp                        :  1;
-		uint64_t irde                        :  1;
-		uint64_t nqerr                       :  1;
-		uint64_t reserved_0_0                :  1;
-#else /* Word 0 - Little Endian */
-		uint64_t reserved_0_0                :  1;
-		uint64_t nqerr                       :  1;
-		uint64_t irde                        :  1;
-		uint64_t nwrp                        :  1;
-		uint64_t reserved_4_4                :  1;
-		uint64_t hwerr                       :  1;
-		uint64_t fault                       :  1;
-		uint64_t reserved_7_63               : 57;
-#endif
+		u64 reserved_0:1;
+		u64 nqerr:1;
+		u64 irde:1;
+		u64 nwrp:1;
+		u64 reserved_4:1;
+		u64 hwerr:1;
+		u64 fault:1;
+		u64 reserved_7_63:57;
 	} s;
 };
 
@@ -365,27 +318,16 @@ union otx2_cptx_lf_misc_int {
  *
  */
 union otx2_cptx_lf_misc_int_ena_w1s {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_lf_misc_int_ena_w1s_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		uint64_t reserved_7_63               : 57;
-		uint64_t fault                       :  1;
-		uint64_t hwerr                       :  1;
-		uint64_t reserved_4_4                :  1;
-		uint64_t nwrp                        :  1;
-		uint64_t irde                        :  1;
-		uint64_t nqerr                       :  1;
-		uint64_t reserved_0_0                :  1;
-#else /* Word 0 - Little Endian */
-		uint64_t reserved_0_0                :  1;
-		uint64_t nqerr                       :  1;
-		uint64_t irde                        :  1;
-		uint64_t nwrp                        :  1;
-		uint64_t reserved_4_4                :  1;
-		uint64_t hwerr                       :  1;
-		uint64_t fault                       :  1;
-		uint64_t reserved_7_63               : 57;
-#endif
+		u64 reserved_0:1;
+		u64 nqerr:1;
+		u64 irde:1;
+		u64 nwrp:1;
+		u64 reserved_4:1;
+		u64 hwerr:1;
+		u64 fault:1;
+		u64 reserved_7_63:57;
 	} s;
 };
 
@@ -398,23 +340,14 @@ union otx2_cptx_lf_misc_int_ena_w1s {
  * software must only write this register with [ENA]=0.
  */
 union otx2_cptx_lf_ctl {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_lf_ctl_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		uint64_t reserved_8_63               : 56;
-		uint64_t fc_hyst_bits                :  4;
-		uint64_t reserved_3_3                :  1;
-		uint64_t fc_up_crossing              :  1;
-		uint64_t fc_ena                      :  1;
-		uint64_t ena                         :  1;
-#else /* Word 0 - Little Endian */
-		uint64_t ena                         :  1;
-		uint64_t fc_ena                      :  1;
-		uint64_t fc_up_crossing              :  1;
-		uint64_t reserved_3_3                :  1;
-		uint64_t fc_hyst_bits                :  4;
-		uint64_t reserved_8_63               : 56;
-#endif
+		u64 ena:1;
+		u64 fc_ena:1;
+		u64 fc_up_crossing:1;
+		u64 reserved_3:1;
+		u64 fc_hyst_bits:4;
+		u64 reserved_8_63:56;
 	} s;
 };
 
@@ -426,17 +359,10 @@ union otx2_cptx_lf_ctl {
 union otx2_cptx_lf_done_wait {
 	u64 u;
 	struct otx2_cptx_lf_done_wait_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		u64 reserved_48_63:16;
-		u64 time_wait:16;
-		u64 reserved_20_31:12;
-		u64 num_wait:20;
-#else /* Word 0 - Little Endian */
 		u64 num_wait:20;
 		u64 reserved_20_31:12;
 		u64 time_wait:16;
 		u64 reserved_48_63:16;
-#endif /* Word 0 - End */
 	} s;
 };
 
@@ -448,13 +374,8 @@ union otx2_cptx_lf_done_wait {
 union otx2_cptx_lf_done {
 	u64 u;
 	struct otx2_cptx_lf_done_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		u64 reserved_20_63:44;
-		u64 done:20;
-#else /* Word 0 - Little Endian */
 		u64 done:20;
 		u64 reserved_20_63:44;
-#endif /* Word 0 - End */
 	} s;
 };
 
@@ -465,29 +386,17 @@ union otx2_cptx_lf_done {
  *
  */
 union otx2_cptx_lf_inprog {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_lf_inprog_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-		uint64_t reserved_48_63              : 16;
-		uint64_t gwb_cnt                     :  8;
-		uint64_t grb_cnt                     :  8;
-		uint64_t grb_partial                 :  1;
-		uint64_t reserved_18_30              : 13;
-		uint64_t grp_drp                     :  1;
-		uint64_t eena                        :  1;
-		uint64_t reserved_9_15               :  7;
-		uint64_t inflight                    :  9;
-#else /* Word 0 - Little Endian */
-		uint64_t inflight                    :	9;
-		uint64_t reserved_9_15               :	7;
-		uint64_t eena                        :	1;
-		uint64_t grp_drp                     :	1;
-		uint64_t reserved_18_30              :	13;
-		uint64_t grb_partial                 :	1;
-		uint64_t grb_cnt                     :	8;
-		uint64_t gwb_cnt                     :	8;
-		uint64_t reserved_48_63		     :	16;
-#endif
+		u64 inflight:9;
+		u64 reserved_9_15:7;
+		u64 eena:1;
+		u64 grp_drp:1;
+		u64 reserved_18_30:13;
+		u64 grb_partial:1;
+		u64 grb_cnt:8;
+		u64 gwb_cnt:8;
+		u64 reserved_48_63:16;
 	} s;
 };
 
@@ -503,19 +412,12 @@ union otx2_cptx_lf_inprog {
  * _ CPT_LF_Q_GRP_PTR[DQ_PTR]=1.
  */
 union otx2_cptx_lf_q_base {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_lf_q_base_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-	uint64_t reserved_53_63              : 11;
-	uint64_t addr                        : 46;
-	uint64_t reserved_1_6                :  6;
-	uint64_t fault                       :  1;
-#else /* Word 0 - Little Endian */
-	uint64_t fault                       :  1;
-	uint64_t reserved_1_6                :  6;
-	uint64_t addr                        : 46;
-	uint64_t reserved_53_63              : 11;
-#endif
+		u64 fault:1;
+		u64 reserved_1_6:6;
+		u64 addr:46;
+		u64 reserved_53_63:11;
 	} s;
 };
 
@@ -531,15 +433,10 @@ union otx2_cptx_lf_q_base {
  * _ CPT_LF_Q_GRP_PTR[DQ_PTR]=1.
  */
 union otx2_cptx_lf_q_size {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_lf_q_size_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-	uint64_t reserved_15_63              : 49;
-	uint64_t size_div40                  : 15;
-#else /* Word 0 - Little Endian */
-	uint64_t size_div40                  : 15;
-	uint64_t reserved_15_63              : 49;
-#endif
+		u64 size_div40:15;
+		u64 reserved_15_63:49;
 	} s;
 };
 
@@ -550,29 +447,17 @@ union otx2_cptx_lf_q_size {
  * when the queue is execution-quiescent (see CPT_LF_INPROG[INFLIGHT]).
  */
 union otx2_cptx_af_lf_ctrl {
-	uint64_t u;
+	u64 u;
 	struct otx2_cptx_af_lf_ctrl_s {
-#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
-	uint64_t reserved_56_63              :	8;
-	uint64_t grp                         :	8;
-	uint64_t reserved_17_47              : 31;
-	uint64_t nixtx_en                    :	1;
-	uint64_t reserved_11_15              :	5;
-	uint64_t cont_err                    :	1;
-	uint64_t pf_func_inst                :	1;
-	uint64_t reserved_1_8                :	8;
-	uint64_t pri                         :	1;
-#else /* Word 0 - Little Endian */
-	uint64_t pri                         :	1;
-	uint64_t reserved_1_8                :	8;
-	uint64_t pf_func_inst                :	1;
-	uint64_t cont_err                    :	1;
-	uint64_t reserved_11_15              :	5;
-	uint64_t nixtx_en                    :	1;
-	uint64_t reserved_17_47              :	31;
-	uint64_t grp                         :	8;
-	uint64_t reserved_56_63              :	8;
-#endif
+		u64 pri:1;
+		u64 reserved_1_8:8;
+		u64 pf_func_inst:1;
+		u64 cont_err:1;
+		u64 reserved_11_15:5;
+		u64 nixtx_en:1;
+		u64 reserved_17_47:31;
+		u64 grp:8;
+		u64 reserved_56_63:8;
 	} s;
 };
 
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.c b/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.c
index 838a3f66e14a..92a87fad90fd 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.c
@@ -1,155 +1,124 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-
-#include "otx2_cpt_mbox_common.h"
-
-static inline struct otx2_mbox *get_mbox(struct pci_dev *pdev)
-{
-	struct otx2_cptpf_dev *cptpf;
-	struct otx2_cptvf_dev *cptvf;
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
 
-	if (pdev->is_physfn) {
-		cptpf = (struct otx2_cptpf_dev *) pci_get_drvdata(pdev);
-		return &cptpf->afpf_mbox;
-	}
+#include "otx2_cpt_common.h"
+#include "otx2_cptlf.h"
 
-	cptvf = (struct otx2_cptvf_dev *) pci_get_drvdata(pdev);
-	return &cptvf->pfvf_mbox;
-}
-
-static inline int get_pf_id(struct pci_dev *pdev)
+int otx2_cpt_send_mbox_msg(struct otx2_mbox *mbox, struct pci_dev *pdev)
 {
-	struct otx2_cptpf_dev *cptpf;
+	int ret;
 
-	if (pdev->is_physfn) {
-		cptpf = (struct otx2_cptpf_dev *) pci_get_drvdata(pdev);
-		return cptpf->pf_id;
+	otx2_mbox_msg_send(mbox, 0);
+	ret = otx2_mbox_wait_for_rsp(mbox, 0);
+	if (ret == -EIO) {
+		dev_err(&pdev->dev, "RVU MBOX timeout.\n");
+		return ret;
+	} else if (ret) {
+		dev_err(&pdev->dev, "RVU MBOX error: %d.\n", ret);
+		return -EFAULT;
 	}
-
-	return 0;
+	return ret;
 }
 
-static inline int get_vf_id(struct pci_dev *pdev)
+int otx2_cpt_send_ready_msg(struct otx2_mbox *mbox, struct pci_dev *pdev)
 {
-	struct otx2_cptvf_dev *cptvf;
+	struct mbox_msghdr *req;
 
-	if (pdev->is_virtfn) {
-		cptvf = (struct otx2_cptvf_dev *) pci_get_drvdata(pdev);
-		return cptvf->vf_id;
+	req = otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+				      sizeof(struct ready_msg_rsp));
+	if (req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		return -EFAULT;
 	}
+	req->id = MBOX_MSG_READY;
+	req->sig = OTX2_MBOX_REQ_SIG;
+	req->pcifunc = 0;
 
-	return 0;
+	return otx2_cpt_send_mbox_msg(mbox, pdev);
 }
 
-static inline u8 cpt_get_blkaddr(struct pci_dev *pdev)
+int otx2_cpt_send_af_reg_requests(struct otx2_mbox *mbox, struct pci_dev *pdev)
 {
-	struct otx2_cptpf_dev *cptpf;
-	struct otx2_cptvf_dev *cptvf;
-
-	if (pdev->is_physfn) {
-		cptpf = (struct otx2_cptpf_dev *) pci_get_drvdata(pdev);
-		return cptpf->blkaddr;
-	}
-
-	cptvf = (struct otx2_cptvf_dev *) pci_get_drvdata(pdev);
-	return cptvf->blkaddr;
+	return otx2_cpt_send_mbox_msg(mbox, pdev);
 }
 
-char *otx2_cpt_get_mbox_opcode_str(int msg_opcode)
+int otx2_cpt_add_read_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			     u64 reg, u64 *val, int blkaddr)
 {
-	char *str = "Unknown";
-
-	switch (msg_opcode) {
-	case MBOX_MSG_READY:
-		str = "READY";
-		break;
+	struct cpt_rd_wr_reg_msg *reg_msg;
 
-	case MBOX_MSG_ATTACH_RESOURCES:
-		str = "ATTACH_RESOURCES";
-		break;
+	reg_msg = (struct cpt_rd_wr_reg_msg *)
+			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*reg_msg),
+						sizeof(*reg_msg));
+	if (reg_msg == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		return -EFAULT;
+	}
 
-	case MBOX_MSG_DETACH_RESOURCES:
-		str = "DETACH_RESOURCES";
-		break;
+	reg_msg->hdr.id = MBOX_MSG_CPT_RD_WR_REGISTER;
+	reg_msg->hdr.sig = OTX2_MBOX_REQ_SIG;
+	reg_msg->hdr.pcifunc = 0;
 
-	case MBOX_MSG_MSIX_OFFSET:
-		str = "MSIX_OFFSET";
-		break;
+	reg_msg->is_write = 0;
+	reg_msg->reg_offset = reg;
+	reg_msg->ret_val = val;
+	reg_msg->blkaddr = blkaddr;
 
-	case MBOX_MSG_CPT_RD_WR_REGISTER:
-		str = "RD_WR_REGISTER";
-		break;
+	return 0;
+}
 
-	case MBOX_MSG_GET_ENG_GRP_NUM:
-		str = "GET_ENG_GRP_NUM";
-		break;
+int otx2_cpt_add_write_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			      u64 reg, u64 val, int blkaddr)
+{
+	struct cpt_rd_wr_reg_msg *reg_msg;
 
-	case MBOX_MSG_RX_INLINE_IPSEC_LF_CFG:
-		str = "RX_INLINE_IPSEC_LF_CFG";
-		break;
+	reg_msg = (struct cpt_rd_wr_reg_msg *)
+			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*reg_msg),
+						sizeof(*reg_msg));
+	if (reg_msg == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		return -EFAULT;
+	}
 
-	case MBOX_MSG_GET_CAPS:
-		str = "GET_CAPS";
-		break;
+	reg_msg->hdr.id = MBOX_MSG_CPT_RD_WR_REGISTER;
+	reg_msg->hdr.sig = OTX2_MBOX_REQ_SIG;
+	reg_msg->hdr.pcifunc = 0;
 
-	case MBOX_MSG_GET_KCRYPTO_LIMITS:
-		str = "GET_KCRYPTO_LIMITS";
-		break;
-	}
+	reg_msg->is_write = 1;
+	reg_msg->reg_offset = reg;
+	reg_msg->val = val;
+	reg_msg->blkaddr = blkaddr;
 
-	return str;
+	return 0;
 }
 
-int otx2_cpt_send_mbox_msg(struct pci_dev *pdev)
+int otx2_cpt_read_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			 u64 reg, u64 *val, int blkaddr)
 {
-	struct otx2_mbox *mbox = get_mbox(pdev);
 	int ret;
 
-	otx2_mbox_msg_send(mbox, 0);
-	ret = otx2_mbox_wait_for_rsp(mbox, 0);
-	if (ret == -EIO) {
-		dev_err(&pdev->dev, "RVU MBOX timeout.\n");
+	ret = otx2_cpt_add_read_af_reg(mbox, pdev, reg, val, blkaddr);
+	if (ret)
 		return ret;
-	} else if (ret) {
-		dev_err(&pdev->dev, "RVU MBOX error: %d.\n", ret);
-		return -EFAULT;
-	}
-	return ret;
+
+	return otx2_cpt_send_mbox_msg(mbox, pdev);
 }
 
-int otx2_cpt_send_ready_msg(struct pci_dev *pdev)
+int otx2_cpt_write_af_reg(struct otx2_mbox *mbox, struct pci_dev *pdev,
+			  u64 reg, u64 val, int blkaddr)
 {
-	struct otx2_mbox *mbox = get_mbox(pdev);
-	struct mbox_msghdr *req;
 	int ret;
 
-	req = otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
-				      sizeof(struct ready_msg_rsp));
-
-	if (req == NULL) {
-		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
-		return -EFAULT;
-	}
-
-	req->id = MBOX_MSG_READY;
-	req->sig = OTX2_MBOX_REQ_SIG;
-	req->pcifunc = OTX2_CPT_RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
-	ret = otx2_cpt_send_mbox_msg(pdev);
+	ret = otx2_cpt_add_write_af_reg(mbox, pdev, reg, val, blkaddr);
+	if (ret)
+		return ret;
 
-	return ret;
+	return otx2_cpt_send_mbox_msg(mbox, pdev);
 }
 
-int otx2_cpt_attach_rscrs_msg(struct pci_dev *pdev)
+int otx2_cpt_attach_rscrs_msg(struct otx2_cptlfs_info *lfs)
 {
-	struct otx2_cptlfs_info *lfs = otx2_cpt_get_lfs_info(pdev);
-	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct otx2_mbox *mbox = lfs->mbox;
 	struct rsrc_attach *req;
 	int ret;
 
@@ -157,17 +126,16 @@ int otx2_cpt_attach_rscrs_msg(struct pci_dev *pdev)
 			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
 						sizeof(struct msg_rsp));
 	if (req == NULL) {
-		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		dev_err(&lfs->pdev->dev, "RVU MBOX failed to get message.\n");
 		return -EFAULT;
 	}
 
 	req->hdr.id = MBOX_MSG_ATTACH_RESOURCES;
 	req->hdr.sig = OTX2_MBOX_REQ_SIG;
-	req->hdr.pcifunc = OTX2_CPT_RVU_PFFUNC(get_pf_id(pdev),
-					       get_vf_id(pdev));
+	req->hdr.pcifunc = 0;
 	req->cptlfs = lfs->lfs_num;
 	req->cpt_blkaddr = lfs->blkaddr;
-	ret = otx2_cpt_send_mbox_msg(pdev);
+	ret = otx2_cpt_send_mbox_msg(mbox, lfs->pdev);
 	if (ret)
 		return ret;
 
@@ -177,10 +145,9 @@ int otx2_cpt_attach_rscrs_msg(struct pci_dev *pdev)
 	return ret;
 }
 
-int otx2_cpt_detach_rsrcs_msg(struct pci_dev *pdev)
+int otx2_cpt_detach_rsrcs_msg(struct otx2_cptlfs_info *lfs)
 {
-	struct otx2_cptlfs_info *lfs = otx2_cpt_get_lfs_info(pdev);
-	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct otx2_mbox *mbox = lfs->mbox;
 	struct rsrc_detach *req;
 	int ret;
 
@@ -188,15 +155,14 @@ int otx2_cpt_detach_rsrcs_msg(struct pci_dev *pdev)
 				otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
 							sizeof(struct msg_rsp));
 	if (req == NULL) {
-		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		dev_err(&lfs->pdev->dev, "RVU MBOX failed to get message.\n");
 		return -EFAULT;
 	}
 
 	req->hdr.id = MBOX_MSG_DETACH_RESOURCES;
 	req->hdr.sig = OTX2_MBOX_REQ_SIG;
-	req->hdr.pcifunc = OTX2_CPT_RVU_PFFUNC(get_pf_id(pdev),
-					       get_vf_id(pdev));
-	ret = otx2_cpt_send_mbox_msg(pdev);
+	req->hdr.pcifunc = 0;
+	ret = otx2_cpt_send_mbox_msg(mbox, lfs->pdev);
 	if (ret)
 		return ret;
 
@@ -206,10 +172,10 @@ int otx2_cpt_detach_rsrcs_msg(struct pci_dev *pdev)
 	return ret;
 }
 
-int otx2_cpt_msix_offset_msg(struct pci_dev *pdev)
+int otx2_cpt_msix_offset_msg(struct otx2_cptlfs_info *lfs)
 {
-	struct otx2_cptlfs_info *lfs = otx2_cpt_get_lfs_info(pdev);
-	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct otx2_mbox *mbox = lfs->mbox;
+	struct pci_dev *pdev = lfs->pdev;
 	struct mbox_msghdr *req;
 	int ret, i;
 
@@ -222,8 +188,8 @@ int otx2_cpt_msix_offset_msg(struct pci_dev *pdev)
 
 	req->id = MBOX_MSG_MSIX_OFFSET;
 	req->sig = OTX2_MBOX_REQ_SIG;
-	req->pcifunc = OTX2_CPT_RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
-	ret = otx2_cpt_send_mbox_msg(pdev);
+	req->pcifunc = 0;
+	ret = otx2_cpt_send_mbox_msg(mbox, pdev);
 	if (ret)
 		return ret;
 
@@ -237,82 +203,3 @@ int otx2_cpt_msix_offset_msg(struct pci_dev *pdev)
 	}
 	return ret;
 }
-
-int otx2_cpt_send_af_reg_requests(struct pci_dev *pdev)
-{
-	return otx2_cpt_send_mbox_msg(pdev);
-}
-
-int otx2_cpt_add_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val)
-{
-	struct otx2_mbox *mbox = get_mbox(pdev);
-	struct cpt_rd_wr_reg_msg *reg_msg;
-
-	reg_msg = (struct cpt_rd_wr_reg_msg *)
-			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*reg_msg),
-						sizeof(*reg_msg));
-	if (reg_msg == NULL) {
-		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
-		return -EFAULT;
-	}
-
-	reg_msg->hdr.id = MBOX_MSG_CPT_RD_WR_REGISTER;
-	reg_msg->hdr.sig = OTX2_MBOX_REQ_SIG;
-	reg_msg->hdr.pcifunc = OTX2_CPT_RVU_PFFUNC(get_pf_id(pdev),
-						   get_vf_id(pdev));
-	reg_msg->is_write = 0;
-	reg_msg->reg_offset = reg;
-	reg_msg->ret_val = val;
-	reg_msg->blkaddr = cpt_get_blkaddr(pdev);
-
-	return 0;
-}
-
-int otx2_cpt_add_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val)
-{
-	struct otx2_mbox *mbox = get_mbox(pdev);
-	struct cpt_rd_wr_reg_msg *reg_msg;
-
-	reg_msg = (struct cpt_rd_wr_reg_msg *)
-			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*reg_msg),
-						sizeof(*reg_msg));
-	if (reg_msg == NULL) {
-		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
-		return -EFAULT;
-	}
-
-	reg_msg->hdr.id = MBOX_MSG_CPT_RD_WR_REGISTER;
-	reg_msg->hdr.sig = OTX2_MBOX_REQ_SIG;
-	reg_msg->hdr.pcifunc = OTX2_CPT_RVU_PFFUNC(get_pf_id(pdev),
-						   get_vf_id(pdev));
-	reg_msg->is_write = 1;
-	reg_msg->reg_offset = reg;
-	reg_msg->val = val;
-	reg_msg->blkaddr = cpt_get_blkaddr(pdev);
-
-	return 0;
-}
-
-int otx2_cpt_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val)
-{
-	int ret;
-
-	ret = otx2_cpt_add_read_af_reg(pdev, reg, val);
-	if (ret)
-		return ret;
-	ret = otx2_cpt_send_mbox_msg(pdev);
-
-	return ret;
-}
-
-int otx2_cpt_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val)
-{
-	int ret;
-
-	ret = otx2_cpt_add_write_af_reg(pdev, reg, val);
-	if (ret)
-		return ret;
-	ret = otx2_cpt_send_mbox_msg(pdev);
-
-	return ret;
-}
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.h b/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.h
deleted file mode 100644
index b45a308e887c..000000000000
--- a/drivers/crypto/marvell/octeontx2/otx2_cpt_mbox_common.h
+++ /dev/null
@@ -1,106 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-
-#ifndef __OTX2_CPT_MBOX_COMMON_H
-#define __OTX2_CPT_MBOX_COMMON_H
-
-#include "otx2_cptpf.h"
-#include "otx2_cptvf.h"
-
-/* Take mbox id from end of CPT mbox range in AF (range 0xA00 - 0xBFF) */
-#define MBOX_MSG_RX_INLINE_IPSEC_LF_CFG 0xBFE
-#define MBOX_MSG_GET_ENG_GRP_NUM 0xBFF
-#define MBOX_MSG_GET_CAPS 0xBFD
-#define MBOX_MSG_GET_KCRYPTO_LIMITS 0xBFC
-
-/*
- * Message request and response to get engine group number
- * which has attached a given type of engines (SE, AE, IE)
- * This messages are only used between CPT PF <-> CPT VF
- */
-struct otx2_cpt_eng_grp_num_msg {
-	struct mbox_msghdr hdr;
-	u8 eng_type;
-};
-
-struct otx2_cpt_eng_grp_num_rsp {
-	struct mbox_msghdr hdr;
-	u8 eng_type;
-	u8 eng_grp_num;
-};
-
-/*
- * Message request to config cpt lf for inline inbound ipsec.
- * This message is only used between CPT PF <-> CPT VF
- */
-struct otx2_cpt_rx_inline_lf_cfg {
-	struct mbox_msghdr hdr;
-	u16 sso_pf_func;
-};
-
-/*
- * Message request and response to get HW capabilities for each
- * engine type (SE, IE, AE).
- * This messages are only used between CPT PF <-> CPT VF
- */
-struct otx2_cpt_caps_msg {
-	struct mbox_msghdr hdr;
-};
-
-struct otx2_cpt_caps_rsp {
-	struct mbox_msghdr hdr;
-	u16 cpt_pf_drv_version;
-	u8 cpt_revision;
-	union otx2_cpt_eng_caps eng_caps[OTX2_CPT_MAX_ENG_TYPES];
-};
-
-/*
- * Message request and response to get kernel crypto limits
- * This messages are only used between CPT PF <-> CPT VF
- */
-struct otx2_cpt_kcrypto_limits_msg {
-	struct mbox_msghdr hdr;
-};
-
-struct otx2_cpt_kcrypto_limits_rsp {
-	struct mbox_msghdr hdr;
-	u8 kcrypto_limits;
-};
-
-static inline struct otx2_cptlfs_info *
-		     otx2_cpt_get_lfs_info(struct pci_dev *pdev)
-{
-	struct otx2_cptpf_dev *cptpf;
-	struct otx2_cptvf_dev *cptvf;
-
-	if (pdev->is_physfn) {
-		cptpf = (struct otx2_cptpf_dev *) pci_get_drvdata(pdev);
-		return &cptpf->lfs;
-	}
-
-	cptvf = (struct otx2_cptvf_dev *) pci_get_drvdata(pdev);
-	return &cptvf->lfs;
-}
-
-int otx2_cpt_send_ready_msg(struct pci_dev *pdev);
-int otx2_cpt_attach_rscrs_msg(struct pci_dev *pdev);
-int otx2_cpt_detach_rsrcs_msg(struct pci_dev *pdev);
-int otx2_cpt_msix_offset_msg(struct pci_dev *pdev);
-
-int otx2_cpt_send_af_reg_requests(struct pci_dev *pdev);
-int otx2_cpt_add_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val);
-int otx2_cpt_add_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val);
-int otx2_cpt_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val);
-int otx2_cpt_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val);
-
-int otx2_cpt_send_mbox_msg(struct pci_dev *pdev);
-char *otx2_cpt_get_mbox_opcode_str(int msg_opcode);
-
-#endif /* __OTX2_CPT_MBOX_COMMON_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cpt_reqmgr.h b/drivers/crypto/marvell/octeontx2/otx2_cpt_reqmgr.h
index f07a52c4b8cd..2e4532c382cc 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cpt_reqmgr.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cpt_reqmgr.h
@@ -1,11 +1,5 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell CPT OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
 
 #ifndef __OTX2_CPT_REQMGR_H
@@ -15,17 +9,17 @@
 
 /* Completion code size and initial value */
 #define OTX2_CPT_COMPLETION_CODE_SIZE 8
-#define OTX2_CPT_COMPLETION_CODE_INIT 0
+#define OTX2_CPT_COMPLETION_CODE_INIT OTX2_CPT_COMP_E_NOTDONE
 /*
  * Maximum total number of SG buffers is 100, we divide it equally
  * between input and output
  */
-#define OTX2_CPT_MAX_SG_IN_CNT 50
+#define OTX2_CPT_MAX_SG_IN_CNT  50
 #define OTX2_CPT_MAX_SG_OUT_CNT 50
 
 /* DMA mode direct or SG */
-#define OTX2_CPT_DMA_DIRECT_DIRECT 0
-#define OTX2_CPT_DMA_GATHER_SCATTER 1
+#define OTX2_CPT_DMA_MODE_DIRECT 0
+#define OTX2_CPT_DMA_MODE_SG     1
 
 /* Context source CPTR or DPTR */
 #define OTX2_CPT_FROM_CPTR 0
@@ -33,7 +27,7 @@
 
 #define OTX2_CPT_MAX_REQ_SIZE 65535
 
-union otx2_cpt_opcode_info {
+union otx2_cpt_opcode {
 	u16 flags;
 	struct {
 		u8 major;
@@ -45,7 +39,7 @@ struct otx2_cptvf_request {
 	u32 param1;
 	u32 param2;
 	u16 dlen;
-	union otx2_cpt_opcode_info opcode;
+	union otx2_cpt_opcode opcode;
 };
 
 /*
@@ -53,25 +47,20 @@ struct otx2_cptvf_request {
  * Words EI (0-3)
  */
 union otx2_cpt_iq_cmd_word0 {
-	u64 u64;
+	u64 u;
 	struct {
-		u16 opcode;
-		u16 param1;
-		u16 param2;
-		u16 dlen;
+		__be16 opcode;
+		__be16 param1;
+		__be16 param2;
+		__be16 dlen;
 	} s;
 };
 
 union otx2_cpt_iq_cmd_word3 {
-	u64 u64;
+	u64 u;
 	struct {
-#if defined(__BIG_ENDIAN_BITFIELD)
-		u64 grp:3;
-		u64 cptr:61;
-#else
 		u64 cptr:61;
 		u64 grp:3;
-#endif
 	} s;
 };
 
@@ -83,7 +72,7 @@ struct otx2_cpt_iq_command {
 };
 
 struct otx2_cpt_pending_entry {
-	u64 *completion_addr;	/* Completion address */
+	void *completion_addr;	/* Completion address */
 	void *info;
 	/* Kernel async request callback */
 	void (*callback)(int status, void *arg1, void *arg2);
@@ -111,7 +100,7 @@ union otx2_cpt_ctrl_info {
 	u32 flags;
 	struct {
 #if defined(__BIG_ENDIAN_BITFIELD)
-		u32 reserved0:26;
+		u32 reserved_6_31:26;
 		u32 grp:3;	/* Group bits */
 		u32 dma_mode:2;	/* DMA mode */
 		u32 se_req:1;	/* To SE core */
@@ -119,7 +108,7 @@ union otx2_cpt_ctrl_info {
 		u32 se_req:1;	/* To SE core */
 		u32 dma_mode:2;	/* DMA mode */
 		u32 grp:3;	/* Group bits */
-		u32 reserved0:26;
+		u32 reserved_6_31:26;
 #endif
 	} s;
 };
@@ -134,18 +123,18 @@ struct otx2_cpt_req_info {
 	struct otx2_cpt_buf_ptr out[OTX2_CPT_MAX_SG_OUT_CNT];
 	u8 *iv_out;     /* IV to send back */
 	u16 rlen;	/* Output length */
-	u8 incnt;	/* Number of input buffers */
-	u8 outcnt;	/* Number of output buffers */
+	u8 in_cnt;	/* Number of input buffers */
+	u8 out_cnt;	/* Number of output buffers */
 	u8 req_type;	/* Type of request */
 	u8 is_enc;	/* Is a request an encryption request */
 	u8 is_trunc_hmac;/* Is truncated hmac used */
 };
 
-struct otx2_cpt_info_buffer {
+struct otx2_cpt_inst_info {
 	struct otx2_cpt_pending_entry *pentry;
 	struct otx2_cpt_req_info *req;
 	struct pci_dev *pdev;
-	u64 *completion_addr;
+	void *completion_addr;
 	u8 *out_buffer;
 	u8 *in_buffer;
 	dma_addr_t dptr_baddr;
@@ -158,23 +147,18 @@ struct otx2_cpt_info_buffer {
 };
 
 struct otx2_cpt_sglist_component {
-	union {
-		u64 len;
-		struct {
-			u16 len0;
-			u16 len1;
-			u16 len2;
-			u16 len3;
-		} s;
-	} u;
-	u64 ptr0;
-	u64 ptr1;
-	u64 ptr2;
-	u64 ptr3;
+	__be16 len0;
+	__be16 len1;
+	__be16 len2;
+	__be16 len3;
+	__be64 ptr0;
+	__be64 ptr1;
+	__be64 ptr2;
+	__be64 ptr3;
 };
 
-static inline void do_request_cleanup(struct pci_dev *pdev,
-				      struct otx2_cpt_info_buffer *info)
+static inline void otx2_cpt_info_destroy(struct pci_dev *pdev,
+					 struct otx2_cpt_inst_info *info)
 {
 	struct otx2_cpt_req_info *req;
 	int i;
@@ -185,7 +169,7 @@ static inline void do_request_cleanup(struct pci_dev *pdev,
 
 	if (info->req) {
 		req = info->req;
-		for (i = 0; i < req->outcnt; i++) {
+		for (i = 0; i < req->out_cnt; i++) {
 			if (req->out[i].dma_addr)
 				dma_unmap_single(&pdev->dev,
 						 req->out[i].dma_addr,
@@ -193,7 +177,7 @@ static inline void do_request_cleanup(struct pci_dev *pdev,
 						 DMA_BIDIRECTIONAL);
 		}
 
-		for (i = 0; i < req->incnt; i++) {
+		for (i = 0; i < req->in_cnt; i++) {
 			if (req->in[i].dma_addr)
 				dma_unmap_single(&pdev->dev,
 						 req->in[i].dma_addr,
@@ -201,13 +185,13 @@ static inline void do_request_cleanup(struct pci_dev *pdev,
 						 DMA_BIDIRECTIONAL);
 		}
 	}
-	kzfree(info);
+	kfree(info);
 }
 
 struct otx2_cptlf_wqe;
-int otx2_cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev);
 int otx2_cpt_do_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 			int cpu_num);
 void otx2_cpt_post_process(struct otx2_cptlf_wqe *wqe);
+int otx2_cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev);
 
 #endif /* __OTX2_CPT_REQMGR_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptlf.c b/drivers/crypto/marvell/octeontx2/otx2_cptlf.c
new file mode 100644
index 000000000000..0e06e72c3077
--- /dev/null
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptlf.c
@@ -0,0 +1,429 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
+
+#include "otx2_cpt_common.h"
+#include "otx2_cptlf.h"
+#include "rvu_reg.h"
+
+#define CPT_TIMER_HOLD 0x03F
+#define CPT_COUNT_HOLD 32
+
+static void cptlf_do_set_done_time_wait(struct otx2_cptlf_info *lf,
+					int time_wait)
+{
+	union otx2_cptx_lf_done_wait done_wait;
+
+	done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
+				      lf->slot, OTX2_CPT_LF_DONE_WAIT);
+	done_wait.s.time_wait = time_wait;
+	otx2_cpt_write64(lf->lfs->reg_base, lf->lfs->blkaddr, lf->slot,
+			 OTX2_CPT_LF_DONE_WAIT, done_wait.u);
+}
+
+static void cptlf_do_set_done_num_wait(struct otx2_cptlf_info *lf, int num_wait)
+{
+	union otx2_cptx_lf_done_wait done_wait;
+
+	done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
+				      lf->slot, OTX2_CPT_LF_DONE_WAIT);
+	done_wait.s.num_wait = num_wait;
+	otx2_cpt_write64(lf->lfs->reg_base, lf->lfs->blkaddr, lf->slot,
+			 OTX2_CPT_LF_DONE_WAIT, done_wait.u);
+}
+
+static void cptlf_set_done_time_wait(struct otx2_cptlfs_info *lfs,
+				     int time_wait)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cptlf_do_set_done_time_wait(&lfs->lf[slot], time_wait);
+}
+
+static void cptlf_set_done_num_wait(struct otx2_cptlfs_info *lfs, int num_wait)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cptlf_do_set_done_num_wait(&lfs->lf[slot], num_wait);
+}
+
+static int cptlf_set_pri(struct otx2_cptlf_info *lf, int pri)
+{
+	struct otx2_cptlfs_info *lfs = lf->lfs;
+	union otx2_cptx_af_lf_ctrl lf_ctrl;
+	int ret;
+
+	ret = otx2_cpt_read_af_reg(lfs->mbox, lfs->pdev,
+				   CPT_AF_LFX_CTL(lf->slot),
+				   &lf_ctrl.u, lfs->blkaddr);
+	if (ret)
+		return ret;
+
+	lf_ctrl.s.pri = pri ? 1 : 0;
+
+	ret = otx2_cpt_write_af_reg(lfs->mbox, lfs->pdev,
+				    CPT_AF_LFX_CTL(lf->slot),
+				    lf_ctrl.u, lfs->blkaddr);
+	return ret;
+}
+
+static int cptlf_set_eng_grps_mask(struct otx2_cptlf_info *lf,
+				   int eng_grps_mask)
+{
+	struct otx2_cptlfs_info *lfs = lf->lfs;
+	union otx2_cptx_af_lf_ctrl lf_ctrl;
+	int ret;
+
+	ret = otx2_cpt_read_af_reg(lfs->mbox, lfs->pdev,
+				   CPT_AF_LFX_CTL(lf->slot),
+				   &lf_ctrl.u, lfs->blkaddr);
+	if (ret)
+		return ret;
+
+	lf_ctrl.s.grp = eng_grps_mask;
+
+	ret = otx2_cpt_write_af_reg(lfs->mbox, lfs->pdev,
+				    CPT_AF_LFX_CTL(lf->slot),
+				    lf_ctrl.u, lfs->blkaddr);
+	return ret;
+}
+
+static int cptlf_set_grp_and_pri(struct otx2_cptlfs_info *lfs,
+				 int eng_grp_mask, int pri)
+{
+	int slot, ret = 0;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		ret = cptlf_set_pri(&lfs->lf[slot], pri);
+		if (ret)
+			return ret;
+
+		ret = cptlf_set_eng_grps_mask(&lfs->lf[slot], eng_grp_mask);
+		if (ret)
+			return ret;
+	}
+	return ret;
+}
+
+static void cptlf_hw_init(struct otx2_cptlfs_info *lfs)
+{
+	/* Disable instruction queues */
+	otx2_cptlf_disable_iqueues(lfs);
+
+	/* Set instruction queues base addresses */
+	otx2_cptlf_set_iqueues_base_addr(lfs);
+
+	/* Set instruction queues sizes */
+	otx2_cptlf_set_iqueues_size(lfs);
+
+	/* Set done interrupts time wait */
+	cptlf_set_done_time_wait(lfs, CPT_TIMER_HOLD);
+
+	/* Set done interrupts num wait */
+	cptlf_set_done_num_wait(lfs, CPT_COUNT_HOLD);
+
+	/* Enable instruction queues */
+	otx2_cptlf_enable_iqueues(lfs);
+}
+
+static void cptlf_hw_cleanup(struct otx2_cptlfs_info *lfs)
+{
+	/* Disable instruction queues */
+	otx2_cptlf_disable_iqueues(lfs);
+}
+
+static void cptlf_set_misc_intrs(struct otx2_cptlfs_info *lfs, u8 enable)
+{
+	union otx2_cptx_lf_misc_int_ena_w1s irq_misc = { .u = 0x0 };
+	u64 reg = enable ? OTX2_CPT_LF_MISC_INT_ENA_W1S :
+			   OTX2_CPT_LF_MISC_INT_ENA_W1C;
+	int slot;
+
+	irq_misc.s.fault = 0x1;
+	irq_misc.s.hwerr = 0x1;
+	irq_misc.s.irde = 0x1;
+	irq_misc.s.nqerr = 0x1;
+	irq_misc.s.nwrp = 0x1;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		otx2_cpt_write64(lfs->reg_base, lfs->blkaddr, slot, reg,
+				 irq_misc.u);
+}
+
+static void cptlf_enable_intrs(struct otx2_cptlfs_info *lfs)
+{
+	int slot;
+
+	/* Enable done interrupts */
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		otx2_cpt_write64(lfs->reg_base, lfs->blkaddr, slot,
+				 OTX2_CPT_LF_DONE_INT_ENA_W1S, 0x1);
+	/* Enable Misc interrupts */
+	cptlf_set_misc_intrs(lfs, true);
+}
+
+static void cptlf_disable_intrs(struct otx2_cptlfs_info *lfs)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		otx2_cpt_write64(lfs->reg_base, lfs->blkaddr, slot,
+				 OTX2_CPT_LF_DONE_INT_ENA_W1C, 0x1);
+	cptlf_set_misc_intrs(lfs, false);
+}
+
+static inline int cptlf_read_done_cnt(struct otx2_cptlf_info *lf)
+{
+	union otx2_cptx_lf_done irq_cnt;
+
+	irq_cnt.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr, lf->slot,
+				    OTX2_CPT_LF_DONE);
+	return irq_cnt.s.done;
+}
+
+static irqreturn_t cptlf_misc_intr_handler(int __always_unused irq, void *arg)
+{
+	union otx2_cptx_lf_misc_int irq_misc, irq_misc_ack;
+	struct otx2_cptlf_info *lf = arg;
+	struct device *dev;
+
+	dev = &lf->lfs->pdev->dev;
+	irq_misc.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
+				     lf->slot, OTX2_CPT_LF_MISC_INT);
+	irq_misc_ack.u = 0x0;
+
+	if (irq_misc.s.fault) {
+		dev_err(dev, "Memory error detected while executing CPT_INST_S, LF %d.\n",
+			lf->slot);
+		irq_misc_ack.s.fault = 0x1;
+
+	} else if (irq_misc.s.hwerr) {
+		dev_err(dev, "HW error from an engine executing CPT_INST_S, LF %d.",
+			lf->slot);
+		irq_misc_ack.s.hwerr = 0x1;
+
+	} else if (irq_misc.s.nwrp) {
+		dev_err(dev, "SMMU fault while writing CPT_RES_S to CPT_INST_S[RES_ADDR], LF %d.\n",
+			lf->slot);
+		irq_misc_ack.s.nwrp = 0x1;
+
+	} else if (irq_misc.s.irde) {
+		dev_err(dev, "Memory error when accessing instruction memory queue CPT_LF_Q_BASE[ADDR].\n");
+		irq_misc_ack.s.irde = 0x1;
+
+	} else if (irq_misc.s.nqerr) {
+		dev_err(dev, "Error enqueuing an instruction received at CPT_LF_NQ.\n");
+		irq_misc_ack.s.nqerr = 0x1;
+
+	} else {
+		dev_err(dev, "Unhandled interrupt in CPT LF %d\n", lf->slot);
+		return IRQ_NONE;
+	}
+
+	/* Acknowledge interrupts */
+	otx2_cpt_write64(lf->lfs->reg_base, lf->lfs->blkaddr, lf->slot,
+			 OTX2_CPT_LF_MISC_INT, irq_misc_ack.u);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cptlf_done_intr_handler(int irq, void *arg)
+{
+	union otx2_cptx_lf_done_wait done_wait;
+	struct otx2_cptlf_info *lf = arg;
+	int irq_cnt;
+
+	/* Read the number of completed requests */
+	irq_cnt = cptlf_read_done_cnt(lf);
+	if (irq_cnt) {
+		done_wait.u = otx2_cpt_read64(lf->lfs->reg_base,
+					      lf->lfs->blkaddr,
+					      lf->slot, OTX2_CPT_LF_DONE_WAIT);
+		/* Acknowledge the number of completed requests */
+		otx2_cpt_write64(lf->lfs->reg_base, lf->lfs->blkaddr, lf->slot,
+				 OTX2_CPT_LF_DONE_ACK, irq_cnt);
+
+		otx2_cpt_write64(lf->lfs->reg_base, lf->lfs->blkaddr, lf->slot,
+				 OTX2_CPT_LF_DONE_WAIT, done_wait.u);
+		if (unlikely(!lf->wqe)) {
+			dev_err(&lf->lfs->pdev->dev, "No work for LF %d\n",
+				lf->slot);
+			return IRQ_NONE;
+		}
+
+		/* Schedule processing of completed requests */
+		tasklet_hi_schedule(&lf->wqe->work);
+	}
+	return IRQ_HANDLED;
+}
+
+void otx2_cptlf_unregister_interrupts(struct otx2_cptlfs_info *lfs)
+{
+	int i, offs, vector;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		for (offs = 0; offs < OTX2_CPT_LF_MSIX_VECTORS; offs++) {
+			if (!lfs->lf[i].is_irq_reg[offs])
+				continue;
+
+			vector = pci_irq_vector(lfs->pdev,
+						lfs->lf[i].msix_offset + offs);
+			free_irq(vector, &lfs->lf[i]);
+			lfs->lf[i].is_irq_reg[offs] = false;
+		}
+	}
+	cptlf_disable_intrs(lfs);
+}
+
+static int cptlf_do_register_interrrupts(struct otx2_cptlfs_info *lfs,
+					 int lf_num, int irq_offset,
+					 irq_handler_t handler)
+{
+	int ret, vector;
+
+	vector = pci_irq_vector(lfs->pdev, lfs->lf[lf_num].msix_offset +
+				irq_offset);
+	ret = request_irq(vector, handler, 0,
+			  lfs->lf[lf_num].irq_name[irq_offset],
+			  &lfs->lf[lf_num]);
+	if (ret)
+		return ret;
+
+	lfs->lf[lf_num].is_irq_reg[irq_offset] = true;
+
+	return ret;
+}
+
+int otx2_cptlf_register_interrupts(struct otx2_cptlfs_info *lfs)
+{
+	int irq_offs, ret, i;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		irq_offs = OTX2_CPT_LF_INT_VEC_E_MISC;
+		snprintf(lfs->lf[i].irq_name[irq_offs], 32, "CPTLF Misc%d", i);
+		ret = cptlf_do_register_interrrupts(lfs, i, irq_offs,
+						    cptlf_misc_intr_handler);
+		if (ret)
+			goto free_irq;
+
+		irq_offs = OTX2_CPT_LF_INT_VEC_E_DONE;
+		snprintf(lfs->lf[i].irq_name[irq_offs], 32, "OTX2_CPTLF Done%d",
+			 i);
+		ret = cptlf_do_register_interrrupts(lfs, i, irq_offs,
+						    cptlf_done_intr_handler);
+		if (ret)
+			goto free_irq;
+	}
+	cptlf_enable_intrs(lfs);
+	return 0;
+
+free_irq:
+	otx2_cptlf_unregister_interrupts(lfs);
+	return ret;
+}
+
+void otx2_cptlf_free_irqs_affinity(struct otx2_cptlfs_info *lfs)
+{
+	int slot, offs;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		for (offs = 0; offs < OTX2_CPT_LF_MSIX_VECTORS; offs++)
+			irq_set_affinity_hint(pci_irq_vector(lfs->pdev,
+					      lfs->lf[slot].msix_offset +
+					      offs), NULL);
+		free_cpumask_var(lfs->lf[slot].affinity_mask);
+	}
+}
+
+int otx2_cptlf_set_irqs_affinity(struct otx2_cptlfs_info *lfs)
+{
+	struct otx2_cptlf_info *lf = lfs->lf;
+	int slot, offs, ret;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		if (!zalloc_cpumask_var(&lf[slot].affinity_mask, GFP_KERNEL)) {
+			dev_err(&lfs->pdev->dev,
+				"cpumask allocation failed for LF %d", slot);
+			ret = -ENOMEM;
+			goto free_affinity_mask;
+		}
+
+		cpumask_set_cpu(cpumask_local_spread(slot,
+				dev_to_node(&lfs->pdev->dev)),
+				lf[slot].affinity_mask);
+
+		for (offs = 0; offs < OTX2_CPT_LF_MSIX_VECTORS; offs++) {
+			ret = irq_set_affinity_hint(pci_irq_vector(lfs->pdev,
+						lf[slot].msix_offset + offs),
+						lf[slot].affinity_mask);
+			if (ret)
+				goto free_affinity_mask;
+		}
+	}
+	return 0;
+
+free_affinity_mask:
+	otx2_cptlf_free_irqs_affinity(lfs);
+	return ret;
+}
+
+int otx2_cptlf_init(struct otx2_cptlfs_info *lfs, u8 eng_grp_mask, int pri,
+		    int lfs_num)
+{
+	int slot, ret;
+
+	if (!lfs->pdev || !lfs->reg_base)
+		return -EINVAL;
+
+	lfs->lfs_num = lfs_num;
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		lfs->lf[slot].lfs = lfs;
+		lfs->lf[slot].slot = slot;
+		lfs->lf[slot].lmtline = lfs->reg_base +
+			OTX2_CPT_RVU_FUNC_ADDR_S(BLKADDR_LMT, slot,
+						 OTX2_CPT_LMT_LF_LMTLINEX(0));
+		lfs->lf[slot].ioreg = lfs->reg_base +
+			OTX2_CPT_RVU_FUNC_ADDR_S(lfs->blkaddr, slot,
+						 OTX2_CPT_LF_NQX(0));
+	}
+	/* Send request to attach LFs */
+	ret = otx2_cpt_attach_rscrs_msg(lfs);
+	if (ret)
+		goto clear_lfs_num;
+
+	ret = otx2_cpt_alloc_instruction_queues(lfs);
+	if (ret) {
+		dev_err(&lfs->pdev->dev,
+			"Allocating instruction queues failed\n");
+		goto detach_rsrcs;
+	}
+	cptlf_hw_init(lfs);
+	/*
+	 * Allow each LF to execute requests destined to any of 8 engine
+	 * groups and set queue priority of each LF to high
+	 */
+	ret = cptlf_set_grp_and_pri(lfs, eng_grp_mask, pri);
+	if (ret)
+		goto free_iq;
+
+	return 0;
+
+free_iq:
+	otx2_cpt_free_instruction_queues(lfs);
+	cptlf_hw_cleanup(lfs);
+detach_rsrcs:
+	otx2_cpt_detach_rsrcs_msg(lfs);
+clear_lfs_num:
+	lfs->lfs_num = 0;
+	return ret;
+}
+
+void otx2_cptlf_shutdown(struct otx2_cptlfs_info *lfs)
+{
+	lfs->lfs_num = 0;
+	/* Cleanup LFs hardware side */
+	cptlf_hw_cleanup(lfs);
+	/* Send request to detach LFs */
+	otx2_cpt_detach_rsrcs_msg(lfs);
+}
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptlf.h b/drivers/crypto/marvell/octeontx2/otx2_cptlf.h
index 96dfcf01b564..4ae08c8b7db0 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptlf.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptlf.h
@@ -1,16 +1,12 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
-
 #ifndef __OTX2_CPTLF_H
 #define __OTX2_CPTLF_H
 
+#include <mbox.h>
+#include <rvu.h>
+#include "otx2_cpt_common.h"
 #include "otx2_cpt_reqmgr.h"
 
 /*
@@ -22,14 +18,12 @@
  * CPT instruction queue size passed to HW is in units of 40*CPT_INST_S
  * messages.
  */
-#define DIV40 40
-#define OTX2_CPT_SIZE_DIV40 ((OTX2_CPT_USER_REQUESTED_QLEN_MSGS + \
-			      DIV40 - 1)/DIV40)
+#define OTX2_CPT_SIZE_DIV40 (OTX2_CPT_USER_REQUESTED_QLEN_MSGS/40)
 
 /*
  * CPT instruction and pending queues length in CPT_INST_S messages
  */
-#define OTX2_CPT_INST_QLEN_MSGS	((OTX2_CPT_SIZE_DIV40 - 1) * DIV40)
+#define OTX2_CPT_INST_QLEN_MSGS	((OTX2_CPT_SIZE_DIV40 - 1) * 40)
 
 /*
  * LDWB is getting incorrectly used when IQB_LDWB = 1 and CPT instruction
@@ -41,7 +35,7 @@
 
 /* CPT instruction queue length in bytes */
 #define OTX2_CPT_INST_QLEN_BYTES                                               \
-		((OTX2_CPT_SIZE_DIV40 * DIV40 * OTX2_CPT_INST_SIZE) +          \
+		((OTX2_CPT_SIZE_DIV40 * 40 * OTX2_CPT_INST_SIZE) +             \
 		OTX2_CPT_INST_QLEN_EXTRA_BYTES)
 
 /* CPT instruction group queue length in bytes */
@@ -52,26 +46,42 @@
 #define OTX2_CPT_Q_FC_LEN 128
 
 /* CPT instruction queue alignment */
-#define OTX2_CPT_INST_Q_ALIGNMENT 128
+#define OTX2_CPT_INST_Q_ALIGNMENT  128
 
 /* Mask which selects all engine groups */
 #define OTX2_CPT_ALL_ENG_GRPS_MASK 0xFF
 
+/* Maximum LFs supported in OcteonTX2 for CPT */
+#define OTX2_CPT_MAX_LFS_NUM    64
+
 /* Queue priority */
-#define OTX2_CPT_QUEUE_HI_PRIO	0x1
-#define OTX2_CPT_QUEUE_LOW_PRIO	0x0
-
-
-struct otx2_cptlf_sysfs_cfg {
-	char name[OTX2_CPT_NAME_LENGTH];
-	struct device_attribute eng_grps_mask_attr;
-	struct device_attribute coalesc_tw_attr;
-	struct device_attribute coalesc_nw_attr;
-	struct device_attribute prio_attr;
-#define ATTRS_NUM 5
-	struct attribute *attrs[ATTRS_NUM];
-	struct attribute_group attr_grp;
-	bool is_sysfs_grp_created;
+#define OTX2_CPT_QUEUE_HI_PRIO  0x1
+#define OTX2_CPT_QUEUE_LOW_PRIO 0x0
+
+#if defined(CONFIG_ARM64)
+/*
+ * otx2_lmt_flush is used for LMT store operation.
+ * On octeontx2 platform CPT instruction enqueue and
+ * NIX packet send are only possible via LMTST
+ * operations and it uses LDEOR instruction targeting
+ * the coprocessor address.
+ */
+#define otx2_lmt_flush(ioaddr)                          \
+({                                                      \
+	u64 result = 0;                                 \
+	__asm__ volatile(".cpu  generic+lse\n"          \
+			 "ldeor xzr, %x[rf], [%[rs]]"   \
+			 : [rf]"=r" (result)            \
+			 : [rs]"r" (ioaddr));           \
+	(result);                                       \
+})
+#else
+#define otx2_lmt_flush(ioaddr)          ({ 0; })
+#endif
+
+enum otx2_cptlf_state {
+	OTX2_CPTLF_IN_RESET,
+	OTX2_CPTLF_STARTED,
 };
 
 struct otx2_cpt_inst_queue {
@@ -90,19 +100,18 @@ struct otx2_cptlf_wqe {
 };
 
 struct otx2_cptlf_info {
-	struct otx2_cptlfs_info *lfs;		/* Ptr to cptlfs_info struct */
-	struct otx2_cptlf_sysfs_cfg sysfs_cfg;	/* LF sysfs config entries */
-	void *lmtline;				/* Address of LMTLINE */
-	void *ioreg;                            /* LMTLINE send register */
-	int msix_offset;			/* MSI-X interrupts offset */
-	cpumask_var_t affinity_mask;		/* IRQs affinity mask */
+	struct otx2_cptlfs_info *lfs;           /* Ptr to cptlfs_info struct */
+	void __iomem *lmtline;                  /* Address of LMTLINE */
+	void __iomem *ioreg;                    /* LMTLINE send register */
+	int msix_offset;                        /* MSI-X interrupts offset */
+	cpumask_var_t affinity_mask;            /* IRQs affinity mask */
 	u8 irq_name[OTX2_CPT_LF_MSIX_VECTORS][32];/* Interrupts name */
 	u8 is_irq_reg[OTX2_CPT_LF_MSIX_VECTORS];  /* Is interrupt registered */
-	u8 slot;				/* Slot number of this LF */
+	u8 slot;                                /* Slot number of this LF */
 
 	struct otx2_cpt_inst_queue iqueue;/* Instruction queue */
 	struct otx2_cpt_pending_queue pqueue; /* Pending queue */
-	struct otx2_cptlf_wqe *wqe;	/* Tasklet work info */
+	struct otx2_cptlf_wqe *wqe;       /* Tasklet work info */
 };
 
 struct otx2_cptlfs_info {
@@ -110,69 +119,64 @@ struct otx2_cptlfs_info {
 	void __iomem *reg_base;
 	struct pci_dev *pdev;   /* Device LFs are attached to */
 	struct otx2_cptlf_info lf[OTX2_CPT_MAX_LFS_NUM];
-	u8 kcrypto_eng_grp_num;	/* Kernel crypto engine group number */
+	struct otx2_mbox *mbox;
 	u8 are_lfs_attached;	/* Whether CPT LFs are attached */
 	u8 lfs_num;		/* Number of CPT LFs */
-	u8 kcrypto_limits;      /* Kernel crypto limits */
-	u8 blkaddr;             /* BLKADDR_CPT0/BLKADDR_CPT1 */
+	u8 kcrypto_eng_grp_num;	/* Kernel crypto engine group number */
+	u8 kvf_limits;          /* Kernel crypto limits */
+	atomic_t state;         /* LF's state. started/reset */
+	int blkaddr;            /* CPT blkaddr: BLKADDR_CPT0/BLKADDR_CPT1 */
 };
 
 static inline void otx2_cpt_free_instruction_queues(
 					struct otx2_cptlfs_info *lfs)
 {
+	struct otx2_cpt_inst_queue *iq;
 	int i;
 
 	for (i = 0; i < lfs->lfs_num; i++) {
-		if (lfs->lf[i].iqueue.real_vaddr)
+		iq = &lfs->lf[i].iqueue;
+		if (iq->real_vaddr)
 			dma_free_coherent(&lfs->pdev->dev,
-					  lfs->lf[i].iqueue.size,
-					  lfs->lf[i].iqueue.real_vaddr,
-					  lfs->lf[i].iqueue.real_dma_addr);
-		lfs->lf[i].iqueue.real_vaddr = NULL;
-		lfs->lf[i].iqueue.vaddr = NULL;
+					  iq->size,
+					  iq->real_vaddr,
+					  iq->real_dma_addr);
+		iq->real_vaddr = NULL;
+		iq->vaddr = NULL;
 	}
 }
 
 static inline int otx2_cpt_alloc_instruction_queues(
 					struct otx2_cptlfs_info *lfs)
 {
+	struct otx2_cpt_inst_queue *iq;
 	int ret = 0, i;
 
 	if (!lfs->lfs_num)
 		return -EINVAL;
 
 	for (i = 0; i < lfs->lfs_num; i++) {
-
-		lfs->lf[i].iqueue.size = OTX2_CPT_INST_QLEN_BYTES +
-					 OTX2_CPT_Q_FC_LEN +
-					 OTX2_CPT_INST_GRP_QLEN_BYTES +
-					 OTX2_CPT_INST_Q_ALIGNMENT;
-		lfs->lf[i].iqueue.real_vaddr =
-				dma_alloc_coherent(&lfs->pdev->dev,
-					lfs->lf[i].iqueue.size,
-					&lfs->lf[i].iqueue.real_dma_addr,
-					GFP_KERNEL);
-		if (!lfs->lf[i].iqueue.real_vaddr) {
-			dev_err(&lfs->pdev->dev,
-				"Inst queue allocation failed for LF %d\n", i);
+		iq = &lfs->lf[i].iqueue;
+		iq->size = OTX2_CPT_INST_QLEN_BYTES +
+			   OTX2_CPT_Q_FC_LEN +
+			   OTX2_CPT_INST_GRP_QLEN_BYTES +
+			   OTX2_CPT_INST_Q_ALIGNMENT;
+		iq->real_vaddr = dma_alloc_coherent(&lfs->pdev->dev, iq->size,
+					&iq->real_dma_addr, GFP_KERNEL);
+		if (!iq->real_vaddr) {
 			ret = -ENOMEM;
 			goto error;
 		}
-		lfs->lf[i].iqueue.vaddr = lfs->lf[i].iqueue.real_vaddr +
-					  OTX2_CPT_INST_GRP_QLEN_BYTES;
-		lfs->lf[i].iqueue.dma_addr = lfs->lf[i].iqueue.real_dma_addr +
-					     OTX2_CPT_INST_GRP_QLEN_BYTES;
+		iq->vaddr = iq->real_vaddr + OTX2_CPT_INST_GRP_QLEN_BYTES;
+		iq->dma_addr = iq->real_dma_addr + OTX2_CPT_INST_GRP_QLEN_BYTES;
 
 		/* Align pointers */
-		lfs->lf[i].iqueue.vaddr =
-			(uint8_t *) PTR_ALIGN(lfs->lf[i].iqueue.vaddr,
-					      OTX2_CPT_INST_Q_ALIGNMENT);
-		lfs->lf[i].iqueue.dma_addr =
-			(dma_addr_t) PTR_ALIGN(lfs->lf[i].iqueue.dma_addr,
-					       OTX2_CPT_INST_Q_ALIGNMENT);
+		iq->vaddr = PTR_ALIGN(iq->vaddr, OTX2_CPT_INST_Q_ALIGNMENT);
+		iq->dma_addr = PTR_ALIGN(iq->dma_addr,
+					 OTX2_CPT_INST_Q_ALIGNMENT);
 	}
-
 	return 0;
+
 error:
 	otx2_cpt_free_instruction_queues(lfs);
 	return ret;
@@ -317,10 +321,10 @@ static inline void otx2_cpt_fill_inst(union otx2_cpt_inst_s *cptinst,
 	cptinst->s.res_addr = comp_baddr;
 	cptinst->u[2] = 0x0;
 	cptinst->u[3] = 0x0;
-	cptinst->s.ei0 = iq_cmd->cmd.u64;
+	cptinst->s.ei0 = iq_cmd->cmd.u;
 	cptinst->s.ei1 = iq_cmd->dptr;
 	cptinst->s.ei2 = iq_cmd->rptr;
-	cptinst->s.ei3 = iq_cmd->cptr.u64;
+	cptinst->s.ei3 = iq_cmd->cptr.u;
 }
 
 /*
@@ -330,30 +334,21 @@ static inline void otx2_cpt_fill_inst(union otx2_cpt_inst_s *cptinst,
  * 2 - 2 CPT instructions will be enqueued during LMTST operation
  */
 static inline void otx2_cpt_send_cmd(union otx2_cpt_inst_s *cptinst,
-				     u32 insts_num, void *obj)
+				     u32 insts_num, struct otx2_cptlf_info *lf)
 {
-	struct otx2_cptlf_info *lf = obj;
-	void *lmtline = lf->lmtline;
-	void *ioreg = lf->ioreg;
+	void __iomem *lmtline = lf->lmtline;
 	long ret;
 
 	/*
 	 * Make sure memory areas pointed in CPT_INST_S
 	 * are flushed before the instruction is sent to CPT
 	 */
-	smp_wmb();
+	dma_wmb();
 
 	do {
 		/* Copy CPT command to LMTLINE */
-		memcpy(lmtline, cptinst, insts_num * OTX2_CPT_INST_SIZE);
+		memcpy_toio(lmtline, cptinst, insts_num * OTX2_CPT_INST_SIZE);
 
-		/*
-		 * Make sure compiler does not reorder memcpy and ldeor.
-		 * LMTST transactions are always flushed from the write
-		 * buffer immediately, a DMB is not required to push out
-		 * LMTSTs.
-		 */
-		barrier();
 		/*
 		 * LDEOR initiates atomic transfer to I/O device
 		 * The following will cause the LMTST to fail (the LDEOR
@@ -372,15 +367,22 @@ static inline void otx2_cpt_send_cmd(union otx2_cpt_inst_s *cptinst,
 		 * need to store to LMTCANCEL. Also note as LMTLINE data cannot
 		 * be read, there is no information leakage between processes.
 		 */
-		__asm__ volatile(
-			"  .cpu		generic+lse\n"
-			"  ldeor	xzr, %0, [%1]\n"
-			: "=r" (ret) : "r" (ioreg) : "memory");
+		ret = otx2_lmt_flush(lf->ioreg);
+
 	} while (!ret);
 }
 
-int otx2_cptvf_lf_init(struct pci_dev *pdev, void *reg_base,
-		       struct otx2_cptlfs_info *lfs, int lfs_num);
-int otx2_cptvf_lf_shutdown(struct pci_dev *pdev, struct otx2_cptlfs_info *lfs);
+static inline bool otx2_cptlf_started(struct otx2_cptlfs_info *lfs)
+{
+	return atomic_read(&lfs->state) == OTX2_CPTLF_STARTED;
+}
+
+int otx2_cptlf_init(struct otx2_cptlfs_info *lfs, u8 eng_grp_msk, int pri,
+		    int lfs_num);
+void otx2_cptlf_shutdown(struct otx2_cptlfs_info *lfs);
+int otx2_cptlf_register_interrupts(struct otx2_cptlfs_info *lfs);
+void otx2_cptlf_unregister_interrupts(struct otx2_cptlfs_info *lfs);
+void otx2_cptlf_free_irqs_affinity(struct otx2_cptlfs_info *lfs);
+int otx2_cptlf_set_irqs_affinity(struct otx2_cptlfs_info *lfs);
 
 #endif /* __OTX2_CPTLF_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptlf_main.c b/drivers/crypto/marvell/octeontx2/otx2_cptlf_main.c
deleted file mode 100644
index 661e1f4704a2..000000000000
--- a/drivers/crypto/marvell/octeontx2/otx2_cptlf_main.c
+++ /dev/null
@@ -1,967 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-
-#include "otx2_cpt_common.h"
-#include "otx2_cpt_reqmgr.h"
-#include "otx2_cptvf_algs.h"
-#include "otx2_cpt_mbox_common.h"
-#include "rvu_reg.h"
-
-#define CPT_TIMER_HOLD 0x03F
-#define CPT_COUNT_HOLD 32
-/* Minimum and maximum values for interrupt coalescing */
-#define CPT_COALESC_MIN_TIME_WAIT  0x0
-#define CPT_COALESC_MAX_TIME_WAIT  ((1<<16)-1)
-#define CPT_COALESC_MIN_NUM_WAIT   0x0
-#define CPT_COALESC_MAX_NUM_WAIT   ((1<<20)-1)
-
-static int cptlf_get_done_time_wait(struct otx2_cptlf_info *lf)
-{
-	union otx2_cptx_lf_done_wait done_wait;
-
-	done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
-				      lf->slot, OTX2_CPT_LF_DONE_WAIT);
-	return done_wait.s.time_wait;
-}
-
-static void cptlf_do_set_done_time_wait(struct otx2_cptlf_info *lf,
-					int time_wait)
-{
-	union otx2_cptx_lf_done_wait done_wait;
-	u8 blkaddr = lf->lfs->blkaddr;
-
-	done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, blkaddr, lf->slot,
-				      OTX2_CPT_LF_DONE_WAIT);
-	done_wait.s.time_wait = time_wait;
-	otx2_cpt_write64(lf->lfs->reg_base, blkaddr, lf->slot,
-			 OTX2_CPT_LF_DONE_WAIT, done_wait.u);
-}
-
-static int cptlf_get_done_num_wait(struct otx2_cptlf_info *lf)
-{
-	union otx2_cptx_lf_done_wait done_wait;
-
-	done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
-				      lf->slot, OTX2_CPT_LF_DONE_WAIT);
-	return done_wait.s.num_wait;
-}
-
-static void cptlf_do_set_done_num_wait(struct otx2_cptlf_info *lf, int num_wait)
-{
-	union otx2_cptx_lf_done_wait done_wait;
-	u8 blkaddr = lf->lfs->blkaddr;
-
-	done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, blkaddr, lf->slot,
-				      OTX2_CPT_LF_DONE_WAIT);
-	done_wait.s.num_wait = num_wait;
-	otx2_cpt_write64(lf->lfs->reg_base, blkaddr, lf->slot,
-			 OTX2_CPT_LF_DONE_WAIT, done_wait.u);
-}
-
-static void cptlf_set_done_time_wait(struct otx2_cptlfs_info *lfs,
-				     int time_wait)
-{
-	int slot;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++)
-		cptlf_do_set_done_time_wait(&lfs->lf[slot], time_wait);
-}
-
-static void cptlf_set_done_num_wait(struct otx2_cptlfs_info *lfs, int num_wait)
-{
-	int slot;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++)
-		cptlf_do_set_done_num_wait(&lfs->lf[slot], num_wait);
-}
-
-static int cptlf_get_inflight(struct otx2_cptlf_info *lf)
-{
-	union otx2_cptx_lf_inprog lf_inprog;
-
-	lf_inprog.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
-				      lf->slot, OTX2_CPT_LF_INPROG);
-
-	return lf_inprog.s.inflight;
-}
-
-static int cptlf_get_pri(struct pci_dev *pdev, struct otx2_cptlf_info *lf,
-			 int *pri)
-{
-	union otx2_cptx_af_lf_ctrl lf_ctrl;
-	int ret;
-
-	ret = otx2_cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
-	if (ret)
-		return ret;
-
-	*pri = lf_ctrl.s.pri;
-
-	return ret;
-}
-
-static int cptlf_set_pri(struct pci_dev *pdev, struct otx2_cptlf_info *lf,
-			 int pri)
-{
-	union otx2_cptx_af_lf_ctrl lf_ctrl;
-	int ret;
-
-	ret = otx2_cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
-	if (ret)
-		return ret;
-
-	lf_ctrl.s.pri = pri ? 1 : 0;
-
-	ret = otx2_cpt_write_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), lf_ctrl.u);
-	return ret;
-}
-
-static int cptlf_get_eng_grps_mask(struct pci_dev *pdev,
-				   struct otx2_cptlf_info *lf,
-				   int *eng_grps_mask)
-{
-	union otx2_cptx_af_lf_ctrl lf_ctrl;
-	int ret;
-
-	ret = otx2_cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
-	if (ret)
-		return ret;
-
-	*eng_grps_mask = lf_ctrl.s.grp;
-
-	return ret;
-}
-
-static int cptlf_set_eng_grps_mask(struct pci_dev *pdev,
-				   struct otx2_cptlf_info *lf,
-				   int eng_grps_mask)
-{
-	union otx2_cptx_af_lf_ctrl lf_ctrl;
-	int ret;
-
-	ret = otx2_cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
-	if (ret)
-		return ret;
-
-	lf_ctrl.s.grp = eng_grps_mask;
-
-	ret = otx2_cpt_write_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), lf_ctrl.u);
-	return ret;
-}
-
-static int cptlf_set_grp_and_pri(struct pci_dev *pdev,
-				 struct otx2_cptlfs_info *lfs,
-				 int eng_grp_mask, int pri)
-{
-	int slot, ret;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++) {
-		ret = cptlf_set_pri(pdev, &lfs->lf[slot], pri);
-		if (ret)
-			return ret;
-
-		ret = cptlf_set_eng_grps_mask(pdev, &lfs->lf[slot],
-					      eng_grp_mask);
-		if (ret)
-			return ret;
-	}
-	return ret;
-}
-
-static void cptlf_hw_init(struct otx2_cptlfs_info *lfs)
-{
-	/* Disable instruction queues */
-	otx2_cptlf_disable_iqueues(lfs);
-
-	/* Set instruction queues base addresses */
-	otx2_cptlf_set_iqueues_base_addr(lfs);
-
-	/* Set instruction queues sizes */
-	otx2_cptlf_set_iqueues_size(lfs);
-
-	/* Set done interrupts time wait */
-	cptlf_set_done_time_wait(lfs, CPT_TIMER_HOLD);
-
-	/* Set done interrupts num wait */
-	cptlf_set_done_num_wait(lfs, CPT_COUNT_HOLD);
-
-	/* Enable instruction queues */
-	otx2_cptlf_enable_iqueues(lfs);
-}
-
-static void cptlf_hw_cleanup(struct otx2_cptlfs_info *lfs)
-{
-	/* Disable instruction queues */
-	otx2_cptlf_disable_iqueues(lfs);
-}
-
-static void cptlf_set_misc_intrs(struct otx2_cptlfs_info *lfs, u8 enable)
-{
-	union otx2_cptx_lf_misc_int_ena_w1s irq_misc = { .u = 0x0 };
-	u64 reg = enable ? OTX2_CPT_LF_MISC_INT_ENA_W1S :
-			   OTX2_CPT_LF_MISC_INT_ENA_W1C;
-	int slot;
-
-	irq_misc.s.fault = 0x1;
-	irq_misc.s.hwerr = 0x1;
-	irq_misc.s.irde = 0x1;
-	irq_misc.s.nqerr = 0x1;
-	irq_misc.s.nwrp = 0x1;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++)
-		otx2_cpt_write64(lfs->reg_base, lfs->blkaddr, slot, reg,
-				 irq_misc.u);
-}
-
-static void cptlf_enable_misc_intrs(struct otx2_cptlfs_info *lfs)
-{
-	cptlf_set_misc_intrs(lfs, true);
-}
-
-static void cptlf_disable_misc_intrs(struct otx2_cptlfs_info *lfs)
-{
-	cptlf_set_misc_intrs(lfs, false);
-}
-
-static void cptlf_enable_done_intr(struct otx2_cptlfs_info *lfs)
-{
-	int slot;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++)
-		otx2_cpt_write64(lfs->reg_base, lfs->blkaddr, slot,
-				 OTX2_CPT_LF_DONE_INT_ENA_W1S, 0x1);
-}
-
-static void cptlf_disable_done_intr(struct otx2_cptlfs_info *lfs)
-{
-	int slot;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++)
-		otx2_cpt_write64(lfs->reg_base, lfs->blkaddr, slot,
-				 OTX2_CPT_LF_DONE_INT_ENA_W1C, 0x1);
-}
-
-static inline int cptlf_read_done_cnt(struct otx2_cptlf_info *lf)
-{
-	union otx2_cptx_lf_done irq_cnt;
-
-	irq_cnt.u = otx2_cpt_read64(lf->lfs->reg_base, lf->lfs->blkaddr,
-				    lf->slot, OTX2_CPT_LF_DONE);
-	return irq_cnt.s.done;
-}
-
-static irqreturn_t cptlf_misc_intr_handler(int __always_unused irq, void *arg)
-{
-	union otx2_cptx_lf_misc_int irq_misc, irq_misc_ack;
-	struct otx2_cptlf_info *lf = arg;
-	u8 blkaddr = lf->lfs->blkaddr;
-	struct device *dev;
-
-	dev = &lf->lfs->pdev->dev;
-	irq_misc.u = otx2_cpt_read64(lf->lfs->reg_base, blkaddr, lf->slot,
-				     OTX2_CPT_LF_MISC_INT);
-	irq_misc_ack.u = 0x0;
-
-	if (irq_misc.s.fault) {
-		dev_err(dev, "Memory error detected while executing CPT_INST_S, LF %d.\n",
-			lf->slot);
-		irq_misc_ack.s.fault = 0x1;
-
-	} else if (irq_misc.s.hwerr) {
-		dev_err(dev, "HW error from an engine executing CPT_INST_S, LF %d.",
-			lf->slot);
-		irq_misc_ack.s.hwerr = 0x1;
-
-	} else if (irq_misc.s.nwrp) {
-		dev_err(dev, "SMMU fault while writing CPT_RES_S to CPT_INST_S[RES_ADDR], LF %d.\n",
-			lf->slot);
-		irq_misc_ack.s.nwrp = 0x1;
-
-	} else if (irq_misc.s.irde) {
-		dev_err(dev, "Memory error when accessing instruction memory queue CPT_LF_Q_BASE[ADDR].\n");
-		irq_misc_ack.s.irde = 0x1;
-
-	} else if (irq_misc.s.nqerr) {
-		dev_err(dev, "Error enqueuing an instruction received at CPT_LF_NQ.\n");
-		irq_misc_ack.s.nqerr = 0x1;
-
-	} else {
-		dev_err(dev, "Unhandled interrupt in CPT LF %d\n", lf->slot);
-		return IRQ_NONE;
-	}
-
-	/* Acknowledge interrupts */
-	otx2_cpt_write64(lf->lfs->reg_base, blkaddr, lf->slot,
-			 OTX2_CPT_LF_MISC_INT, irq_misc_ack.u);
-
-	return IRQ_HANDLED;
-}
-
-static irqreturn_t cptlf_done_intr_handler(int irq, void *arg)
-{
-	union otx2_cptx_lf_done_wait done_wait;
-	struct otx2_cptlf_info *lf = arg;
-	u8 blkaddr = lf->lfs->blkaddr;
-	int irq_cnt;
-
-	/* Read the number of completed requests */
-	irq_cnt = cptlf_read_done_cnt(lf);
-	if (irq_cnt) {
-		done_wait.u = otx2_cpt_read64(lf->lfs->reg_base, blkaddr,
-					      lf->slot, OTX2_CPT_LF_DONE_WAIT);
-		/* Acknowledge the number of completed requests */
-		otx2_cpt_write64(lf->lfs->reg_base, blkaddr, lf->slot,
-				 OTX2_CPT_LF_DONE_ACK, irq_cnt);
-
-		otx2_cpt_write64(lf->lfs->reg_base, blkaddr, lf->slot,
-				 OTX2_CPT_LF_DONE_WAIT, done_wait.u);
-		if (unlikely(!lf->wqe)) {
-			dev_err(&lf->lfs->pdev->dev, "No work for LF %d\n",
-				lf->slot);
-			return IRQ_NONE;
-		}
-
-		/* Schedule processing of completed requests */
-		tasklet_hi_schedule(&lf->wqe->work);
-	}
-	return IRQ_HANDLED;
-}
-
-static int cptlf_do_register_interrrupts(struct otx2_cptlfs_info *lfs,
-					 int lf_num, int irq_offset,
-					 irq_handler_t handler)
-{
-	int ret;
-
-	ret = request_irq(pci_irq_vector(lfs->pdev, lfs->lf[lf_num].msix_offset
-			  + irq_offset), handler, 0,
-			  lfs->lf[lf_num].irq_name[irq_offset],
-			  &lfs->lf[lf_num]);
-	if (ret)
-		return ret;
-
-	lfs->lf[lf_num].is_irq_reg[irq_offset] = true;
-
-	return ret;
-}
-
-static int cptlf_register_interrupts(struct otx2_cptlfs_info *lfs)
-{
-	int irq_offs, ret, i;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		irq_offs = OTX2_CPT_LF_INT_VEC_E_MISC;
-		snprintf(lfs->lf[i].irq_name[irq_offs], 32, "CPTLF Misc%d", i);
-		ret = cptlf_do_register_interrrupts(lfs, i, irq_offs,
-						    cptlf_misc_intr_handler);
-		if (ret)
-			return ret;
-
-		irq_offs = OTX2_CPT_LF_INT_VEC_E_DONE;
-		snprintf(lfs->lf[i].irq_name[irq_offs], 32, "OTX2_CPTLF Done%d",
-			 i);
-		ret = cptlf_do_register_interrrupts(lfs, i, irq_offs,
-						    cptlf_done_intr_handler);
-		if (ret)
-			return ret;
-	}
-	return ret;
-}
-
-static void cptlf_unregister_interrupts(struct otx2_cptlfs_info *lfs)
-{
-	int i, offs;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		for (offs = 0; offs < OTX2_CPT_LF_MSIX_VECTORS; offs++) {
-			if (lfs->lf[i].is_irq_reg[offs]) {
-				free_irq(pci_irq_vector(lfs->pdev,
-							lfs->lf[i].msix_offset
-							+ offs),
-							&lfs->lf[i]);
-				lfs->lf[i].is_irq_reg[offs] = false;
-			}
-		}
-	}
-}
-
-static int cptlf_set_irqs_affinity(struct otx2_cptlfs_info *lfs)
-{
-	int slot, offs, ret;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++) {
-		if (!zalloc_cpumask_var(&lfs->lf[slot].affinity_mask,
-					GFP_KERNEL)) {
-			dev_err(&lfs->pdev->dev,
-				"cpumask allocation failed for LF %d", slot);
-			return -ENOMEM;
-		}
-
-		cpumask_set_cpu(cpumask_local_spread(slot,
-				dev_to_node(&lfs->pdev->dev)),
-				lfs->lf[slot].affinity_mask);
-
-		for (offs = 0; offs < OTX2_CPT_LF_MSIX_VECTORS; offs++) {
-			ret = irq_set_affinity_hint(pci_irq_vector(lfs->pdev,
-					lfs->lf[slot].msix_offset + offs),
-					lfs->lf[slot].affinity_mask);
-			if (ret)
-				return ret;
-		}
-	}
-	return ret;
-}
-
-static void cptlf_free_irqs_affinity(struct otx2_cptlfs_info *lfs)
-{
-	int slot, offs;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++) {
-		for (offs = 0; offs < OTX2_CPT_LF_MSIX_VECTORS; offs++)
-			irq_set_affinity_hint(pci_irq_vector(lfs->pdev,
-					      lfs->lf[slot].msix_offset +
-					      offs), NULL);
-		free_cpumask_var(lfs->lf[slot].affinity_mask);
-	}
-}
-
-static void cptlf_work_handler(unsigned long data)
-{
-	otx2_cpt_post_process((struct otx2_cptlf_wqe *) data);
-}
-
-static int init_tasklet_work(struct otx2_cptlfs_info *lfs)
-{
-	struct otx2_cptlf_wqe *wqe;
-	int i;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		wqe = kzalloc(sizeof(struct otx2_cptlf_wqe), GFP_KERNEL);
-		if (!wqe)
-			return -ENOMEM;
-
-		tasklet_init(&wqe->work, cptlf_work_handler, (u64) wqe);
-		wqe->lfs = lfs;
-		wqe->lf_num = i;
-		lfs->lf[i].wqe = wqe;
-	}
-	return 0;
-}
-
-static void cleanup_tasklet_work(struct otx2_cptlfs_info *lfs)
-{
-	int i;
-
-	for (i = 0; i <  lfs->lfs_num; i++) {
-		if (!lfs->lf[i].wqe)
-			continue;
-
-		tasklet_kill(&lfs->lf[i].wqe->work);
-		kfree(lfs->lf[i].wqe);
-		lfs->lf[i].wqe = NULL;
-	}
-}
-
-static void free_pending_queues(struct otx2_cptlfs_info *lfs)
-{
-	int i;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		kfree(lfs->lf[i].pqueue.head);
-		lfs->lf[i].pqueue.head = NULL;
-	}
-}
-
-static int alloc_pending_queues(struct otx2_cptlfs_info *lfs)
-{
-	int size, ret, i;
-
-	if (!lfs->lfs_num)
-		return -EINVAL;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		lfs->lf[i].pqueue.qlen = OTX2_CPT_INST_QLEN_MSGS;
-		size = lfs->lf[i].pqueue.qlen *
-		       sizeof(struct otx2_cpt_pending_entry);
-
-		lfs->lf[i].pqueue.head = kzalloc(size, GFP_KERNEL);
-		if (!lfs->lf[i].pqueue.head) {
-			ret = -ENOMEM;
-			goto error;
-		}
-
-		/* Initialize spin lock */
-		spin_lock_init(&lfs->lf[i].pqueue.lock);
-	}
-	return 0;
-error:
-	free_pending_queues(lfs);
-	return ret;
-}
-
-static int cptlf_sw_init(struct otx2_cptlfs_info *lfs)
-{
-	int ret;
-
-	ret = otx2_cpt_alloc_instruction_queues(lfs);
-	if (ret) {
-		dev_err(&lfs->pdev->dev,
-			"Allocating instruction queues failed\n");
-		return ret;
-	}
-
-	ret = alloc_pending_queues(lfs);
-	if (ret) {
-		dev_err(&lfs->pdev->dev,
-			"Allocating pending queues failed\n");
-		goto instruction_queues_free;
-	}
-
-	ret = init_tasklet_work(lfs);
-	if (ret) {
-		dev_err(&lfs->pdev->dev,
-			"Tasklet work init failed\n");
-		goto pending_queues_free;
-	}
-	return 0;
-
-pending_queues_free:
-	free_pending_queues(lfs);
-instruction_queues_free:
-	otx2_cpt_free_instruction_queues(lfs);
-	return ret;
-}
-
-static void cptlf_sw_cleanup(struct otx2_cptlfs_info *lfs)
-{
-	cleanup_tasklet_work(lfs);
-	free_pending_queues(lfs);
-	otx2_cpt_free_instruction_queues(lfs);
-}
-
-static ssize_t cptlf_coalesc_time_wait_show(struct device *dev,
-					    struct device_attribute *attr,
-					    char *buf)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg, coalesc_tw_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-
-	return scnprintf(buf, PAGE_SIZE, "%d\n", cptlf_get_done_time_wait(lf));
-}
-
-static ssize_t cptlf_coalesc_time_wait_store(struct device *dev,
-					     struct device_attribute *attr,
-					     const char *buf, size_t count)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-	long val;
-	int ret;
-
-	ret = kstrtol(buf, 10, &val);
-	if (ret != 0)
-		return ret;
-
-	if (val < CPT_COALESC_MIN_TIME_WAIT ||
-	    val > CPT_COALESC_MAX_TIME_WAIT)
-		return -EINVAL;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg, coalesc_tw_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-
-	cptlf_do_set_done_time_wait(lf, val);
-	return count;
-}
-
-static ssize_t cptlf_coalesc_num_wait_show(struct device *dev,
-					   struct device_attribute *attr,
-					   char *buf)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg, coalesc_nw_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-
-	return scnprintf(buf, PAGE_SIZE, "%d\n", cptlf_get_done_num_wait(lf));
-}
-
-static ssize_t cptlf_coalesc_num_wait_store(struct device *dev,
-					    struct device_attribute *attr,
-					    const char *buf, size_t count)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-	long val;
-	int ret;
-
-	ret = kstrtol(buf, 10, &val);
-	if (ret != 0)
-		return ret;
-
-	if (val < CPT_COALESC_MIN_NUM_WAIT ||
-	    val > CPT_COALESC_MAX_NUM_WAIT)
-		return -EINVAL;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg, coalesc_nw_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-
-	cptlf_do_set_done_num_wait(lf, val);
-	return count;
-}
-
-static ssize_t cptlf_priority_show(struct device *dev,
-				   struct device_attribute *attr, char *buf)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-	struct pci_dev *pdev;
-	int pri, ret;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg, prio_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-	pdev = container_of(dev, struct pci_dev, dev);
-
-	ret = cptlf_get_pri(pdev, lf, &pri);
-	if (ret)
-		return ret;
-
-	return scnprintf(buf, PAGE_SIZE, "%d\n", pri);
-}
-
-static ssize_t cptlf_priority_store(struct device *dev,
-				    struct device_attribute *attr,
-				    const char *buf, size_t count)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-	struct pci_dev *pdev;
-	long val;
-	int ret;
-
-	ret = kstrtol(buf, 10, &val);
-	if (ret)
-		return ret;
-
-	if (val < OTX2_CPT_QUEUE_LOW_PRIO ||
-	    val > OTX2_CPT_QUEUE_HI_PRIO)
-		return -EINVAL;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg, prio_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-	pdev = container_of(dev, struct pci_dev, dev);
-
-	/* Queue's priority can be modified only if queue is quiescent */
-	if (cptlf_get_inflight(lf)) {
-		ret = -EPERM;
-		goto err_print;
-	}
-
-	otx2_cptlf_disable_iqueue_exec(lf);
-
-	if (cptlf_get_inflight(lf)) {
-		ret = -EPERM;
-		otx2_cptlf_enable_iqueue_exec(lf);
-		goto err_print;
-	}
-
-	ret = cptlf_set_pri(pdev, lf, val);
-	if (ret) {
-		otx2_cptlf_enable_iqueue_exec(lf);
-		goto err;
-	}
-
-	otx2_cptlf_enable_iqueue_exec(lf);
-	return count;
-
-err_print:
-	dev_err(&pdev->dev,
-		"Disable traffic before modifying queue's priority");
-err:
-	return ret;
-}
-
-static ssize_t cptlf_eng_grps_mask_show(struct device *dev,
-					struct device_attribute *attr,
-					char *buf)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-	struct pci_dev *pdev;
-	int eng_grps_mask;
-	int ret;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg,
-			   eng_grps_mask_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-	pdev = container_of(dev, struct pci_dev, dev);
-
-	ret = cptlf_get_eng_grps_mask(pdev, lf, &eng_grps_mask);
-	if (ret)
-		return ret;
-
-	return scnprintf(buf, PAGE_SIZE, "0x%2.2X\n", eng_grps_mask);
-}
-
-static ssize_t cptlf_eng_grps_mask_store(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	struct otx2_cptlf_info *lf;
-	struct pci_dev *pdev;
-	long val;
-	int ret;
-
-	ret = kstrtol(buf, 16, &val);
-	if (ret)
-		return ret;
-
-	if (val < 1 ||
-	    val > OTX2_CPT_ALL_ENG_GRPS_MASK)
-		return -EINVAL;
-
-	cfg = container_of(attr, struct otx2_cptlf_sysfs_cfg,
-			   eng_grps_mask_attr);
-	lf = container_of(cfg, struct otx2_cptlf_info, sysfs_cfg);
-	pdev = container_of(dev, struct pci_dev, dev);
-
-	/*
-	 * Queue's engine groups mask can be modified only if queue is
-	 * quiescent
-	 */
-	if (cptlf_get_inflight(lf)) {
-		ret = -EPERM;
-		goto err_print;
-	}
-
-	otx2_cptlf_disable_iqueue_exec(lf);
-
-	if (cptlf_get_inflight(lf)) {
-		ret = -EPERM;
-		otx2_cptlf_enable_iqueue_exec(lf);
-		goto err_print;
-	}
-
-	ret = cptlf_set_eng_grps_mask(pdev, lf, val);
-	if (ret) {
-		otx2_cptlf_enable_iqueue_exec(lf);
-		goto err;
-	}
-
-	otx2_cptlf_enable_iqueue_exec(lf);
-	return count;
-
-err_print:
-	dev_err(&pdev->dev,
-		"Disable traffic before modifying queue's engine groups mask");
-err:
-	return ret;
-}
-
-static void cptlf_delete_sysfs_cfg(struct otx2_cptlfs_info *lfs)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	int i;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		cfg = &lfs->lf[i].sysfs_cfg;
-		if (cfg->is_sysfs_grp_created) {
-			sysfs_remove_group(&lfs->pdev->dev.kobj,
-					   &cfg->attr_grp);
-			cfg->is_sysfs_grp_created = false;
-		}
-	}
-}
-
-static int cptlf_create_sysfs_cfg(struct otx2_cptlfs_info *lfs)
-{
-	struct otx2_cptlf_sysfs_cfg *cfg;
-	int i, ret = 0;
-
-	for (i = 0; i < lfs->lfs_num; i++) {
-		cfg = &lfs->lf[i].sysfs_cfg;
-		snprintf(cfg->name, OTX2_CPT_NAME_LENGTH, "cpt_queue%d", i);
-
-		cfg->eng_grps_mask_attr.show = cptlf_eng_grps_mask_show;
-		cfg->eng_grps_mask_attr.store = cptlf_eng_grps_mask_store;
-		cfg->eng_grps_mask_attr.attr.name = "eng_grps_mask";
-		cfg->eng_grps_mask_attr.attr.mode = 0664;
-		sysfs_attr_init(&cfg->eng_grps_mask_attr.attr);
-
-		cfg->coalesc_tw_attr.show = cptlf_coalesc_time_wait_show;
-		cfg->coalesc_tw_attr.store = cptlf_coalesc_time_wait_store;
-		cfg->coalesc_tw_attr.attr.name = "coalescence_time_wait";
-		cfg->coalesc_tw_attr.attr.mode = 0664;
-		sysfs_attr_init(&cfg->coalesc_tw_attr.attr);
-
-		cfg->coalesc_nw_attr.show = cptlf_coalesc_num_wait_show;
-		cfg->coalesc_nw_attr.store = cptlf_coalesc_num_wait_store;
-		cfg->coalesc_nw_attr.attr.name = "coalescence_num_wait";
-		cfg->coalesc_nw_attr.attr.mode = 0664;
-		sysfs_attr_init(&cfg->coalesc_nw_attr.attr);
-
-		cfg->prio_attr.show = cptlf_priority_show;
-		cfg->prio_attr.store = cptlf_priority_store;
-		cfg->prio_attr.attr.name = "priority";
-		cfg->prio_attr.attr.mode = 0664;
-		sysfs_attr_init(&cfg->prio_attr.attr);
-
-		cfg->attrs[0] = &cfg->eng_grps_mask_attr.attr;
-		cfg->attrs[1] = &cfg->coalesc_tw_attr.attr;
-		cfg->attrs[2] = &cfg->coalesc_nw_attr.attr;
-		cfg->attrs[3] = &cfg->prio_attr.attr;
-		cfg->attrs[ATTRS_NUM - 1] = NULL;
-
-		cfg->attr_grp.name = cfg->name;
-		cfg->attr_grp.attrs = cfg->attrs;
-		ret = sysfs_create_group(&lfs->pdev->dev.kobj,
-					 &cfg->attr_grp);
-		if (ret)
-			goto err;
-		cfg->is_sysfs_grp_created = true;
-	}
-
-	return 0;
-err:
-	cptlf_delete_sysfs_cfg(lfs);
-	return ret;
-}
-
-int otx2_cptvf_lf_init(struct pci_dev *pdev, void *reg_base,
-		       struct otx2_cptlfs_info *lfs, int lfs_num)
-{
-	struct otx2_cptvf_dev *cptvf;
-	int slot, ret;
-
-	lfs->reg_base = reg_base;
-	lfs->lfs_num = lfs_num;
-	lfs->pdev = pdev;
-
-	cptvf = (struct otx2_cptvf_dev *) pci_get_drvdata(pdev);
-	lfs->blkaddr = cptvf->blkaddr;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++) {
-		lfs->lf[slot].lfs = lfs;
-		lfs->lf[slot].slot = slot;
-		lfs->lf[slot].lmtline = lfs->reg_base +
-			OTX2_CPT_RVU_FUNC_ADDR_S(BLKADDR_LMT, slot,
-						 OTX2_CPT_LMT_LF_LMTLINEX(0));
-		lfs->lf[slot].ioreg = lfs->reg_base +
-			OTX2_CPT_RVU_FUNC_ADDR_S(lfs->blkaddr, slot,
-						 OTX2_CPT_LF_NQX(0));
-	}
-
-	/* Send request to attach LFs */
-	ret = otx2_cpt_attach_rscrs_msg(pdev);
-	if (ret)
-		return ret;
-
-	/* Get msix offsets for attached LFs */
-	ret = otx2_cpt_msix_offset_msg(pdev);
-	if (ret)
-		goto detach_rscrs;
-
-	/* Initialize LFs software side */
-	ret = cptlf_sw_init(lfs);
-	if (ret)
-		goto detach_rscrs;
-
-	/* Register LFs interrupts */
-	ret = cptlf_register_interrupts(lfs);
-	if (ret)
-		goto sw_cleanup;
-
-	/* Initialize LFs hardware side */
-	cptlf_hw_init(lfs);
-
-	/*
-	 * Allow each LF to execute requests destined to any of 8 engine
-	 * groups and set queue priority of each LF to high
-	 */
-	ret = cptlf_set_grp_and_pri(pdev, lfs, OTX2_CPT_ALL_ENG_GRPS_MASK,
-				    OTX2_CPT_QUEUE_HI_PRIO);
-	if (ret)
-		goto hw_cleanup;
-
-	/* Create sysfs configuration entries */
-	ret = cptlf_create_sysfs_cfg(lfs);
-	if (ret)
-		goto hw_cleanup;
-
-	/* Set interrupts affinity */
-	ret = cptlf_set_irqs_affinity(lfs);
-	if (ret)
-		goto delete_sysfs_cfg;
-
-	/* Enable interrupts */
-	cptlf_enable_misc_intrs(lfs);
-	cptlf_enable_done_intr(lfs);
-
-	/* Register crypto algorithms */
-	ret = otx2_cpt_crypto_init(pdev, THIS_MODULE, OTX2_CPT_SE_TYPES,
-				   lfs_num, 1);
-	if (ret) {
-		dev_err(&pdev->dev, "algorithms registration failed\n");
-		goto disable_irqs;
-	}
-	return 0;
-
-disable_irqs:
-	cptlf_disable_done_intr(lfs);
-	cptlf_disable_misc_intrs(lfs);
-	cptlf_free_irqs_affinity(lfs);
-delete_sysfs_cfg:
-	cptlf_delete_sysfs_cfg(lfs);
-hw_cleanup:
-	cptlf_hw_cleanup(lfs);
-	cptlf_unregister_interrupts(lfs);
-sw_cleanup:
-	cptlf_sw_cleanup(lfs);
-detach_rscrs:
-	otx2_cpt_detach_rsrcs_msg(pdev);
-
-	return ret;
-}
-
-int otx2_cptvf_lf_shutdown(struct pci_dev *pdev, struct otx2_cptlfs_info *lfs)
-{
-	int ret;
-
-	/* Unregister crypto algorithms */
-	otx2_cpt_crypto_exit(pdev, THIS_MODULE, OTX2_CPT_SE_TYPES);
-
-	/* Disable interrupts */
-	cptlf_disable_done_intr(lfs);
-	cptlf_disable_misc_intrs(lfs);
-
-	/* Remove interrupts affinity */
-	cptlf_free_irqs_affinity(lfs);
-
-	/* Remove sysfs configuration entries */
-	cptlf_delete_sysfs_cfg(lfs);
-
-	/* Cleanup LFs hardware side */
-	cptlf_hw_cleanup(lfs);
-
-	/* Unregister LFs interrupts */
-	cptlf_unregister_interrupts(lfs);
-
-	/* Cleanup LFs software side */
-	cptlf_sw_cleanup(lfs);
-
-	/* Send request to detach LFs */
-	ret = otx2_cpt_detach_rsrcs_msg(pdev);
-
-	return ret;
-}
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptpf.h b/drivers/crypto/marvell/octeontx2/otx2_cptpf.h
index d52e60be9aab..d0aae97688ff 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptpf.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptpf.h
@@ -1,16 +1,11 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
 
 #ifndef __OTX2_CPTPF_H
 #define __OTX2_CPTPF_H
 
+#include "otx2_cpt_common.h"
 #include "otx2_cptpf_ucode.h"
 #include "otx2_cptlf.h"
 
@@ -23,29 +18,6 @@ struct otx2_cptvf_info {
 	int intr_idx;
 };
 
-struct otx2_cpt_kvf_limits {
-	struct device_attribute kvf_limits_attr;
-	int lfs_num; /* Number of LFs allocated for kernel VF driver */
-};
-
-/* CPT HW capabilities */
-union otx2_cpt_eng_caps {
-	u64 u;
-	struct {
-		u64 reserved_0_4:5;
-		u64 mul:1;
-		u64 sha1_sha2:1;
-		u64 chacha20:1;
-		u64 zuc_snow3g:1;
-		u64 sha3:1;
-		u64 aes:1;
-		u64 kasumi:1;
-		u64 des:1;
-		u64 crc:1;
-		u64 reserved_14_63:50;
-	};
-};
-
 struct cptpf_flr_work {
 	struct work_struct work;
 	struct otx2_cptpf_dev *pf;
@@ -57,8 +29,8 @@ struct otx2_cptpf_dev {
 	void __iomem *vfpf_mbox_base;   /* VF-PF mbox start address */
 	struct pci_dev *pdev;		/* PCI device handle */
 	struct otx2_cptvf_info vf[OTX2_CPT_MAX_VFS_NUM];
-	struct otx2_cptlfs_info lfs;	/* CPT LFs attached to this PF */
 	struct otx2_cpt_eng_grps eng_grps;/* Engine groups information */
+	struct otx2_cptlfs_info lfs;      /* CPT LFs attached to this PF */
 	/* HW capabilities for each engine type */
 	union otx2_cpt_eng_caps eng_caps[OTX2_CPT_MAX_ENG_TYPES];
 	bool is_eng_caps_discovered;
@@ -75,21 +47,17 @@ struct otx2_cptpf_dev {
 	struct workqueue_struct	*flr_wq;
 	struct cptpf_flr_work   *flr_work;
 
-	bool irq_registered[OTX2_CPT_PF_MSIX_VECTORS];	/* Is IRQ registered */
-	u8 pf_id;		/* RVU PF number */
+	u8 pf_id;               /* RVU PF number */
 	u8 max_vfs;		/* Maximum number of VFs supported by CPT */
 	u8 enabled_vfs;		/* Number of enabled VFs */
-	u8 crypto_eng_grp;	/* Symmetric crypto engine group number */
 	u8 sso_pf_func_ovrd;	/* SSO PF_FUNC override bit */
 	u8 kvf_limits;		/* Kernel VF limits */
-	/* BLKADDR_CPT0/BLKADDR_CPT1 or 0 for BLKADDR_CPT0 */
-	u8 blkaddr;
-	bool cpt1_implemented;
+	bool has_cpt1;
 };
 
 irqreturn_t otx2_cptpf_afpf_mbox_intr(int irq, void *arg);
-irqreturn_t otx2_cptpf_vfpf_mbox_intr(int irq, void *arg);
 void otx2_cptpf_afpf_mbox_handler(struct work_struct *work);
+irqreturn_t otx2_cptpf_vfpf_mbox_intr(int irq, void *arg);
 void otx2_cptpf_vfpf_mbox_handler(struct work_struct *work);
 int otx2_cptpf_lf_init(struct otx2_cptpf_dev *cptpf, u8 eng_grp_mask,
 		       int pri, int lfs_num);
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptpf_main.c b/drivers/crypto/marvell/octeontx2/otx2_cptpf_main.c
index 142649e6b606..15db1c5ba14e 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptpf_main.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptpf_main.c
@@ -1,95 +1,80 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
 
 #include <linux/firmware.h>
-#include "otx2_cpt_mbox_common.h"
+#include "otx2_cpt_hw_types.h"
+#include "otx2_cpt_common.h"
+#include "otx2_cptpf_ucode.h"
+#include "otx2_cptpf.h"
 #include "rvu_reg.h"
 
-#define OTX2_CPT_DRV_NAME "octeontx2-cpt"
-#define OTX2_CPT_DRV_VERSION "1.0"
+#define OTX2_CPT_DRV_NAME    "octeontx2-cpt"
+#define OTX2_CPT_DRV_STRING  "Marvell OcteonTX2 CPT Physical Function Driver"
 
-static void cptpf_enable_vf_flr_me_intrs(struct otx2_cptpf_dev *cptpf)
+static void cptpf_enable_vf_flr_me_intrs(struct otx2_cptpf_dev *cptpf,
+					 int numvfs)
 {
-	int vfs = cptpf->max_vfs;
-
 	/* Clear FLR interrupt if any */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(0),
-			 INTR_MASK(vfs));
+			 INTR_MASK(numvfs));
 
 	/* Enable VF FLR interrupts */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFFLR_INT_ENA_W1SX(0), INTR_MASK(vfs));
+			 RVU_PF_VFFLR_INT_ENA_W1SX(0), INTR_MASK(numvfs));
 	/* Clear ME interrupt if any */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFME_INTX(0),
-			 INTR_MASK(vfs));
+			 INTR_MASK(numvfs));
 	/* Enable VF ME interrupts */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFME_INT_ENA_W1SX(0), INTR_MASK(vfs));
+			 RVU_PF_VFME_INT_ENA_W1SX(0), INTR_MASK(numvfs));
 
-	if (vfs <= 64)
+	if (numvfs <= 64)
 		return;
 
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(1),
-			 INTR_MASK(vfs - 64));
+			 INTR_MASK(numvfs - 64));
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFFLR_INT_ENA_W1SX(1), INTR_MASK(vfs - 64));
+			 RVU_PF_VFFLR_INT_ENA_W1SX(1), INTR_MASK(numvfs - 64));
 
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFME_INTX(1),
-			 INTR_MASK(vfs - 64));
+			 INTR_MASK(numvfs - 64));
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFME_INT_ENA_W1SX(1), INTR_MASK(vfs - 64));
+			 RVU_PF_VFME_INT_ENA_W1SX(1), INTR_MASK(numvfs - 64));
 }
 
-static void cptpf_disable_vf_flr_me_intrs(struct otx2_cptpf_dev *cptpf)
+static void cptpf_disable_vf_flr_me_intrs(struct otx2_cptpf_dev *cptpf,
+					  int numvfs)
 {
-	int vfs = cptpf->max_vfs;
+	int vector;
 
 	/* Disable VF FLR interrupts */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFFLR_INT_ENA_W1CX(0), INTR_MASK(vfs));
+			 RVU_PF_VFFLR_INT_ENA_W1CX(0), INTR_MASK(numvfs));
+	vector = pci_irq_vector(cptpf->pdev, RVU_PF_INT_VEC_VFFLR0);
+	free_irq(vector, cptpf);
+
 	/* Disable VF ME interrupts */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFME_INT_ENA_W1CX(0), INTR_MASK(vfs));
+			 RVU_PF_VFME_INT_ENA_W1CX(0), INTR_MASK(numvfs));
+	vector = pci_irq_vector(cptpf->pdev, RVU_PF_INT_VEC_VFME0);
+	free_irq(vector, cptpf);
 
-	if (vfs <= 64)
+	if (numvfs <= 64)
 		return;
 
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFFLR_INT_ENA_W1CX(1), INTR_MASK(vfs - 64));
+			 RVU_PF_VFFLR_INT_ENA_W1CX(1), INTR_MASK(numvfs - 64));
+	vector = pci_irq_vector(cptpf->pdev, RVU_PF_INT_VEC_VFFLR1);
+	free_irq(vector, cptpf);
 
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFME_INT_ENA_W1CX(1), INTR_MASK(vfs - 64));
-}
-
-static void cptpf_enable_afpf_mbox_intrs(struct otx2_cptpf_dev *cptpf)
-{
-	/* Clear interrupt if any */
-	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT, 0x1ULL);
-
-	/* Enable AF-PF interrupt */
-	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT_ENA_W1S,
-			 0x1ULL);
-}
-
-static void cptpf_disable_afpf_mbox_intrs(struct otx2_cptpf_dev *cptpf)
-{
-	/* Disable AF-PF interrupt */
-	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT_ENA_W1C,
-			 0x1ULL);
-
-	/* Clear interrupt if any */
-	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT, 0x1ULL);
+			 RVU_PF_VFME_INT_ENA_W1CX(1), INTR_MASK(numvfs - 64));
+	vector = pci_irq_vector(cptpf->pdev, RVU_PF_INT_VEC_VFME1);
+	free_irq(vector, cptpf);
 }
 
-static void cptpf_enable_vfpf_mbox_intrs(struct otx2_cptpf_dev *cptpf,
-					 int numvfs)
+static void cptpf_enable_vfpf_mbox_intr(struct otx2_cptpf_dev *cptpf,
+					int num_vfs)
 {
 	int ena_bits;
 
@@ -100,22 +85,25 @@ static void cptpf_enable_vfpf_mbox_intrs(struct otx2_cptpf_dev *cptpf,
 			 RVU_PF_VFPF_MBOX_INTX(1), ~0x0ULL);
 
 	/* Enable VF interrupts for VFs from 0 to 63 */
-	ena_bits = ((numvfs - 1) % 64);
+	ena_bits = ((num_vfs - 1) % 64);
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
 			 RVU_PF_VFPF_MBOX_INT_ENA_W1SX(0),
 			 GENMASK_ULL(ena_bits, 0));
 
-	if (numvfs > 64) {
+	if (num_vfs > 64) {
 		/* Enable VF interrupts for VFs from 64 to 127 */
-		ena_bits = numvfs - 64 - 1;
+		ena_bits = num_vfs - 64 - 1;
 		otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
 				RVU_PF_VFPF_MBOX_INT_ENA_W1SX(1),
 				GENMASK_ULL(ena_bits, 0));
 	}
 }
 
-static void cptpf_disable_vfpf_mbox_intrs(struct otx2_cptpf_dev *cptpf)
+static void cptpf_disable_vfpf_mbox_intr(struct otx2_cptpf_dev *cptpf,
+					  int num_vfs)
 {
+	int vector;
+
 	/* Disable VF-PF interrupts */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
 			 RVU_PF_VFPF_MBOX_INT_ENA_W1CX(0), ~0x0ULL);
@@ -125,8 +113,16 @@ static void cptpf_disable_vfpf_mbox_intrs(struct otx2_cptpf_dev *cptpf)
 	/* Clear any pending interrupts */
 	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
 			 RVU_PF_VFPF_MBOX_INTX(0), ~0x0ULL);
-	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
-			 RVU_PF_VFPF_MBOX_INTX(1), ~0x0ULL);
+
+	vector = pci_irq_vector(cptpf->pdev, RVU_PF_INT_VEC_VFPF_MBOX0);
+	free_irq(vector, cptpf);
+
+	if (num_vfs > 64) {
+		otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+				 RVU_PF_VFPF_MBOX_INTX(1), ~0ULL);
+		vector = pci_irq_vector(cptpf->pdev, RVU_PF_INT_VEC_VFPF_MBOX1);
+		free_irq(vector, cptpf);
+	}
 }
 
 static void cptpf_flr_wq_handler(struct work_struct *work)
@@ -153,7 +149,7 @@ static void cptpf_flr_wq_handler(struct work_struct *work)
 	req->pcifunc &= RVU_PFVF_FUNC_MASK;
 	req->pcifunc |= (vf + 1) & RVU_PFVF_FUNC_MASK;
 
-	otx2_cpt_send_mbox_msg(pf->pdev);
+	otx2_cpt_send_mbox_msg(mbox, pf->pdev);
 
 	if (vf >= 64) {
 		reg = 1;
@@ -225,92 +221,93 @@ static irqreturn_t cptpf_vf_me_intr(int __always_unused irq, void *arg)
 	return IRQ_HANDLED;
 }
 
-static void cptpf_unregister_interrupts(struct otx2_cptpf_dev *cptpf)
+static void cptpf_unregister_vfpf_intr(struct otx2_cptpf_dev *cptpf,
+				       int num_vfs)
 {
-	int i;
-
-	for (i = 0; i < OTX2_CPT_PF_MSIX_VECTORS; i++) {
-		if (cptpf->irq_registered[i])
-			free_irq(pci_irq_vector(cptpf->pdev, i), cptpf);
-		cptpf->irq_registered[i] = false;
-	}
-
-	pci_free_irq_vectors(cptpf->pdev);
+	cptpf_disable_vfpf_mbox_intr(cptpf, num_vfs);
+	cptpf_disable_vf_flr_me_intrs(cptpf, num_vfs);
 }
 
-static int cptpf_register_interrupts(struct otx2_cptpf_dev *cptpf)
+static int cptpf_register_vfpf_intr(struct otx2_cptpf_dev *cptpf, int num_vfs)
 {
-	u32 num_vec;
-	int ret;
-
-	num_vec = OTX2_CPT_PF_MSIX_VECTORS;
+	struct pci_dev *pdev = cptpf->pdev;
+	struct device *dev = &pdev->dev;
+	int ret, vector;
 
-	/* Enable MSI-X */
-	ret = pci_alloc_irq_vectors(cptpf->pdev, num_vec, num_vec,
-				    PCI_IRQ_MSIX);
-	if (ret < 0) {
-		dev_err(&cptpf->pdev->dev,
-			"Request for %d msix vectors failed\n", num_vec);
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFPF_MBOX0);
+	/* Register VF-PF mailbox interrupt handler */
+	ret = request_irq(vector, otx2_cptpf_vfpf_mbox_intr, 0, "CPTVFPF Mbox0",
+			  cptpf);
+	if (ret) {
+		dev_err(dev,
+			"IRQ registration failed for PFVF mbox0 irq\n");
 		return ret;
 	}
-
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFFLR0);
 	/* Register VF FLR interrupt handler */
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_VFFLR0), cptpf_vf_flr_intr, 0,
-			  "CPTPF FLR0", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_VFFLR0] = true;
-
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_VFFLR1), cptpf_vf_flr_intr, 0,
-			  "CPTPF FLR1", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_VFFLR1] = true;
-
+	ret = request_irq(vector, cptpf_vf_flr_intr, 0, "CPTPF FLR0", cptpf);
+	if (ret) {
+		dev_err(dev,
+			"IRQ registration failed for VFFLR0 irq\n");
+		goto free_mbox0_irq;
+	}
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFME0);
 	/* Register VF ME interrupt handler */
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_VFME0), cptpf_vf_me_intr, 0,
-			  "CPTPF ME0", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_VFME0] = true;
-
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_VFME1), cptpf_vf_me_intr, 0,
-			  "CPTPF ME1", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_VFME1] = true;
-
-	/* Register AF-PF mailbox interrupt handler */
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_AFPF_MBOX), otx2_cptpf_afpf_mbox_intr,
-			  0, "CPTAFPF Mbox", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_AFPF_MBOX] = true;
-
-	/* Register VF-PF mailbox interrupt handler */
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_VFPF_MBOX0), otx2_cptpf_vfpf_mbox_intr,
-			  0, "CPTVFPF Mbox0", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_VFPF_MBOX0] = true;
+	ret = request_irq(vector, cptpf_vf_me_intr, 0, "CPTPF ME0", cptpf);
+	if (ret) {
+		dev_err(dev,
+			"IRQ registration failed for PFVF mbox0 irq\n");
+		goto free_flr0_irq;
+	}
 
-	ret = request_irq(pci_irq_vector(cptpf->pdev,
-			  RVU_PF_INT_VEC_VFPF_MBOX1), otx2_cptpf_vfpf_mbox_intr,
-			  0, "CPTVFPF Mbox1", cptpf);
-	if (ret)
-		goto err;
-	cptpf->irq_registered[RVU_PF_INT_VEC_VFPF_MBOX1] = true;
+	if (num_vfs > 64) {
+		vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFPF_MBOX1);
+		ret = request_irq(vector, otx2_cptpf_vfpf_mbox_intr, 0,
+				  "CPTVFPF Mbox1", cptpf);
+		if (ret) {
+			dev_err(dev,
+				"IRQ registration failed for PFVF mbox1 irq\n");
+			goto free_me0_irq;
+		}
+		vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFFLR1);
+		/* Register VF FLR interrupt handler */
+		ret = request_irq(vector, cptpf_vf_flr_intr, 0, "CPTPF FLR1",
+				  cptpf);
+		if (ret) {
+			dev_err(dev,
+				"IRQ registration failed for VFFLR1 irq\n");
+			goto free_mbox1_irq;
+		}
+		vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFME1);
+		/* Register VF FLR interrupt handler */
+		ret = request_irq(vector, cptpf_vf_me_intr, 0, "CPTPF ME1",
+				  cptpf);
+		if (ret) {
+			dev_err(dev,
+				"IRQ registration failed for VFFLR1 irq\n");
+			goto free_flr1_irq;
+		}
+	}
+	cptpf_enable_vfpf_mbox_intr(cptpf, num_vfs);
+	cptpf_enable_vf_flr_me_intrs(cptpf, num_vfs);
 
 	return 0;
-err:
-	dev_err(&cptpf->pdev->dev, "Failed to register interrupts\n");
-	cptpf_unregister_interrupts(cptpf);
+
+free_flr1_irq:
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFFLR1);
+	free_irq(vector, cptpf);
+free_mbox1_irq:
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFPF_MBOX1);
+	free_irq(vector, cptpf);
+free_me0_irq:
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFME0);
+	free_irq(vector, cptpf);
+free_flr0_irq:
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFFLR0);
+	free_irq(vector, cptpf);
+free_mbox0_irq:
+	vector = pci_irq_vector(pdev, RVU_PF_INT_VEC_VFPF_MBOX0);
+	free_irq(vector, cptpf);
 	return ret;
 }
 
@@ -323,9 +320,8 @@ static void cptpf_flr_wq_destroy(struct otx2_cptpf_dev *pf)
 	kfree(pf->flr_work);
 }
 
-static int cptpf_flr_wq_init(struct otx2_cptpf_dev *cptpf)
+static int cptpf_flr_wq_init(struct otx2_cptpf_dev *cptpf, int num_vfs)
 {
-	int num_vfs = cptpf->max_vfs;
 	int vf;
 
 	cptpf->flr_wq = alloc_ordered_workqueue("cptpf_flr_wq", 0);
@@ -349,6 +345,46 @@ static int cptpf_flr_wq_init(struct otx2_cptpf_dev *cptpf)
 	return -ENOMEM;
 }
 
+static void cptpf_disable_afpf_mbox_intr(struct otx2_cptpf_dev *cptpf)
+{
+	/* Disable AF-PF interrupt */
+	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT_ENA_W1C,
+			 0x1ULL);
+	/* Clear interrupt if any */
+	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT, 0x1ULL);
+}
+
+static int cptpf_register_afpf_mbox_intr(struct otx2_cptpf_dev *cptpf)
+{
+	struct pci_dev *pdev = cptpf->pdev;
+	struct device *dev = &pdev->dev;
+	int ret, irq;
+
+	irq = pci_irq_vector(pdev, RVU_PF_INT_VEC_AFPF_MBOX);
+	/* Register AF-PF mailbox interrupt handler */
+	ret = devm_request_irq(dev, irq, otx2_cptpf_afpf_mbox_intr, 0,
+			       "CPTAFPF Mbox", cptpf);
+	if (ret) {
+		dev_err(dev,
+			"IRQ registration failed for PFAF mbox irq\n");
+		return ret;
+	}
+	/* Clear interrupt if any, to avoid spurious interrupts */
+	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT, 0x1ULL);
+	/* Enable AF-PF interrupt */
+	otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT_ENA_W1S,
+			 0x1ULL);
+
+	ret = otx2_cpt_send_ready_msg(&cptpf->afpf_mbox, cptpf->pdev);
+	if (ret) {
+		dev_warn(dev,
+			 "AF not responding to mailbox, deferring probe\n");
+		cptpf_disable_afpf_mbox_intr(cptpf);
+		return -EPROBE_DEFER;
+	}
+	return 0;
+}
+
 static int cptpf_afpf_mbox_init(struct otx2_cptpf_dev *cptpf)
 {
 	int err;
@@ -371,8 +407,16 @@ static int cptpf_afpf_mbox_init(struct otx2_cptpf_dev *cptpf)
 	return err;
 }
 
+static void cptpf_afpf_mbox_destroy(struct otx2_cptpf_dev *cptpf)
+{
+	destroy_workqueue(cptpf->afpf_mbox_wq);
+	otx2_mbox_destroy(&cptpf->afpf_mbox);
+}
+
 static int cptpf_vfpf_mbox_init(struct otx2_cptpf_dev *cptpf, int numvfs)
 {
+	struct device *dev = &cptpf->pdev->dev;
+	u64 vfpf_mbox_base;
 	int err, i;
 
 	cptpf->vfpf_mbox_wq = alloc_workqueue("cpt_vfpf_mailbox",
@@ -381,11 +425,25 @@ static int cptpf_vfpf_mbox_init(struct otx2_cptpf_dev *cptpf, int numvfs)
 	if (!cptpf->vfpf_mbox_wq)
 		return -ENOMEM;
 
+	/* Map VF-PF mailbox memory */
+	vfpf_mbox_base = readq(cptpf->reg_base + RVU_PF_VF_BAR4_ADDR);
+	if (!vfpf_mbox_base) {
+		dev_err(dev, "VF-PF mailbox address not configured\n");
+		err = -ENOMEM;
+		goto free_wqe;
+	}
+	cptpf->vfpf_mbox_base = devm_ioremap_wc(dev, vfpf_mbox_base,
+						MBOX_SIZE * cptpf->max_vfs);
+	if (!cptpf->vfpf_mbox_base) {
+		dev_err(dev, "Mapping of VF-PF mailbox address failed\n");
+		err = -ENOMEM;
+		goto free_wqe;
+	}
 	err = otx2_mbox_init(&cptpf->vfpf_mbox, cptpf->vfpf_mbox_base,
 			     cptpf->pdev, cptpf->reg_base, MBOX_DIR_PFVF,
 			     numvfs);
 	if (err)
-		goto error;
+		goto free_wqe;
 
 	for (i = 0; i < numvfs; i++) {
 		cptpf->vf[i].vf_id = i;
@@ -395,38 +453,31 @@ static int cptpf_vfpf_mbox_init(struct otx2_cptpf_dev *cptpf, int numvfs)
 			  otx2_cptpf_vfpf_mbox_handler);
 	}
 	return 0;
-error:
-	flush_workqueue(cptpf->vfpf_mbox_wq);
+
+free_wqe:
 	destroy_workqueue(cptpf->vfpf_mbox_wq);
 	return err;
 }
 
-static void cptpf_afpf_mbox_destroy(struct otx2_cptpf_dev *cptpf)
-{
-	flush_workqueue(cptpf->afpf_mbox_wq);
-	destroy_workqueue(cptpf->afpf_mbox_wq);
-	otx2_mbox_destroy(&cptpf->afpf_mbox);
-}
-
 static void cptpf_vfpf_mbox_destroy(struct otx2_cptpf_dev *cptpf)
 {
-	flush_workqueue(cptpf->vfpf_mbox_wq);
 	destroy_workqueue(cptpf->vfpf_mbox_wq);
 	otx2_mbox_destroy(&cptpf->vfpf_mbox);
 }
 
-static int cptx_device_reset(struct otx2_cptpf_dev *cptpf)
+static int cptx_device_reset(struct otx2_cptpf_dev *cptpf, int blkaddr)
 {
 	int timeout = 10, ret;
-	u64 reg;
+	u64 reg = 0;
 
-	ret = otx2_cpt_write_af_reg(cptpf->pdev, CPT_AF_BLK_RST, 0x1);
+	ret = otx2_cpt_write_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+				    CPT_AF_BLK_RST, 0x1, blkaddr);
 	if (ret)
 		return ret;
 
 	do {
-		ret = otx2_cpt_read_af_reg(cptpf->pdev, CPT_AF_BLK_RST,
-					   &reg);
+		ret = otx2_cpt_read_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+					   CPT_AF_BLK_RST, &reg, blkaddr);
 		if (ret)
 			return ret;
 
@@ -445,26 +496,22 @@ static int cptpf_device_reset(struct otx2_cptpf_dev *cptpf)
 {
 	int ret = 0;
 
-	if (cptpf->cpt1_implemented) {
-		cptpf->blkaddr = BLKADDR_CPT1;
-		ret = cptx_device_reset(cptpf);
+	if (cptpf->has_cpt1) {
+		ret = cptx_device_reset(cptpf, BLKADDR_CPT1);
 		if (ret)
 			return ret;
 	}
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = cptx_device_reset(cptpf);
-
-	return ret;
+	return cptx_device_reset(cptpf, BLKADDR_CPT0);
 }
 
-static void cpt_check_block_implemented(struct otx2_cptpf_dev *cptpf)
+static void cptpf_check_block_implemented(struct otx2_cptpf_dev *cptpf)
 {
 	u64 cfg;
 
 	cfg = otx2_cpt_read64(cptpf->reg_base, BLKADDR_RVUM, 0,
 			      RVU_PF_BLOCK_ADDRX_DISC(BLKADDR_CPT1));
 	if (cfg & BIT_ULL(11))
-		cptpf->cpt1_implemented = true;
+		cptpf->has_cpt1 = true;
 }
 
 static int cptpf_device_init(struct otx2_cptpf_dev *cptpf)
@@ -473,15 +520,16 @@ static int cptpf_device_init(struct otx2_cptpf_dev *cptpf)
 	int ret = 0;
 
 	/* check if 'implemented' bit is set for block BLKADDR_CPT1 */
-	cpt_check_block_implemented(cptpf);
+	cptpf_check_block_implemented(cptpf);
 	/* Reset the CPT PF device */
 	ret = cptpf_device_reset(cptpf);
 	if (ret)
 		return ret;
 
 	/* Get number of SE, IE and AE engines */
-	ret = otx2_cpt_read_af_reg(cptpf->pdev, CPT_AF_CONSTANTS1,
-				   &af_cnsts1.u);
+	ret = otx2_cpt_read_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+				   CPT_AF_CONSTANTS1, &af_cnsts1.u,
+				   BLKADDR_CPT0);
 	if (ret)
 		return ret;
 
@@ -580,172 +628,157 @@ static int cpt_is_pf_usable(struct otx2_cptpf_dev *cptpf)
 	return 0;
 }
 
-static int otx2_cptpf_sriov_configure(struct pci_dev *pdev, int numvfs)
+static int cptpf_sriov_disable(struct pci_dev *pdev)
 {
 	struct otx2_cptpf_dev *cptpf = pci_get_drvdata(pdev);
-	int ret = 0;
+	int num_vfs = pci_num_vf(pdev);
 
-	if (numvfs > cptpf->max_vfs)
-		numvfs = cptpf->max_vfs;
+	if (!num_vfs)
+		return 0;
 
-	if (numvfs > 0) {
-		/* Get CPT HW capabilities using LOAD_FVC operation. */
-		ret = otx2_cpt_discover_eng_capabilities(cptpf);
-		if (ret)
-			return ret;
-		ret = otx2_cpt_try_create_default_eng_grps(cptpf->pdev,
-							   &cptpf->eng_grps);
-		if (ret)
-			return ret;
+	pci_disable_sriov(pdev);
+	cptpf_unregister_vfpf_intr(cptpf, num_vfs);
+	cptpf_flr_wq_destroy(cptpf);
+	cptpf_vfpf_mbox_destroy(cptpf);
+	module_put(THIS_MODULE);
+	cptpf->enabled_vfs = 0;
 
-		cptpf->enabled_vfs = numvfs;
+	return 0;
+}
 
-		ret = pci_enable_sriov(pdev, numvfs);
-		if (ret)
-			goto reset_numvfs;
+static int cptpf_sriov_enable(struct pci_dev *pdev, int num_vfs)
+{
+	struct otx2_cptpf_dev *cptpf = pci_get_drvdata(pdev);
+	int ret;
 
-		otx2_cpt_set_eng_grps_is_rdonly(&cptpf->eng_grps, true);
-		try_module_get(THIS_MODULE);
-		ret = numvfs;
-	} else {
-		pci_disable_sriov(pdev);
-		otx2_cpt_set_eng_grps_is_rdonly(&cptpf->eng_grps, false);
-		module_put(THIS_MODULE);
-		cptpf->enabled_vfs = 0;
-	}
+	/* Initialize VF<=>PF mailbox */
+	ret = cptpf_vfpf_mbox_init(cptpf, num_vfs);
+	if (ret)
+		return ret;
 
-	dev_notice(&cptpf->pdev->dev, "VFs enabled: %d\n", ret);
-	return ret;
-reset_numvfs:
+	ret = cptpf_flr_wq_init(cptpf, num_vfs);
+	if (ret)
+		goto destroy_mbox;
+	/* Register VF<=>PF mailbox interrupt */
+	ret = cptpf_register_vfpf_intr(cptpf, num_vfs);
+	if (ret)
+		goto destroy_flr;
+
+	/* Get CPT HW capabilities using LOAD_FVC operation. */
+	ret = otx2_cpt_discover_eng_capabilities(cptpf);
+	if (ret)
+		goto disable_intr;
+
+	ret = otx2_cpt_create_eng_grps(cptpf->pdev, &cptpf->eng_grps);
+	if (ret)
+		goto disable_intr;
+
+	cptpf->enabled_vfs = num_vfs;
+	ret = pci_enable_sriov(pdev, num_vfs);
+	if (ret)
+		goto disable_intr;
+
+	dev_notice(&cptpf->pdev->dev, "VFs enabled: %d\n", num_vfs);
+
+	try_module_get(THIS_MODULE);
+	return num_vfs;
+
+disable_intr:
+	cptpf_unregister_vfpf_intr(cptpf, num_vfs);
 	cptpf->enabled_vfs = 0;
+destroy_flr:
+	cptpf_flr_wq_destroy(cptpf);
+destroy_mbox:
+	cptpf_vfpf_mbox_destroy(cptpf);
 	return ret;
 }
 
+static int otx2_cptpf_sriov_configure(struct pci_dev *pdev, int num_vfs)
+{
+	if (num_vfs > 0) {
+		return cptpf_sriov_enable(pdev, num_vfs);
+	} else {
+		return cptpf_sriov_disable(pdev);
+	}
+}
+
 static int otx2_cptpf_probe(struct pci_dev *pdev,
 			    const struct pci_device_id *ent)
 {
 	struct device *dev = &pdev->dev;
+	resource_size_t offset, size;
 	struct otx2_cptpf_dev *cptpf;
-	u64 vfpf_mbox_base;
 	int err;
 
-	cptpf = kzalloc(sizeof(*cptpf), GFP_KERNEL);
+	cptpf = devm_kzalloc(dev, sizeof(*cptpf), GFP_KERNEL);
 	if (!cptpf)
 		return -ENOMEM;
 
-	pci_set_drvdata(pdev, cptpf);
-	cptpf->pdev = pdev;
-	cptpf->crypto_eng_grp = OTX2_CPT_INVALID_CRYPTO_ENG_GRP;
-	cptpf->max_vfs = pci_sriov_get_totalvfs(pdev);
-
-	err = pci_enable_device(pdev);
+	err = pcim_enable_device(pdev);
 	if (err) {
 		dev_err(dev, "Failed to enable PCI device\n");
 		goto clear_drvdata;
 	}
 
-	pci_set_master(pdev);
-
-	err = pci_request_regions(pdev, OTX2_CPT_DRV_NAME);
-	if (err) {
-		dev_err(dev, "PCI request regions failed 0x%x\n", err);
-		goto disable_device;
-	}
-
-	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	err = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(48));
 	if (err) {
 		dev_err(dev, "Unable to get usable DMA configuration\n");
-		goto release_regions;
+		goto clear_drvdata;
 	}
-
-	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	/* Map PF's configuration registers */
+	err = pcim_iomap_regions_request_all(pdev, 1 << PCI_PF_REG_BAR_NUM,
+					     OTX2_CPT_DRV_NAME);
 	if (err) {
-		dev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");
-		goto release_regions;
+		dev_err(dev, "Couldn't get PCI resources 0x%x\n", err);
+		goto clear_drvdata;
 	}
+	pci_set_master(pdev);
+	pci_set_drvdata(pdev, cptpf);
+	cptpf->pdev = pdev;
 
-	/* Map PF's configuration registers */
-	cptpf->reg_base = pci_iomap(pdev, PCI_PF_REG_BAR_NUM, 0);
-	if (!cptpf->reg_base) {
-		dev_err(&pdev->dev, "Unable to map BAR2\n");
-		err = -ENODEV;
-		goto release_regions;
-	}
+	cptpf->reg_base = pcim_iomap_table(pdev)[PCI_PF_REG_BAR_NUM];
 
 	/* Check if AF driver is up, otherwise defer probe */
 	err = cpt_is_pf_usable(cptpf);
 	if (err)
-		goto pci_unmap_region;
+		goto clear_drvdata;
 
+	offset = pci_resource_start(pdev, PCI_MBOX_BAR_NUM);
+	size = pci_resource_len(pdev, PCI_MBOX_BAR_NUM);
 	/* Map AF-PF mailbox memory */
-	cptpf->afpf_mbox_base = ioremap_wc(pci_resource_start(cptpf->pdev,
-					   PCI_MBOX_BAR_NUM),
-					   pci_resource_len(cptpf->pdev,
-					   PCI_MBOX_BAR_NUM));
+	cptpf->afpf_mbox_base = devm_ioremap_wc(dev, offset, size);
 	if (!cptpf->afpf_mbox_base) {
 		dev_err(&pdev->dev, "Unable to map BAR4\n");
 		err = -ENODEV;
-		goto pci_unmap_region;
-	}
-
-	/* Map VF-PF mailbox memory */
-	vfpf_mbox_base = readq((void __iomem *) ((u64)cptpf->reg_base +
-			       RVU_PF_VF_BAR4_ADDR));
-	if (!vfpf_mbox_base) {
-		dev_err(&pdev->dev, "VF-PF mailbox address not configured\n");
-		err = -ENOMEM;
-		goto iounmap_afpf;
+		goto clear_drvdata;
 	}
-	cptpf->vfpf_mbox_base = ioremap_wc(vfpf_mbox_base,
-					   MBOX_SIZE * cptpf->max_vfs);
-	if (!cptpf->vfpf_mbox_base) {
-		dev_err(&pdev->dev,
-			"Mapping of VF-PF mailbox address failed\n");
-		err = -ENOMEM;
-		goto iounmap_afpf;
+	err = pci_alloc_irq_vectors(pdev, RVU_PF_INT_VEC_CNT,
+				    RVU_PF_INT_VEC_CNT, PCI_IRQ_MSIX);
+	if (err < 0) {
+		dev_err(dev, "Request for %d msix vectors failed\n",
+			RVU_PF_INT_VEC_CNT);
+		goto clear_drvdata;
 	}
-
 	/* Initialize AF-PF mailbox */
 	err = cptpf_afpf_mbox_init(cptpf);
 	if (err)
-		goto iounmap_vfpf;
-
-	/* Initialize VF-PF mailbox */
-	err = cptpf_vfpf_mbox_init(cptpf, cptpf->max_vfs);
+		goto clear_drvdata;
+	/* Register mailbox interrupt */
+	err = cptpf_register_afpf_mbox_intr(cptpf);
 	if (err)
 		goto destroy_afpf_mbox;
 
-	err = cptpf_flr_wq_init(cptpf);
-	if (err)
-		goto destroy_vfpf_mbox;
-
-	/* Register interrupts */
-	err = cptpf_register_interrupts(cptpf);
-	if (err)
-		goto destroy_flr;
-
-	/* Enable VF FLR interrupts */
-	cptpf_enable_vf_flr_me_intrs(cptpf);
-
-	/* Enable AF-PF mailbox interrupts */
-	cptpf_enable_afpf_mbox_intrs(cptpf);
-
-	/* Enable VF-PF mailbox interrupts */
-	cptpf_enable_vfpf_mbox_intrs(cptpf, cptpf->max_vfs);
+	cptpf->max_vfs = pci_sriov_get_totalvfs(pdev);
 
 	/* Initialize CPT PF device */
 	err = cptpf_device_init(cptpf);
 	if (err)
-		goto unregister_interrupts;
-
-	err = otx2_cpt_send_ready_msg(cptpf->pdev);
-	if (err)
-		goto unregister_interrupts;
+		goto unregister_intr;
 
 	/* Initialize engine groups */
 	err = otx2_cpt_init_eng_grps(pdev, &cptpf->eng_grps);
 	if (err)
-		goto unregister_interrupts;
+		goto unregister_intr;
 
 	err = sysfs_create_group(&dev->kobj, &cptpf_sysfs_group);
 	if (err)
@@ -754,31 +787,12 @@ static int otx2_cptpf_probe(struct pci_dev *pdev,
 
 cleanup_eng_grps:
 	otx2_cpt_cleanup_eng_grps(pdev, &cptpf->eng_grps);
-unregister_interrupts:
-	cptpf_disable_vfpf_mbox_intrs(cptpf);
-	cptpf_disable_afpf_mbox_intrs(cptpf);
-	cptpf_disable_vf_flr_me_intrs(cptpf);
-	cptpf_unregister_interrupts(cptpf);
-destroy_flr:
-	cptpf_flr_wq_destroy(cptpf);
-destroy_vfpf_mbox:
-	cptpf_vfpf_mbox_destroy(cptpf);
+unregister_intr:
+	cptpf_disable_afpf_mbox_intr(cptpf);
 destroy_afpf_mbox:
 	cptpf_afpf_mbox_destroy(cptpf);
-iounmap_vfpf:
-	iounmap(cptpf->vfpf_mbox_base);
-iounmap_afpf:
-	iounmap(cptpf->afpf_mbox_base);
-pci_unmap_region:
-	pci_iounmap(pdev, cptpf->reg_base);
-release_regions:
-	pci_release_regions(pdev);
-disable_device:
-	pci_disable_device(pdev);
 clear_drvdata:
 	pci_set_drvdata(pdev, NULL);
-	kfree(cptpf);
-
 	return err;
 }
 
@@ -789,38 +803,16 @@ static void otx2_cptpf_remove(struct pci_dev *pdev)
 	if (!cptpf)
 		return;
 
-	/* Disable SRIOV */
-	pci_disable_sriov(pdev);
-	/*
-	 * Delete sysfs entry created for kernel VF limits
-	 * and sso_pf_func_ovrd bit.
-	 */
+	cptpf_sriov_disable(pdev);
+	/* Delete sysfs entry created for kernel VF limits */
 	sysfs_remove_group(&pdev->dev.kobj, &cptpf_sysfs_group);
 	/* Cleanup engine groups */
 	otx2_cpt_cleanup_eng_grps(pdev, &cptpf->eng_grps);
-	/* Disable VF-PF interrupts */
-	cptpf_disable_vfpf_mbox_intrs(cptpf);
 	/* Disable AF-PF mailbox interrupt */
-	cptpf_disable_afpf_mbox_intrs(cptpf);
-	/* Disable VF FLR interrupts */
-	cptpf_disable_vf_flr_me_intrs(cptpf);
-	/* Unregister CPT interrupts */
-	cptpf_unregister_interrupts(cptpf);
-	/* Destroy FLR work queue */
-	cptpf_flr_wq_destroy(cptpf);
+	cptpf_disable_afpf_mbox_intr(cptpf);
 	/* Destroy AF-PF mbox */
 	cptpf_afpf_mbox_destroy(cptpf);
-	/* Destroy VF-PF mbox */
-	cptpf_vfpf_mbox_destroy(cptpf);
-	/* Unmap VF-PF mailbox memory */
-	iounmap(cptpf->vfpf_mbox_base);
-	/* Unmap AF-PF mailbox memory */
-	iounmap(cptpf->afpf_mbox_base);
-	pci_iounmap(pdev, cptpf->reg_base);
-	pci_release_regions(pdev);
-	pci_disable_device(pdev);
 	pci_set_drvdata(pdev, NULL);
-	kfree(cptpf);
 }
 
 /* Supported devices */
@@ -839,8 +831,7 @@ static struct pci_driver otx2_cpt_pci_driver = {
 
 module_pci_driver(otx2_cpt_pci_driver);
 
-MODULE_AUTHOR("Marvell International Ltd.");
-MODULE_DESCRIPTION("Marvell OcteonTX2 CPT Physical Function Driver");
+MODULE_AUTHOR("Marvell");
+MODULE_DESCRIPTION(OTX2_CPT_DRV_STRING);
 MODULE_LICENSE("GPL v2");
-MODULE_VERSION(OTX2_CPT_DRV_VERSION);
 MODULE_DEVICE_TABLE(pci, otx2_cpt_id_table);
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptpf_mbox.c b/drivers/crypto/marvell/octeontx2/otx2_cptpf_mbox.c
index 06fe731dd4d7..2cce58d1a107 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptpf_mbox.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptpf_mbox.c
@@ -1,209 +1,18 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
 
-#include "otx2_cpt_mbox_common.h"
+#include "otx2_cpt_common.h"
+#include "otx2_cptpf.h"
 #include "rvu_reg.h"
 
 /* Fastpath ipsec opcode with inplace processing */
 #define CPT_INLINE_RX_OPCODE (0x26 | (1 << 6))
 /*
  * CPT PF driver version, It will be incremented by 1 for every feature
- * addition in CPT PF driver.
+ * addition in CPT mailbox messages.
  */
 #define OTX2_CPT_PF_DRV_VERSION 0x3
 
-static void dump_mbox_msg(struct mbox_msghdr *msg, int size)
-{
-	u16 pf_id, vf_id;
-
-	pf_id = (msg->pcifunc >> RVU_PFVF_PF_SHIFT) & RVU_PFVF_PF_MASK;
-	vf_id = (msg->pcifunc >> RVU_PFVF_FUNC_SHIFT) & RVU_PFVF_FUNC_MASK;
-
-	pr_debug("MBOX opcode %s received from (PF%d/VF%d), size %d, rc %d",
-		 otx2_cpt_get_mbox_opcode_str(msg->id), pf_id, vf_id, size,
-		 msg->rc);
-	print_hex_dump_debug("", DUMP_PREFIX_OFFSET, 16, 2, msg, size, false);
-}
-
-static int get_eng_grp(struct otx2_cptpf_dev *cptpf, u8 eng_type)
-{
-	int eng_grp_num = OTX2_CPT_INVALID_CRYPTO_ENG_GRP;
-	struct otx2_cpt_eng_grp_info *grp;
-	int i;
-
-
-	mutex_lock(&cptpf->eng_grps.lock);
-
-	switch (eng_type) {
-	case OTX2_CPT_SE_TYPES:
-		/*
-		 * Find engine group for kernel crypto functionality, select
-		 * first engine group which is configured and has only
-		 * SE engines attached
-		 */
-		for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-			grp = &cptpf->eng_grps.grp[i];
-			if (!grp->is_enabled)
-				continue;
-
-			if (otx2_cpt_eng_grp_has_eng_type(grp,
-							  OTX2_CPT_SE_TYPES) &&
-			    !otx2_cpt_eng_grp_has_eng_type(grp,
-							   OTX2_CPT_IE_TYPES)) {
-				eng_grp_num = i;
-				break;
-			}
-		}
-		break;
-
-	case OTX2_CPT_AE_TYPES:
-	case OTX2_CPT_IE_TYPES:
-		for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-			grp = &cptpf->eng_grps.grp[i];
-			if (!grp->is_enabled)
-				continue;
-
-			if (otx2_cpt_eng_grp_has_eng_type(grp, eng_type)) {
-				eng_grp_num = i;
-				break;
-			}
-		}
-		break;
-
-	default:
-		dev_err(&cptpf->pdev->dev, "Invalid engine type %d\n",
-			eng_type);
-	}
-	mutex_unlock(&cptpf->eng_grps.lock);
-
-	return eng_grp_num;
-}
-
-static int cptlf_set_pri(struct pci_dev *pdev, struct otx2_cptlf_info *lf,
-			 int pri)
-{
-	union otx2_cptx_af_lf_ctrl lf_ctrl;
-	int ret;
-
-	ret = otx2_cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
-	if (ret)
-		return ret;
-
-	lf_ctrl.s.pri = pri ? 1 : 0;
-
-	ret = otx2_cpt_write_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), lf_ctrl.u);
-
-	return ret;
-}
-
-static int cptlf_set_eng_grps_mask(struct pci_dev *pdev,
-				   struct otx2_cptlf_info *lf,
-				   u8 eng_grp_mask)
-{
-	union otx2_cptx_af_lf_ctrl lf_ctrl;
-	int ret;
-
-	ret = otx2_cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
-	if (ret)
-		return ret;
-
-	lf_ctrl.s.grp = eng_grp_mask;
-
-	ret = otx2_cpt_write_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), lf_ctrl.u);
-
-	return ret;
-}
-
-static int cptlf_set_grp_and_pri(struct pci_dev *pdev,
-				 struct otx2_cptlfs_info *lfs,
-				 u8 eng_grp_mask, int pri)
-{
-	int slot, ret;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++) {
-		ret = cptlf_set_pri(pdev, &lfs->lf[slot], pri);
-		if (ret)
-			return ret;
-
-		ret = cptlf_set_eng_grps_mask(pdev, &lfs->lf[slot],
-					      eng_grp_mask);
-		if (ret)
-			return ret;
-	}
-	return 0;
-}
-
-void otx2_cptpf_lf_cleanup(struct otx2_cptlfs_info *lfs)
-{
-	otx2_cptlf_disable_iqueues(lfs);
-	otx2_cpt_free_instruction_queues(lfs);
-	otx2_cpt_detach_rsrcs_msg(lfs->pdev);
-	lfs->lfs_num = 0;
-}
-
-int otx2_cptpf_lf_init(struct otx2_cptpf_dev *cptpf, u8 eng_grp_mask, int pri,
-		       int lfs_num)
-{
-	struct otx2_cptlfs_info *lfs = &cptpf->lfs;
-	struct pci_dev *pdev = cptpf->pdev;
-	int ret, slot;
-
-	lfs->reg_base = cptpf->reg_base;
-	lfs->lfs_num = lfs_num;
-	lfs->pdev = pdev;
-	lfs->blkaddr = cptpf->blkaddr;
-
-	for (slot = 0; slot < lfs->lfs_num; slot++) {
-		lfs->lf[slot].lfs = lfs;
-		lfs->lf[slot].slot = slot;
-		lfs->lf[slot].lmtline = lfs->reg_base +
-			OTX2_CPT_RVU_FUNC_ADDR_S(BLKADDR_LMT, slot,
-			OTX2_CPT_LMT_LF_LMTLINEX(0));
-		lfs->lf[slot].ioreg = lfs->reg_base +
-			OTX2_CPT_RVU_FUNC_ADDR_S(lfs->blkaddr, slot,
-			OTX2_CPT_LF_NQX(0));
-	}
-	ret = otx2_cpt_attach_rscrs_msg(pdev);
-	if (ret)
-		goto clear_lfs_num;
-
-	ret = otx2_cpt_alloc_instruction_queues(lfs);
-	if (ret) {
-		dev_err(&pdev->dev,
-			"Allocating instruction queues failed\n");
-		goto detach_rsrcs;
-	}
-	otx2_cptlf_disable_iqueues(lfs);
-
-	otx2_cptlf_set_iqueues_base_addr(lfs);
-
-	otx2_cptlf_set_iqueues_size(lfs);
-
-	otx2_cptlf_enable_iqueues(lfs);
-
-	ret = cptlf_set_grp_and_pri(pdev, lfs, eng_grp_mask, pri);
-	if (ret)
-		goto free_iqueue;
-
-	return 0;
-
-free_iqueue:
-	otx2_cptlf_disable_iqueues(lfs);
-	otx2_cpt_free_instruction_queues(lfs);
-detach_rsrcs:
-	otx2_cpt_detach_rsrcs_msg(pdev);
-clear_lfs_num:
-	lfs->lfs_num = 0;
-	return ret;
-}
-
 static int forward_to_af(struct otx2_cptpf_dev *cptpf,
 			 struct otx2_cptvf_info *vf,
 			 struct mbox_msghdr *req, int size)
@@ -234,43 +43,66 @@ static int forward_to_af(struct otx2_cptpf_dev *cptpf,
 	return 0;
 }
 
-static int reply_ready_msg(struct otx2_cptpf_dev *cptpf,
-			   struct otx2_cptvf_info *vf,
-			   struct mbox_msghdr *req)
+static int handle_msg_get_caps(struct otx2_cptpf_dev *cptpf,
+			       struct otx2_cptvf_info *vf,
+			       struct mbox_msghdr *req)
+{
+	struct otx2_cpt_caps_rsp *rsp;
+
+	rsp = (struct otx2_cpt_caps_rsp *)
+	      otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id,
+				  sizeof(*rsp));
+	if (!rsp)
+		return -ENOMEM;
+
+	rsp->hdr.id = MBOX_MSG_GET_CAPS;
+	rsp->hdr.sig = OTX2_MBOX_RSP_SIG;
+	rsp->hdr.pcifunc = req->pcifunc;
+	rsp->cpt_pf_drv_version = OTX2_CPT_PF_DRV_VERSION;
+	rsp->cpt_revision = cptpf->pdev->revision;
+	memcpy(&rsp->eng_caps, &cptpf->eng_caps, sizeof(rsp->eng_caps));
+
+	return 0;
+}
+
+static int handle_msg_get_eng_grp_num(struct otx2_cptpf_dev *cptpf,
+				      struct otx2_cptvf_info *vf,
+				      struct mbox_msghdr *req)
 {
-	struct ready_msg_rsp *rsp;
+	struct otx2_cpt_egrp_num_msg *grp_req;
+	struct otx2_cpt_egrp_num_rsp *rsp;
 
-	rsp = (struct ready_msg_rsp *)
+	grp_req = (struct otx2_cpt_egrp_num_msg *)req;
+	rsp = (struct otx2_cpt_egrp_num_rsp *)
 	       otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id, sizeof(*rsp));
 	if (!rsp)
 		return -ENOMEM;
 
-	rsp->hdr.id = MBOX_MSG_READY;
+	rsp->hdr.id = MBOX_MSG_GET_ENG_GRP_NUM;
 	rsp->hdr.sig = OTX2_MBOX_RSP_SIG;
 	rsp->hdr.pcifunc = req->pcifunc;
+	rsp->eng_type = grp_req->eng_type;
+	rsp->eng_grp_num = otx2_cpt_get_eng_grp(&cptpf->eng_grps,
+						grp_req->eng_type);
 
 	return 0;
 }
 
-static int reply_eng_grp_num_msg(struct otx2_cptpf_dev *cptpf,
+static int handle_msg_kvf_limits(struct otx2_cptpf_dev *cptpf,
 				 struct otx2_cptvf_info *vf,
 				 struct mbox_msghdr *req)
 {
-	struct otx2_cpt_eng_grp_num_msg *grp_req =
-			(struct otx2_cpt_eng_grp_num_msg *)req;
-	struct otx2_cpt_eng_grp_num_rsp *rsp;
+	struct otx2_cpt_kvf_limits_rsp *rsp;
 
-	rsp = (struct otx2_cpt_eng_grp_num_rsp *)
-			      otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id,
-						  sizeof(*rsp));
+	rsp = (struct otx2_cpt_kvf_limits_rsp *)
+	      otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id, sizeof(*rsp));
 	if (!rsp)
 		return -ENOMEM;
 
-	rsp->hdr.id = MBOX_MSG_GET_ENG_GRP_NUM;
+	rsp->hdr.id = MBOX_MSG_GET_KVF_LIMITS;
 	rsp->hdr.sig = OTX2_MBOX_RSP_SIG;
 	rsp->hdr.pcifunc = req->pcifunc;
-	rsp->eng_type = grp_req->eng_type;
-	rsp->eng_grp_num = get_eng_grp(cptpf, grp_req->eng_type);
+	rsp->kvf_limits = cptpf->kvf_limits;
 
 	return 0;
 }
@@ -284,9 +116,9 @@ static int rx_inline_ipsec_lf_cfg(struct otx2_cptpf_dev *cptpf, u8 egrp,
 	int ret;
 
 	nix_req = (struct nix_inline_ipsec_cfg *)
-			otx2_mbox_alloc_msg_rsp(&cptpf->afpf_mbox, 0,
-						sizeof(*nix_req),
-						sizeof(struct msg_rsp));
+		   otx2_mbox_alloc_msg_rsp(&cptpf->afpf_mbox, 0,
+					   sizeof(*nix_req),
+					   sizeof(struct msg_rsp));
 	if (nix_req == NULL) {
 		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
 		return -EFAULT;
@@ -300,14 +132,13 @@ static int rx_inline_ipsec_lf_cfg(struct otx2_cptpf_dev *cptpf, u8 egrp,
 	nix_req->gen_cfg.opcode = CPT_INLINE_RX_OPCODE;
 	nix_req->inst_qsel.cpt_pf_func = OTX2_CPT_RVU_PFFUNC(cptpf->pf_id, 0);
 	nix_req->inst_qsel.cpt_slot = 0;
-	ret = otx2_cpt_send_mbox_msg(pdev);
+	ret = otx2_cpt_send_mbox_msg(&cptpf->afpf_mbox, pdev);
 	if (ret)
 		return ret;
 
 	req = (struct cpt_inline_ipsec_cfg_msg *)
-			otx2_mbox_alloc_msg_rsp(&cptpf->afpf_mbox, 0,
-						sizeof(*req),
-						sizeof(struct msg_rsp));
+	      otx2_mbox_alloc_msg_rsp(&cptpf->afpf_mbox, 0,
+				      sizeof(*req), sizeof(struct msg_rsp));
 	if (req == NULL) {
 		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
 		return -EFAULT;
@@ -321,19 +152,19 @@ static int rx_inline_ipsec_lf_cfg(struct otx2_cptpf_dev *cptpf, u8 egrp,
 	req->sso_pf_func_ovrd = cptpf->sso_pf_func_ovrd;
 	req->sso_pf_func = sso_pf_func;
 	req->enable = enable;
-	ret = otx2_cpt_send_mbox_msg(pdev);
 
-	return ret;
+	return otx2_cpt_send_mbox_msg(&cptpf->afpf_mbox, pdev);
 }
 
-static int rx_inline_ipsec_lf_enable(struct otx2_cptpf_dev *cptpf,
-				     struct mbox_msghdr *req)
+static int handle_msg_rx_inline_ipsec_lf_cfg(struct otx2_cptpf_dev *cptpf,
+					     struct mbox_msghdr *req)
 {
-	struct otx2_cpt_rx_inline_lf_cfg *cfg_req =
-					(struct otx2_cpt_rx_inline_lf_cfg *)req;
+	struct otx2_cptlfs_info *lfs = &cptpf->lfs;
+	struct otx2_cpt_rx_inline_lf_cfg *cfg_req;
 	u8 egrp;
 	int ret;
 
+	cfg_req = (struct otx2_cpt_rx_inline_lf_cfg *)req;
 	if (cptpf->lfs.lfs_num) {
 		dev_err(&cptpf->pdev->dev,
 			"LF is already configured for RX inline ipsec.\n");
@@ -343,14 +174,17 @@ static int rx_inline_ipsec_lf_enable(struct otx2_cptpf_dev *cptpf,
 	 * Allow LFs to execute requests destined to only grp IE_TYPES and
 	 * set queue priority of each LF to high
 	 */
-	egrp = get_eng_grp(cptpf, OTX2_CPT_IE_TYPES);
+	egrp = otx2_cpt_get_eng_grp(&cptpf->eng_grps, OTX2_CPT_IE_TYPES);
 	if (egrp == OTX2_CPT_INVALID_CRYPTO_ENG_GRP) {
 		dev_err(&cptpf->pdev->dev,
 			"Engine group for inline ipsec is not available\n");
 		return -ENOENT;
 	}
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = otx2_cptpf_lf_init(cptpf, 1 << egrp, OTX2_CPT_QUEUE_HI_PRIO, 1);
+	lfs->pdev = cptpf->pdev;
+	lfs->reg_base = cptpf->reg_base;
+	lfs->mbox = &cptpf->afpf_mbox;
+	lfs->blkaddr = BLKADDR_CPT0;
+	ret = otx2_cptlf_init(lfs, 1 << egrp, OTX2_CPT_QUEUE_HI_PRIO, 1);
 	if (ret)
 		return ret;
 
@@ -361,65 +195,10 @@ static int rx_inline_ipsec_lf_enable(struct otx2_cptpf_dev *cptpf,
 	return 0;
 
 lf_cleanup:
-	otx2_cptpf_lf_cleanup(&cptpf->lfs);
+	otx2_cptlf_shutdown(&cptpf->lfs);
 	return ret;
 }
 
-static int reply_caps_msg(struct otx2_cptpf_dev *cptpf,
-			  struct otx2_cptvf_info *vf,
-			  struct mbox_msghdr *req)
-{
-	struct otx2_cpt_caps_rsp *rsp;
-
-	rsp = (struct otx2_cpt_caps_rsp *)
-			      otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id,
-						  sizeof(*rsp));
-	if (!rsp)
-		return -ENOMEM;
-
-	rsp->hdr.id = MBOX_MSG_GET_CAPS;
-	rsp->hdr.sig = OTX2_MBOX_RSP_SIG;
-	rsp->hdr.pcifunc = req->pcifunc;
-	rsp->cpt_pf_drv_version = OTX2_CPT_PF_DRV_VERSION;
-	rsp->cpt_revision = cptpf->pdev->revision;
-	memcpy(&rsp->eng_caps, &cptpf->eng_caps, sizeof(rsp->eng_caps));
-
-	return 0;
-}
-
-static int reply_kcrypto_limits_msg(struct otx2_cptpf_dev *cptpf,
-				    struct otx2_cptvf_info *vf,
-				    struct mbox_msghdr *req)
-{
-	struct otx2_cpt_kcrypto_limits_rsp *rsp;
-
-	rsp = (struct otx2_cpt_kcrypto_limits_rsp *)
-			      otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id,
-						  sizeof(*rsp));
-	if (!rsp)
-		return -ENOMEM;
-
-	rsp->hdr.id = MBOX_MSG_GET_KCRYPTO_LIMITS;
-	rsp->hdr.sig = OTX2_MBOX_RSP_SIG;
-	rsp->hdr.pcifunc = req->pcifunc;
-	rsp->kcrypto_limits = cptpf->kvf_limits;
-
-	return 0;
-}
-
-static int check_cpt_lf_alloc_req(struct otx2_cptpf_dev *cptpf,
-				  struct otx2_cptvf_info *vf,
-				  struct mbox_msghdr *req, int size)
-{
-	struct cpt_lf_alloc_req_msg *alloc_req =
-					(struct cpt_lf_alloc_req_msg *)req;
-
-	if (alloc_req->eng_grpmsk == 0x0)
-		alloc_req->eng_grpmsk = OTX2_CPT_ALL_ENG_GRPS_MASK;
-
-	return forward_to_af(cptpf, vf, req, size);
-}
-
 static int cptpf_handle_vf_req(struct otx2_cptpf_dev *cptpf,
 			       struct otx2_cptvf_info *vf,
 			       struct mbox_msghdr *req, int size)
@@ -428,57 +207,32 @@ static int cptpf_handle_vf_req(struct otx2_cptpf_dev *cptpf,
 
 	/* Check if msg is valid, if not reply with an invalid msg */
 	if (req->sig != OTX2_MBOX_REQ_SIG)
-		return otx2_reply_invalid_msg(&cptpf->vfpf_mbox, vf->vf_id,
-					      req->pcifunc, req->id);
-	switch (req->id) {
-	case MBOX_MSG_READY:
-		err = reply_ready_msg(cptpf, vf, req);
-		break;
+		goto inval_msg;
 
+	switch (req->id) {
 	case MBOX_MSG_GET_ENG_GRP_NUM:
-		err = reply_eng_grp_num_msg(cptpf, vf, req);
-		break;
-
-	case MBOX_MSG_RX_INLINE_IPSEC_LF_CFG:
-		err = rx_inline_ipsec_lf_enable(cptpf, req);
+		err = handle_msg_get_eng_grp_num(cptpf, vf, req);
 		break;
-
 	case MBOX_MSG_GET_CAPS:
-		err = reply_caps_msg(cptpf, vf, req);
+		err = handle_msg_get_caps(cptpf, vf, req);
 		break;
-
-	case MBOX_MSG_GET_KCRYPTO_LIMITS:
-		err = reply_kcrypto_limits_msg(cptpf, vf, req);
+	case MBOX_MSG_GET_KVF_LIMITS:
+		err = handle_msg_kvf_limits(cptpf, vf, req);
 		break;
-
-	case MBOX_MSG_CPT_LF_ALLOC:
-		err = check_cpt_lf_alloc_req(cptpf, vf, req, size);
+	case MBOX_MSG_RX_INLINE_IPSEC_LF_CFG:
+		err = handle_msg_rx_inline_ipsec_lf_cfg(cptpf, req);
 		break;
 
 	default:
 		err = forward_to_af(cptpf, vf, req, size);
 		break;
 	}
-
 	return err;
-}
-
-irqreturn_t otx2_cptpf_afpf_mbox_intr(int __always_unused irq, void *arg)
-{
-	struct otx2_cptpf_dev *cptpf = arg;
-	u64 intr;
 
-	/* Read the interrupt bits */
-	intr = otx2_cpt_read64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT);
-
-	if (intr & 0x1ULL) {
-		/* Schedule work queue function to process the MBOX request */
-		queue_work(cptpf->afpf_mbox_wq, &cptpf->afpf_mbox_work);
-		/* Clear and ack the interrupt */
-		otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT,
-			    0x1ULL);
-	}
-	return IRQ_HANDLED;
+inval_msg:
+	otx2_reply_invalid_msg(&cptpf->vfpf_mbox, vf->vf_id, 0, req->id);
+	otx2_mbox_msg_send(&cptpf->vfpf_mbox, vf->vf_id);
+	return err;
 }
 
 irqreturn_t otx2_cptpf_vfpf_mbox_intr(int __always_unused irq, void *arg)
@@ -512,171 +266,192 @@ irqreturn_t otx2_cptpf_vfpf_mbox_intr(int __always_unused irq, void *arg)
 	return IRQ_HANDLED;
 }
 
-void otx2_cptpf_afpf_mbox_handler(struct work_struct *work)
+void otx2_cptpf_vfpf_mbox_handler(struct work_struct *work)
 {
-	struct cpt_rd_wr_reg_msg *rsp_rd_wr;
-	struct otx2_mbox *afpf_mbox;
-	struct otx2_mbox *vfpf_mbox;
-	struct mbox_hdr *rsp_hdr;
-	struct mbox_msghdr *msg;
-	struct mbox_msghdr *fwd;
 	struct otx2_cptpf_dev *cptpf;
-	int offset, size;
-	int vf_id, i;
+	struct otx2_cptvf_info *vf;
+	struct otx2_mbox_dev *mdev;
+	struct mbox_hdr *req_hdr;
+	struct mbox_msghdr *msg;
+	struct otx2_mbox *mbox;
+	int offset, i, err;
 
-	/* Read latest mbox data */
+	vf = container_of(work, struct otx2_cptvf_info, vfpf_mbox_work);
+	cptpf = vf->cptpf;
+	mbox = &cptpf->vfpf_mbox;
+	/* sync with mbox memory region */
 	smp_rmb();
+	mdev = &mbox->dev[vf->vf_id];
+	/* Process received mbox messages */
+	req_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);
+	offset = mbox->rx_start + ALIGN(sizeof(*req_hdr), MBOX_MSG_ALIGN);
 
-	cptpf = container_of(work, struct otx2_cptpf_dev, afpf_mbox_work);
-	afpf_mbox = &cptpf->afpf_mbox;
-	vfpf_mbox = &cptpf->vfpf_mbox;
-	rsp_hdr = (struct mbox_hdr *)(afpf_mbox->dev->mbase +
-		   afpf_mbox->rx_start);
-	if (rsp_hdr->num_msgs == 0)
+	for (i = 0; i < req_hdr->num_msgs; i++) {
+		msg = (struct mbox_msghdr *)(mdev->mbase + offset);
+
+		/* Set which VF sent this message based on mbox IRQ */
+		msg->pcifunc = ((u16)cptpf->pf_id << RVU_PFVF_PF_SHIFT) |
+				((vf->vf_id + 1) & RVU_PFVF_FUNC_MASK);
+
+		err = cptpf_handle_vf_req(cptpf, vf, msg,
+					  msg->next_msgoff - offset);
+		/*
+		 * Behave as the AF, drop the msg if there is
+		 * no memory, timeout handling also goes here
+		 */
+		if (err == -ENOMEM || err == -EIO)
+			break;
+		offset = msg->next_msgoff;
+	}
+	/* Send mbox responses to VF */
+	if (mdev->num_msgs)
+		otx2_mbox_msg_send(mbox, vf->vf_id);
+}
+
+irqreturn_t otx2_cptpf_afpf_mbox_intr(int __always_unused irq, void *arg)
+{
+	struct otx2_cptpf_dev *cptpf = arg;
+	u64 intr;
+
+	/* Read the interrupt bits */
+	intr = otx2_cpt_read64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT);
+
+	if (intr & 0x1ULL) {
+		/* Schedule work queue function to process the MBOX request */
+		queue_work(cptpf->afpf_mbox_wq, &cptpf->afpf_mbox_work);
+		/* Clear and ack the interrupt */
+		otx2_cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT,
+				 0x1ULL);
+	}
+	return IRQ_HANDLED;
+}
+
+static void process_afpf_mbox_msg(struct otx2_cptpf_dev *cptpf,
+				  struct mbox_msghdr *msg)
+{
+	struct device *dev = &cptpf->pdev->dev;
+	struct cpt_rd_wr_reg_msg *rsp_rd_wr;
+
+	if (msg->id >= MBOX_MSG_MAX) {
+		dev_err(dev, "MBOX msg with unknown ID %d\n", msg->id);
 		return;
-	offset = ALIGN(sizeof(struct mbox_hdr), MBOX_MSG_ALIGN);
+	}
+	if (msg->sig != OTX2_MBOX_RSP_SIG) {
+		dev_err(dev, "MBOX msg with wrong signature %x, ID %d\n",
+			msg->sig, msg->id);
+		return;
+	}
 
-	for (i = 0; i < rsp_hdr->num_msgs; i++) {
-		msg = (struct mbox_msghdr *)(afpf_mbox->dev->mbase +
-					     afpf_mbox->rx_start + offset);
-		size = msg->next_msgoff - offset;
-
-		if (msg->id >= MBOX_MSG_MAX) {
-			dev_err(&cptpf->pdev->dev,
-				"MBOX msg with unknown ID %d\n", msg->id);
-			goto error;
+	switch (msg->id) {
+	case MBOX_MSG_READY:
+		cptpf->pf_id = (msg->pcifunc >> RVU_PFVF_PF_SHIFT) &
+				RVU_PFVF_PF_MASK;
+		break;
+	case MBOX_MSG_CPT_RD_WR_REGISTER:
+		rsp_rd_wr = (struct cpt_rd_wr_reg_msg *)msg;
+		if (msg->rc) {
+			dev_err(dev, "Reg %llx rd/wr(%d) failed %d\n",
+				rsp_rd_wr->reg_offset, rsp_rd_wr->is_write,
+				msg->rc);
+			return;
 		}
+		if (!rsp_rd_wr->is_write)
+			*rsp_rd_wr->ret_val = rsp_rd_wr->val;
+		break;
+	case MBOX_MSG_ATTACH_RESOURCES:
+		if (!msg->rc)
+			cptpf->lfs.are_lfs_attached = 1;
+		break;
+	case MBOX_MSG_DETACH_RESOURCES:
+		if (!msg->rc)
+			cptpf->lfs.are_lfs_attached = 0;
+		break;
+	case MBOX_MSG_CPT_INLINE_IPSEC_CFG:
+	case MBOX_MSG_NIX_INLINE_IPSEC_CFG:
+		break;
 
-		if (msg->sig != OTX2_MBOX_RSP_SIG) {
-			dev_err(&cptpf->pdev->dev,
-				"MBOX msg with wrong signature %x, ID %d\n",
-				msg->sig, msg->id);
-			goto error;
-		}
+	default:
+		dev_err(dev,
+			"Unsupported msg %d received.\n", msg->id);
+		break;
+	}
+}
 
-		offset = msg->next_msgoff;
-		vf_id = (msg->pcifunc >> RVU_PFVF_FUNC_SHIFT) &
-			 RVU_PFVF_FUNC_MASK;
-		if (vf_id > 0) {
-			vf_id--;
-			if (vf_id >= cptpf->enabled_vfs) {
-				dev_err(&cptpf->pdev->dev,
-					"MBOX msg to unknown VF: %d >= %d\n",
-					vf_id, cptpf->enabled_vfs);
-				goto error;
-			}
-			if (msg->id == MBOX_MSG_VF_FLR)
-				goto error;
-
-			fwd = otx2_mbox_alloc_msg(vfpf_mbox, vf_id, size);
-			if (!fwd) {
-				dev_err(&cptpf->pdev->dev,
-					"Forwarding to VF%d failed.\n", vf_id);
-				goto error;
-			}
-			memcpy((uint8_t *)fwd + sizeof(struct mbox_msghdr),
-			       (uint8_t *)msg + sizeof(struct mbox_msghdr),
-			       size);
-			fwd->id = msg->id;
-			fwd->pcifunc = msg->pcifunc;
-			fwd->sig = msg->sig;
-			fwd->ver = msg->ver;
-			fwd->rc = msg->rc;
-		} else {
-			dump_mbox_msg(msg, size);
-			switch (msg->id) {
-			case MBOX_MSG_READY:
-				cptpf->pf_id =
-					(msg->pcifunc >> RVU_PFVF_PF_SHIFT) &
-					RVU_PFVF_PF_MASK;
-				break;
-
-			case MBOX_MSG_CPT_RD_WR_REGISTER:
-				rsp_rd_wr = (struct cpt_rd_wr_reg_msg *)
-					     msg;
-				if (msg->rc) {
-					dev_err(&cptpf->pdev->dev,
-						"Reg %llx rd/wr(%d) failed %d\n",
-						rsp_rd_wr->reg_offset,
-						rsp_rd_wr->is_write,
-						msg->rc);
-					continue;
-				}
-
-				if (!rsp_rd_wr->is_write)
-					*rsp_rd_wr->ret_val = rsp_rd_wr->val;
-				break;
-
-			case MBOX_MSG_ATTACH_RESOURCES:
-				if (!msg->rc)
-					cptpf->lfs.are_lfs_attached = 1;
-				break;
-
-			case MBOX_MSG_DETACH_RESOURCES:
-				if (!msg->rc)
-					cptpf->lfs.are_lfs_attached = 0;
-				break;
-			case MBOX_MSG_CPT_INLINE_IPSEC_CFG:
-			case MBOX_MSG_NIX_INLINE_IPSEC_CFG:
-				break;
-			default:
-				dev_err(&cptpf->pdev->dev,
-					"Unsupported msg %d received.\n",
-					msg->id);
-				break;
-			}
-		}
-error:
-		afpf_mbox->dev->msgs_acked++;
+static void forward_to_vf(struct otx2_cptpf_dev *cptpf, struct mbox_msghdr *msg,
+			  int vf_id, int size)
+{
+	struct otx2_mbox *vfpf_mbox;
+	struct mbox_msghdr *fwd;
+
+	if (msg->id >= MBOX_MSG_MAX) {
+		dev_err(&cptpf->pdev->dev,
+			"MBOX msg with unknown ID %d\n", msg->id);
+		return;
 	}
+	if (msg->sig != OTX2_MBOX_RSP_SIG) {
+		dev_err(&cptpf->pdev->dev,
+			"MBOX msg with wrong signature %x, ID %d\n",
+			msg->sig, msg->id);
+		return;
+	}
+	vfpf_mbox = &cptpf->vfpf_mbox;
+	vf_id--;
+	if (vf_id >= cptpf->enabled_vfs) {
+		dev_err(&cptpf->pdev->dev,
+			"MBOX msg to unknown VF: %d >= %d\n",
+			vf_id, cptpf->enabled_vfs);
+		return;
+	}
+	if (msg->id == MBOX_MSG_VF_FLR)
+		return;
 
-	otx2_mbox_reset(afpf_mbox, 0);
+	fwd = otx2_mbox_alloc_msg(vfpf_mbox, vf_id, size);
+	if (!fwd) {
+		dev_err(&cptpf->pdev->dev,
+			"Forwarding to VF%d failed.\n", vf_id);
+		return;
+	}
+	memcpy((uint8_t *)fwd + sizeof(struct mbox_msghdr),
+		(uint8_t *)msg + sizeof(struct mbox_msghdr), size);
+	fwd->id = msg->id;
+	fwd->pcifunc = msg->pcifunc;
+	fwd->sig = msg->sig;
+	fwd->ver = msg->ver;
+	fwd->rc = msg->rc;
 }
 
-void otx2_cptpf_vfpf_mbox_handler(struct work_struct *work)
+/* Handle mailbox messages received from AF */
+void otx2_cptpf_afpf_mbox_handler(struct work_struct *work)
 {
-	struct otx2_cptvf_info *vf = container_of(work, struct otx2_cptvf_info,
-						  vfpf_mbox_work);
-	struct otx2_cptpf_dev *cptpf = vf->cptpf;
-	struct otx2_mbox *mbox = &cptpf->vfpf_mbox;
-	struct otx2_mbox_dev *mdev = &mbox->dev[vf->vf_id];
-	struct mbox_hdr *req_hdr;
+	struct otx2_cptpf_dev *cptpf;
+	struct otx2_mbox *afpf_mbox;
+	struct otx2_mbox_dev *mdev;
+	struct mbox_hdr *rsp_hdr;
 	struct mbox_msghdr *msg;
-	int offset, id, err;
+	int offset, vf_id, i;
 
-	/* sync with mbox memory region */
-	rmb();
+	cptpf = container_of(work, struct otx2_cptpf_dev, afpf_mbox_work);
+	afpf_mbox = &cptpf->afpf_mbox;
+	mdev = &afpf_mbox->dev[0];
+	/* Sync mbox data into memory */
+	smp_wmb();
 
-	/* Process received mbox messages */
-	req_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);
-	offset = ALIGN(sizeof(*req_hdr), MBOX_MSG_ALIGN);
-	id = 0;
-	while (id < req_hdr->num_msgs) {
-		while (id < req_hdr->num_msgs) {
-			msg = (struct mbox_msghdr *)(mdev->mbase +
-						     mbox->rx_start + offset);
-
-			/* Set which VF sent this message based on mbox IRQ */
-			msg->pcifunc = ((u16)cptpf->pf_id << RVU_PFVF_PF_SHIFT)
-				| ((vf->vf_id + 1) & RVU_PFVF_FUNC_MASK);
-
-			err = cptpf_handle_vf_req(cptpf, vf, msg,
-						  msg->next_msgoff - offset);
-
-			/*
-			 * Behave as the AF, drop the msg if there is
-			 * no memory, timeout handling also goes here
-			 */
-			if (err == -ENOMEM ||
-			    err == -EIO)
-				break;
-
-			offset = msg->next_msgoff;
-			id++;
-		}
+	rsp_hdr = (struct mbox_hdr *)(mdev->mbase + afpf_mbox->rx_start);
+	offset = ALIGN(sizeof(*rsp_hdr), MBOX_MSG_ALIGN);
+
+	for (i = 0; i < rsp_hdr->num_msgs; i++) {
+		msg = (struct mbox_msghdr *)(mdev->mbase + afpf_mbox->rx_start +
+					     offset);
+		vf_id = (msg->pcifunc >> RVU_PFVF_FUNC_SHIFT) &
+			 RVU_PFVF_FUNC_MASK;
+		if (vf_id > 0)
+			forward_to_vf(cptpf, msg, vf_id,
+				      msg->next_msgoff - offset);
+		else
+			process_afpf_mbox_msg(cptpf, msg);
 
-		/* Send mbox responses to VF */
-		if (mdev->num_msgs)
-			otx2_mbox_msg_send(mbox, vf->vf_id);
+		offset = msg->next_msgoff;
+		mdev->msgs_acked++;
 	}
+	otx2_mbox_reset(afpf_mbox, 0);
 }
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.c b/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.c
index 452f12a4c533..6a4994906663 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.c
@@ -1,68 +1,29 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
 
 #include <linux/ctype.h>
 #include <linux/firmware.h>
 #include "otx2_cptpf_ucode.h"
 #include "otx2_cpt_common.h"
-#include "otx2_cpt_mbox_common.h"
+#include "otx2_cptpf.h"
+#include "otx2_cptlf.h"
+#include "otx2_cpt_reqmgr.h"
 #include "rvu_reg.h"
 
 #define CSR_DELAY 30
-/* Tar archive defines */
-#define TAR_MAGIC "ustar"
-#define TAR_MAGIC_LEN 6
-#define TAR_BLOCK_LEN 512
-#define REGTYPE '0'
-#define AREGTYPE '\0'
 
 #define LOADFVC_RLEN 8
 #define LOADFVC_MAJOR_OP 0x01
 #define LOADFVC_MINOR_OP 0x08
 
-/* tar header as defined in POSIX 1003.1-1990. */
-struct tar_hdr_t {
-	char name[100];
-	char mode[8];
-	char uid[8];
-	char gid[8];
-	char size[12];
-	char mtime[12];
-	char chksum[8];
-	char typeflag;
-	char linkname[100];
-	char magic[6];
-	char version[2];
-	char uname[32];
-	char gname[32];
-	char devmajor[8];
-	char devminor[8];
-	char prefix[155];
-};
-
-struct tar_blk_t {
-	union {
-		struct tar_hdr_t hdr;
-		char block[TAR_BLOCK_LEN];
-	};
-};
-
-struct tar_arch_info_t {
+struct fw_info_t {
 	struct list_head ucodes;
-	const struct firmware *fw;
 };
 
 static struct otx2_cpt_bitmap get_cores_bmap(struct device *dev,
 					struct otx2_cpt_eng_grp_info *eng_grp)
 {
-	struct otx2_cpt_bitmap bmap = { 0 };
+	struct otx2_cpt_bitmap bmap = { {0} };
 	bool found = false;
 	int i;
 
@@ -93,12 +54,6 @@ static int is_eng_type(int val, int eng_type)
 	return val & (1 << eng_type);
 }
 
-static int dev_supports_eng_type(struct otx2_cpt_eng_grps *eng_grps,
-				 int eng_type)
-{
-	return is_eng_type(eng_grps->eng_types_supported, eng_type);
-}
-
 static int is_2nd_ucode_used(struct otx2_cpt_eng_grp_info *eng_grp)
 {
 	if (eng_grp->ucode[1].type)
@@ -157,29 +112,14 @@ static char *get_ucode_type_str(int ucode_type)
 	return str;
 }
 
-static void swap_engines(struct otx2_cpt_engines *engsl,
-			 struct otx2_cpt_engines *engsr)
-{
-	struct otx2_cpt_engines engs;
-
-	engs = *engsl;
-	*engsl = *engsr;
-	*engsr = engs;
-}
-
-static void swap_ucodes(struct otx2_cpt_ucode *ucodel,
-			struct otx2_cpt_ucode *ucoder)
-{
-	struct otx2_cpt_ucode ucode;
-
-	ucode = *ucodel;
-	*ucodel = *ucoder;
-	*ucoder = ucode;
-}
-
-static int get_ucode_type(struct otx2_cpt_ucode_hdr *ucode_hdr, int *ucode_type)
+static int get_ucode_type(struct device *dev,
+			  struct otx2_cpt_ucode_hdr *ucode_hdr,
+			  int *ucode_type)
 {
+	struct otx2_cptpf_dev *cptpf = dev_get_drvdata(dev);
+	char ver_str_prefix[OTX2_CPT_UCODE_VER_STR_SZ];
 	char tmp_ver_str[OTX2_CPT_UCODE_VER_STR_SZ];
+	struct pci_dev *pdev = cptpf->pdev;
 	int i, val = 0;
 	u8 nn;
 
@@ -187,12 +127,16 @@ static int get_ucode_type(struct otx2_cpt_ucode_hdr *ucode_hdr, int *ucode_type)
 	for (i = 0; i < strlen(tmp_ver_str); i++)
 		tmp_ver_str[i] = tolower(tmp_ver_str[i]);
 
+	sprintf(ver_str_prefix, "ocpt-%02d", pdev->revision);
+	if (!strnstr(tmp_ver_str, ver_str_prefix, OTX2_CPT_UCODE_VER_STR_SZ))
+		return -EINVAL;
+
 	nn = ucode_hdr->ver_num.nn;
 	if (strnstr(tmp_ver_str, "se-", OTX2_CPT_UCODE_VER_STR_SZ) &&
 	    (nn == OTX2_CPT_SE_UC_TYPE1 || nn == OTX2_CPT_SE_UC_TYPE2 ||
 	     nn == OTX2_CPT_SE_UC_TYPE3))
 		val |= 1 << OTX2_CPT_SE_TYPES;
-	if (strnstr(tmp_ver_str, "ipsec", OTX2_CPT_UCODE_VER_STR_SZ) &&
+	if (strnstr(tmp_ver_str, "ie-", OTX2_CPT_UCODE_VER_STR_SZ) &&
 	    (nn == OTX2_CPT_IE_UC_TYPE1 || nn == OTX2_CPT_IE_UC_TYPE2 ||
 	     nn == OTX2_CPT_IE_UC_TYPE3))
 		val |= 1 << OTX2_CPT_IE_TYPES;
@@ -204,35 +148,29 @@ static int get_ucode_type(struct otx2_cpt_ucode_hdr *ucode_hdr, int *ucode_type)
 
 	if (!val)
 		return -EINVAL;
-	if (is_eng_type(val, OTX2_CPT_AE_TYPES) &&
-	    (is_eng_type(val, OTX2_CPT_SE_TYPES) ||
-	    is_eng_type(val, OTX2_CPT_IE_TYPES)))
-		return -EINVAL;
 
 	return 0;
 }
 
-static int is_mem_zero(const char *ptr, int size)
+static int __write_ucode_base(struct otx2_cptpf_dev *cptpf, int eng,
+			      dma_addr_t dma_addr, int blkaddr)
 {
-	int i;
-
-	for (i = 0; i < size; i++) {
-		if (ptr[i])
-			return 0;
-	}
-	return 1;
+	return otx2_cpt_write_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+				     CPT_AF_EXEX_UCODE_BASE(eng),
+				     (u64)dma_addr, blkaddr);
 }
 
 static int cptx_set_ucode_base(struct otx2_cpt_eng_grp_info *eng_grp,
-			       struct otx2_cptpf_dev *cptpf)
+			       struct otx2_cptpf_dev *cptpf, int blkaddr)
 {
 	struct otx2_cpt_engs_rsvd *engs;
 	dma_addr_t dma_addr;
 	int i, bit, ret;
 
 	/* Set PF number for microcode fetches */
-	ret = otx2_cpt_write_af_reg(cptpf->pdev, CPT_AF_PF_FUNC,
-				    cptpf->pf_id << RVU_PFVF_PF_SHIFT);
+	ret = otx2_cpt_write_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+				    CPT_AF_PF_FUNC,
+				    cptpf->pf_id << RVU_PFVF_PF_SHIFT, blkaddr);
 	if (ret)
 		return ret;
 
@@ -241,7 +179,7 @@ static int cptx_set_ucode_base(struct otx2_cpt_eng_grp_info *eng_grp,
 		if (!engs->type)
 			continue;
 
-		dma_addr = engs->ucode->align_dma;
+		dma_addr = engs->ucode->dma;
 
 		/*
 		 * Set UCODE_BASE only for the cores which are not used,
@@ -249,9 +187,8 @@ static int cptx_set_ucode_base(struct otx2_cpt_eng_grp_info *eng_grp,
 		 */
 		for_each_set_bit(bit, engs->bmap, eng_grp->g->engs_num)
 			if (!eng_grp->g->eng_ref_cnt[bit]) {
-				ret = otx2_cpt_write_af_reg(cptpf->pdev,
-						CPT_AF_EXEX_UCODE_BASE(bit),
-						(u64) dma_addr);
+				ret = __write_ucode_base(cptpf, bit, dma_addr,
+							 blkaddr);
 				if (ret)
 					return ret;
 			}
@@ -264,30 +201,27 @@ static int cpt_set_ucode_base(struct otx2_cpt_eng_grp_info *eng_grp, void *obj)
 	struct otx2_cptpf_dev *cptpf = obj;
 	int ret;
 
-	if (cptpf->cpt1_implemented) {
-		cptpf->blkaddr = BLKADDR_CPT1;
-		ret = cptx_set_ucode_base(eng_grp, cptpf);
+	if (cptpf->has_cpt1) {
+		ret = cptx_set_ucode_base(eng_grp, cptpf, BLKADDR_CPT1);
 		if (ret)
 			return ret;
 	}
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = cptx_set_ucode_base(eng_grp, cptpf);
-
-	return ret;
+	return cptx_set_ucode_base(eng_grp, cptpf, BLKADDR_CPT0);
 }
 
 static int cptx_detach_and_disable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 					 struct otx2_cptpf_dev *cptpf,
-					 struct otx2_cpt_bitmap bmap)
+					 struct otx2_cpt_bitmap bmap,
+					 int blkaddr)
 {
-	int i, busy, ret;
-	int timeout = 10;
-	u64 reg;
+	int i, timeout = 10;
+	int busy, ret;
+	u64 reg = 0;
 
 	/* Detach the cores from group */
 	for_each_set_bit(i, bmap.bits, bmap.size) {
-		ret = otx2_cpt_read_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL2(i),
-					   &reg);
+		ret = otx2_cpt_read_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+					   CPT_AF_EXEX_CTL2(i), &reg, blkaddr);
 		if (ret)
 			return ret;
 
@@ -295,8 +229,10 @@ static int cptx_detach_and_disable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 			eng_grp->g->eng_ref_cnt[i]--;
 			reg &= ~(1ull << eng_grp->idx);
 
-			ret = otx2_cpt_write_af_reg(cptpf->pdev,
-						    CPT_AF_EXEX_CTL2(i), reg);
+			ret = otx2_cpt_write_af_reg(&cptpf->afpf_mbox,
+						    cptpf->pdev,
+						    CPT_AF_EXEX_CTL2(i), reg,
+						    blkaddr);
 			if (ret)
 				return ret;
 		}
@@ -310,8 +246,10 @@ static int cptx_detach_and_disable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 			return -EBUSY;
 
 		for_each_set_bit(i, bmap.bits, bmap.size) {
-			ret = otx2_cpt_read_af_reg(cptpf->pdev,
-						   CPT_AF_EXEX_STS(i), &reg);
+			ret = otx2_cpt_read_af_reg(&cptpf->afpf_mbox,
+						   cptpf->pdev,
+						   CPT_AF_EXEX_STS(i), &reg,
+						   blkaddr);
 			if (ret)
 				return ret;
 
@@ -325,8 +263,10 @@ static int cptx_detach_and_disable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 	/* Disable the cores only if they are not used anymore */
 	for_each_set_bit(i, bmap.bits, bmap.size) {
 		if (!eng_grp->g->eng_ref_cnt[i]) {
-			ret = otx2_cpt_write_af_reg(cptpf->pdev,
-						    CPT_AF_EXEX_CTL(i), 0x0);
+			ret = otx2_cpt_write_af_reg(&cptpf->afpf_mbox,
+						    cptpf->pdev,
+						    CPT_AF_EXEX_CTL(i), 0x0,
+						    blkaddr);
 			if (ret)
 				return ret;
 		}
@@ -346,29 +286,28 @@ static int cpt_detach_and_disable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 	if (!bmap.size)
 		return -EINVAL;
 
-	if (cptpf->cpt1_implemented) {
-		cptpf->blkaddr = BLKADDR_CPT1;
-		ret = cptx_detach_and_disable_cores(eng_grp, cptpf, bmap);
+	if (cptpf->has_cpt1) {
+		ret = cptx_detach_and_disable_cores(eng_grp, cptpf, bmap,
+						    BLKADDR_CPT1);
 		if (ret)
 			return ret;
 	}
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = cptx_detach_and_disable_cores(eng_grp, cptpf, bmap);
-
-	return ret;
+	return cptx_detach_and_disable_cores(eng_grp, cptpf, bmap,
+					     BLKADDR_CPT0);
 }
 
 static int cptx_attach_and_enable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 					struct otx2_cptpf_dev *cptpf,
-					struct otx2_cpt_bitmap bmap)
+					struct otx2_cpt_bitmap bmap,
+					int blkaddr)
 {
+	u64 reg = 0;
 	int i, ret;
-	u64 reg;
 
 	/* Attach the cores to the group */
 	for_each_set_bit(i, bmap.bits, bmap.size) {
-		ret = otx2_cpt_read_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL2(i),
-					   &reg);
+		ret = otx2_cpt_read_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+					   CPT_AF_EXEX_CTL2(i), &reg, blkaddr);
 		if (ret)
 			return ret;
 
@@ -376,8 +315,10 @@ static int cptx_attach_and_enable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 			eng_grp->g->eng_ref_cnt[i]++;
 			reg |= 1ull << eng_grp->idx;
 
-			ret = otx2_cpt_write_af_reg(cptpf->pdev,
-						    CPT_AF_EXEX_CTL2(i), reg);
+			ret = otx2_cpt_write_af_reg(&cptpf->afpf_mbox,
+						    cptpf->pdev,
+						    CPT_AF_EXEX_CTL2(i), reg,
+						    blkaddr);
 			if (ret)
 				return ret;
 		}
@@ -385,14 +326,13 @@ static int cptx_attach_and_enable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 
 	/* Enable the cores */
 	for_each_set_bit(i, bmap.bits, bmap.size) {
-		ret = otx2_cpt_add_write_af_reg(cptpf->pdev,
-						CPT_AF_EXEX_CTL(i), 0x1);
+		ret = otx2_cpt_add_write_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+						CPT_AF_EXEX_CTL(i), 0x1,
+						blkaddr);
 		if (ret)
 			return ret;
 	}
-	ret = otx2_cpt_send_af_reg_requests(cptpf->pdev);
-
-	return ret;
+	return otx2_cpt_send_af_reg_requests(&cptpf->afpf_mbox, cptpf->pdev);
 }
 
 static int cpt_attach_and_enable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
@@ -406,136 +346,93 @@ static int cpt_attach_and_enable_cores(struct otx2_cpt_eng_grp_info *eng_grp,
 	if (!bmap.size)
 		return -EINVAL;
 
-	if (cptpf->cpt1_implemented) {
-		cptpf->blkaddr = BLKADDR_CPT1;
-		ret = cptx_attach_and_enable_cores(eng_grp, cptpf, bmap);
+	if (cptpf->has_cpt1) {
+		ret = cptx_attach_and_enable_cores(eng_grp, cptpf, bmap,
+						   BLKADDR_CPT1);
 		if (ret)
 			return ret;
 	}
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = cptx_attach_and_enable_cores(eng_grp, cptpf, bmap);
-
-	return ret;
+	return cptx_attach_and_enable_cores(eng_grp, cptpf, bmap, BLKADDR_CPT0);
 }
 
-static int process_tar_file(struct device *dev,
-			    struct tar_arch_info_t *tar_arch, char *filename,
-			    const u8 *data, int size)
+static int load_fw(struct device *dev, struct fw_info_t *fw_info,
+		   char *filename)
 {
-	struct tar_ucode_info_t *tar_ucode_info;
 	struct otx2_cpt_ucode_hdr *ucode_hdr;
+	struct otx2_cpt_uc_info_t *uc_info;
 	int ucode_type, ucode_size;
+	int ret;
 
-	/*
-	 * If size is less than microcode header size then don't report
-	 * an error because it might not be microcode file, just process
-	 * next file from archive
-	 */
-	if (size < sizeof(struct otx2_cpt_ucode_hdr))
-		return 0;
+	uc_info = kzalloc(sizeof(*uc_info), GFP_KERNEL);
+	if (!uc_info)
+		return -ENOMEM;
 
-	ucode_hdr = (struct otx2_cpt_ucode_hdr *) data;
-	/*
-	 * If microcode version can't be found don't report an error
-	 * because it might not be microcode file, just process next file
-	 */
-	if (get_ucode_type(ucode_hdr, &ucode_type))
-		return 0;
+	ret = request_firmware(&uc_info->fw, filename, dev);
+	if (ret)
+		goto free_uc_info;
+
+	ucode_hdr = (struct otx2_cpt_ucode_hdr *)uc_info->fw->data;
+	ret = get_ucode_type(dev, ucode_hdr, &ucode_type);
+	if (ret)
+		goto release_fw;
 
 	ucode_size = ntohl(ucode_hdr->code_length) * 2;
-	if (!ucode_size || (size < round_up(ucode_size, 16) +
-	    sizeof(struct otx2_cpt_ucode_hdr) + OTX2_CPT_UCODE_SIGN_LEN)) {
+	if (!ucode_size) {
 		dev_err(dev, "Ucode %s invalid size\n", filename);
-		return -EINVAL;
+		ret = -EINVAL;
+		goto release_fw;
 	}
 
-	tar_ucode_info = kzalloc(sizeof(*tar_ucode_info), GFP_KERNEL);
-	if (!tar_ucode_info)
-		return -ENOMEM;
-
-	tar_ucode_info->ucode_ptr = data;
-	set_ucode_filename(&tar_ucode_info->ucode, filename);
-	memcpy(tar_ucode_info->ucode.ver_str, ucode_hdr->ver_str,
+	set_ucode_filename(&uc_info->ucode, filename);
+	memcpy(uc_info->ucode.ver_str, ucode_hdr->ver_str,
 	       OTX2_CPT_UCODE_VER_STR_SZ);
-	tar_ucode_info->ucode.ver_num = ucode_hdr->ver_num;
-	tar_ucode_info->ucode.type = ucode_type;
-	tar_ucode_info->ucode.size = ucode_size;
-	list_add_tail(&tar_ucode_info->list, &tar_arch->ucodes);
+	uc_info->ucode.ver_num = ucode_hdr->ver_num;
+	uc_info->ucode.type = ucode_type;
+	uc_info->ucode.size = ucode_size;
+	list_add_tail(&uc_info->list, &fw_info->ucodes);
 
 	return 0;
+
+release_fw:
+	release_firmware(uc_info->fw);
+free_uc_info:
+	kfree(uc_info);
+	return ret;
 }
 
-static void release_tar_archive(struct tar_arch_info_t *tar_arch)
+static void cpt_ucode_release_fw(struct fw_info_t *fw_info)
 {
-	struct tar_ucode_info_t *curr, *temp;
+	struct otx2_cpt_uc_info_t *curr, *temp;
 
-	if (!tar_arch)
+	if (!fw_info)
 		return;
 
-	list_for_each_entry_safe(curr, temp, &tar_arch->ucodes, list) {
+	list_for_each_entry_safe(curr, temp, &fw_info->ucodes, list) {
 		list_del(&curr->list);
+		release_firmware(curr->fw);
 		kfree(curr);
 	}
-
-	if (tar_arch->fw)
-		release_firmware(tar_arch->fw);
-	kfree(tar_arch);
 }
 
-static struct tar_ucode_info_t *get_uc_from_tar_archive(
-					struct tar_arch_info_t *tar_arch,
-					int ucode_type)
+static struct otx2_cpt_uc_info_t *get_ucode(struct fw_info_t *fw_info,
+					    int ucode_type)
 {
-	struct tar_ucode_info_t *curr, *uc_found = NULL;
+	struct otx2_cpt_uc_info_t *curr;
 
-	list_for_each_entry(curr, &tar_arch->ucodes, list) {
+	list_for_each_entry(curr, &fw_info->ucodes, list) {
 		if (!is_eng_type(curr->ucode.type, ucode_type))
 			continue;
 
-		if (ucode_type == OTX2_CPT_IE_TYPES &&
-		    is_eng_type(curr->ucode.type, OTX2_CPT_SE_TYPES))
-			continue;
-
-		if (!uc_found) {
-			uc_found = curr;
-			continue;
-		}
-
-		switch (ucode_type) {
-		case OTX2_CPT_AE_TYPES:
-			break;
-
-		case OTX2_CPT_SE_TYPES:
-			if (uc_found->ucode.ver_num.nn ==
-							OTX2_CPT_SE_UC_TYPE2 ||
-			    (uc_found->ucode.ver_num.nn ==
-							OTX2_CPT_SE_UC_TYPE3 &&
-			     curr->ucode.ver_num.nn == OTX2_CPT_SE_UC_TYPE1))
-				uc_found = curr;
-			break;
-
-		case OTX2_CPT_IE_TYPES:
-			if (uc_found->ucode.ver_num.nn ==
-							OTX2_CPT_IE_UC_TYPE2 ||
-			    (uc_found->ucode.ver_num.nn ==
-							OTX2_CPT_IE_UC_TYPE3 &&
-			     curr->ucode.ver_num.nn == OTX2_CPT_IE_UC_TYPE1))
-				uc_found = curr;
-			break;
-		}
+		return curr;
 	}
-	return uc_found;
+	return NULL;
 }
 
-static void print_tar_dbg_info(struct tar_arch_info_t *tar_arch,
-			       char *tar_filename)
+static void print_uc_info(struct fw_info_t *fw_info)
 {
-	struct tar_ucode_info_t *curr;
+	struct otx2_cpt_uc_info_t *curr;
 
-	pr_debug("Tar archive filename %s\n", tar_filename);
-	pr_debug("Tar archive pointer %p, size %ld\n", tar_arch->fw->data,
-		 tar_arch->fw->size);
-	list_for_each_entry(curr, &tar_arch->ucodes, list) {
+	list_for_each_entry(curr, &fw_info->ucodes, list) {
 		pr_debug("Ucode filename %s\n", curr->ucode.filename);
 		pr_debug("Ucode version string %s\n", curr->ucode.ver_str);
 		pr_debug("Ucode version %d.%d.%d.%d\n",
@@ -544,90 +441,36 @@ static void print_tar_dbg_info(struct tar_arch_info_t *tar_arch,
 		pr_debug("Ucode type (%d) %s\n", curr->ucode.type,
 			 get_ucode_type_str(curr->ucode.type));
 		pr_debug("Ucode size %d\n", curr->ucode.size);
-		pr_debug("Ucode ptr %p\n", curr->ucode_ptr);
+		pr_debug("Ucode ptr %p\n", curr->fw->data);
 	}
 }
 
-static struct tar_arch_info_t *load_tar_archive(struct device *dev,
-						char *tar_filename)
+static int cpt_ucode_load_fw(struct pci_dev *pdev, struct fw_info_t *fw_info)
 {
-	struct tar_arch_info_t *tar_arch = NULL;
-	struct tar_blk_t *tar_blk;
-	unsigned int cur_size;
-	size_t tar_offs = 0;
-	size_t tar_size;
-	int ret;
-
-	tar_arch = kzalloc(sizeof(struct tar_arch_info_t), GFP_KERNEL);
-	if (!tar_arch)
-		return NULL;
-
-	INIT_LIST_HEAD(&tar_arch->ucodes);
-
-	/* Load tar archive */
-	ret = request_firmware(&tar_arch->fw, tar_filename, dev);
-	if (ret)
-		goto release_tar_arch;
+	char filename[OTX2_CPT_NAME_LENGTH];
+	char eng_type[8] = {0};
+	int ret, e, i;
 
-	if (tar_arch->fw->size < TAR_BLOCK_LEN) {
-		dev_err(dev, "Invalid tar archive %s\n", tar_filename);
-		goto release_tar_arch;
-	}
+	INIT_LIST_HEAD(&fw_info->ucodes);
 
-	tar_size = tar_arch->fw->size;
-	tar_blk = (struct tar_blk_t *) tar_arch->fw->data;
-	if (strncmp(tar_blk->hdr.magic, TAR_MAGIC, TAR_MAGIC_LEN - 1)) {
-		dev_err(dev, "Unsupported format of tar archive %s\n",
-			tar_filename);
-		goto release_tar_arch;
-	}
+	for (e = 1; e < OTX2_CPT_MAX_ENG_TYPES; e++) {
+		strcpy(eng_type, get_eng_type_str(e));
+		for (i = 0; i < strlen(eng_type); i++)
+			eng_type[i] = tolower(eng_type[i]);
 
-	while (1) {
-		/* Read current file size */
-		ret = kstrtouint(tar_blk->hdr.size, 8, &cur_size);
+		snprintf(filename, sizeof(filename), "mrvl/cpt%02d/%s.out",
+			 pdev->revision, eng_type);
+		/* Request firmware for each engine type */
+		ret = load_fw(&pdev->dev, fw_info, filename);
 		if (ret)
-			goto release_tar_arch;
-
-		if (tar_offs + cur_size > tar_size ||
-		    tar_offs + 2*TAR_BLOCK_LEN > tar_size) {
-			dev_err(dev, "Invalid tar archive %s\n", tar_filename);
-			goto release_tar_arch;
-		}
-
-		tar_offs += TAR_BLOCK_LEN;
-		if (tar_blk->hdr.typeflag == REGTYPE ||
-		    tar_blk->hdr.typeflag == AREGTYPE) {
-			ret = process_tar_file(dev, tar_arch,
-					       tar_blk->hdr.name,
-					       &tar_arch->fw->data[tar_offs],
-					       cur_size);
-			if (ret)
-				goto release_tar_arch;
-		}
-
-		tar_offs += (cur_size/TAR_BLOCK_LEN) * TAR_BLOCK_LEN;
-		if (cur_size % TAR_BLOCK_LEN)
-			tar_offs += TAR_BLOCK_LEN;
-
-		/* Check for the end of the archive */
-		if (tar_offs + 2*TAR_BLOCK_LEN > tar_size) {
-			dev_err(dev, "Invalid tar archive %s\n", tar_filename);
-			goto release_tar_arch;
-		}
-
-		if (is_mem_zero(&tar_arch->fw->data[tar_offs],
-		    2*TAR_BLOCK_LEN))
-			break;
-
-		/* Read next block from tar archive */
-		tar_blk = (struct tar_blk_t *) &tar_arch->fw->data[tar_offs];
+			goto release_fw;
 	}
+	print_uc_info(fw_info);
+	return 0;
 
-	print_tar_dbg_info(tar_arch, tar_filename);
-	return tar_arch;
-release_tar_arch:
-	release_tar_archive(tar_arch);
-	return NULL;
+release_fw:
+	cpt_ucode_release_fw(fw_info);
+	return ret;
 }
 
 static struct otx2_cpt_engs_rsvd *find_engines_by_type(
@@ -646,13 +489,8 @@ static struct otx2_cpt_engs_rsvd *find_engines_by_type(
 	return NULL;
 }
 
-int otx2_cpt_uc_supports_eng_type(struct otx2_cpt_ucode *ucode, int eng_type)
-{
-	return is_eng_type(ucode->type, eng_type);
-}
-
-int otx2_cpt_eng_grp_has_eng_type(struct otx2_cpt_eng_grp_info *eng_grp,
-				  int eng_type)
+static int eng_grp_has_eng_type(struct otx2_cpt_eng_grp_info *eng_grp,
+				int eng_type)
 {
 	struct otx2_cpt_engs_rsvd *engs;
 
@@ -661,156 +499,6 @@ int otx2_cpt_eng_grp_has_eng_type(struct otx2_cpt_eng_grp_info *eng_grp,
 	return (engs != NULL ? 1 : 0);
 }
 
-static void print_ucode_info(struct otx2_cpt_eng_grp_info *eng_grp,
-			     char *buf, int size)
-{
-	int len;
-
-	if (eng_grp->mirror.is_ena) {
-		scnprintf(buf, size, "%s (shared with engine_group%d)",
-			  eng_grp->g->grp[eng_grp->mirror.idx].ucode[0].ver_str,
-			  eng_grp->mirror.idx);
-	} else {
-		scnprintf(buf, size, "%s", eng_grp->ucode[0].ver_str);
-	}
-
-	if (is_2nd_ucode_used(eng_grp)) {
-		len = strlen(buf);
-		scnprintf(buf + len, size - len, ", %s (used by IE engines)",
-			  eng_grp->ucode[1].ver_str);
-	}
-}
-
-static void print_engs_info(struct otx2_cpt_eng_grp_info *eng_grp,
-			    char *buf, int size, int idx)
-{
-	struct otx2_cpt_engs_rsvd *mirrored_engs = NULL;
-	struct otx2_cpt_engs_rsvd *engs;
-	int len, i;
-
-	buf[0] = '\0';
-	for (i = 0; i < OTX2_CPT_MAX_ETYPES_PER_GRP; i++) {
-		engs = &eng_grp->engs[i];
-		if (!engs->type)
-			continue;
-		if (idx != -1 && idx != i)
-			continue;
-
-		if (eng_grp->mirror.is_ena)
-			mirrored_engs = find_engines_by_type(
-					&eng_grp->g->grp[eng_grp->mirror.idx],
-					engs->type);
-		if (i > 0 && idx == -1) {
-			len = strlen(buf);
-			scnprintf(buf+len, size-len, ", ");
-		}
-
-		len = strlen(buf);
-		scnprintf(buf+len, size-len, "%d %s ", mirrored_engs ?
-			  engs->count + mirrored_engs->count : engs->count,
-			  get_eng_type_str(engs->type));
-		if (mirrored_engs) {
-			len = strlen(buf);
-			scnprintf(buf+len, size-len,
-				  "(%d shared with engine_group%d) ",
-				  engs->count <= 0 ? engs->count +
-				  mirrored_engs->count : mirrored_engs->count,
-				  eng_grp->mirror.idx);
-		}
-	}
-}
-
-static void print_ucode_dbg_info(struct otx2_cpt_ucode *ucode)
-{
-	pr_debug("Ucode info\n");
-	pr_debug("Ucode version string %s\n", ucode->ver_str);
-	pr_debug("Ucode version %d.%d.%d.%d\n", ucode->ver_num.nn,
-		 ucode->ver_num.xx, ucode->ver_num.yy, ucode->ver_num.zz);
-	pr_debug("Ucode type %s\n", get_ucode_type_str(ucode->type));
-	pr_debug("Ucode size %d\n", ucode->size);
-	pr_debug("Ucode virt address %16.16llx\n", (u64)ucode->align_va);
-	pr_debug("Ucode phys address %16.16llx\n", ucode->align_dma);
-}
-
-static void print_engines_mask(struct otx2_cpt_eng_grp_info *eng_grp,
-			       void *obj, char *buf, int size)
-{
-	struct otx2_cptpf_dev *cptpf = obj;
-	struct otx2_cpt_bitmap bmap;
-	u32 mask[4];
-
-	bmap = get_cores_bmap(&cptpf->pdev->dev, eng_grp);
-	if (!bmap.size) {
-		scnprintf(buf, size, "unknown");
-		return;
-	}
-	bitmap_to_arr32(mask, bmap.bits, bmap.size);
-	scnprintf(buf, size, "%8.8x %8.8x %8.8x %8.8x", mask[3], mask[2],
-		  mask[1], mask[0]);
-}
-
-static void print_dbg_info(struct device *dev,
-			   struct otx2_cpt_eng_grps *eng_grps)
-{
-	struct otx2_cpt_eng_grp_info *mirrored_grp;
-	char engs_info[2*OTX2_CPT_NAME_LENGTH];
-	char engs_mask[OTX2_CPT_NAME_LENGTH];
-	struct otx2_cpt_eng_grp_info *grp;
-	struct otx2_cpt_engs_rsvd *engs;
-	u32 mask[4];
-	int i, j;
-
-	pr_debug("Engine groups global info\n");
-	pr_debug("max SE %d, max IE %d, max AE %d\n",
-		 eng_grps->avail.max_se_cnt, eng_grps->avail.max_ie_cnt,
-		 eng_grps->avail.max_ae_cnt);
-	pr_debug("free SE %d\n", eng_grps->avail.se_cnt);
-	pr_debug("free IE %d\n", eng_grps->avail.ie_cnt);
-	pr_debug("free AE %d\n", eng_grps->avail.ae_cnt);
-
-	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-		grp = &eng_grps->grp[i];
-		pr_debug("engine_group%d, state %s\n", i, grp->is_enabled ?
-			 "enabled" : "disabled");
-		if (grp->is_enabled) {
-			mirrored_grp = &eng_grps->grp[grp->mirror.idx];
-			pr_debug("Ucode0 filename %s, version %s\n",
-				 grp->mirror.is_ena ?
-				 mirrored_grp->ucode[0].filename :
-				 grp->ucode[0].filename,
-				 grp->mirror.is_ena ?
-				 mirrored_grp->ucode[0].ver_str :
-				 grp->ucode[0].ver_str);
-			if (is_2nd_ucode_used(grp))
-				pr_debug("Ucode1 filename %s, version %s\n",
-					 grp->ucode[1].filename,
-					 grp->ucode[1].ver_str);
-			else
-				pr_debug("Ucode1 not used\n");
-		}
-
-		for (j = 0; j < OTX2_CPT_MAX_ETYPES_PER_GRP; j++) {
-			engs = &grp->engs[j];
-			if (engs->type) {
-				print_engs_info(grp, engs_info,
-						2*OTX2_CPT_NAME_LENGTH,
-						j);
-				pr_debug("Slot%d: %s\n", j, engs_info);
-				bitmap_to_arr32(mask, engs->bmap,
-						eng_grps->engs_num);
-				pr_debug("Mask:  %8.8x %8.8x %8.8x %8.8x\n",
-					 mask[3], mask[2], mask[1], mask[0]);
-			} else
-				pr_debug("Slot%d not used\n", j);
-		}
-		if (grp->is_enabled) {
-			print_engines_mask(grp, eng_grps->obj, engs_mask,
-					   OTX2_CPT_NAME_LENGTH);
-			pr_debug("Cmask: %s\n", engs_mask);
-		}
-	}
-}
-
 static int update_engines_avail_count(struct device *dev,
 				      struct otx2_cpt_engs_available *avail,
 				      struct otx2_cpt_engs_rsvd *engs, int val)
@@ -832,7 +520,6 @@ static int update_engines_avail_count(struct device *dev,
 		dev_err(dev, "Invalid engine type %d\n", engs->type);
 		return -EINVAL;
 	}
-
 	return 0;
 }
 
@@ -857,7 +544,6 @@ static int update_engines_offset(struct device *dev,
 		dev_err(dev, "Invalid engine type %d\n", engs->type);
 		return -EINVAL;
 	}
-
 	return 0;
 }
 
@@ -957,19 +643,19 @@ static int check_engines_availability(struct device *dev,
 
 static int reserve_engines(struct device *dev,
 			   struct otx2_cpt_eng_grp_info *grp,
-			   struct otx2_cpt_engines *req_engs, int req_cnt)
+			   struct otx2_cpt_engines *req_engs, int ucodes_cnt)
 {
 	int i, ret = 0;
 
-	/* Validate if a number of requested engines is available */
-	for (i = 0; i < req_cnt; i++) {
+	/* Validate if a number of requested engines are available */
+	for (i = 0; i < ucodes_cnt; i++) {
 		ret = check_engines_availability(dev, grp, &req_engs[i]);
 		if (ret)
 			return ret;
 	}
 
 	/* Reserve requested engines for this engine group */
-	for (i = 0; i < req_cnt; i++) {
+	for (i = 0; i < ucodes_cnt; i++) {
 		ret = do_reserve_engines(dev, grp, &req_engs[i]);
 		if (ret)
 			return ret;
@@ -977,57 +663,12 @@ static int reserve_engines(struct device *dev,
 	return 0;
 }
 
-static ssize_t eng_grp_info_show(struct device *dev,
-				 struct device_attribute *attr,
-				 char *buf)
-{
-	struct otx2_cpt_eng_grp_info *eng_grp;
-	char ucode_info[2*OTX2_CPT_NAME_LENGTH];
-	char engs_info[2*OTX2_CPT_NAME_LENGTH];
-	char engs_mask[OTX2_CPT_NAME_LENGTH];
-	int ret;
-
-	eng_grp = container_of(attr, struct otx2_cpt_eng_grp_info, info_attr);
-	mutex_lock(&eng_grp->g->lock);
-
-	print_engs_info(eng_grp, engs_info, 2 * OTX2_CPT_NAME_LENGTH, -1);
-	print_ucode_info(eng_grp, ucode_info, 2 * OTX2_CPT_NAME_LENGTH);
-	print_engines_mask(eng_grp, eng_grp->g, engs_mask,
-			   OTX2_CPT_NAME_LENGTH);
-	ret = scnprintf(buf, PAGE_SIZE,
-			"Microcode : %s\nEngines: %s\nEngines mask: %s\n",
-			ucode_info, engs_info, engs_mask);
-
-	mutex_unlock(&eng_grp->g->lock);
-	return ret;
-}
-
-static int create_sysfs_eng_grps_info(struct device *dev,
-				      struct otx2_cpt_eng_grp_info *eng_grp)
-{
-	int ret;
-
-	eng_grp->info_attr.show = eng_grp_info_show;
-	eng_grp->info_attr.store = NULL;
-	eng_grp->info_attr.attr.name = eng_grp->sysfs_info_name;
-	eng_grp->info_attr.attr.mode = 0440;
-	sysfs_attr_init(&eng_grp->info_attr.attr);
-	ret = device_create_file(dev, &eng_grp->info_attr);
-	if (ret)
-		return ret;
-
-	return 0;
-}
-
 static void ucode_unload(struct device *dev, struct otx2_cpt_ucode *ucode)
 {
 	if (ucode->va) {
-		dma_free_coherent(dev, ucode->size + OTX2_CPT_UCODE_ALIGNMENT,
-				  ucode->va, ucode->dma);
+		dma_free_coherent(dev, ucode->size, ucode->va, ucode->dma);
 		ucode->va = NULL;
-		ucode->align_va = NULL;
 		ucode->dma = 0;
-		ucode->align_dma = 0;
 		ucode->size = 0;
 	}
 
@@ -1044,70 +685,23 @@ static int copy_ucode_to_dma_mem(struct device *dev,
 	u32 i;
 
 	/*  Allocate DMAable space */
-	ucode->va = dma_alloc_coherent(dev, ucode->size +
-				       OTX2_CPT_UCODE_ALIGNMENT,
-				       &ucode->dma, GFP_KERNEL);
-	if (!ucode->va) {
-		dev_err(dev, "Unable to allocate space for microcode\n");
+	ucode->va = dma_alloc_coherent(dev, ucode->size, &ucode->dma,
+				       GFP_KERNEL);
+	if (!ucode->va)
 		return -ENOMEM;
-	}
-	ucode->align_va = PTR_ALIGN(ucode->va, OTX2_CPT_UCODE_ALIGNMENT);
-	ucode->align_dma = PTR_ALIGN(ucode->dma, OTX2_CPT_UCODE_ALIGNMENT);
 
-	memcpy((void *) ucode->align_va, (void *) ucode_data +
-	       sizeof(struct otx2_cpt_ucode_hdr), ucode->size);
+	memcpy(ucode->va, ucode_data + sizeof(struct otx2_cpt_ucode_hdr),
+	       ucode->size);
 
 	/* Byte swap 64-bit */
 	for (i = 0; i < (ucode->size / 8); i++)
-		((u64 *)ucode->align_va)[i] =
-				cpu_to_be64(((u64 *)ucode->align_va)[i]);
+		cpu_to_be64s(&((u64 *)ucode->va)[i]);
 	/*  Ucode needs 16-bit swap */
 	for (i = 0; i < (ucode->size / 2); i++)
-		((u16 *)ucode->align_va)[i] =
-				cpu_to_be16(((u16 *)ucode->align_va)[i]);
+		cpu_to_be16s(&((u16 *)ucode->va)[i]);
 	return 0;
 }
 
-static int ucode_load(struct device *dev, struct otx2_cpt_ucode *ucode,
-		      const char *ucode_filename)
-{
-	struct otx2_cpt_ucode_hdr *ucode_hdr;
-	const struct firmware *fw;
-	int ret;
-
-	set_ucode_filename(ucode, ucode_filename);
-	ret = request_firmware(&fw, ucode->filename, dev);
-	if (ret)
-		return ret;
-
-	ucode_hdr = (struct otx2_cpt_ucode_hdr *) fw->data;
-	memcpy(ucode->ver_str, ucode_hdr->ver_str, OTX2_CPT_UCODE_VER_STR_SZ);
-	ucode->ver_num = ucode_hdr->ver_num;
-	ucode->size = ntohl(ucode_hdr->code_length) * 2;
-	if (!ucode->size || (fw->size < round_up(ucode->size, 16)
-	    + sizeof(struct otx2_cpt_ucode_hdr) + OTX2_CPT_UCODE_SIGN_LEN)) {
-		dev_err(dev, "Ucode %s invalid size\n", ucode_filename);
-		ret = -EINVAL;
-		goto release_fw;
-	}
-
-	ret = get_ucode_type(ucode_hdr, &ucode->type);
-	if (ret) {
-		dev_err(dev, "Microcode %s unknown type 0x%x\n",
-			ucode->filename, ucode->type);
-		goto release_fw;
-	}
-
-	ret = copy_ucode_to_dma_mem(dev, ucode, fw->data);
-	if (ret)
-		goto release_fw;
-
-	print_ucode_dbg_info(ucode);
-release_fw:
-	release_firmware(fw);
-	return ret;
-}
-
 static int enable_eng_grp(struct otx2_cpt_eng_grp_info *eng_grp,
 			  void *obj)
 {
@@ -1250,7 +844,7 @@ static int eng_grp_update_masks(struct device *dev,
 				struct otx2_cpt_eng_grp_info *eng_grp)
 {
 	struct otx2_cpt_engs_rsvd *engs, *mirrored_engs;
-	struct otx2_cpt_bitmap tmp_bmap = { 0 };
+	struct otx2_cpt_bitmap tmp_bmap = { {0} };
 	int i, j, cnt, max_cnt;
 	int bit;
 
@@ -1328,22 +922,13 @@ static int eng_grp_update_masks(struct device *dev,
 static int delete_engine_group(struct device *dev,
 			       struct otx2_cpt_eng_grp_info *eng_grp)
 {
-	int i, ret;
+	int ret;
 
 	if (!eng_grp->is_enabled)
-		return -EINVAL;
+		return 0;
 
-	if (eng_grp->mirror.ref_count) {
-		dev_err(dev, "Can't delete engine_group%d as it is used by engine group(s):",
-			eng_grp->idx);
-		for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-			if (eng_grp->g->grp[i].mirror.is_ena &&
-			    eng_grp->g->grp[i].mirror.idx == eng_grp->idx)
-				pr_cont("%d", i);
-		}
-		pr_cont("\n");
+	if (eng_grp->mirror.ref_count)
 		return -EINVAL;
-	}
 
 	/* Removing engine group mirroring if enabled */
 	remove_eng_grp_mirroring(eng_grp);
@@ -1358,86 +943,11 @@ static int delete_engine_group(struct device *dev,
 	if (ret)
 		return ret;
 
-	device_remove_file(dev, &eng_grp->info_attr);
 	eng_grp->is_enabled = false;
 
 	return 0;
 }
 
-static int validate_2_ucodes_scenario(struct device *dev,
-				      struct otx2_cpt_eng_grp_info *eng_grp)
-{
-	struct otx2_cpt_ucode *se_ucode = NULL, *ie_ucode = NULL;
-	struct otx2_cpt_ucode *ucode;
-	int i;
-
-	/*
-	 * Find ucode which supports SE engines and ucode which supports
-	 * IE engines only
-	 */
-	for (i = 0; i < OTX2_CPT_MAX_ETYPES_PER_GRP; i++) {
-		ucode = &eng_grp->ucode[i];
-		if (otx2_cpt_uc_supports_eng_type(ucode, OTX2_CPT_SE_TYPES))
-			se_ucode = ucode;
-		else if (otx2_cpt_uc_supports_eng_type(ucode,
-						       OTX2_CPT_IE_TYPES) &&
-			 !otx2_cpt_uc_supports_eng_type(ucode,
-							OTX2_CPT_SE_TYPES))
-			ie_ucode = ucode;
-	}
-
-	if (!se_ucode || !ie_ucode) {
-		dev_err(dev,
-			"Only combination of SE+IE microcodes is supported.\n");
-		return -EINVAL;
-	}
-
-	/* Keep SE ucode at index 0 */
-	if (otx2_cpt_uc_supports_eng_type(&eng_grp->ucode[1],
-					  OTX2_CPT_SE_TYPES))
-		swap_ucodes(&eng_grp->ucode[0], &eng_grp->ucode[1]);
-
-	return 0;
-}
-
-static int validate_1_ucode_scenario(struct device *dev,
-				     struct otx2_cpt_eng_grp_info *eng_grp,
-				     struct otx2_cpt_engines *engs,
-				     int engs_cnt)
-{
-	int i;
-
-	/* Verify that ucode loaded supports requested engine types */
-	for (i = 0; i < engs_cnt; i++) {
-		if (otx2_cpt_uc_supports_eng_type(&eng_grp->ucode[0],
-						  OTX2_CPT_SE_TYPES) &&
-		    engs[i].type == OTX2_CPT_IE_TYPES) {
-			dev_err(dev,
-				"IE engines can't be used with SE microcode\n");
-			return -EINVAL;
-		}
-
-		if (!otx2_cpt_uc_supports_eng_type(&eng_grp->ucode[0],
-						   engs[i].type)) {
-			/*
-			 * Exception to this rule is the case
-			 * where IPSec ucode can use SE engines
-			 */
-			if (otx2_cpt_uc_supports_eng_type(&eng_grp->ucode[0],
-							  OTX2_CPT_IE_TYPES) &&
-			    engs[i].type == OTX2_CPT_SE_TYPES)
-				continue;
-
-			dev_err(dev,
-				"Microcode %s does not support %s engines\n",
-				eng_grp->ucode[0].filename,
-				get_eng_type_str(engs[i].type));
-			return -EINVAL;
-		}
-	}
-	return 0;
-}
-
 static void update_ucode_ptrs(struct otx2_cpt_eng_grp_info *eng_grp)
 {
 	struct otx2_cpt_ucode *ucode;
@@ -1457,136 +967,31 @@ static void update_ucode_ptrs(struct otx2_cpt_eng_grp_info *eng_grp)
 	}
 }
 
-static int get_eng_caps_discovery_grp(struct otx2_cpt_eng_grps *eng_grps,
-				      u8 eng_type)
+static int create_engine_group(struct device *dev,
+			       struct otx2_cpt_eng_grps *eng_grps,
+			       struct otx2_cpt_engines *engs, int ucodes_cnt,
+			       void *ucode_data[], int is_print)
 {
-	struct otx2_cpt_eng_grp_info *grp;
-	int eng_grp_num = 0xff, i;
-
-	switch (eng_type) {
-	case OTX2_CPT_SE_TYPES:
-		for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-			grp = &eng_grps->grp[i];
-			if (!grp->is_enabled)
-				continue;
-
-			if (otx2_cpt_eng_grp_has_eng_type(grp,
-							  OTX2_CPT_SE_TYPES) &&
-			    !otx2_cpt_eng_grp_has_eng_type(grp,
-							   OTX2_CPT_IE_TYPES) &&
-			    !otx2_cpt_eng_grp_has_eng_type(grp,
-							   OTX2_CPT_AE_TYPES)) {
-				eng_grp_num = i;
-				break;
-			}
-		}
-		break;
+	struct otx2_cpt_eng_grp_info *mirrored_eng_grp;
+	struct otx2_cpt_eng_grp_info *eng_grp;
+	struct otx2_cpt_uc_info_t *uc_info;
+	int i, ret = 0;
 
-	case OTX2_CPT_IE_TYPES:
-		for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-			grp = &eng_grps->grp[i];
-			if (!grp->is_enabled)
-				continue;
-
-			if (otx2_cpt_eng_grp_has_eng_type(grp,
-							  OTX2_CPT_IE_TYPES) &&
-			    !otx2_cpt_eng_grp_has_eng_type(grp,
-							   OTX2_CPT_SE_TYPES)) {
-				eng_grp_num = i;
-				break;
-			}
-		}
-		break;
-
-	case OTX2_CPT_AE_TYPES:
-		for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-			grp = &eng_grps->grp[i];
-			if (!grp->is_enabled)
-				continue;
-
-			if (otx2_cpt_eng_grp_has_eng_type(grp, eng_type)) {
-				eng_grp_num = i;
-				break;
-			}
-		}
-		break;
-	}
-	return eng_grp_num;
-}
-
-static int delete_eng_caps_discovery_grps(struct pci_dev *pdev,
-					  struct otx2_cpt_eng_grps *eng_grps)
-{
-	struct otx2_cpt_eng_grp_info *grp;
-	int i, ret;
-
-	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
-		grp = &eng_grps->grp[i];
-		ret = delete_engine_group(&pdev->dev, grp);
-		if (ret)
-			return ret;
-	}
-	return ret;
-}
-
-static int create_engine_group(struct device *dev,
-			       struct otx2_cpt_eng_grps *eng_grps,
-			       struct otx2_cpt_engines *engs, int engs_cnt,
-			       void *ucode_data[], int ucodes_cnt,
-			       bool use_uc_from_tar_arch)
-{
-	struct otx2_cpt_eng_grp_info *mirrored_eng_grp;
-	struct tar_ucode_info_t *tar_ucode_info;
-	struct otx2_cpt_eng_grp_info *eng_grp;
-	int i, ret = 0;
-
-	if (ucodes_cnt > OTX2_CPT_MAX_ETYPES_PER_GRP)
-		return -EINVAL;
-
-	/* Validate if requested engine types are supported by this device */
-	for (i = 0; i < engs_cnt; i++)
-		if (!dev_supports_eng_type(eng_grps, engs[i].type)) {
-			dev_err(dev, "Device does not support %s engines\n",
-				get_eng_type_str(engs[i].type));
-			return -EPERM;
-		}
-
-	/* Find engine group which is not used */
-	eng_grp = find_unused_eng_grp(eng_grps);
-	if (!eng_grp) {
-		dev_err(dev, "Error all engine groups are being used\n");
-		return -ENOSPC;
-	}
-
-	/* Load ucode */
-	for (i = 0; i < ucodes_cnt; i++) {
-		if (use_uc_from_tar_arch) {
-			tar_ucode_info =
-				     (struct tar_ucode_info_t *) ucode_data[i];
-			eng_grp->ucode[i] = tar_ucode_info->ucode;
-			ret = copy_ucode_to_dma_mem(dev, &eng_grp->ucode[i],
-						    tar_ucode_info->ucode_ptr);
-		} else
-			ret = ucode_load(dev, &eng_grp->ucode[i],
-					 (char *) ucode_data[i]);
-		if (ret)
-			goto err_ucode_unload;
-	}
-
-	if (ucodes_cnt > 1) {
-		/*
-		 * Validate scenario where 2 ucodes are used - this
-		 * is only allowed for combination of SE+IE ucodes
-		 */
-		ret = validate_2_ucodes_scenario(dev, eng_grp);
-		if (ret)
-			goto err_ucode_unload;
-	} else {
-		/* Validate scenario where 1 ucode is used */
-		ret = validate_1_ucode_scenario(dev, eng_grp, engs, engs_cnt);
-		if (ret)
-			goto err_ucode_unload;
-	}
+	/* Find engine group which is not used */
+	eng_grp = find_unused_eng_grp(eng_grps);
+	if (!eng_grp) {
+		dev_err(dev, "Error all engine groups are being used\n");
+		return -ENOSPC;
+	}
+	/* Load ucode */
+	for (i = 0; i < ucodes_cnt; i++) {
+		uc_info = (struct otx2_cpt_uc_info_t *) ucode_data[i];
+		eng_grp->ucode[i] = uc_info->ucode;
+		ret = copy_ucode_to_dma_mem(dev, &eng_grp->ucode[i],
+					    uc_info->fw->data);
+		if (ret)
+			goto unload_ucode;
+	}
 
 	/* Check if this group mirrors another existing engine group */
 	mirrored_eng_grp = find_mirrored_eng_grp(eng_grp);
@@ -1598,12 +1003,11 @@ static int create_engine_group(struct device *dev,
 		 * Update count of requested engines because some
 		 * of them might be shared with mirrored group
 		 */
-		update_requested_engs(mirrored_eng_grp, engs, engs_cnt);
+		update_requested_engs(mirrored_eng_grp, engs, ucodes_cnt);
 	}
-
-	ret = reserve_engines(dev, eng_grp, engs, engs_cnt);
+	ret = reserve_engines(dev, eng_grp, engs, ucodes_cnt);
 	if (ret)
-		goto err_ucode_unload;
+		goto unload_ucode;
 
 	/* Update ucode pointers used by engines */
 	update_ucode_ptrs(eng_grp);
@@ -1611,17 +1015,12 @@ static int create_engine_group(struct device *dev,
 	/* Update engine masks used by this group */
 	ret = eng_grp_update_masks(dev, eng_grp);
 	if (ret)
-		goto err_release_engs;
-
-	/* Create sysfs entry for engine group info */
-	ret = create_sysfs_eng_grps_info(dev, eng_grp);
-	if (ret)
-		goto err_release_engs;
+		goto release_engs;
 
 	/* Enable engine group */
 	ret = enable_eng_grp(eng_grp, eng_grps->obj);
 	if (ret)
-		goto err_release_engs;
+		goto release_engs;
 
 	/*
 	 * If this engine group mirrors another engine group
@@ -1632,6 +1031,10 @@ static int create_engine_group(struct device *dev,
 		ucode_unload(dev, &eng_grp->ucode[0]);
 
 	eng_grp->is_enabled = true;
+
+	if (!is_print)
+		return 0;
+
 	if (mirrored_eng_grp)
 		dev_info(dev,
 			 "Engine_group%d: reuse microcode %s from group %d\n",
@@ -1646,439 +1049,148 @@ static int create_engine_group(struct device *dev,
 
 	return 0;
 
-err_release_engs:
+release_engs:
 	release_engines(dev, eng_grp);
-err_ucode_unload:
+unload_ucode:
 	ucode_unload(dev, &eng_grp->ucode[0]);
 	ucode_unload(dev, &eng_grp->ucode[1]);
 	return ret;
 }
 
-static ssize_t ucode_load_store(struct device *dev,
-				struct device_attribute *attr,
-				const char *buf, size_t count)
+static void delete_engine_grps(struct pci_dev *pdev,
+			       struct otx2_cpt_eng_grps *eng_grps)
 {
-	struct otx2_cpt_engines engs[OTX2_CPT_MAX_ETYPES_PER_GRP] = { 0 };
-	char *ucode_filename[OTX2_CPT_MAX_ETYPES_PER_GRP];
-	char tmp_buf[OTX2_CPT_NAME_LENGTH] = { 0 };
-	struct otx2_cpt_eng_grps *eng_grps;
-	char *start, *val, *err_msg, *tmp;
-	int grp_idx = 0, ret = -EINVAL;
-	bool has_se, has_ie, has_ae;
-	int del_grp_idx = -1;
-	int ucode_idx = 0;
-
-	if (strlen(buf) > OTX2_CPT_NAME_LENGTH)
-		return -EINVAL;
-
-	eng_grps = container_of(attr, struct otx2_cpt_eng_grps,
-				ucode_load_attr);
-	err_msg = "Invalid engine group format";
-	strlcpy(tmp_buf, buf, OTX2_CPT_NAME_LENGTH);
-	start = tmp_buf;
-
-	has_se = has_ie = has_ae = false;
-
-	for (;;) {
-		val = strsep(&start, ";");
-		if (!val)
-			break;
-		val = strim(val);
-		if (!*val)
-			continue;
-
-		if (!strncasecmp(val, "engine_group", 12)) {
-			if (del_grp_idx != -1)
-				goto err_print;
-			tmp = strim(strsep(&val, ":"));
-			if (!val)
-				goto err_print;
-			if (strlen(tmp) != 13)
-				goto err_print;
-			if (kstrtoint((tmp + 12), 10, &del_grp_idx))
-				goto err_print;
-			val = strim(val);
-			if (strncasecmp(val, "null", 4))
-				goto err_print;
-			if (strlen(val) != 4)
-				goto err_print;
-		} else if (!strncasecmp(val, "se", 2) && strchr(val, ':')) {
-			if (has_se || ucode_idx)
-				goto err_print;
-			tmp = strim(strsep(&val, ":"));
-			if (!val)
-				goto err_print;
-			if (strlen(tmp) != 2)
-				goto err_print;
-			if (kstrtoint(strim(val), 10, &engs[grp_idx].count))
-				goto err_print;
-			engs[grp_idx++].type = OTX2_CPT_SE_TYPES;
-			has_se = true;
-		} else if (!strncasecmp(val, "ae", 2) && strchr(val, ':')) {
-			if (has_ae || ucode_idx)
-				goto err_print;
-			tmp = strim(strsep(&val, ":"));
-			if (!val)
-				goto err_print;
-			if (strlen(tmp) != 2)
-				goto err_print;
-			if (kstrtoint(strim(val), 10, &engs[grp_idx].count))
-				goto err_print;
-			engs[grp_idx++].type = OTX2_CPT_AE_TYPES;
-			has_ae = true;
-		} else if (!strncasecmp(val, "ie", 2) && strchr(val, ':')) {
-			if (has_ie || ucode_idx)
-				goto err_print;
-			tmp = strim(strsep(&val, ":"));
-			if (!val)
-				goto err_print;
-			if (strlen(tmp) != 2)
-				goto err_print;
-			if (kstrtoint(strim(val), 10, &engs[grp_idx].count))
-				goto err_print;
-			engs[grp_idx++].type = OTX2_CPT_IE_TYPES;
-			has_ie = true;
-		} else {
-			if (ucode_idx > 1)
-				goto err_print;
-			if (!strlen(val))
-				goto err_print;
-			if (strnstr(val, " ", strlen(val)))
-				goto err_print;
-			ucode_filename[ucode_idx++] = val;
-		}
-	}
-
-	/* Validate input parameters */
-	if (del_grp_idx == -1) {
-		if (!(grp_idx && ucode_idx))
-			goto err_print;
-
-		if (ucode_idx > 1 && grp_idx < 2)
-			goto err_print;
-
-		if (grp_idx > OTX2_CPT_MAX_ETYPES_PER_GRP) {
-			err_msg = "Error max 2 engine types can be attached";
-			goto err_print;
-		}
-
-		if (grp_idx > 1) {
-			if ((engs[0].type + engs[1].type) !=
-			    (OTX2_CPT_SE_TYPES + OTX2_CPT_IE_TYPES)) {
-				err_msg =
-				"Only combination of SE+IE engines is allowed";
-				goto err_print;
-			}
-
-			/* Keep SE engines at zero index */
-			if (engs[1].type == OTX2_CPT_SE_TYPES)
-				swap_engines(&engs[0], &engs[1]);
-		}
-
-	} else {
-		if (del_grp_idx < 0 || del_grp_idx >=
-						OTX2_CPT_MAX_ENGINE_GROUPS) {
-			dev_err(dev, "Invalid engine group index %d\n",
-				del_grp_idx);
-			return -EINVAL;
-		}
-
-		if (!eng_grps->grp[del_grp_idx].is_enabled) {
-			dev_err(dev, "Error engine_group%d is not configured\n",
-				del_grp_idx);
-			return -EINVAL;
-		}
-
-		if (grp_idx || ucode_idx)
-			goto err_print;
-	}
-
-	mutex_lock(&eng_grps->lock);
-
-	if (eng_grps->is_rdonly) {
-		dev_err(dev, "Disable VFs before modifying engine groups\n");
-		ret = -EACCES;
-		goto err_unlock;
-	}
-
-	if (del_grp_idx == -1)
-		/* create engine group */
-		ret = create_engine_group(dev, eng_grps, engs, grp_idx,
-					  (void **) ucode_filename,
-					  ucode_idx, false);
-	else
-		/* delete engine group */
-		ret = delete_engine_group(dev, &eng_grps->grp[del_grp_idx]);
-	if (ret)
-		goto err_unlock;
+	int i;
 
-	print_dbg_info(dev, eng_grps);
-err_unlock:
-	mutex_unlock(&eng_grps->lock);
-	return ret ? ret : count;
-err_print:
-	dev_err(dev, "%s\n", err_msg);
+	/* First delete all mirroring engine groups */
+	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++)
+		if (eng_grps->grp[i].mirror.is_ena)
+			delete_engine_group(&pdev->dev, &eng_grps->grp[i]);
 
-	return ret;
+	/* Delete remaining engine groups */
+	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++)
+		delete_engine_group(&pdev->dev, &eng_grps->grp[i]);
 }
 
-static int create_eng_caps_discovery_grps(struct pci_dev *pdev,
-					  struct otx2_cpt_eng_grps *eng_grps)
+int otx2_cpt_get_eng_grp(struct otx2_cpt_eng_grps *eng_grps, int eng_type)
 {
-	struct tar_ucode_info_t *tar_info[OTX2_CPT_MAX_ETYPES_PER_GRP] = { 0 };
-	struct otx2_cpt_engines engs[OTX2_CPT_MAX_ETYPES_PER_GRP] = { 0 };
-	struct tar_arch_info_t *tar_arch = NULL;
-	char tar_filename[OTX2_CPT_NAME_LENGTH];
-	int ret = -EINVAL;
-
-	sprintf(tar_filename, "cpt%02d-mc.tar", pdev->revision);
-	tar_arch = load_tar_archive(&pdev->dev, tar_filename);
-	if (!tar_arch)
-		return -EINVAL;
-	/*
-	 * If device supports AE engines and there is AE microcode in tar
-	 * archive try to create engine group with AE engines.
-	 */
-	tar_info[0] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_AE_TYPES);
-	if (tar_info[0] && dev_supports_eng_type(eng_grps, OTX2_CPT_AE_TYPES)) {
 
-		engs[0].type = OTX2_CPT_AE_TYPES;
-		engs[0].count = 2;
-
-		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
-					  (void **) tar_info, 1, true);
-		if (ret)
-			goto release_tar;
-	}
-	/*
-	 * If device supports SE engines and there is SE microcode in tar
-	 * archive try to create engine group with SE engines.
-	 */
-	tar_info[0] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_SE_TYPES);
-	if (tar_info[0] && dev_supports_eng_type(eng_grps, OTX2_CPT_SE_TYPES)) {
-
-		engs[0].type = OTX2_CPT_SE_TYPES;
-		engs[0].count = 2;
-
-		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
-					  (void **) tar_info, 1, true);
-		if (ret)
-			goto release_tar;
-	}
-	/*
-	 * If device supports IE engines and there is IE microcode in tar
-	 * archive try to create engine group with IE engines.
-	 */
-	tar_info[0] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_IE_TYPES);
-	if (tar_info[0] && dev_supports_eng_type(eng_grps, OTX2_CPT_IE_TYPES)) {
+	int eng_grp_num = OTX2_CPT_INVALID_CRYPTO_ENG_GRP;
+	struct otx2_cpt_eng_grp_info *grp;
+	int i;
 
-		engs[0].type = OTX2_CPT_IE_TYPES;
-		engs[0].count = 2;
+	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
+		grp = &eng_grps->grp[i];
+		if (!grp->is_enabled)
+			continue;
 
-		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
-					  (void **) tar_info, 1, true);
-		if (ret)
-			goto release_tar;
+		if (eng_type == OTX2_CPT_SE_TYPES) {
+			if (eng_grp_has_eng_type(grp, eng_type) &&
+			    !eng_grp_has_eng_type(grp, OTX2_CPT_IE_TYPES)) {
+				eng_grp_num = i;
+				break;
+			}
+		} else {
+			if (eng_grp_has_eng_type(grp, eng_type)) {
+				eng_grp_num = i;
+				break;
+			}
+		}
 	}
-release_tar:
-	release_tar_archive(tar_arch);
-	return ret;
+	return eng_grp_num;
 }
 
-/*
- * Get CPT HW capabilities using LOAD_FVC operation.
- */
-int otx2_cpt_discover_eng_capabilities(void *obj)
+int otx2_cpt_create_eng_grps(struct pci_dev *pdev,
+			     struct otx2_cpt_eng_grps *eng_grps)
 {
-	struct otx2_cptpf_dev *cptpf = obj;
-	struct otx2_cpt_iq_command iq_cmd;
-	union otx2_cpt_opcode_info opcode;
-	union otx2_cpt_res_s *result;
-	union otx2_cpt_inst_s inst;
-	dma_addr_t rptr_baddr;
-	struct pci_dev *pdev;
-	u32 len, compl_rlen;
-	int ret, etype;
-	void *rptr;
+	struct otx2_cpt_uc_info_t *uc_info[OTX2_CPT_MAX_ETYPES_PER_GRP] = {  };
+	struct otx2_cpt_engines engs[OTX2_CPT_MAX_ETYPES_PER_GRP] = { {0} };
+	struct fw_info_t fw_info;
+	int ret;
 
 	/*
-	 * We don't get capabilities if it was already done
-	 * (when user enabled VFs for the first time)
+	 * We don't create engine groups if it was already
+	 * made (when user enabled VFs for the first time)
 	 */
-	if (cptpf->is_eng_caps_discovered)
+	if (eng_grps->is_grps_created)
 		return 0;
 
-	pdev = cptpf->pdev;
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = create_eng_caps_discovery_grps(pdev, &cptpf->eng_grps);
-	if (ret)
-		goto delete_grps;
-
-	ret = otx2_cptpf_lf_init(cptpf, OTX2_CPT_ALL_ENG_GRPS_MASK,
-				 OTX2_CPT_QUEUE_HI_PRIO, 1);
+	ret = cpt_ucode_load_fw(pdev, &fw_info);
 	if (ret)
-		goto delete_grps;
-
-	compl_rlen = ALIGN(sizeof(union otx2_cpt_res_s), OTX2_CPT_DMA_MINALIGN);
-	len = compl_rlen + LOADFVC_RLEN;
-
-	result = kzalloc(len, GFP_KERNEL);
-	if (!result) {
-		ret = -ENOMEM;
-		goto lf_cleanup;
-	}
-	rptr_baddr = dma_map_single(&pdev->dev, (void *)result, len,
-				    DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(&pdev->dev, rptr_baddr)) {
-		dev_err(&pdev->dev, "DMA mapping failed\n");
-		ret = -EFAULT;
-		goto free_result;
-	}
-	rptr = (u8 *)result + compl_rlen;
-
-	/* Fill in the command */
-	opcode.s.major = LOADFVC_MAJOR_OP;
-	opcode.s.minor = LOADFVC_MINOR_OP;
-
-	iq_cmd.cmd.u64 = 0;
-	iq_cmd.cmd.s.opcode = cpu_to_be16(opcode.flags);
-
-	/* 64-bit swap for microcode data reads, not needed for addresses */
-	cpu_to_be64s(&iq_cmd.cmd.u64);
-	iq_cmd.dptr = 0;
-	iq_cmd.rptr = rptr_baddr + compl_rlen;
-	iq_cmd.cptr.u64 = 0;
-
-	for (etype = 1; etype < OTX2_CPT_MAX_ENG_TYPES; etype++) {
-		result->s.compcode = OTX2_CPT_COMPLETION_CODE_INIT;
-		iq_cmd.cptr.s.grp = get_eng_caps_discovery_grp(
-						&cptpf->eng_grps, etype);
-		otx2_cpt_fill_inst(&inst, &iq_cmd, rptr_baddr);
-		otx2_cpt_send_cmd(&inst, 1, &cptpf->lfs.lf[0]);
-
-		while (result->s.compcode == OTX2_CPT_COMPLETION_CODE_INIT)
-			cpu_relax();
-
-		cptpf->eng_caps[etype].u = be64_to_cpup(rptr);
-	}
-	dma_unmap_single(&pdev->dev, rptr_baddr, len, DMA_BIDIRECTIONAL);
-	cptpf->is_eng_caps_discovered = true;
-free_result:
-	kzfree(result);
-lf_cleanup:
-	otx2_cptpf_lf_cleanup(&cptpf->lfs);
-delete_grps:
-	delete_eng_caps_discovery_grps(pdev, &cptpf->eng_grps);
-
-	return ret;
-}
-
-
-int otx2_cpt_try_create_default_eng_grps(struct pci_dev *pdev,
-					 struct otx2_cpt_eng_grps *eng_grps)
-{
-	struct tar_ucode_info_t *tar_info[OTX2_CPT_MAX_ETYPES_PER_GRP] = { 0 };
-	struct otx2_cpt_engines engs[OTX2_CPT_MAX_ETYPES_PER_GRP] = { 0 };
-	struct tar_arch_info_t *tar_arch = NULL;
-	char tar_filename[OTX2_CPT_NAME_LENGTH];
-	int i, ret = 0;
-
-	mutex_lock(&eng_grps->lock);
-
-	/*
-	 * We don't create engine group for kernel crypto if attempt to create
-	 * it was already made (when user enabled VFs for the first time)
-	 */
-	if (eng_grps->is_first_try)
-		goto unlock_mutex;
-	eng_grps->is_first_try = true;
-
-	/* We create group for kcrypto only if no groups are configured */
-	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++)
-		if (eng_grps->grp[i].is_enabled)
-			goto unlock_mutex;
-
-	sprintf(tar_filename, "cpt%02d-mc.tar", pdev->revision);
-
-	tar_arch = load_tar_archive(&pdev->dev, tar_filename);
-	if (!tar_arch)
-		goto unlock_mutex;
+		return ret;
 
 	/*
-	 * If device supports SE engines and there is SE microcode in tar
-	 * archive try to create engine group with SE engines for kernel
+	 * Create engine group with SE engines for kernel
 	 * crypto functionality (symmetric crypto)
 	 */
-	tar_info[0] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_SE_TYPES);
-	if (tar_info[0] && dev_supports_eng_type(eng_grps, OTX2_CPT_SE_TYPES)) {
-
-		engs[0].type = OTX2_CPT_SE_TYPES;
-		engs[0].count = eng_grps->avail.max_se_cnt;
-
-		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
-					  (void **) tar_info, 1, true);
-		if (ret)
-			goto release_tar_arch;
+	uc_info[0] = get_ucode(&fw_info, OTX2_CPT_SE_TYPES);
+	if (uc_info[0] == NULL) {
+		dev_err(&pdev->dev, "Unable to find firmware for SE\n");
+		ret = -EINVAL;
+		goto release_fw;
 	}
+	engs[0].type = OTX2_CPT_SE_TYPES;
+	engs[0].count = eng_grps->avail.max_se_cnt;
+
+	ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+				  (void **) uc_info, 1);
+	if (ret)
+		goto release_fw;
 
 	/*
-	 * If device supports SE+IE engines and there is SE and IE microcode in
-	 * tar archive try to create engine group with SE+IE engines for IPSec.
+	 * Create engine group with SE+IE engines for IPSec.
 	 * All SE engines will be shared with engine group 0.
 	 */
-	tar_info[0] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_SE_TYPES);
-	tar_info[1] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_IE_TYPES);
-	if (tar_info[0] && tar_info[1] &&
-	    dev_supports_eng_type(eng_grps, OTX2_CPT_SE_TYPES) &&
-	    dev_supports_eng_type(eng_grps, OTX2_CPT_IE_TYPES)) {
-
-		engs[0].type = OTX2_CPT_SE_TYPES;
-		engs[0].count = eng_grps->avail.max_se_cnt;
-		engs[1].type = OTX2_CPT_IE_TYPES;
-		engs[1].count = eng_grps->avail.max_ie_cnt;
-
-		ret = create_engine_group(&pdev->dev, eng_grps, engs, 2,
-					  (void **) tar_info, 2, true);
-		if (ret)
-			goto release_tar_arch;
+	uc_info[0] = get_ucode(&fw_info, OTX2_CPT_SE_TYPES);
+	uc_info[1] = get_ucode(&fw_info, OTX2_CPT_IE_TYPES);
+
+	if (uc_info[1] == NULL) {
+		dev_err(&pdev->dev, "Unable to find firmware for IE");
+		ret = -EINVAL;
+		goto delete_eng_grp;
 	}
+	engs[0].type = OTX2_CPT_SE_TYPES;
+	engs[0].count = eng_grps->avail.max_se_cnt;
+	engs[1].type = OTX2_CPT_IE_TYPES;
+	engs[1].count = eng_grps->avail.max_ie_cnt;
+
+	ret = create_engine_group(&pdev->dev, eng_grps, engs, 2,
+				  (void **) uc_info, 1);
+	if (ret)
+		goto delete_eng_grp;
 
 	/*
-	 * If device supports AE engines and there is AE microcode in tar
-	 * archive try to create engine group with AE engines for asymmetric
+	 * Create engine group with AE engines for asymmetric
 	 * crypto functionality.
 	 */
-	tar_info[0] = get_uc_from_tar_archive(tar_arch, OTX2_CPT_AE_TYPES);
-	if (tar_info[0] && dev_supports_eng_type(eng_grps, OTX2_CPT_AE_TYPES)) {
-
-		engs[0].type = OTX2_CPT_AE_TYPES;
-		engs[0].count = eng_grps->avail.max_ae_cnt;
-
-		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
-					  (void **) tar_info, 1, true);
-		if (ret)
-			goto release_tar_arch;
+	uc_info[0] = get_ucode(&fw_info, OTX2_CPT_AE_TYPES);
+	if (uc_info[0] == NULL) {
+		dev_err(&pdev->dev, "Unable to find firmware for AE");
+		ret = -EINVAL;
+		goto delete_eng_grp;
 	}
+	engs[0].type = OTX2_CPT_AE_TYPES;
+	engs[0].count = eng_grps->avail.max_ae_cnt;
 
-	print_dbg_info(&pdev->dev, eng_grps);
-release_tar_arch:
-	release_tar_archive(tar_arch);
-unlock_mutex:
-	mutex_unlock(&eng_grps->lock);
-	return ret;
-}
+	ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+				  (void **) uc_info, 1);
+	if (ret)
+		goto delete_eng_grp;
 
-void otx2_cpt_set_eng_grps_is_rdonly(struct otx2_cpt_eng_grps *eng_grps,
-				     bool is_rdonly)
-{
-	mutex_lock(&eng_grps->lock);
+	eng_grps->is_grps_created = true;
 
-	eng_grps->is_rdonly = is_rdonly;
+	cpt_ucode_release_fw(&fw_info);
+	return 0;
 
-	mutex_unlock(&eng_grps->lock);
+delete_eng_grp:
+	delete_engine_grps(pdev, eng_grps);
+release_fw:
+	cpt_ucode_release_fw(&fw_info);
+	return ret;
 }
 
-static int cptx_disable_all_cores(struct otx2_cptpf_dev *cptpf, int total_cores)
+static int cptx_disable_all_cores(struct otx2_cptpf_dev *cptpf, int total_cores,
+				  int blkaddr)
 {
 	int timeout = 10, ret;
 	int i, busy;
@@ -2086,14 +1198,15 @@ static int cptx_disable_all_cores(struct otx2_cptpf_dev *cptpf, int total_cores)
 
 	/* Disengage the cores from groups */
 	for (i = 0; i < total_cores; i++) {
-		ret = otx2_cpt_add_write_af_reg(cptpf->pdev,
-						CPT_AF_EXEX_CTL2(i), 0x0);
+		ret = otx2_cpt_add_write_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+						CPT_AF_EXEX_CTL2(i), 0x0,
+						blkaddr);
 		if (ret)
 			return ret;
 
 		cptpf->eng_grps.eng_ref_cnt[i] = 0;
 	}
-	ret = otx2_cpt_send_af_reg_requests(cptpf->pdev);
+	ret = otx2_cpt_send_af_reg_requests(&cptpf->afpf_mbox, cptpf->pdev);
 	if (ret)
 		return ret;
 
@@ -2105,8 +1218,10 @@ static int cptx_disable_all_cores(struct otx2_cptpf_dev *cptpf, int total_cores)
 			return -EBUSY;
 
 		for (i = 0; i < total_cores; i++) {
-			ret = otx2_cpt_read_af_reg(cptpf->pdev,
-						   CPT_AF_EXEX_STS(i), &reg);
+			ret = otx2_cpt_read_af_reg(&cptpf->afpf_mbox,
+						   cptpf->pdev,
+						   CPT_AF_EXEX_STS(i), &reg,
+						   blkaddr);
 			if (ret)
 				return ret;
 
@@ -2119,14 +1234,13 @@ static int cptx_disable_all_cores(struct otx2_cptpf_dev *cptpf, int total_cores)
 
 	/* Disable the cores */
 	for (i = 0; i < total_cores; i++) {
-		ret = otx2_cpt_add_write_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL(i),
-						0x0);
+		ret = otx2_cpt_add_write_af_reg(&cptpf->afpf_mbox, cptpf->pdev,
+						CPT_AF_EXEX_CTL(i), 0x0,
+						blkaddr);
 		if (ret)
 			return ret;
 	}
-	ret = otx2_cpt_send_af_reg_requests(cptpf->pdev);
-
-	return ret;
+	return otx2_cpt_send_af_reg_requests(&cptpf->afpf_mbox, cptpf->pdev);
 }
 
 int otx2_cpt_disable_all_cores(struct otx2_cptpf_dev *cptpf)
@@ -2137,16 +1251,12 @@ int otx2_cpt_disable_all_cores(struct otx2_cptpf_dev *cptpf)
 		      cptpf->eng_grps.avail.max_ie_cnt +
 		      cptpf->eng_grps.avail.max_ae_cnt;
 
-	if (cptpf->cpt1_implemented) {
-		cptpf->blkaddr = BLKADDR_CPT1;
-		ret = cptx_disable_all_cores(cptpf, total_cores);
+	if (cptpf->has_cpt1) {
+		ret = cptx_disable_all_cores(cptpf, total_cores, BLKADDR_CPT1);
 		if (ret)
 			return ret;
 	}
-	cptpf->blkaddr = BLKADDR_CPT0;
-	ret = cptx_disable_all_cores(cptpf, total_cores);
-
-	return ret;
+	return cptx_disable_all_cores(cptpf, total_cores, BLKADDR_CPT0);
 }
 
 void otx2_cpt_cleanup_eng_grps(struct pci_dev *pdev,
@@ -2155,22 +1265,7 @@ void otx2_cpt_cleanup_eng_grps(struct pci_dev *pdev,
 	struct otx2_cpt_eng_grp_info *grp;
 	int i, j;
 
-	mutex_lock(&eng_grps->lock);
-	if (eng_grps->is_ucode_load_created) {
-		device_remove_file(&pdev->dev,
-				   &eng_grps->ucode_load_attr);
-		eng_grps->is_ucode_load_created = false;
-	}
-
-	/* First delete all mirroring engine groups */
-	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++)
-		if (eng_grps->grp[i].mirror.is_ena)
-			delete_engine_group(&pdev->dev, &eng_grps->grp[i]);
-
-	/* Delete remaining engine groups */
-	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++)
-		delete_engine_group(&pdev->dev, &eng_grps->grp[i]);
-
+	delete_engine_grps(pdev, eng_grps);
 	/* Release memory */
 	for (i = 0; i < OTX2_CPT_MAX_ENGINE_GROUPS; i++) {
 		grp = &eng_grps->grp[i];
@@ -2179,7 +1274,6 @@ void otx2_cpt_cleanup_eng_grps(struct pci_dev *pdev,
 			grp->engs[j].bmap = NULL;
 		}
 	}
-	mutex_unlock(&eng_grps->lock);
 }
 
 int otx2_cpt_init_eng_grps(struct pci_dev *pdev,
@@ -2188,7 +1282,6 @@ int otx2_cpt_init_eng_grps(struct pci_dev *pdev,
 	struct otx2_cpt_eng_grp_info *grp;
 	int i, j, ret;
 
-	mutex_init(&eng_grps->lock);
 	eng_grps->obj = pci_get_drvdata(pdev);
 	eng_grps->avail.se_cnt = eng_grps->avail.max_se_cnt;
 	eng_grps->avail.ie_cnt = eng_grps->avail.max_ie_cnt;
@@ -2210,8 +1303,6 @@ int otx2_cpt_init_eng_grps(struct pci_dev *pdev,
 		grp->g = eng_grps;
 		grp->idx = i;
 
-		snprintf(grp->sysfs_info_name, OTX2_CPT_NAME_LENGTH,
-			 "engine_group%d", i);
 		for (j = 0; j < OTX2_CPT_MAX_ETYPES_PER_GRP; j++) {
 			grp->engs[j].bmap =
 				kcalloc(BITS_TO_LONGS(eng_grps->engs_num),
@@ -2222,25 +1313,169 @@ int otx2_cpt_init_eng_grps(struct pci_dev *pdev,
 			}
 		}
 	}
+	return 0;
+
+cleanup_eng_grps:
+	otx2_cpt_cleanup_eng_grps(pdev, eng_grps);
+	return ret;
+}
+
+static int create_eng_caps_discovery_grps(struct pci_dev *pdev,
+					  struct otx2_cpt_eng_grps *eng_grps)
+{
+	struct otx2_cpt_uc_info_t *uc_info[OTX2_CPT_MAX_ETYPES_PER_GRP] = {  };
+	struct otx2_cpt_engines engs[OTX2_CPT_MAX_ETYPES_PER_GRP] = { {0} };
+	struct fw_info_t fw_info;
+	int ret;
+
+	ret = cpt_ucode_load_fw(pdev, &fw_info);
+	if (ret)
+		return ret;
+
+	uc_info[0] = get_ucode(&fw_info, OTX2_CPT_SE_TYPES);
+	if (uc_info[0] == NULL) {
+		dev_err(&pdev->dev, "Unable to find firmware for AE\n");
+		ret = -EINVAL;
+		goto release_fw;
+	}
+	engs[0].type = OTX2_CPT_AE_TYPES;
+	engs[0].count = 2;
 
-	eng_grps->eng_types_supported = 1 << OTX2_CPT_SE_TYPES |
-					1 << OTX2_CPT_IE_TYPES |
-					1 << OTX2_CPT_AE_TYPES;
-
-	eng_grps->ucode_load_attr.show = NULL;
-	eng_grps->ucode_load_attr.store = ucode_load_store;
-	eng_grps->ucode_load_attr.attr.name = "ucode_load";
-	eng_grps->ucode_load_attr.attr.mode = 0220;
-	sysfs_attr_init(&eng_grps->ucode_load_attr.attr);
-	ret = device_create_file(&pdev->dev,
-				 &eng_grps->ucode_load_attr);
+	ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+				  (void **) uc_info, 0);
 	if (ret)
-		goto cleanup_eng_grps;
-	eng_grps->is_ucode_load_created = true;
+		goto release_fw;
 
-	print_dbg_info(&pdev->dev, eng_grps);
+	uc_info[0] = get_ucode(&fw_info, OTX2_CPT_SE_TYPES);
+	if (uc_info[0] == NULL) {
+		dev_err(&pdev->dev, "Unable to find firmware for SE\n");
+		ret = -EINVAL;
+		goto delete_eng_grp;
+	}
+	engs[0].type = OTX2_CPT_SE_TYPES;
+	engs[0].count = 2;
+
+	ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+				  (void **) uc_info, 0);
+	if (ret)
+		goto delete_eng_grp;
+
+	uc_info[0] = get_ucode(&fw_info, OTX2_CPT_IE_TYPES);
+	if (uc_info[0] == NULL) {
+		dev_err(&pdev->dev, "Unable to find firmware for IE\n");
+		ret = -EINVAL;
+		goto delete_eng_grp;
+	}
+	engs[0].type = OTX2_CPT_IE_TYPES;
+	engs[0].count = 2;
+
+	ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+				  (void **) uc_info, 0);
+	if (ret)
+		goto delete_eng_grp;
+
+	cpt_ucode_release_fw(&fw_info);
 	return 0;
-cleanup_eng_grps:
-	otx2_cpt_cleanup_eng_grps(pdev, eng_grps);
+
+delete_eng_grp:
+	delete_engine_grps(pdev, eng_grps);
+release_fw:
+	cpt_ucode_release_fw(&fw_info);
+	return ret;
+}
+
+/*
+ * Get CPT HW capabilities using LOAD_FVC operation.
+ */
+int otx2_cpt_discover_eng_capabilities(struct otx2_cptpf_dev *cptpf)
+{
+	struct otx2_cptlfs_info *lfs = &cptpf->lfs;
+	struct otx2_cpt_iq_command iq_cmd;
+	union otx2_cpt_opcode opcode;
+	union otx2_cpt_res_s *result;
+	union otx2_cpt_inst_s inst;
+	dma_addr_t rptr_baddr;
+	struct pci_dev *pdev;
+	u32 len, compl_rlen;
+	int ret, etype;
+	void *rptr;
+
+	/*
+	 * We don't get capabilities if it was already done
+	 * (when user enabled VFs for the first time)
+	 */
+	if (cptpf->is_eng_caps_discovered)
+		return 0;
+
+	pdev = cptpf->pdev;
+	/*
+	 * Create engine groups for each type to submit LOAD_FVC op and
+	 * get engine's capabilities.
+	 */
+	ret = create_eng_caps_discovery_grps(pdev, &cptpf->eng_grps);
+	if (ret)
+		goto delete_grps;
+
+	lfs->pdev = pdev;
+	lfs->reg_base = cptpf->reg_base;
+	lfs->mbox = &cptpf->afpf_mbox;
+	lfs->blkaddr = BLKADDR_CPT0;
+	ret = otx2_cptlf_init(&cptpf->lfs, OTX2_CPT_ALL_ENG_GRPS_MASK,
+			      OTX2_CPT_QUEUE_HI_PRIO, 1);
+	if (ret)
+		goto delete_grps;
+
+	compl_rlen = ALIGN(sizeof(union otx2_cpt_res_s), OTX2_CPT_DMA_MINALIGN);
+	len = compl_rlen + LOADFVC_RLEN;
+
+	result = kzalloc(len, GFP_KERNEL);
+	if (!result) {
+		ret = -ENOMEM;
+		goto lf_cleanup;
+	}
+	rptr_baddr = dma_map_single(&pdev->dev, (void *)result, len,
+				    DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(&pdev->dev, rptr_baddr)) {
+		dev_err(&pdev->dev, "DMA mapping failed\n");
+		ret = -EFAULT;
+		goto free_result;
+	}
+	rptr = (u8 *)result + compl_rlen;
+
+	/* Fill in the command */
+	opcode.s.major = LOADFVC_MAJOR_OP;
+	opcode.s.minor = LOADFVC_MINOR_OP;
+
+	iq_cmd.cmd.u = 0;
+	iq_cmd.cmd.s.opcode = cpu_to_be16(opcode.flags);
+
+	/* 64-bit swap for microcode data reads, not needed for addresses */
+	cpu_to_be64s(&iq_cmd.cmd.u);
+	iq_cmd.dptr = 0;
+	iq_cmd.rptr = rptr_baddr + compl_rlen;
+	iq_cmd.cptr.u = 0;
+
+	for (etype = 1; etype < OTX2_CPT_MAX_ENG_TYPES; etype++) {
+		result->s.compcode = OTX2_CPT_COMPLETION_CODE_INIT;
+		iq_cmd.cptr.s.grp = otx2_cpt_get_eng_grp(&cptpf->eng_grps,
+							 etype);
+		otx2_cpt_fill_inst(&inst, &iq_cmd, rptr_baddr);
+		otx2_cpt_send_cmd(&inst, 1, &cptpf->lfs.lf[0]);
+
+		while (result->s.compcode == OTX2_CPT_COMPLETION_CODE_INIT)
+			cpu_relax();
+
+		cptpf->eng_caps[etype].u = be64_to_cpup(rptr);
+	}
+	dma_unmap_single(&pdev->dev, rptr_baddr, len, DMA_BIDIRECTIONAL);
+	cptpf->is_eng_caps_discovered = true;
+
+free_result:
+	kfree(result);
+lf_cleanup:
+	otx2_cptlf_shutdown(&cptpf->lfs);
+delete_grps:
+	delete_engine_grps(pdev, &cptpf->eng_grps);
+
 	return ret;
 }
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.h b/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.h
index 698867219f89..113d86cb0c88 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptpf_ucode.h
@@ -1,11 +1,5 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
 
 #ifndef __OTX2_CPTPF_UCODE_H
@@ -23,9 +17,6 @@
  */
 #define OTX2_CPT_MAX_ETYPES_PER_GRP 2
 
-/* CPT ucode alignment */
-#define OTX2_CPT_UCODE_ALIGNMENT    128
-
 /* CPT ucode signature size */
 #define OTX2_CPT_UCODE_SIGN_LEN     256
 
@@ -35,8 +26,7 @@
 /* Maximum number of supported engines/cores on OcteonTX2 platform */
 #define OTX2_CPT_MAX_ENGINES        128
 
-#define OTX2_CPT_ENGS_BITMASK_LEN   (OTX2_CPT_MAX_ENGINES/(BITS_PER_BYTE * \
-					 sizeof(unsigned long)))
+#define OTX2_CPT_ENGS_BITMASK_LEN   BITS_TO_LONGS(OTX2_CPT_MAX_ENGINES)
 
 /* Microcode types */
 enum otx2_cpt_ucode_type {
@@ -76,7 +66,7 @@ struct otx2_cpt_ucode_ver_num {
 struct otx2_cpt_ucode_hdr {
 	struct otx2_cpt_ucode_ver_num ver_num;
 	u8 ver_str[OTX2_CPT_UCODE_VER_STR_SZ];
-	u32 code_length;
+	__be32 code_length;
 	u32 padding[3];
 };
 
@@ -88,17 +78,15 @@ struct otx2_cpt_ucode {
 	struct otx2_cpt_ucode_ver_num ver_num;/* ucode version number */
 	char filename[OTX2_CPT_NAME_LENGTH];/* ucode filename */
 	dma_addr_t dma;		/* phys address of ucode image */
-	dma_addr_t align_dma;	/* aligned phys address of ucode image */
 	void *va;		/* virt address of ucode image */
-	void *align_va;		/* aligned virt address of ucode image */
 	u32 size;		/* ucode image size */
 	int type;		/* ucode image type SE, IE, AE or SE+IE */
 };
 
-struct tar_ucode_info_t {
+struct otx2_cpt_uc_info_t {
 	struct list_head list;
 	struct otx2_cpt_ucode ucode;/* microcode information */
-	const u8 *ucode_ptr;	/* pointer to microcode in tar archive */
+	const struct firmware *fw;
 };
 
 /* Maximum and current number of engines available for all engine groups */
@@ -139,13 +127,10 @@ struct otx2_cpt_mirror_info {
 
 struct otx2_cpt_eng_grp_info {
 	struct otx2_cpt_eng_grps *g; /* pointer to engine_groups structure */
-	struct device_attribute info_attr; /* group info entry attr */
 	/* engines attached */
 	struct otx2_cpt_engs_rsvd engs[OTX2_CPT_MAX_ETYPES_PER_GRP];
 	/* ucodes information */
 	struct otx2_cpt_ucode ucode[OTX2_CPT_MAX_ETYPES_PER_GRP];
-	/* sysfs info entry name */
-	char sysfs_info_name[OTX2_CPT_NAME_LENGTH];
 	/* engine group mirroring information */
 	struct otx2_cpt_mirror_info mirror;
 	int idx;	 /* engine group index */
@@ -157,30 +142,21 @@ struct otx2_cpt_eng_grp_info {
 
 struct otx2_cpt_eng_grps {
 	struct otx2_cpt_eng_grp_info grp[OTX2_CPT_MAX_ENGINE_GROUPS];
-	struct device_attribute ucode_load_attr;/* ucode load attr */
 	struct otx2_cpt_engs_available avail;
-	struct mutex lock;
 	void *obj;			/* device specific data */
 	int engs_num;			/* total number of engines supported */
-	int eng_types_supported;	/* engine types supported SE, IE, AE */
 	u8 eng_ref_cnt[OTX2_CPT_MAX_ENGINES];/* engines reference count */
-	bool is_ucode_load_created;	/* is ucode_load sysfs entry created */
-	bool is_first_try; /* is this first try to create kcrypto engine grp */
-	bool is_rdonly;	/* do engine groups configuration can be modified */
+	bool is_grps_created; /* Is the engine groups are already created */
 };
 struct otx2_cptpf_dev;
 int otx2_cpt_init_eng_grps(struct pci_dev *pdev,
 			   struct otx2_cpt_eng_grps *eng_grps);
 void otx2_cpt_cleanup_eng_grps(struct pci_dev *pdev,
 			       struct otx2_cpt_eng_grps *eng_grps);
-int otx2_cpt_try_create_default_eng_grps(struct pci_dev *pdev,
-					 struct otx2_cpt_eng_grps *eng_grps);
-void otx2_cpt_set_eng_grps_is_rdonly(struct otx2_cpt_eng_grps *eng_grps,
-				     bool is_rdonly);
-int otx2_cpt_uc_supports_eng_type(struct otx2_cpt_ucode *ucode, int eng_type);
-int otx2_cpt_eng_grp_has_eng_type(struct otx2_cpt_eng_grp_info *eng_grp,
-				  int eng_type);
+int otx2_cpt_create_eng_grps(struct pci_dev *pdev,
+			     struct otx2_cpt_eng_grps *eng_grps);
 int otx2_cpt_disable_all_cores(struct otx2_cptpf_dev *cptpf);
-int otx2_cpt_discover_eng_capabilities(void *obj);
+int otx2_cpt_get_eng_grp(struct otx2_cpt_eng_grps *eng_grps, int eng_type);
+int otx2_cpt_discover_eng_capabilities(struct otx2_cptpf_dev *cptpf);
 
 #endif /* __OTX2_CPTPF_UCODE_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptvf.h b/drivers/crypto/marvell/octeontx2/otx2_cptvf.h
index 51a661bc4c1b..b8ce3a6dc8a8 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptvf.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptvf.h
@@ -1,11 +1,5 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2018 Marvell.
  */
 
 #ifndef __OTX2_CPTVF_H
@@ -25,12 +19,12 @@ struct otx2_cptvf_dev {
 	struct otx2_mbox	pfvf_mbox;
 	struct work_struct	pfvf_mbox_work;
 	struct workqueue_struct *pfvf_mbox_wq;
-	u8 blkaddr;
+	int blkaddr;
 };
 
 irqreturn_t otx2_cptvf_pfvf_mbox_intr(int irq, void *arg);
 void otx2_cptvf_pfvf_mbox_handler(struct work_struct *work);
 int otx2_cptvf_send_eng_grp_num_msg(struct otx2_cptvf_dev *cptvf, int eng_type);
-int otx2_cptvf_send_kcrypto_limits_msg(struct otx2_cptvf_dev *cptvf);
+int otx2_cptvf_send_kvf_limits_msg(struct otx2_cptvf_dev *cptvf);
 
 #endif /* __OTX2_CPTVF_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.c b/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.c
index 7686d9490089..ddfa82566bfc 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.c
@@ -1,12 +1,5 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX CPT driver
- *
- * Copyright (C) 2019 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2019 Marvell. */
 
 #include <crypto/aes.h>
 #include <crypto/authenc.h>
@@ -24,7 +17,6 @@
 #include "otx2_cptvf_algs.h"
 #include "otx2_cpt_reqmgr.h"
 
-#define CPT_MAX_LF_NUM 64
 /* Size of salt in AES GCM mode */
 #define AES_GCM_SALT_SIZE 4
 /* Size of IV in AES GCM mode */
@@ -36,7 +28,7 @@
 #define CONTROL_WORD_LEN 8
 #define KEY2_OFFSET 48
 #define DMA_MODE_FLAG(dma_mode) \
-	(((dma_mode) == OTX2_CPT_DMA_GATHER_SCATTER) ? (1 << 7) : 0)
+	(((dma_mode) == OTX2_CPT_DMA_MODE_SG) ? (1 << 7) : 0)
 
 /* Truncated SHA digest size */
 #define SHA1_TRUNC_DIGEST_SIZE 12
@@ -54,17 +46,13 @@ struct cpt_device_desc {
 
 struct cpt_device_table {
 	atomic_t count;
-	struct cpt_device_desc desc[CPT_MAX_LF_NUM];
+	struct cpt_device_desc desc[OTX2_CPT_MAX_LFS_NUM];
 };
 
 static struct cpt_device_table se_devices = {
 	.count = ATOMIC_INIT(0)
 };
 
-static struct cpt_device_table ae_devices = {
-	.count = ATOMIC_INIT(0)
-};
-
 static inline int get_se_device(struct pci_dev **pdev, int *cpu_num)
 {
 	int count;
@@ -109,13 +97,13 @@ static inline int validate_hmac_cipher_null(struct otx2_cpt_req_info *cpt_req)
 
 static void otx2_cpt_aead_callback(int status, void *arg1, void *arg2)
 {
-	struct otx2_cpt_info_buffer *cpt_info = arg2;
+	struct otx2_cpt_inst_info *inst_info = arg2;
 	struct crypto_async_request *areq = arg1;
 	struct otx2_cpt_req_info *cpt_req;
 	struct pci_dev *pdev;
 
-	if (cpt_info) {
-		cpt_req = cpt_info->req;
+	if (inst_info) {
+		cpt_req = inst_info->req;
 		if (!status) {
 			/*
 			 * When selected cipher is NULL we need to manually
@@ -127,8 +115,8 @@ static void otx2_cpt_aead_callback(int status, void *arg1, void *arg2)
 			    !cpt_req->is_enc)
 				status = validate_hmac_cipher_null(cpt_req);
 		}
-		pdev = cpt_info->pdev;
-		do_request_cleanup(pdev, cpt_info);
+		pdev = inst_info->pdev;
+		otx2_cpt_info_destroy(pdev, inst_info);
 	}
 	if (areq)
 		areq->complete(areq, status);
@@ -137,9 +125,9 @@ static void otx2_cpt_aead_callback(int status, void *arg1, void *arg2)
 static void output_iv_copyback(struct crypto_async_request *areq)
 {
 	struct otx2_cpt_req_info *req_info;
+	struct otx2_cpt_req_ctx *rctx;
 	struct skcipher_request *sreq;
 	struct crypto_skcipher *stfm;
-	struct otx2_cpt_req_ctx *rctx;
 	struct otx2_cpt_enc_ctx *ctx;
 	u32 start, ivsize;
 
@@ -170,16 +158,16 @@ static void output_iv_copyback(struct crypto_async_request *areq)
 
 static void otx2_cpt_skcipher_callback(int status, void *arg1, void *arg2)
 {
-	struct otx2_cpt_info_buffer *cpt_info = arg2;
+	struct otx2_cpt_inst_info *inst_info = arg2;
 	struct crypto_async_request *areq = arg1;
 	struct pci_dev *pdev;
 
 	if (areq) {
 		if (!status)
 			output_iv_copyback(areq);
-		if (cpt_info) {
-			pdev = cpt_info->pdev;
-			do_request_cleanup(pdev, cpt_info);
+		if (inst_info) {
+			pdev = inst_info->pdev;
+			otx2_cpt_info_destroy(pdev, inst_info);
 		}
 		areq->complete(areq, status);
 	}
@@ -192,7 +180,7 @@ static inline void update_input_data(struct otx2_cpt_req_info *req_info,
 	req_info->req.dlen += nbytes;
 
 	while (nbytes) {
-		u32 len = min(nbytes, inp_sg->length);
+		u32 len = (nbytes < inp_sg->length) ? nbytes : inp_sg->length;
 		u8 *ptr = sg_virt(inp_sg);
 
 		req_info->in[*argcnt].vptr = (void *)ptr;
@@ -207,11 +195,15 @@ static inline void update_output_data(struct otx2_cpt_req_info *req_info,
 				      struct scatterlist *outp_sg,
 				      u32 offset, u32 nbytes, u32 *argcnt)
 {
+	u32 len, sg_len;
+	u8 *ptr;
+
 	req_info->rlen += nbytes;
 
 	while (nbytes) {
-		u32 len = min(nbytes, outp_sg->length - offset);
-		u8 *ptr = sg_virt(outp_sg);
+		sg_len = outp_sg->length - offset;
+		len = (nbytes < sg_len) ? nbytes : sg_len;
+		ptr = sg_virt(outp_sg);
 
 		req_info->out[*argcnt].vptr = (void *) (ptr + offset);
 		req_info->out[*argcnt].size = len;
@@ -222,7 +214,7 @@ static inline void update_output_data(struct otx2_cpt_req_info *req_info,
 	}
 }
 
-static inline u32 create_ctx_hdr(struct skcipher_request *req, u32 enc,
+static inline int create_ctx_hdr(struct skcipher_request *req, u32 enc,
 				 u32 *argcnt)
 {
 	struct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);
@@ -232,16 +224,15 @@ static inline u32 create_ctx_hdr(struct skcipher_request *req, u32 enc,
 	struct otx2_cpt_fc_ctx *fctx = &rctx->fctx;
 	int ivsize = crypto_skcipher_ivsize(stfm);
 	u32 start = req->cryptlen - ivsize;
-	u64 *ctrl_flags = NULL;
 	gfp_t flags;
 
 	flags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?
 			GFP_KERNEL : GFP_ATOMIC;
-	req_info->ctrl.s.dma_mode = OTX2_CPT_DMA_GATHER_SCATTER;
-	req_info->ctrl.s.se_req = OTX2_CPT_SE_CORE_REQ;
+	req_info->ctrl.s.dma_mode = OTX2_CPT_DMA_MODE_SG;
+	req_info->ctrl.s.se_req = 1;
 
 	req_info->req.opcode.s.major = OTX2_CPT_MAJOR_OP_FC |
-				DMA_MODE_FLAG(OTX2_CPT_DMA_GATHER_SCATTER);
+				DMA_MODE_FLAG(OTX2_CPT_DMA_MODE_SG);
 	if (enc) {
 		req_info->req.opcode.s.minor = 2;
 	} else {
@@ -273,8 +264,7 @@ static inline u32 create_ctx_hdr(struct skcipher_request *req, u32 enc,
 
 	memcpy(fctx->enc.encr_iv, req->iv, crypto_skcipher_ivsize(stfm));
 
-	ctrl_flags = (u64 *)&fctx->enc.enc_ctrl.flags;
-	*ctrl_flags = cpu_to_be64(*ctrl_flags);
+	cpu_to_be64s(&fctx->enc.enc_ctrl.u);
 
 	/*
 	 * Storing  Packet Data Information in offset
@@ -294,7 +284,7 @@ static inline u32 create_ctx_hdr(struct skcipher_request *req, u32 enc,
 	return 0;
 }
 
-static inline u32 create_input_list(struct skcipher_request *req, u32 enc,
+static inline int create_input_list(struct skcipher_request *req, u32 enc,
 				    u32 enc_iv_len)
 {
 	struct otx2_cpt_req_ctx *rctx = skcipher_request_ctx(req);
@@ -307,7 +297,7 @@ static inline u32 create_input_list(struct skcipher_request *req, u32 enc,
 		return ret;
 
 	update_input_data(req_info, req->src, req->cryptlen, &argcnt);
-	req_info->incnt = argcnt;
+	req_info->in_cnt = argcnt;
 
 	return 0;
 }
@@ -328,7 +318,30 @@ static inline void create_output_list(struct skcipher_request *req,
 	 * [ 16 Bytes/     [   Request Enc/Dec/ DATA Len AES CBC ]
 	 */
 	update_output_data(req_info, req->dst, 0, req->cryptlen, &argcnt);
-	req_info->outcnt = argcnt;
+	req_info->out_cnt = argcnt;
+}
+
+static int skcipher_do_fallback(struct skcipher_request *req, bool is_enc)
+{
+	struct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);
+	struct otx2_cpt_req_ctx *rctx = skcipher_request_ctx(req);
+	struct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(stfm);
+	int ret;
+
+	if (ctx->fbk_cipher) {
+		skcipher_request_set_tfm(&rctx->sk_fbk_req, ctx->fbk_cipher);
+		skcipher_request_set_callback(&rctx->sk_fbk_req,
+					      req->base.flags,
+					      req->base.complete,
+					      req->base.data);
+		skcipher_request_set_crypt(&rctx->sk_fbk_req, req->src,
+					   req->dst, req->cryptlen, req->iv);
+		ret = is_enc ? crypto_skcipher_encrypt(&rctx->sk_fbk_req) :
+			       crypto_skcipher_decrypt(&rctx->sk_fbk_req);
+	} else {
+		ret = -EINVAL;
+	}
+	return ret;
 }
 
 static inline int cpt_enc_dec(struct skcipher_request *req, u32 enc)
@@ -344,13 +357,15 @@ static inline int cpt_enc_dec(struct skcipher_request *req, u32 enc)
 	if (req->cryptlen == 0)
 		return 0;
 
-	if (req->cryptlen > OTX2_CPT_MAX_REQ_SIZE ||
-	    !IS_ALIGNED(req->cryptlen, ctx->enc_align_len))
+	if (!IS_ALIGNED(req->cryptlen, ctx->enc_align_len))
 		return -EINVAL;
 
+	if (req->cryptlen > OTX2_CPT_MAX_REQ_SIZE)
+		return skcipher_do_fallback(req, enc);
+
 	/* Clear control words */
 	rctx->ctrl_word.flags = 0;
-	rctx->fctx.enc.enc_ctrl.flags = 0;
+	rctx->fctx.enc.enc_ctrl.u = 0;
 
 	status = create_input_list(req, enc, enc_iv_len);
 	if (status)
@@ -408,14 +423,16 @@ static int otx2_cpt_skcipher_xts_setkey(struct crypto_skcipher *tfm,
 	case 2 * AES_KEYSIZE_128:
 		ctx->key_type = OTX2_CPT_AES_128_BIT;
 		break;
+	case 2 * AES_KEYSIZE_192:
+		ctx->key_type = OTX2_CPT_AES_192_BIT;
+		break;
 	case 2 * AES_KEYSIZE_256:
 		ctx->key_type = OTX2_CPT_AES_256_BIT;
 		break;
 	default:
 		return -EINVAL;
 	}
-
-	return 0;
+	return crypto_skcipher_setkey(ctx->fbk_cipher, key, keylen);
 }
 
 static int cpt_des_setkey(struct crypto_skcipher *tfm, const u8 *key,
@@ -432,7 +449,7 @@ static int cpt_des_setkey(struct crypto_skcipher *tfm, const u8 *key,
 
 	memcpy(ctx->enc_key, key, keylen);
 
-	return 0;
+	return crypto_skcipher_setkey(ctx->fbk_cipher, key, keylen);
 }
 
 static int cpt_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,
@@ -463,7 +480,7 @@ static int cpt_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,
 
 	memcpy(ctx->enc_key, key, keylen);
 
-	return 0;
+	return crypto_skcipher_setkey(ctx->fbk_cipher, key, keylen);
 }
 
 static int otx2_cpt_skcipher_cbc_aes_setkey(struct crypto_skcipher *tfm,
@@ -478,12 +495,6 @@ static int otx2_cpt_skcipher_ecb_aes_setkey(struct crypto_skcipher *tfm,
 	return cpt_aes_setkey(tfm, key, keylen, OTX2_CPT_AES_ECB);
 }
 
-static int otx2_cpt_skcipher_cfb_aes_setkey(struct crypto_skcipher *tfm,
-					    const u8 *key, u32 keylen)
-{
-	return cpt_aes_setkey(tfm, key, keylen, OTX2_CPT_AES_CFB);
-}
-
 static int otx2_cpt_skcipher_cbc_des3_setkey(struct crypto_skcipher *tfm,
 					     const u8 *key, u32 keylen)
 {
@@ -496,25 +507,73 @@ static int otx2_cpt_skcipher_ecb_des3_setkey(struct crypto_skcipher *tfm,
 	return cpt_des_setkey(tfm, key, keylen, OTX2_CPT_DES3_ECB);
 }
 
-static int otx2_cpt_enc_dec_init(struct crypto_skcipher *tfm)
+static int cpt_skcipher_fallback_init(struct otx2_cpt_enc_ctx *ctx,
+				      struct crypto_alg *alg)
 {
-	struct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);
+	if (alg->cra_flags & CRYPTO_ALG_NEED_FALLBACK) {
+		ctx->fbk_cipher =
+				crypto_alloc_skcipher(alg->cra_name, 0,
+						      CRYPTO_ALG_ASYNC |
+						      CRYPTO_ALG_NEED_FALLBACK);
+		if (IS_ERR(ctx->fbk_cipher)) {
+			pr_err("%s() failed to allocate fallback for %s\n",
+				__func__, alg->cra_name);
+			return PTR_ERR(ctx->fbk_cipher);
+		}
+	}
+	return 0;
+}
+
+static int otx2_cpt_enc_dec_init(struct crypto_skcipher *stfm)
+{
+	struct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(stfm);
+	struct crypto_tfm *tfm = crypto_skcipher_tfm(stfm);
+	struct crypto_alg *alg = tfm->__crt_alg;
 
 	memset(ctx, 0, sizeof(*ctx));
 	/*
-	 * Additional memory for ablkcipher_request is
+	 * Additional memory for skcipher_request is
 	 * allocated since the cryptd daemon uses
 	 * this memory for request_ctx information
 	 */
-	crypto_skcipher_set_reqsize(tfm, sizeof(struct otx2_cpt_req_ctx) +
+	crypto_skcipher_set_reqsize(stfm, sizeof(struct otx2_cpt_req_ctx) +
 					sizeof(struct skcipher_request));
 
+	return cpt_skcipher_fallback_init(ctx, alg);
+}
+
+static void otx2_cpt_skcipher_exit(struct crypto_skcipher *tfm)
+{
+	struct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);
+
+	if (ctx->fbk_cipher) {
+		crypto_free_skcipher(ctx->fbk_cipher);
+		ctx->fbk_cipher = NULL;
+	}
+}
+
+static int cpt_aead_fallback_init(struct otx2_cpt_aead_ctx *ctx,
+				  struct crypto_alg *alg)
+{
+	if (alg->cra_flags & CRYPTO_ALG_NEED_FALLBACK) {
+		ctx->fbk_cipher =
+			    crypto_alloc_aead(alg->cra_name, 0,
+					      CRYPTO_ALG_ASYNC |
+					      CRYPTO_ALG_NEED_FALLBACK);
+		if (IS_ERR(ctx->fbk_cipher)) {
+			pr_err("%s() failed to allocate fallback for %s\n",
+				__func__, alg->cra_name);
+			return PTR_ERR(ctx->fbk_cipher);
+		}
+	}
 	return 0;
 }
 
-static int cpt_aead_init(struct crypto_aead *tfm, u8 cipher_type, u8 mac_type)
+static int cpt_aead_init(struct crypto_aead *atfm, u8 cipher_type, u8 mac_type)
 {
-	struct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx(tfm);
+	struct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx(atfm);
+	struct crypto_tfm *tfm = crypto_aead_tfm(atfm);
+	struct crypto_alg *alg = tfm->__crt_alg;
 
 	ctx->cipher_type = cipher_type;
 	ctx->mac_type = mac_type;
@@ -569,9 +628,9 @@ static int cpt_aead_init(struct crypto_aead *tfm, u8 cipher_type, u8 mac_type)
 		ctx->enc_align_len = 1;
 		break;
 	}
-	crypto_aead_set_reqsize(tfm, sizeof(struct otx2_cpt_req_ctx));
+	crypto_aead_set_reqsize(atfm, sizeof(struct otx2_cpt_req_ctx));
 
-	return 0;
+	return cpt_aead_fallback_init(ctx, alg);
 }
 
 static int otx2_cpt_aead_cbc_aes_sha1_init(struct crypto_aead *tfm)
@@ -628,15 +687,26 @@ static void otx2_cpt_aead_exit(struct crypto_aead *tfm)
 	if (ctx->hashalg)
 		crypto_free_shash(ctx->hashalg);
 	kfree(ctx->sdesc);
+
+	if (ctx->fbk_cipher) {
+		crypto_free_aead(ctx->fbk_cipher);
+		ctx->fbk_cipher = NULL;
+	}
 }
 
 static int otx2_cpt_aead_gcm_set_authsize(struct crypto_aead *tfm,
 					  unsigned int authsize)
 {
+	struct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx(tfm);
+
 	if (crypto_rfc4106_check_authsize(authsize))
 		return -EINVAL;
 
 	tfm->authsize = authsize;
+	/* Set authsize for fallback case */
+	if (ctx->fbk_cipher)
+		ctx->fbk_cipher->authsize = authsize;
+
 	return 0;
 }
 
@@ -676,20 +746,16 @@ static struct otx2_cpt_sdesc *alloc_sdesc(struct crypto_shash *alg)
 
 static inline void swap_data32(void *buf, u32 len)
 {
-	u32 *store = (u32 *) buf;
-	int i = 0;
-
-	for (i = 0 ; i < len/sizeof(u32); i++, store++)
-		*store = cpu_to_be32(*store);
+	cpu_to_be32_array(buf, buf, len / 4);
 }
 
 static inline void swap_data64(void *buf, u32 len)
 {
-	u64 *store = (u64 *) buf;
+	u64 *src = buf;
 	int i = 0;
 
-	for (i = 0 ; i < len/sizeof(u64); i++, store++)
-		*store = cpu_to_be64(*store);
+	for (i = 0 ; i < len / 8; i++, src++)
+		cpu_to_be64s(src);
 }
 
 static int copy_pad(u8 mac_type, u8 *out_pad, u8 *in_pad)
@@ -829,26 +895,26 @@ static int otx2_cpt_aead_cbc_aes_sha_setkey(struct crypto_aead *cipher,
 	struct crypto_authenc_key_param *param;
 	int enckeylen = 0, authkeylen = 0;
 	struct rtattr *rta = (void *)key;
-	int status = -EINVAL;
+	int status;
 
 	if (!RTA_OK(rta, keylen))
-		goto badkey;
+		return -EINVAL;
 
 	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)
-		goto badkey;
+		return -EINVAL;
 
 	if (RTA_PAYLOAD(rta) < sizeof(*param))
-		goto badkey;
+		return -EINVAL;
 
 	param = RTA_DATA(rta);
 	enckeylen = be32_to_cpu(param->enckeylen);
 	key += RTA_ALIGN(rta->rta_len);
 	keylen -= RTA_ALIGN(rta->rta_len);
 	if (keylen < enckeylen)
-		goto badkey;
+		return -EINVAL;
 
 	if (keylen > OTX2_CPT_MAX_KEY_SIZE)
-		goto badkey;
+		return -EINVAL;
 
 	authkeylen = keylen - enckeylen;
 	memcpy(ctx->key, key, keylen);
@@ -865,7 +931,7 @@ static int otx2_cpt_aead_cbc_aes_sha_setkey(struct crypto_aead *cipher,
 		break;
 	default:
 		/* Invalid key length */
-		goto badkey;
+		return -EINVAL;
 	}
 
 	ctx->enc_key_len = enckeylen;
@@ -873,11 +939,9 @@ static int otx2_cpt_aead_cbc_aes_sha_setkey(struct crypto_aead *cipher,
 
 	status = aead_hmac_init(cipher);
 	if (status)
-		goto badkey;
+		return status;
 
 	return 0;
-badkey:
-	return status;
 }
 
 static int otx2_cpt_aead_ecb_null_sha_setkey(struct crypto_aead *cipher,
@@ -890,30 +954,29 @@ static int otx2_cpt_aead_ecb_null_sha_setkey(struct crypto_aead *cipher,
 	int enckeylen = 0;
 
 	if (!RTA_OK(rta, keylen))
-		goto badkey;
+		return -EINVAL;
 
 	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)
-		goto badkey;
+		return -EINVAL;
 
 	if (RTA_PAYLOAD(rta) < sizeof(*param))
-		goto badkey;
+		return -EINVAL;
 
 	param = RTA_DATA(rta);
 	enckeylen = be32_to_cpu(param->enckeylen);
 	key += RTA_ALIGN(rta->rta_len);
 	keylen -= RTA_ALIGN(rta->rta_len);
 	if (enckeylen != 0)
-		goto badkey;
+		return -EINVAL;
 
 	if (keylen > OTX2_CPT_MAX_KEY_SIZE)
-		goto badkey;
+		return -EINVAL;
 
 	memcpy(ctx->key, key, keylen);
 	ctx->enc_key_len = enckeylen;
 	ctx->auth_key_len = keylen;
+
 	return 0;
-badkey:
-	return -EINVAL;
 }
 
 static int otx2_cpt_aead_gcm_aes_setkey(struct crypto_aead *cipher,
@@ -947,10 +1010,10 @@ static int otx2_cpt_aead_gcm_aes_setkey(struct crypto_aead *cipher,
 	/* Store encryption key and salt */
 	memcpy(ctx->key, key, keylen);
 
-	return 0;
+	return crypto_aead_setkey(ctx->fbk_cipher, key, keylen);
 }
 
-static inline u32 create_aead_ctx_hdr(struct aead_request *req, u32 enc,
+static inline int create_aead_ctx_hdr(struct aead_request *req, u32 enc,
 				      u32 *argcnt)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
@@ -1002,12 +1065,12 @@ static inline u32 create_aead_ctx_hdr(struct aead_request *req, u32 enc,
 		/* Unknown cipher type */
 		return -EINVAL;
 	}
-	rctx->ctrl_word.flags = cpu_to_be64(rctx->ctrl_word.flags);
+	cpu_to_be64s(&rctx->ctrl_word.flags);
 
-	req_info->ctrl.s.dma_mode = OTX2_CPT_DMA_GATHER_SCATTER;
-	req_info->ctrl.s.se_req = OTX2_CPT_SE_CORE_REQ;
+	req_info->ctrl.s.dma_mode = OTX2_CPT_DMA_MODE_SG;
+	req_info->ctrl.s.se_req = 1;
 	req_info->req.opcode.s.major = OTX2_CPT_MAJOR_OP_FC |
-				 DMA_MODE_FLAG(OTX2_CPT_DMA_GATHER_SCATTER);
+				 DMA_MODE_FLAG(OTX2_CPT_DMA_MODE_SG);
 	if (enc) {
 		req_info->req.opcode.s.minor = 2;
 		req_info->req.param1 = req->cryptlen;
@@ -1022,7 +1085,7 @@ static inline u32 create_aead_ctx_hdr(struct aead_request *req, u32 enc,
 	fctx->enc.enc_ctrl.e.aes_key = ctx->key_type;
 	fctx->enc.enc_ctrl.e.mac_type = ctx->mac_type;
 	fctx->enc.enc_ctrl.e.mac_len = mac_len;
-	fctx->enc.enc_ctrl.flags = cpu_to_be64(fctx->enc.enc_ctrl.flags);
+	cpu_to_be64s(&fctx->enc.enc_ctrl.u);
 
 	/*
 	 * Storing Packet Data Information in offset
@@ -1041,7 +1104,7 @@ static inline u32 create_aead_ctx_hdr(struct aead_request *req, u32 enc,
 	return 0;
 }
 
-static inline u32 create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,
+static inline void create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,
 				      u32 enc)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
@@ -1049,10 +1112,10 @@ static inline u32 create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,
 	struct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx(tfm);
 	struct otx2_cpt_req_info *req_info = &rctx->cpt_req;
 
-	req_info->ctrl.s.dma_mode = OTX2_CPT_DMA_GATHER_SCATTER;
-	req_info->ctrl.s.se_req = OTX2_CPT_SE_CORE_REQ;
+	req_info->ctrl.s.dma_mode = OTX2_CPT_DMA_MODE_SG;
+	req_info->ctrl.s.se_req = 1;
 	req_info->req.opcode.s.major = OTX2_CPT_MAJOR_OP_HMAC |
-				 DMA_MODE_FLAG(OTX2_CPT_DMA_GATHER_SCATTER);
+				 DMA_MODE_FLAG(OTX2_CPT_DMA_MODE_SG);
 	req_info->is_trunc_hmac = ctx->is_trunc_hmac;
 
 	req_info->req.opcode.s.minor = 0;
@@ -1064,11 +1127,9 @@ static inline u32 create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,
 	req_info->in[*argcnt].size = round_up(ctx->auth_key_len, 8);
 	req_info->req.dlen += round_up(ctx->auth_key_len, 8);
 	++(*argcnt);
-
-	return 0;
 }
 
-static inline u32 create_aead_input_list(struct aead_request *req, u32 enc)
+static inline int create_aead_input_list(struct aead_request *req, u32 enc)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
 	struct otx2_cpt_req_info *req_info = &rctx->cpt_req;
@@ -1079,13 +1140,13 @@ static inline u32 create_aead_input_list(struct aead_request *req, u32 enc)
 	if (status)
 		return status;
 	update_input_data(req_info, req->src, inputlen, &argcnt);
-	req_info->incnt = argcnt;
+	req_info->in_cnt = argcnt;
 
 	return 0;
 }
 
-static inline u32 create_aead_output_list(struct aead_request *req, u32 enc,
-					  u32 mac_len)
+static inline void create_aead_output_list(struct aead_request *req, u32 enc,
+					   u32 mac_len)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
 	struct otx2_cpt_req_info *req_info =  &rctx->cpt_req;
@@ -1097,13 +1158,11 @@ static inline u32 create_aead_output_list(struct aead_request *req, u32 enc,
 		outputlen = req->cryptlen + req->assoclen - mac_len;
 
 	update_output_data(req_info, req->dst, 0, outputlen, &argcnt);
-	req_info->outcnt = argcnt;
-
-	return 0;
+	req_info->out_cnt = argcnt;
 }
 
-static inline u32 create_aead_null_input_list(struct aead_request *req,
-					      u32 enc, u32 mac_len)
+static inline void create_aead_null_input_list(struct aead_request *req,
+					       u32 enc, u32 mac_len)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
 	struct otx2_cpt_req_info *req_info = &rctx->cpt_req;
@@ -1116,12 +1175,10 @@ static inline u32 create_aead_null_input_list(struct aead_request *req,
 
 	create_hmac_ctx_hdr(req, &argcnt, enc);
 	update_input_data(req_info, req->src, inputlen, &argcnt);
-	req_info->incnt = argcnt;
-
-	return 0;
+	req_info->in_cnt = argcnt;
 }
 
-static inline u32 create_aead_null_output_list(struct aead_request *req,
+static inline int create_aead_null_output_list(struct aead_request *req,
 					       u32 enc, u32 mac_len)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
@@ -1145,10 +1202,8 @@ static inline u32 create_aead_null_output_list(struct aead_request *req,
 		ptr = kmalloc(inputlen, (req_info->areq->flags &
 					 CRYPTO_TFM_REQ_MAY_SLEEP) ?
 					 GFP_KERNEL : GFP_ATOMIC);
-		if (!ptr) {
-			status = -ENOMEM;
-			goto error;
-		}
+		if (!ptr)
+			return -ENOMEM;
 
 		status = sg_copy_to_buffer(req->src, sg_nents(req->src), ptr,
 					   inputlen);
@@ -1175,10 +1230,8 @@ static inline u32 create_aead_null_output_list(struct aead_request *req,
 		while (offset >= dst->length) {
 			offset -= dst->length;
 			dst = sg_next(dst);
-			if (!dst) {
-				status = -ENOENT;
-				goto error;
-			}
+			if (!dst)
+				return -ENOENT;
 		}
 
 		update_output_data(req_info, dst, offset, mac_len, &argcnt);
@@ -1190,37 +1243,57 @@ static inline u32 create_aead_null_output_list(struct aead_request *req,
 		status = sg_copy_buffer(req->src, sg_nents(req->src),
 					rctx->fctx.hmac.s.hmac_recv, mac_len,
 					inputlen, true);
-		if (status != mac_len) {
-			status = -EINVAL;
-			goto error;
-		}
+		if (status != mac_len)
+			return -EINVAL;
 
 		req_info->out[argcnt].vptr = rctx->fctx.hmac.s.hmac_calc;
 		req_info->out[argcnt].size = mac_len;
 		argcnt++;
 	}
 
-	req_info->outcnt = argcnt;
+	req_info->out_cnt = argcnt;
 	return 0;
 
 error_free:
 	kfree(ptr);
-error:
 	return status;
 }
 
-static u32 cpt_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)
+static int aead_do_fallback(struct aead_request *req, bool is_enc)
+{
+	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx(aead);
+	int ret;
+
+	if (ctx->fbk_cipher) {
+		/* Store the cipher tfm and then use the fallback tfm */
+		aead_request_set_tfm(&rctx->fbk_req, ctx->fbk_cipher);
+		aead_request_set_callback(&rctx->fbk_req, req->base.flags,
+					  req->base.complete, req->base.data);
+		aead_request_set_crypt(&rctx->fbk_req, req->src,
+				       req->dst, req->cryptlen, req->iv);
+		ret = is_enc ? crypto_aead_encrypt(&rctx->fbk_req) :
+			       crypto_aead_decrypt(&rctx->fbk_req);
+	} else {
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int cpt_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)
 {
 	struct otx2_cpt_req_ctx *rctx = aead_request_ctx(req);
 	struct otx2_cpt_req_info *req_info = &rctx->cpt_req;
 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 	struct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx(tfm);
 	struct pci_dev *pdev;
-	u32 status, cpu_num;
+	int status, cpu_num;
 
 	/* Clear control words */
 	rctx->ctrl_word.flags = 0;
-	rctx->fctx.enc.enc_ctrl.flags = 0;
+	rctx->fctx.enc.enc_ctrl.u = 0;
 
 	req_info->callback = otx2_cpt_aead_callback;
 	req_info->areq = &req->base;
@@ -1233,17 +1306,12 @@ static u32 cpt_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)
 		status = create_aead_input_list(req, enc);
 		if (status)
 			return status;
-		status = create_aead_output_list(req, enc,
-						 crypto_aead_authsize(tfm));
-		if (status)
-			return status;
+		create_aead_output_list(req, enc, crypto_aead_authsize(tfm));
 		break;
 
 	case OTX2_CPT_AEAD_ENC_DEC_NULL_REQ:
-		status = create_aead_null_input_list(req, enc,
-						     crypto_aead_authsize(tfm));
-		if (status)
-			return status;
+		create_aead_null_input_list(req, enc,
+					    crypto_aead_authsize(tfm));
 		status = create_aead_null_output_list(req, enc,
 						crypto_aead_authsize(tfm));
 		if (status)
@@ -1253,26 +1321,26 @@ static u32 cpt_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)
 	default:
 		return -EINVAL;
 	}
-	if (req_info->req.param1 == 0)
-		return 0;
-	if (req_info->req.param1 > OTX2_CPT_MAX_REQ_SIZE ||
-	    req_info->req.param2 > OTX2_CPT_MAX_REQ_SIZE ||
-	    !IS_ALIGNED(req_info->req.param1, ctx->enc_align_len))
+	if (!IS_ALIGNED(req_info->req.param1, ctx->enc_align_len))
 		return -EINVAL;
 
+	if (!req_info->req.param2 ||
+	    (req_info->req.param1 > OTX2_CPT_MAX_REQ_SIZE) ||
+	    (req_info->req.param2 > OTX2_CPT_MAX_REQ_SIZE))
+		return aead_do_fallback(req, enc);
+
 	status = get_se_device(&pdev, &cpu_num);
 	if (status)
 		return status;
 
 	req_info->ctrl.s.grp = otx2_cpt_get_kcrypto_eng_grp_num(pdev);
 
-	status = otx2_cpt_do_request(pdev, req_info, cpu_num);
 	/*
 	 * We perform an asynchronous send and once
 	 * the request is completed the driver would
 	 * intimate through registered call back functions
 	 */
-	return status;
+	return otx2_cpt_do_request(pdev, req_info, cpu_num);
 }
 
 static int otx2_cpt_aead_encrypt(struct aead_request *req)
@@ -1298,7 +1366,7 @@ static int otx2_cpt_aead_null_decrypt(struct aead_request *req)
 static struct skcipher_alg otx2_cpt_skciphers[] = { {
 	.base.cra_name = "xts(aes)",
 	.base.cra_driver_name = "cpt_xts_aes",
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 	.base.cra_blocksize = AES_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),
 	.base.cra_alignmask = 7,
@@ -1306,6 +1374,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 	.base.cra_module = THIS_MODULE,
 
 	.init = otx2_cpt_enc_dec_init,
+	.exit = otx2_cpt_skcipher_exit,
 	.ivsize = AES_BLOCK_SIZE,
 	.min_keysize = 2 * AES_MIN_KEY_SIZE,
 	.max_keysize = 2 * AES_MAX_KEY_SIZE,
@@ -1315,7 +1384,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 }, {
 	.base.cra_name = "cbc(aes)",
 	.base.cra_driver_name = "cpt_cbc_aes",
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 	.base.cra_blocksize = AES_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),
 	.base.cra_alignmask = 7,
@@ -1323,6 +1392,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 	.base.cra_module = THIS_MODULE,
 
 	.init = otx2_cpt_enc_dec_init,
+	.exit = otx2_cpt_skcipher_exit,
 	.ivsize = AES_BLOCK_SIZE,
 	.min_keysize = AES_MIN_KEY_SIZE,
 	.max_keysize = AES_MAX_KEY_SIZE,
@@ -1332,7 +1402,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 }, {
 	.base.cra_name = "ecb(aes)",
 	.base.cra_driver_name = "cpt_ecb_aes",
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 	.base.cra_blocksize = AES_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),
 	.base.cra_alignmask = 7,
@@ -1340,33 +1410,17 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 	.base.cra_module = THIS_MODULE,
 
 	.init = otx2_cpt_enc_dec_init,
+	.exit = otx2_cpt_skcipher_exit,
 	.ivsize = 0,
 	.min_keysize = AES_MIN_KEY_SIZE,
 	.max_keysize = AES_MAX_KEY_SIZE,
 	.setkey = otx2_cpt_skcipher_ecb_aes_setkey,
 	.encrypt = otx2_cpt_skcipher_encrypt,
 	.decrypt = otx2_cpt_skcipher_decrypt,
-}, {
-	.base.cra_name = "cfb(aes)",
-	.base.cra_driver_name = "cpt_cfb_aes",
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
-	.base.cra_blocksize = AES_BLOCK_SIZE,
-	.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),
-	.base.cra_alignmask = 7,
-	.base.cra_priority = 4001,
-	.base.cra_module = THIS_MODULE,
-
-	.init = otx2_cpt_enc_dec_init,
-	.ivsize = AES_BLOCK_SIZE,
-	.min_keysize = AES_MIN_KEY_SIZE,
-	.max_keysize = AES_MAX_KEY_SIZE,
-	.setkey = otx2_cpt_skcipher_cfb_aes_setkey,
-	.encrypt = otx2_cpt_skcipher_encrypt,
-	.decrypt = otx2_cpt_skcipher_decrypt,
 }, {
 	.base.cra_name = "cbc(des3_ede)",
 	.base.cra_driver_name = "cpt_cbc_des3_ede",
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 	.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),
 	.base.cra_alignmask = 7,
@@ -1374,6 +1428,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 	.base.cra_module = THIS_MODULE,
 
 	.init = otx2_cpt_enc_dec_init,
+	.exit = otx2_cpt_skcipher_exit,
 	.min_keysize = DES3_EDE_KEY_SIZE,
 	.max_keysize = DES3_EDE_KEY_SIZE,
 	.ivsize = DES_BLOCK_SIZE,
@@ -1383,7 +1438,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 }, {
 	.base.cra_name = "ecb(des3_ede)",
 	.base.cra_driver_name = "cpt_ecb_des3_ede",
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 	.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),
 	.base.cra_alignmask = 7,
@@ -1391,6 +1446,7 @@ static struct skcipher_alg otx2_cpt_skciphers[] = { {
 	.base.cra_module = THIS_MODULE,
 
 	.init = otx2_cpt_enc_dec_init,
+	.exit = otx2_cpt_skcipher_exit,
 	.min_keysize = DES3_EDE_KEY_SIZE,
 	.max_keysize = DES3_EDE_KEY_SIZE,
 	.ivsize = 0,
@@ -1404,7 +1460,7 @@ static struct aead_alg otx2_cpt_aeads[] = { {
 		.cra_name = "authenc(hmac(sha1),cbc(aes))",
 		.cra_driver_name = "cpt_hmac_sha1_cbc_aes",
 		.cra_blocksize = AES_BLOCK_SIZE,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 		.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx),
 		.cra_priority = 4001,
 		.cra_alignmask = 0,
@@ -1423,7 +1479,7 @@ static struct aead_alg otx2_cpt_aeads[] = { {
 		.cra_name = "authenc(hmac(sha256),cbc(aes))",
 		.cra_driver_name = "cpt_hmac_sha256_cbc_aes",
 		.cra_blocksize = AES_BLOCK_SIZE,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 		.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx),
 		.cra_priority = 4001,
 		.cra_alignmask = 0,
@@ -1442,7 +1498,7 @@ static struct aead_alg otx2_cpt_aeads[] = { {
 		.cra_name = "authenc(hmac(sha384),cbc(aes))",
 		.cra_driver_name = "cpt_hmac_sha384_cbc_aes",
 		.cra_blocksize = AES_BLOCK_SIZE,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 		.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx),
 		.cra_priority = 4001,
 		.cra_alignmask = 0,
@@ -1461,7 +1517,7 @@ static struct aead_alg otx2_cpt_aeads[] = { {
 		.cra_name = "authenc(hmac(sha512),cbc(aes))",
 		.cra_driver_name = "cpt_hmac_sha512_cbc_aes",
 		.cra_blocksize = AES_BLOCK_SIZE,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 		.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx),
 		.cra_priority = 4001,
 		.cra_alignmask = 0,
@@ -1556,7 +1612,7 @@ static struct aead_alg otx2_cpt_aeads[] = { {
 		.cra_name = "rfc4106(gcm(aes))",
 		.cra_driver_name = "cpt_rfc4106_gcm_aes",
 		.cra_blocksize = 1,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 		.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx),
 		.cra_priority = 4001,
 		.cra_alignmask = 0,
@@ -1623,8 +1679,8 @@ static inline void cpt_unregister_algs(void)
 
 static int compare_func(const void *lptr, const void *rptr)
 {
-	struct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;
-	struct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;
+	const struct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;
+	const struct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;
 
 	if (ldesc->dev->devfn < rdesc->dev->devfn)
 		return -1;
@@ -1635,8 +1691,8 @@ static int compare_func(const void *lptr, const void *rptr)
 
 static void swap_func(void *lptr, void *rptr, int size)
 {
-	struct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;
-	struct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;
+	struct cpt_device_desc *ldesc = lptr;
+	struct cpt_device_desc *rdesc = rptr;
 	struct cpt_device_desc desc;
 
 	desc = *ldesc;
@@ -1645,65 +1701,42 @@ static void swap_func(void *lptr, void *rptr, int size)
 }
 
 int otx2_cpt_crypto_init(struct pci_dev *pdev, struct module *mod,
-			 enum otx2_cpt_eng_type engine_type,
 			 int num_queues, int num_devices)
 {
 	int ret = 0;
 	int count;
 
 	mutex_lock(&mutex);
-	switch (engine_type) {
-	case OTX2_CPT_SE_TYPES:
-		count = atomic_read(&se_devices.count);
-		if (count >= CPT_MAX_LF_NUM) {
-			dev_err(&pdev->dev, "No space to add a new device\n");
-			ret = -ENOSPC;
-			goto unlock_mutex;
-		}
-		se_devices.desc[count].num_queues = num_queues;
-		se_devices.desc[count++].dev = pdev;
-		atomic_inc(&se_devices.count);
-
-		if (atomic_read(&se_devices.count) == num_devices &&
-		    is_crypto_registered == false) {
-			if (cpt_register_algs()) {
-				dev_err(&pdev->dev,
-				   "Error in registering crypto algorithms\n");
-				ret =  -EINVAL;
-				goto unlock_mutex;
-			}
-			try_module_get(mod);
-			is_crypto_registered = true;
-		}
-		sort(se_devices.desc, count, sizeof(struct cpt_device_desc),
-		     compare_func, swap_func);
-		break;
-
-	case OTX2_CPT_AE_TYPES:
-		count = atomic_read(&ae_devices.count);
-		if (count >= CPT_MAX_LF_NUM) {
-			dev_err(&pdev->dev, "No space to a add new device\n");
-			ret = -ENOSPC;
-			goto unlock_mutex;
+	count = atomic_read(&se_devices.count);
+	if (count >= OTX2_CPT_MAX_LFS_NUM) {
+		dev_err(&pdev->dev, "No space to add a new device\n");
+		ret = -ENOSPC;
+		goto unlock;
+	}
+	se_devices.desc[count].num_queues = num_queues;
+	se_devices.desc[count++].dev = pdev;
+	atomic_inc(&se_devices.count);
+
+	if (atomic_read(&se_devices.count) == num_devices &&
+	    is_crypto_registered == false) {
+		if (cpt_register_algs()) {
+			dev_err(&pdev->dev,
+				"Error in registering crypto algorithms\n");
+			ret =  -EINVAL;
+			goto unlock;
 		}
-		ae_devices.desc[count].num_queues = num_queues;
-		ae_devices.desc[count++].dev = pdev;
-		atomic_inc(&ae_devices.count);
-		sort(ae_devices.desc, count, sizeof(struct cpt_device_desc),
-		     compare_func, swap_func);
-		break;
-
-	default:
-		dev_err(&pdev->dev, "Unknown VF type %d\n", engine_type);
-		ret = BAD_OTX2_CPT_ENG_TYPE;
+		try_module_get(mod);
+		is_crypto_registered = true;
 	}
-unlock_mutex:
+	sort(se_devices.desc, count, sizeof(struct cpt_device_desc),
+	     compare_func, swap_func);
+
+unlock:
 	mutex_unlock(&mutex);
 	return ret;
 }
 
-void otx2_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod,
-			  enum otx2_cpt_eng_type engine_type)
+void otx2_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod)
 {
 	struct cpt_device_table *dev_tbl;
 	bool dev_found = false;
@@ -1711,31 +1744,28 @@ void otx2_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod,
 
 	mutex_lock(&mutex);
 
-	dev_tbl = (engine_type == OTX2_CPT_AE_TYPES) ? &ae_devices :
-						       &se_devices;
+	dev_tbl = &se_devices;
 	count = atomic_read(&dev_tbl->count);
-	for (i = 0; i < count; i++)
+	for (i = 0; i < count; i++) {
 		if (pdev == dev_tbl->desc[i].dev) {
 			for (j = i; j < count-1; j++)
 				dev_tbl->desc[j] = dev_tbl->desc[j+1];
 			dev_found = true;
 			break;
 		}
+	}
 
 	if (!dev_found) {
 		dev_err(&pdev->dev, "%s device not found\n", __func__);
-		goto exit;
+		goto unlock;
+	}
+	if (atomic_dec_and_test(&se_devices.count) &&
+	    !is_any_alg_used()) {
+		cpt_unregister_algs();
+		module_put(mod);
+		is_crypto_registered = false;
 	}
 
-	if (engine_type == OTX2_CPT_SE_TYPES) {
-		if (atomic_dec_and_test(&se_devices.count) &&
-		    !is_any_alg_used()) {
-			cpt_unregister_algs();
-			module_put(mod);
-			is_crypto_registered = false;
-		}
-	} else
-		atomic_dec(&ae_devices.count);
-exit:
+unlock:
 	mutex_unlock(&mutex);
 }
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.h b/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.h
index c73f5f94e01d..cfd6e2c50424 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.h
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.h
@@ -1,17 +1,13 @@
-/* SPDX-License-Identifier: GPL-2.0
- * Marvell OcteonTX CPT driver
- *
- * Copyright (C) 2019 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2019 Marvell.
  */
 
 #ifndef __OTX2_CPT_ALGS_H
 #define __OTX2_CPT_ALGS_H
 
 #include <crypto/hash.h>
+#include <crypto/skcipher.h>
+#include <crypto/aead.h>
 #include "otx2_cpt_common.h"
 
 #define OTX2_CPT_MAX_ENC_KEY_SIZE    32
@@ -22,7 +18,7 @@ enum otx2_cpt_request_type {
 	OTX2_CPT_ENC_DEC_REQ            = 0x1,
 	OTX2_CPT_AEAD_ENC_DEC_REQ       = 0x2,
 	OTX2_CPT_AEAD_ENC_DEC_NULL_REQ  = 0x3,
-	OTX2_CPT_PASSTHROUGH_REQ	       = 0x4
+	OTX2_CPT_PASSTHROUGH_REQ	= 0x4
 };
 
 enum otx2_cpt_major_opcodes {
@@ -31,11 +27,6 @@ enum otx2_cpt_major_opcodes {
 	OTX2_CPT_MAJOR_OP_HMAC = 0x35,
 };
 
-enum otx2_cpt_req_type {
-	OTX2_CPT_AE_CORE_REQ,
-	OTX2_CPT_SE_CORE_REQ
-};
-
 enum otx2_cpt_cipher_type {
 	OTX2_CPT_CIPHER_NULL = 0x0,
 	OTX2_CPT_DES3_CBC = 0x1,
@@ -66,18 +57,18 @@ enum otx2_cpt_aes_key_len {
 };
 
 union otx2_cpt_encr_ctrl {
-	u64 flags;
+	u64 u;
 	struct {
 #if defined(__BIG_ENDIAN_BITFIELD)
 		u64 enc_cipher:4;
-		u64 reserved1:1;
+		u64 reserved_59:1;
 		u64 aes_key:2;
 		u64 iv_source:1;
 		u64 mac_type:4;
-		u64 reserved2:3;
+		u64 reserved_49_51:3;
 		u64 auth_input_type:1;
 		u64 mac_len:8;
-		u64 reserved3:8;
+		u64 reserved_32_39:8;
 		u64 encr_offset:16;
 		u64 iv_offset:8;
 		u64 auth_offset:8;
@@ -85,14 +76,14 @@ union otx2_cpt_encr_ctrl {
 		u64 auth_offset:8;
 		u64 iv_offset:8;
 		u64 encr_offset:16;
-		u64 reserved3:8;
+		u64 reserved_32_39:8;
 		u64 mac_len:8;
 		u64 auth_input_type:1;
-		u64 reserved2:3;
+		u64 reserved_49_51:3;
 		u64 mac_type:4;
 		u64 iv_source:1;
 		u64 aes_key:2;
-		u64 reserved1:1;
+		u64 reserved_59:1;
 		u64 enc_cipher:4;
 #endif
 	} e;
@@ -103,13 +94,13 @@ struct otx2_cpt_cipher {
 	u8 value;
 };
 
-struct otx2_cpt_enc_context {
+struct otx2_cpt_fc_enc_ctx {
 	union otx2_cpt_encr_ctrl enc_ctrl;
 	u8 encr_key[32];
 	u8 encr_iv[16];
 };
 
-union otx2_cpt_fchmac_ctx {
+union otx2_cpt_fc_hmac_ctx {
 	struct {
 		u8 ipad[64];
 		u8 opad[64];
@@ -121,8 +112,8 @@ union otx2_cpt_fchmac_ctx {
 };
 
 struct otx2_cpt_fc_ctx {
-	struct otx2_cpt_enc_context enc;
-	union otx2_cpt_fchmac_ctx hmac;
+	struct otx2_cpt_fc_enc_ctx enc;
+	union otx2_cpt_fc_hmac_ctx hmac;
 };
 
 struct otx2_cpt_enc_ctx {
@@ -131,9 +122,10 @@ struct otx2_cpt_enc_ctx {
 	u8 cipher_type;
 	u8 key_type;
 	u8 enc_align_len;
+	struct crypto_skcipher *fbk_cipher;
 };
 
-union otx2_cpt_offset_ctrl_word {
+union otx2_cpt_offset_ctrl {
 	u64 flags;
 	struct {
 #if defined(__BIG_ENDIAN_BITFIELD)
@@ -152,8 +144,12 @@ union otx2_cpt_offset_ctrl_word {
 
 struct otx2_cpt_req_ctx {
 	struct otx2_cpt_req_info cpt_req;
-	union otx2_cpt_offset_ctrl_word ctrl_word;
+	union otx2_cpt_offset_ctrl ctrl_word;
 	struct otx2_cpt_fc_ctx fctx;
+	union {
+		struct skcipher_request sk_fbk_req;
+		struct aead_request fbk_req;
+	};
 };
 
 struct otx2_cpt_sdesc {
@@ -164,6 +160,7 @@ struct otx2_cpt_aead_ctx {
 	u8 key[OTX2_CPT_MAX_KEY_SIZE];
 	struct crypto_shash *hashalg;
 	struct otx2_cpt_sdesc *sdesc;
+	struct crypto_aead *fbk_cipher;
 	u8 *ipad;
 	u8 *opad;
 	u32 enc_key_len;
@@ -175,9 +172,7 @@ struct otx2_cpt_aead_ctx {
 	u8 enc_align_len;
 };
 int otx2_cpt_crypto_init(struct pci_dev *pdev, struct module *mod,
-			 enum otx2_cpt_eng_type engine_type,
 			 int num_queues, int num_devices);
-void otx2_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod,
-			  enum otx2_cpt_eng_type engine_type);
+void otx2_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod);
 
 #endif /* __OTX2_CPT_ALGS_H */
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptvf_main.c b/drivers/crypto/marvell/octeontx2/otx2_cptvf_main.c
index 10ec8aa9f280..ef04edd88757 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptvf_main.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptvf_main.c
@@ -1,18 +1,13 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-
-#include "otx2_cpt_mbox_common.h"
-#include "rvu_reg.h"
-
-#define OTX2_CPT_DRV_NAME "octeontx2-cptvf"
-#define OTX2_CPT_DRV_VERSION "1.0"
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
+
+#include "otx2_cpt_common.h"
+#include "otx2_cptvf.h"
+#include "otx2_cptlf.h"
+#include "otx2_cptvf_algs.h"
+#include <rvu_reg.h>
+
+#define OTX2_CPTVF_DRV_NAME "octeontx2-cptvf"
 
 static unsigned int cpt_block_num;
 module_param(cpt_block_num, uint, 0644);
@@ -42,8 +37,8 @@ static void cptvf_disable_pfvf_mbox_intrs(struct otx2_cptvf_dev *cptvf)
 
 static int cptvf_register_interrupts(struct otx2_cptvf_dev *cptvf)
 {
-	u32 num_vec;
-	int ret;
+	int ret, irq;
+	int num_vec;
 
 	num_vec = pci_msix_vec_count(cptvf->pdev);
 	if (num_vec <= 0)
@@ -57,26 +52,24 @@ static int cptvf_register_interrupts(struct otx2_cptvf_dev *cptvf)
 			"Request for %d msix vectors failed\n", num_vec);
 		return ret;
 	}
-
-	/* Register VF-PF mailbox interrupt handler */
-	ret = request_irq(pci_irq_vector(cptvf->pdev,
-			  OTX2_CPT_VF_INT_VEC_E_MBOX),
-			  otx2_cptvf_pfvf_mbox_intr,
-			  0, "CPTPFVF Mbox", cptvf);
+	irq = pci_irq_vector(cptvf->pdev, OTX2_CPT_VF_INT_VEC_E_MBOX);
+	/* Register VF<=>PF mailbox interrupt handler */
+	ret = devm_request_irq(&cptvf->pdev->dev, irq,
+			       otx2_cptvf_pfvf_mbox_intr, 0,
+			       "CPTPFVF Mbox", cptvf);
 	if (ret)
-		goto free_irq;
-	return 0;
-free_irq:
-	dev_err(&cptvf->pdev->dev, "Failed to register interrupts\n");
-	pci_free_irq_vectors(cptvf->pdev);
-	return ret;
-}
+		return ret;
+	/* Enable PF-VF mailbox interrupts */
+	cptvf_enable_pfvf_mbox_intrs(cptvf);
 
-static void cptvf_unregister_interrupts(struct otx2_cptvf_dev *cptvf)
-{
-	free_irq(pci_irq_vector(cptvf->pdev, OTX2_CPT_VF_INT_VEC_E_MBOX),
-		 cptvf);
-	pci_free_irq_vectors(cptvf->pdev);
+	ret = otx2_cpt_send_ready_msg(&cptvf->pfvf_mbox, cptvf->pdev);
+	if (ret) {
+		dev_warn(&cptvf->pdev->dev,
+			 "PF not responding to mailbox, deferring probe\n");
+		cptvf_disable_pfvf_mbox_intrs(cptvf);
+		return -EPROBE_DEFER;
+	}
+	return 0;
 }
 
 static int cptvf_pfvf_mbox_init(struct otx2_cptvf_dev *cptvf)
@@ -96,118 +89,281 @@ static int cptvf_pfvf_mbox_init(struct otx2_cptvf_dev *cptvf)
 
 	INIT_WORK(&cptvf->pfvf_mbox_work, otx2_cptvf_pfvf_mbox_handler);
 	return 0;
+
 free_wqe:
-	flush_workqueue(cptvf->pfvf_mbox_wq);
 	destroy_workqueue(cptvf->pfvf_mbox_wq);
 	return ret;
 }
 
 static void cptvf_pfvf_mbox_destroy(struct otx2_cptvf_dev *cptvf)
 {
-	flush_workqueue(cptvf->pfvf_mbox_wq);
 	destroy_workqueue(cptvf->pfvf_mbox_wq);
 	otx2_mbox_destroy(&cptvf->pfvf_mbox);
 }
 
+static void cptlf_work_handler(unsigned long data)
+{
+	otx2_cpt_post_process((struct otx2_cptlf_wqe *) data);
+}
+
+static void cleanup_tasklet_work(struct otx2_cptlfs_info *lfs)
+{
+	int i;
+
+	for (i = 0; i <  lfs->lfs_num; i++) {
+		if (!lfs->lf[i].wqe)
+			continue;
+
+		tasklet_kill(&lfs->lf[i].wqe->work);
+		kfree(lfs->lf[i].wqe);
+		lfs->lf[i].wqe = NULL;
+	}
+}
+
+static int init_tasklet_work(struct otx2_cptlfs_info *lfs)
+{
+	struct otx2_cptlf_wqe *wqe;
+	int i, ret = 0;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		wqe = kzalloc(sizeof(struct otx2_cptlf_wqe), GFP_KERNEL);
+		if (!wqe) {
+			ret = -ENOMEM;
+			goto cleanup_tasklet;
+		}
+
+		tasklet_init(&wqe->work, cptlf_work_handler, (u64) wqe);
+		wqe->lfs = lfs;
+		wqe->lf_num = i;
+		lfs->lf[i].wqe = wqe;
+	}
+	return 0;
+
+cleanup_tasklet:
+	cleanup_tasklet_work(lfs);
+	return ret;
+}
+
+static void free_pending_queues(struct otx2_cptlfs_info *lfs)
+{
+	int i;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		kfree(lfs->lf[i].pqueue.head);
+		lfs->lf[i].pqueue.head = NULL;
+	}
+}
+
+static int alloc_pending_queues(struct otx2_cptlfs_info *lfs)
+{
+	int size, ret, i;
+
+	if (!lfs->lfs_num)
+		return -EINVAL;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		lfs->lf[i].pqueue.qlen = OTX2_CPT_INST_QLEN_MSGS;
+		size = lfs->lf[i].pqueue.qlen *
+		       sizeof(struct otx2_cpt_pending_entry);
+
+		lfs->lf[i].pqueue.head = kzalloc(size, GFP_KERNEL);
+		if (!lfs->lf[i].pqueue.head) {
+			ret = -ENOMEM;
+			goto error;
+		}
+
+		/* Initialize spin lock */
+		spin_lock_init(&lfs->lf[i].pqueue.lock);
+	}
+	return 0;
+
+error:
+	free_pending_queues(lfs);
+	return ret;
+}
+
+static void lf_sw_cleanup(struct otx2_cptlfs_info *lfs)
+{
+	cleanup_tasklet_work(lfs);
+	free_pending_queues(lfs);
+}
+
+static int lf_sw_init(struct otx2_cptlfs_info *lfs)
+{
+	int ret;
+
+	ret = alloc_pending_queues(lfs);
+	if (ret) {
+		dev_err(&lfs->pdev->dev,
+			"Allocating pending queues failed\n");
+		return ret;
+	}
+	ret = init_tasklet_work(lfs);
+	if (ret) {
+		dev_err(&lfs->pdev->dev,
+			"Tasklet work init failed\n");
+		goto pending_queues_free;
+	}
+	return 0;
+
+pending_queues_free:
+	free_pending_queues(lfs);
+	return ret;
+}
+
+static void cptvf_lf_shutdown(struct otx2_cptlfs_info *lfs)
+{
+	atomic_set(&lfs->state, OTX2_CPTLF_IN_RESET);
+
+	/* Remove interrupts affinity */
+	otx2_cptlf_free_irqs_affinity(lfs);
+	/* Disable instruction queue */
+	otx2_cptlf_disable_iqueues(lfs);
+	/* Unregister crypto algorithms */
+	otx2_cpt_crypto_exit(lfs->pdev, THIS_MODULE);
+	/* Unregister LFs interrupts */
+	otx2_cptlf_unregister_interrupts(lfs);
+	/* Cleanup LFs software side */
+	lf_sw_cleanup(lfs);
+	/* Send request to detach LFs */
+	otx2_cpt_detach_rsrcs_msg(lfs);
+}
+
+static int cptvf_lf_init(struct otx2_cptvf_dev *cptvf)
+{
+	struct otx2_cptlfs_info *lfs = &cptvf->lfs;
+	struct device *dev = &cptvf->pdev->dev;
+	int ret, lfs_num;
+	u8 eng_grp_msk;
+
+	/* Get engine group number for symmetric crypto */
+	cptvf->lfs.kcrypto_eng_grp_num = OTX2_CPT_INVALID_CRYPTO_ENG_GRP;
+	ret = otx2_cptvf_send_eng_grp_num_msg(cptvf, OTX2_CPT_SE_TYPES);
+	if (ret)
+		return ret;
+
+	if (cptvf->lfs.kcrypto_eng_grp_num == OTX2_CPT_INVALID_CRYPTO_ENG_GRP) {
+		dev_err(dev, "Engine group for kernel crypto not available\n");
+		ret = -ENOENT;
+		return ret;
+	}
+	eng_grp_msk = 1 << cptvf->lfs.kcrypto_eng_grp_num;
+
+	ret = otx2_cptvf_send_kvf_limits_msg(cptvf);
+	if (ret)
+		return ret;
+
+	lfs->reg_base = cptvf->reg_base;
+	lfs->pdev = cptvf->pdev;
+	lfs->mbox = &cptvf->pfvf_mbox;
+	lfs->blkaddr = cptvf->blkaddr;
+
+	lfs_num = cptvf->lfs.kvf_limits ? cptvf->lfs.kvf_limits :
+		  num_online_cpus();
+	ret = otx2_cptlf_init(lfs, eng_grp_msk, OTX2_CPT_QUEUE_HI_PRIO,
+			      lfs_num);
+	if (ret)
+		return ret;
+
+	/* Get msix offsets for attached LFs */
+	ret = otx2_cpt_msix_offset_msg(lfs);
+	if (ret)
+		goto cleanup_lf;
+
+	/* Initialize LFs software side */
+	ret = lf_sw_init(lfs);
+	if (ret)
+		goto cleanup_lf;
+
+	/* Register LFs interrupts */
+	ret = otx2_cptlf_register_interrupts(lfs);
+	if (ret)
+		goto cleanup_lf_sw;
+
+	/* Set interrupts affinity */
+	ret = otx2_cptlf_set_irqs_affinity(lfs);
+	if (ret)
+		goto unregister_intr;
+
+	atomic_set(&lfs->state, OTX2_CPTLF_STARTED);
+	/* Register crypto algorithms */
+	ret = otx2_cpt_crypto_init(lfs->pdev, THIS_MODULE, lfs_num, 1);
+	if (ret) {
+		dev_err(&lfs->pdev->dev, "algorithms registration failed\n");
+		goto disable_irqs;
+	}
+	return 0;
+
+disable_irqs:
+	otx2_cptlf_free_irqs_affinity(lfs);
+unregister_intr:
+	otx2_cptlf_unregister_interrupts(lfs);
+cleanup_lf_sw:
+	lf_sw_cleanup(lfs);
+cleanup_lf:
+	otx2_cptlf_shutdown(lfs);
+
+	return ret;
+}
+
 static int otx2_cptvf_probe(struct pci_dev *pdev,
 			    const struct pci_device_id *ent)
 {
 	struct device *dev = &pdev->dev;
+	resource_size_t offset, size;
 	struct otx2_cptvf_dev *cptvf;
-	int ret, kcrypto_lfs;
+	int ret;
 
-	cptvf = kzalloc(sizeof(*cptvf), GFP_KERNEL);
+	cptvf = devm_kzalloc(dev, sizeof(*cptvf), GFP_KERNEL);
 	if (!cptvf)
 		return -ENOMEM;
 
-	pci_set_drvdata(pdev, cptvf);
-	cptvf->pdev = pdev;
-
-	ret = pci_enable_device(pdev);
+	ret = pcim_enable_device(pdev);
 	if (ret) {
 		dev_err(dev, "Failed to enable PCI device\n");
 		goto clear_drvdata;
 	}
 
-	pci_set_master(pdev);
-
-	ret = pci_request_regions(pdev, OTX2_CPT_DRV_NAME);
-	if (ret) {
-		dev_err(dev, "PCI request regions failed 0x%x\n", ret);
-		goto disable_device;
-	}
-
-	ret = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(48));
 	if (ret) {
 		dev_err(dev, "Unable to get usable DMA configuration\n");
-		goto release_regions;
+		goto clear_drvdata;
 	}
-
-	ret = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	/* Map VF's configuration registers */
+	ret = pcim_iomap_regions_request_all(pdev, 1 << PCI_PF_REG_BAR_NUM,
+					     OTX2_CPTVF_DRV_NAME);
 	if (ret) {
-		dev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");
-		goto release_regions;
+		dev_err(dev, "Couldn't get PCI resources 0x%x\n", ret);
+		goto clear_drvdata;
 	}
+	pci_set_master(pdev);
+	pci_set_drvdata(pdev, cptvf);
+	cptvf->pdev = pdev;
 
-	/* Map VF's configuration registers */
-	cptvf->reg_base = pci_iomap(pdev, PCI_PF_REG_BAR_NUM, 0);
-	if (!cptvf->reg_base) {
-		dev_err(dev, "Unable to map BAR2\n");
-		ret = -ENOMEM;
-		goto release_regions;
-	}
+	cptvf->reg_base = pcim_iomap_table(pdev)[PCI_PF_REG_BAR_NUM];
 
+	offset = pci_resource_start(pdev, PCI_MBOX_BAR_NUM);
+	size = pci_resource_len(pdev, PCI_MBOX_BAR_NUM);
 	/* Map PF-VF mailbox memory */
-	cptvf->pfvf_mbox_base = ioremap_wc(pci_resource_start(cptvf->pdev,
-					   PCI_MBOX_BAR_NUM),
-					   pci_resource_len(cptvf->pdev,
-					   PCI_MBOX_BAR_NUM));
+	cptvf->pfvf_mbox_base = devm_ioremap_wc(dev, offset, size);
 	if (!cptvf->pfvf_mbox_base) {
 		dev_err(&pdev->dev, "Unable to map BAR4\n");
 		ret = -ENODEV;
-		goto pci_unmap;
+		goto clear_drvdata;
 	}
-
-	/* Initialize PF-VF mailbox */
+	/* Initialize PF<=>VF mailbox */
 	ret = cptvf_pfvf_mbox_init(cptvf);
 	if (ret)
-		goto iounmap_pfvf;
+		goto clear_drvdata;
 
 	/* Register interrupts */
 	ret = cptvf_register_interrupts(cptvf);
 	if (ret)
 		goto destroy_pfvf_mbox;
 
-	/* Enable PF-VF mailbox interrupts */
-	cptvf_enable_pfvf_mbox_intrs(cptvf);
-
-	/* Send ready message */
-	ret = otx2_cpt_send_ready_msg(cptvf->pdev);
-	if (ret)
-		goto unregister_interrupts;
-
-	/* Get engine group number for symmetric crypto */
-	cptvf->lfs.kcrypto_eng_grp_num = OTX2_CPT_INVALID_CRYPTO_ENG_GRP;
-	ret = otx2_cptvf_send_eng_grp_num_msg(cptvf, OTX2_CPT_SE_TYPES);
-	if (ret)
-		goto unregister_interrupts;
-
-	if (cptvf->lfs.kcrypto_eng_grp_num == OTX2_CPT_INVALID_CRYPTO_ENG_GRP) {
-		dev_err(dev, "Engine group for kernel crypto not available\n");
-		ret = -ENOENT;
-		goto unregister_interrupts;
-	}
-	ret = otx2_cptvf_send_kcrypto_limits_msg(cptvf);
-	if (ret)
-		goto unregister_interrupts;
-
-	kcrypto_lfs = cptvf->lfs.kcrypto_limits ? cptvf->lfs.kcrypto_limits :
-		      num_online_cpus();
-
 	cptvf->blkaddr = (cpt_block_num == 0) ? BLKADDR_CPT0 : BLKADDR_CPT1;
 	/* Initialize CPT LFs */
-	ret = otx2_cptvf_lf_init(pdev, cptvf->reg_base, &cptvf->lfs,
-				 kcrypto_lfs);
+	ret = cptvf_lf_init(cptvf);
 	if (ret)
 		goto unregister_interrupts;
 
@@ -215,20 +371,10 @@ static int otx2_cptvf_probe(struct pci_dev *pdev,
 
 unregister_interrupts:
 	cptvf_disable_pfvf_mbox_intrs(cptvf);
-	cptvf_unregister_interrupts(cptvf);
 destroy_pfvf_mbox:
 	cptvf_pfvf_mbox_destroy(cptvf);
-iounmap_pfvf:
-	iounmap(cptvf->pfvf_mbox_base);
-pci_unmap:
-	pci_iounmap(pdev, cptvf->reg_base);
-release_regions:
-	pci_release_regions(pdev);
-disable_device:
-	pci_disable_device(pdev);
 clear_drvdata:
 	pci_set_drvdata(pdev, NULL);
-	kfree(cptvf);
 
 	return ret;
 }
@@ -241,23 +387,12 @@ static void otx2_cptvf_remove(struct pci_dev *pdev)
 		dev_err(&pdev->dev, "Invalid CPT VF device.\n");
 		return;
 	}
-
-	/* Shutdown CPT LFs */
-	if (otx2_cptvf_lf_shutdown(pdev, &cptvf->lfs))
-		dev_err(&pdev->dev, "CPT LFs shutdown failed.\n");
+	cptvf_lf_shutdown(&cptvf->lfs);
 	/* Disable PF-VF mailbox interrupt */
 	cptvf_disable_pfvf_mbox_intrs(cptvf);
-	/* Unregister interrupts */
-	cptvf_unregister_interrupts(cptvf);
 	/* Destroy PF-VF mbox */
 	cptvf_pfvf_mbox_destroy(cptvf);
-	/* Unmap PF-VF mailbox memory */
-	iounmap(cptvf->pfvf_mbox_base);
-	pci_iounmap(pdev, cptvf->reg_base);
-	pci_release_regions(pdev);
-	pci_disable_device(pdev);
 	pci_set_drvdata(pdev, NULL);
-	kfree(cptvf);
 }
 
 /* Supported devices */
@@ -267,7 +402,7 @@ static const struct pci_device_id otx2_cptvf_id_table[] = {
 };
 
 static struct pci_driver otx2_cptvf_pci_driver = {
-	.name = OTX2_CPT_DRV_NAME,
+	.name = OTX2_CPTVF_DRV_NAME,
 	.id_table = otx2_cptvf_id_table,
 	.probe = otx2_cptvf_probe,
 	.remove = otx2_cptvf_remove,
@@ -275,8 +410,7 @@ static struct pci_driver otx2_cptvf_pci_driver = {
 
 module_pci_driver(otx2_cptvf_pci_driver);
 
-MODULE_AUTHOR("Marvell International Ltd.");
+MODULE_AUTHOR("Marvell");
 MODULE_DESCRIPTION("Marvell OcteonTX2 CPT Virtual Function Driver");
 MODULE_LICENSE("GPL v2");
-MODULE_VERSION(OTX2_CPT_DRV_VERSION);
 MODULE_DEVICE_TABLE(pci, otx2_cptvf_id_table);
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptvf_mbox.c b/drivers/crypto/marvell/octeontx2/otx2_cptvf_mbox.c
index 4f3aa4f478d7..ed7d45c4f649 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptvf_mbox.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptvf_mbox.c
@@ -1,28 +1,9 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX2 CPT driver
- *
- * Copyright (C) 2018 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-
-#include "otx2_cpt_mbox_common.h"
-#include "rvu_reg.h"
-
-static void dump_mbox_msg(struct mbox_msghdr *msg, int size)
-{
-	u16 pf_id, vf_id;
-
-	pf_id = (msg->pcifunc >> RVU_PFVF_PF_SHIFT) & RVU_PFVF_PF_MASK;
-	vf_id = (msg->pcifunc >> RVU_PFVF_FUNC_SHIFT) & RVU_PFVF_FUNC_MASK;
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2018 Marvell. */
 
-	pr_debug("MBOX opcode %s received from (PF%d/VF%d), size %d, rc %d",
-		 otx2_cpt_get_mbox_opcode_str(msg->id), pf_id, vf_id, size,
-		 msg->rc);
-	print_hex_dump_debug("", DUMP_PREFIX_OFFSET, 16, 2, msg, size, false);
-}
+#include "otx2_cpt_common.h"
+#include "otx2_cptvf.h"
+#include <rvu_reg.h>
 
 irqreturn_t otx2_cptvf_pfvf_mbox_intr(int __always_unused irq, void *arg)
 {
@@ -43,125 +24,116 @@ irqreturn_t otx2_cptvf_pfvf_mbox_intr(int __always_unused irq, void *arg)
 	return IRQ_HANDLED;
 }
 
-void otx2_cptvf_pfvf_mbox_handler(struct work_struct *work)
+static void process_pfvf_mbox_mbox_msg(struct otx2_cptvf_dev *cptvf,
+				       struct mbox_msghdr *msg)
 {
-	struct otx2_cpt_kcrypto_limits_rsp *rsp_limits;
-	struct otx2_cpt_eng_grp_num_rsp *rsp_grp;
+	struct otx2_cptlfs_info *lfs = &cptvf->lfs;
+	struct otx2_cpt_kvf_limits_rsp *rsp_limits;
+	struct otx2_cpt_egrp_num_rsp *rsp_grp;
 	struct cpt_rd_wr_reg_msg *rsp_reg;
 	struct msix_offset_rsp *rsp_msix;
+	int i;
+
+	if (msg->id >= MBOX_MSG_MAX) {
+		dev_err(&cptvf->pdev->dev,
+			"MBOX msg with unknown ID %d\n", msg->id);
+		return;
+	}
+	if (msg->sig != OTX2_MBOX_RSP_SIG) {
+		dev_err(&cptvf->pdev->dev,
+			"MBOX msg with wrong signature %x, ID %d\n",
+			msg->sig, msg->id);
+		return;
+	}
+	switch (msg->id) {
+	case MBOX_MSG_READY:
+		cptvf->vf_id = ((msg->pcifunc >> RVU_PFVF_FUNC_SHIFT)
+				& RVU_PFVF_FUNC_MASK) - 1;
+		break;
+	case MBOX_MSG_ATTACH_RESOURCES:
+		/* Check if resources were successfully attached */
+		if (!msg->rc)
+			lfs->are_lfs_attached = 1;
+		break;
+	case MBOX_MSG_DETACH_RESOURCES:
+		/* Check if resources were successfully detached */
+		if (!msg->rc)
+			lfs->are_lfs_attached = 0;
+		break;
+	case MBOX_MSG_MSIX_OFFSET:
+		rsp_msix = (struct msix_offset_rsp *) msg;
+		for (i = 0; i < rsp_msix->cptlfs; i++)
+			lfs->lf[i].msix_offset = rsp_msix->cptlf_msixoff[i];
+
+		for (i = 0; i < rsp_msix->cpt1_lfs; i++)
+			lfs->lf[i].msix_offset = rsp_msix->cpt1_lf_msixoff[i];
+		break;
+	case MBOX_MSG_CPT_RD_WR_REGISTER:
+		rsp_reg = (struct cpt_rd_wr_reg_msg *) msg;
+		if (msg->rc) {
+			dev_err(&cptvf->pdev->dev,
+				"Reg %llx rd/wr(%d) failed %d\n",
+				rsp_reg->reg_offset, rsp_reg->is_write,
+				msg->rc);
+			return;
+		}
+		if (!rsp_reg->is_write)
+			*rsp_reg->ret_val = rsp_reg->val;
+		break;
+	case MBOX_MSG_GET_ENG_GRP_NUM:
+		rsp_grp = (struct otx2_cpt_egrp_num_rsp *) msg;
+		cptvf->lfs.kcrypto_eng_grp_num = rsp_grp->eng_grp_num;
+		break;
+	case MBOX_MSG_GET_KVF_LIMITS:
+		rsp_limits = (struct otx2_cpt_kvf_limits_rsp *) msg;
+		cptvf->lfs.kvf_limits = rsp_limits->kvf_limits;
+		break;
+	default:
+		dev_err(&cptvf->pdev->dev, "Unsupported msg %d received.\n",
+			msg->id);
+		break;
+	}
+}
+
+void otx2_cptvf_pfvf_mbox_handler(struct work_struct *work)
+{
 	struct otx2_cptvf_dev *cptvf;
 	struct otx2_mbox *pfvf_mbox;
+	struct otx2_mbox_dev *mdev;
 	struct mbox_hdr *rsp_hdr;
 	struct mbox_msghdr *msg;
-	int offset, i, j, size;
+	int offset, i;
 
-	/* Read latest mbox data */
+	/* sync with mbox memory region */
 	smp_rmb();
 
 	cptvf = container_of(work, struct otx2_cptvf_dev, pfvf_mbox_work);
 	pfvf_mbox = &cptvf->pfvf_mbox;
-	rsp_hdr = (struct mbox_hdr *)(pfvf_mbox->dev->mbase +
-		   pfvf_mbox->rx_start);
+	mdev = &pfvf_mbox->dev[0];
+	rsp_hdr = (struct mbox_hdr *)(mdev->mbase + pfvf_mbox->rx_start);
 	if (rsp_hdr->num_msgs == 0)
 		return;
 	offset = ALIGN(sizeof(struct mbox_hdr), MBOX_MSG_ALIGN);
 
 	for (i = 0; i < rsp_hdr->num_msgs; i++) {
-		msg = (struct mbox_msghdr *)(pfvf_mbox->dev->mbase +
-					     pfvf_mbox->rx_start + offset);
-		size = msg->next_msgoff - offset;
-
-		if (msg->id >= MBOX_MSG_MAX) {
-			dev_err(&cptvf->pdev->dev,
-				"MBOX msg with unknown ID %d\n", msg->id);
-			goto error;
-		}
-
-		if (msg->sig != OTX2_MBOX_RSP_SIG) {
-			dev_err(&cptvf->pdev->dev,
-				"MBOX msg with wrong signature %x, ID %d\n",
-				msg->sig, msg->id);
-			goto error;
-		}
-
-		dump_mbox_msg(msg, size);
-
+		msg = (struct mbox_msghdr *)(mdev->mbase + pfvf_mbox->rx_start +
+					     offset);
+		process_pfvf_mbox_mbox_msg(cptvf, msg);
 		offset = msg->next_msgoff;
-		switch (msg->id) {
-		case MBOX_MSG_READY:
-			cptvf->vf_id = ((msg->pcifunc >> RVU_PFVF_FUNC_SHIFT)
-					& RVU_PFVF_FUNC_MASK) - 1;
-			break;
-
-		case MBOX_MSG_ATTACH_RESOURCES:
-			/* Check if resources were successfully attached */
-			if (!msg->rc)
-				cptvf->lfs.are_lfs_attached = 1;
-			break;
-
-		case MBOX_MSG_DETACH_RESOURCES:
-			/* Check if resources were successfully detached */
-			if (!msg->rc)
-				cptvf->lfs.are_lfs_attached = 0;
-			break;
-
-		case MBOX_MSG_MSIX_OFFSET:
-			rsp_msix = (struct msix_offset_rsp *) msg;
-			for (j = 0; j < rsp_msix->cptlfs; j++)
-				cptvf->lfs.lf[j].msix_offset =
-						rsp_msix->cptlf_msixoff[j];
-
-			for (j = 0; j < rsp_msix->cpt1_lfs; j++)
-				cptvf->lfs.lf[j].msix_offset =
-						rsp_msix->cpt1_lf_msixoff[j];
-			break;
-
-		case MBOX_MSG_CPT_RD_WR_REGISTER:
-			rsp_reg = (struct cpt_rd_wr_reg_msg *) msg;
-			if (msg->rc) {
-				dev_err(&cptvf->pdev->dev,
-					"Reg %llx rd/wr(%d) failed %d\n",
-					rsp_reg->reg_offset, rsp_reg->is_write,
-					msg->rc);
-				continue;
-			}
-
-			if (!rsp_reg->is_write)
-				*rsp_reg->ret_val = rsp_reg->val;
-			break;
-
-		case MBOX_MSG_GET_ENG_GRP_NUM:
-			rsp_grp = (struct otx2_cpt_eng_grp_num_rsp *) msg;
-			cptvf->lfs.kcrypto_eng_grp_num = rsp_grp->eng_grp_num;
-			break;
-
-		case MBOX_MSG_GET_KCRYPTO_LIMITS:
-			rsp_limits = (struct otx2_cpt_kcrypto_limits_rsp *) msg;
-			cptvf->lfs.kcrypto_limits = rsp_limits->kcrypto_limits;
-			break;
-
-		default:
-			dev_err(&cptvf->pdev->dev,
-				"Unsupported msg %d received.\n",
-				msg->id);
-			break;
-		}
-error:
-		pfvf_mbox->dev->msgs_acked++;
+		mdev->msgs_acked++;
 	}
 	otx2_mbox_reset(pfvf_mbox, 0);
 }
 
 int otx2_cptvf_send_eng_grp_num_msg(struct otx2_cptvf_dev *cptvf, int eng_type)
 {
-	struct otx2_cpt_eng_grp_num_msg *req;
+	struct otx2_mbox *mbox = &cptvf->pfvf_mbox;
 	struct pci_dev *pdev = cptvf->pdev;
-	int ret;
+	struct otx2_cpt_egrp_num_msg *req;
 
-	req = (struct otx2_cpt_eng_grp_num_msg *)
-		otx2_mbox_alloc_msg_rsp(&cptvf->pfvf_mbox, 0,
-				sizeof(*req),
-				sizeof(struct otx2_cpt_eng_grp_num_rsp));
+	req = (struct otx2_cpt_egrp_num_msg *)
+	      otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+				      sizeof(struct otx2_cpt_egrp_num_rsp));
 	if (req == NULL) {
 		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
 		return -EFAULT;
@@ -171,30 +143,28 @@ int otx2_cptvf_send_eng_grp_num_msg(struct otx2_cptvf_dev *cptvf, int eng_type)
 	req->hdr.pcifunc = OTX2_CPT_RVU_PFFUNC(cptvf->vf_id, 0);
 	req->eng_type = eng_type;
 
-	ret = otx2_cpt_send_mbox_msg(pdev);
-
-	return ret;
+	return otx2_cpt_send_mbox_msg(mbox, pdev);
 }
 
-int otx2_cptvf_send_kcrypto_limits_msg(struct otx2_cptvf_dev *cptvf)
+int otx2_cptvf_send_kvf_limits_msg(struct otx2_cptvf_dev *cptvf)
 {
+	struct otx2_mbox *mbox = &cptvf->pfvf_mbox;
 	struct pci_dev *pdev = cptvf->pdev;
 	struct mbox_msghdr *req;
 	int ret;
 
 	req = (struct mbox_msghdr *)
-		otx2_mbox_alloc_msg_rsp(&cptvf->pfvf_mbox, 0,
-				sizeof(*req),
-				sizeof(struct otx2_cpt_kcrypto_limits_rsp));
+	      otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+				      sizeof(struct otx2_cpt_kvf_limits_rsp));
 	if (req == NULL) {
 		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
 		return -EFAULT;
 	}
-	req->id = MBOX_MSG_GET_KCRYPTO_LIMITS;
+	req->id = MBOX_MSG_GET_KVF_LIMITS;
 	req->sig = OTX2_MBOX_REQ_SIG;
 	req->pcifunc = OTX2_CPT_RVU_PFFUNC(cptvf->vf_id, 0);
 
-	ret = otx2_cpt_send_mbox_msg(pdev);
+	ret = otx2_cpt_send_mbox_msg(mbox, pdev);
 
 	return ret;
 }
diff --git a/drivers/crypto/marvell/octeontx2/otx2_cptvf_reqmgr.c b/drivers/crypto/marvell/octeontx2/otx2_cptvf_reqmgr.c
index 9676a3448272..7a217e20ca07 100644
--- a/drivers/crypto/marvell/octeontx2/otx2_cptvf_reqmgr.c
+++ b/drivers/crypto/marvell/octeontx2/otx2_cptvf_reqmgr.c
@@ -1,16 +1,8 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTX CPT driver
- *
- * Copyright (C) 2019 Marvell International Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (C) 2019 Marvell. */
 
 #include "otx2_cptvf.h"
-#include "otx2_cptvf_algs.h"
-#include "otx2_cpt_mbox_common.h"
+#include "otx2_cpt_common.h"
 
 /* SG list header size in bytes */
 #define SG_LIST_HDR_SIZE	8
@@ -32,8 +24,8 @@ static void otx2_cpt_dump_sg_list(struct pci_dev *pdev,
 {
 	int i;
 
-	pr_debug("Gather list size %d\n", req->incnt);
-	for (i = 0; i < req->incnt; i++) {
+	pr_debug("Gather list size %d\n", req->in_cnt);
+	for (i = 0; i < req->in_cnt; i++) {
 		pr_debug("Buffer %d size %d, vptr 0x%p, dmaptr 0x%p\n", i,
 			 req->in[i].size, req->in[i].vptr,
 			 (void *) req->in[i].dma_addr);
@@ -42,9 +34,8 @@ static void otx2_cpt_dump_sg_list(struct pci_dev *pdev,
 		print_hex_dump_debug("", DUMP_PREFIX_NONE, 16, 1,
 				     req->in[i].vptr, req->in[i].size, false);
 	}
-
-	pr_debug("Scatter list size %d\n", req->outcnt);
-	for (i = 0; i < req->outcnt; i++) {
+	pr_debug("Scatter list size %d\n", req->out_cnt);
+	for (i = 0; i < req->out_cnt; i++) {
 		pr_debug("Buffer %d size %d, vptr 0x%p, dmaptr 0x%p\n", i,
 			 req->out[i].size, req->out[i].vptr,
 			 (void *) req->out[i].dma_addr);
@@ -107,27 +98,24 @@ static inline int setup_sgio_components(struct pci_dev *pdev,
 	}
 
 	for (i = 0; i < buf_count; i++) {
-		if (likely(list[i].vptr)) {
-			list[i].dma_addr = dma_map_single(&pdev->dev,
-							  list[i].vptr,
-							  list[i].size,
-							  DMA_BIDIRECTIONAL);
-			if (unlikely(dma_mapping_error(&pdev->dev,
-						       list[i].dma_addr))) {
-				dev_err(&pdev->dev, "Dma mapping failed\n");
-				ret = -EIO;
-				goto sg_cleanup;
-			}
+		if (unlikely(!list[i].vptr))
+			continue;
+		list[i].dma_addr = dma_map_single(&pdev->dev, list[i].vptr,
+						  list[i].size,
+						  DMA_BIDIRECTIONAL);
+		if (unlikely(dma_mapping_error(&pdev->dev, list[i].dma_addr))) {
+			dev_err(&pdev->dev, "Dma mapping failed\n");
+			ret = -EIO;
+			goto sg_cleanup;
 		}
 	}
-
 	components = buf_count / 4;
 	sg_ptr = (struct otx2_cpt_sglist_component *)buffer;
 	for (i = 0; i < components; i++) {
-		sg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);
-		sg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);
-		sg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);
-		sg_ptr->u.s.len3 = cpu_to_be16(list[i * 4 + 3].size);
+		sg_ptr->len0 = cpu_to_be16(list[i * 4 + 0].size);
+		sg_ptr->len1 = cpu_to_be16(list[i * 4 + 1].size);
+		sg_ptr->len2 = cpu_to_be16(list[i * 4 + 2].size);
+		sg_ptr->len3 = cpu_to_be16(list[i * 4 + 3].size);
 		sg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);
 		sg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);
 		sg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);
@@ -138,15 +126,15 @@ static inline int setup_sgio_components(struct pci_dev *pdev,
 
 	switch (components) {
 	case 3:
-		sg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);
+		sg_ptr->len2 = cpu_to_be16(list[i * 4 + 2].size);
 		sg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);
 		fallthrough;
 	case 2:
-		sg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);
+		sg_ptr->len1 = cpu_to_be16(list[i * 4 + 1].size);
 		sg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);
 		fallthrough;
 	case 1:
-		sg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);
+		sg_ptr->len0 = cpu_to_be16(list[i * 4 + 0].size);
 		sg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);
 		break;
 	default:
@@ -157,8 +145,8 @@ static inline int setup_sgio_components(struct pci_dev *pdev,
 sg_cleanup:
 	for (j = 0; j < i; j++) {
 		if (list[j].dma_addr) {
-			dma_unmap_single(&pdev->dev, list[i].dma_addr,
-					 list[i].size, DMA_BIDIRECTIONAL);
+			dma_unmap_single(&pdev->dev, list[j].dma_addr,
+					 list[j].size, DMA_BIDIRECTIONAL);
 		}
 
 		list[j].dma_addr = 0;
@@ -166,92 +154,87 @@ static inline int setup_sgio_components(struct pci_dev *pdev,
 	return ret;
 }
 
-static inline int setup_sgio_list(struct pci_dev *pdev,
-				  struct otx2_cpt_info_buffer **pinfo,
-				  struct otx2_cpt_req_info *req, gfp_t gfp)
+static inline struct otx2_cpt_inst_info *info_create(struct pci_dev *pdev,
+					      struct otx2_cpt_req_info *req,
+					      gfp_t gfp)
 {
-	u32 dlen, align_dlen, info_len, rlen;
-	struct otx2_cpt_info_buffer *info;
 	int align = OTX2_CPT_DMA_MINALIGN;
+	struct otx2_cpt_inst_info *info;
+	u32 dlen, align_dlen, info_len;
 	u16 g_sz_bytes, s_sz_bytes;
 	u32 total_mem_len;
 
-	if (unlikely(req->incnt > OTX2_CPT_MAX_SG_IN_CNT ||
-		     req->outcnt > OTX2_CPT_MAX_SG_OUT_CNT)) {
+	if (unlikely(req->in_cnt > OTX2_CPT_MAX_SG_IN_CNT ||
+		     req->out_cnt > OTX2_CPT_MAX_SG_OUT_CNT)) {
 		dev_err(&pdev->dev, "Error too many sg components\n");
-		return -EINVAL;
+		return NULL;
 	}
 
-	g_sz_bytes = ((req->incnt + 3) / 4) *
+	g_sz_bytes = ((req->in_cnt + 3) / 4) *
 		      sizeof(struct otx2_cpt_sglist_component);
-	s_sz_bytes = ((req->outcnt + 3) / 4) *
+	s_sz_bytes = ((req->out_cnt + 3) / 4) *
 		      sizeof(struct otx2_cpt_sglist_component);
 
 	dlen = g_sz_bytes + s_sz_bytes + SG_LIST_HDR_SIZE;
 	align_dlen = ALIGN(dlen, align);
 	info_len = ALIGN(sizeof(*info), align);
-	rlen = ALIGN(sizeof(union otx2_cpt_res_s), align);
-	total_mem_len = align_dlen + info_len + rlen +
-			OTX2_CPT_COMPLETION_CODE_SIZE;
+	total_mem_len = align_dlen + info_len + sizeof(union otx2_cpt_res_s);
 
 	info = kzalloc(total_mem_len, gfp);
-	if (unlikely(!info)) {
-		dev_err(&pdev->dev, "Memory allocation failed\n");
-		return -ENOMEM;
-	}
-	*pinfo = info;
+	if (unlikely(!info))
+		return NULL;
+
 	info->dlen = dlen;
 	info->in_buffer = (u8 *)info + info_len;
 
-	((u16 *)info->in_buffer)[0] = req->outcnt;
-	((u16 *)info->in_buffer)[1] = req->incnt;
+	((u16 *)info->in_buffer)[0] = req->out_cnt;
+	((u16 *)info->in_buffer)[1] = req->in_cnt;
 	((u16 *)info->in_buffer)[2] = 0;
 	((u16 *)info->in_buffer)[3] = 0;
-	*(u64 *)info->in_buffer = cpu_to_be64p((u64 *)info->in_buffer);
+	cpu_to_be64s((u64 *)info->in_buffer);
 
 	/* Setup gather (input) components */
-	if (setup_sgio_components(pdev, req->in, req->incnt,
+	if (setup_sgio_components(pdev, req->in, req->in_cnt,
 				  &info->in_buffer[8])) {
 		dev_err(&pdev->dev, "Failed to setup gather list\n");
-		return -EFAULT;
+		goto destroy_info;
 	}
 
-	if (setup_sgio_components(pdev, req->out, req->outcnt,
+	if (setup_sgio_components(pdev, req->out, req->out_cnt,
 				  &info->in_buffer[8 + g_sz_bytes])) {
 		dev_err(&pdev->dev, "Failed to setup scatter list\n");
-		return -EFAULT;
+		goto destroy_info;
 	}
 
 	info->dma_len = total_mem_len - info_len;
-	info->dptr_baddr = dma_map_single(&pdev->dev, (void *)info->in_buffer,
+	info->dptr_baddr = dma_map_single(&pdev->dev, info->in_buffer,
 					  info->dma_len, DMA_BIDIRECTIONAL);
 	if (unlikely(dma_mapping_error(&pdev->dev, info->dptr_baddr))) {
 		dev_err(&pdev->dev, "DMA Mapping failed for cpt req\n");
-		return -EIO;
+		goto destroy_info;
 	}
 	/*
 	 * Get buffer for union otx2_cpt_res_s response
 	 * structure and its physical address
 	 */
-	info->completion_addr = (u64 *)(info->in_buffer + align_dlen);
+	info->completion_addr = info->in_buffer + align_dlen;
 	info->comp_baddr = info->dptr_baddr + align_dlen;
 
-	/* Create and initialize RPTR */
-	info->out_buffer = (u8 *)info->completion_addr + rlen;
-	info->rptr_baddr = info->comp_baddr + rlen;
-
-	*((u64 *) info->out_buffer) = ~((u64) OTX2_CPT_COMPLETION_CODE_INIT);
+	return info;
 
-	return 0;
+destroy_info:
+	otx2_cpt_info_destroy(pdev, info);
+	return NULL;
 }
 
 static int process_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
-			   struct otx2_cpt_pending_queue *pqueue, void *obj)
+			   struct otx2_cpt_pending_queue *pqueue,
+			   struct otx2_cptlf_info *lf)
 {
 	struct otx2_cptvf_request *cpt_req = &req->req;
 	struct otx2_cpt_pending_entry *pentry = NULL;
 	union otx2_cpt_ctrl_info *ctrl = &req->ctrl;
-	struct otx2_cpt_info_buffer *info = NULL;
+	struct otx2_cpt_inst_info *info = NULL;
 	union otx2_cpt_res_s *result = NULL;
 	struct otx2_cpt_iq_command iq_cmd;
 	union otx2_cpt_inst_s cptinst;
@@ -261,14 +244,17 @@ static int process_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 
 	gfp = (req->areq->flags & CRYPTO_TFM_REQ_MAY_SLEEP) ? GFP_KERNEL :
 							      GFP_ATOMIC;
-	ret = setup_sgio_list(pdev, &info, req, gfp);
-	if (unlikely(ret)) {
-		dev_err(&pdev->dev, "Setting up SG list failed");
-		goto request_cleanup;
+	if (unlikely(!otx2_cptlf_started(lf->lfs)))
+		return -ENODEV;
+
+	info = info_create(pdev, req, gfp);
+	if (unlikely(!info)) {
+		dev_err(&pdev->dev, "Setting up cpt inst info failed");
+		return -ENOMEM;
 	}
 	cpt_req->dlen = info->dlen;
 
-	result = (union otx2_cpt_res_s *) info->completion_addr;
+	result = info->completion_addr;
 	result->s.compcode = OTX2_CPT_COMPLETION_CODE_INIT;
 
 	spin_lock_bh(&pqueue->lock);
@@ -283,8 +269,7 @@ static int process_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 
 	if (unlikely(!pentry)) {
 		ret = -ENOSPC;
-		spin_unlock_bh(&pqueue->lock);
-		goto request_cleanup;
+		goto destroy_info;
 	}
 
 	/*
@@ -310,17 +295,17 @@ static int process_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 	info->req = req;
 
 	/* Fill in the command */
-	iq_cmd.cmd.u64 = 0;
+	iq_cmd.cmd.u = 0;
 	iq_cmd.cmd.s.opcode = cpu_to_be16(cpt_req->opcode.flags);
 	iq_cmd.cmd.s.param1 = cpu_to_be16(cpt_req->param1);
 	iq_cmd.cmd.s.param2 = cpu_to_be16(cpt_req->param2);
 	iq_cmd.cmd.s.dlen   = cpu_to_be16(cpt_req->dlen);
 
 	/* 64-bit swap for microcode data reads, not needed for addresses*/
-	iq_cmd.cmd.u64 = cpu_to_be64(iq_cmd.cmd.u64);
+	cpu_to_be64s(&iq_cmd.cmd.u);
 	iq_cmd.dptr = info->dptr_baddr;
-	iq_cmd.rptr = info->rptr_baddr;
-	iq_cmd.cptr.u64 = 0;
+	iq_cmd.rptr = 0;
+	iq_cmd.cptr.u = 0;
 	iq_cmd.cptr.s.grp = ctrl->s.grp;
 
 	/* Fill in the CPT_INST_S type command for HW interpretation */
@@ -335,7 +320,7 @@ static int process_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 			     cpt_req->dlen, false);
 
 	/* Send CPT command */
-	otx2_cpt_send_cmd(&cptinst, 1, obj);
+	otx2_cpt_send_cmd(&cptinst, 1, lf);
 
 	/*
 	 * We allocate and prepare pending queue entry in critical section
@@ -348,61 +333,58 @@ static int process_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 	ret = resume_sender ? -EBUSY : -EINPROGRESS;
 	return ret;
 
-request_cleanup:
-	do_request_cleanup(pdev, info);
+destroy_info:
+	spin_unlock_bh(&pqueue->lock);
+	otx2_cpt_info_destroy(pdev, info);
 	return ret;
 }
 
-int otx2_cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev)
-{
-	struct otx2_cptlfs_info *lfs = otx2_cpt_get_lfs_info(pdev);
-
-	return lfs->kcrypto_eng_grp_num;
-}
-
 int otx2_cpt_do_request(struct pci_dev *pdev, struct otx2_cpt_req_info *req,
 			int cpu_num)
 {
-	struct otx2_cptlfs_info *lfs = otx2_cpt_get_lfs_info(pdev);
+	struct otx2_cptvf_dev *cptvf = pci_get_drvdata(pdev);
+	struct otx2_cptlfs_info *lfs = &cptvf->lfs;
 
-	return process_request(pdev, req, &lfs->lf[cpu_num].pqueue,
+	return process_request(lfs->pdev, req, &lfs->lf[cpu_num].pqueue,
 			       &lfs->lf[cpu_num]);
 }
 
 static int cpt_process_ccode(struct pci_dev *pdev,
 			     union otx2_cpt_res_s *cpt_status,
-			     struct otx2_cpt_info_buffer *cpt_info,
-			     struct otx2_cpt_req_info *req, u32 *res_code)
+			     struct otx2_cpt_inst_info *info,
+			     u32 *res_code)
 {
+	u8 uc_ccode = cpt_status->s.uc_compcode;
 	u8 ccode = cpt_status->s.compcode;
 
 	switch (ccode) {
 	case OTX2_CPT_COMP_E_FAULT:
 		dev_err(&pdev->dev,
 			"Request failed with DMA fault\n");
-		otx2_cpt_dump_sg_list(pdev, req);
+		otx2_cpt_dump_sg_list(pdev, info->req);
 		break;
 
 	case OTX2_CPT_COMP_E_HWERR:
 		dev_err(&pdev->dev,
 			"Request failed with hardware error\n");
-		otx2_cpt_dump_sg_list(pdev, req);
+		otx2_cpt_dump_sg_list(pdev, info->req);
 		break;
 
 	case OTX2_CPT_COMP_E_INSTERR:
 		dev_err(&pdev->dev,
 			"Request failed with instruction error\n");
-		otx2_cpt_dump_sg_list(pdev, req);
+		otx2_cpt_dump_sg_list(pdev, info->req);
 		break;
 
-	case OTX2_CPT_COMPLETION_CODE_INIT:
+	case OTX2_CPT_COMP_E_NOTDONE:
 		/* check for timeout */
-		if (time_after_eq(jiffies, cpt_info->time_in +
+		if (time_after_eq(jiffies, info->time_in +
 				  CPT_COMMAND_TIMEOUT * HZ))
-			dev_warn(&pdev->dev, "Request timed out 0x%p", req);
-		else if (cpt_info->extra_time < CPT_TIME_IN_RESET_COUNT) {
-			cpt_info->time_in = jiffies;
-			cpt_info->extra_time++;
+			dev_warn(&pdev->dev,
+				 "Request timed out 0x%p", info->req);
+		else if (info->extra_time < CPT_TIME_IN_RESET_COUNT) {
+			info->time_in = jiffies;
+			info->extra_time++;
 		}
 		return 1;
 
@@ -411,7 +393,7 @@ static int cpt_process_ccode(struct pci_dev *pdev,
 		 * Check microcode completion code, it is only valid
 		 * when completion code is CPT_COMP_E::GOOD
 		 */
-		if (cpt_status->s.uc_compcode) {
+		if (uc_ccode != OTX2_CPT_UCC_SUCCESS) {
 			/*
 			 * If requested hmac is truncated and ucode returns
 			 * s/g write length error then we report success
@@ -420,8 +402,8 @@ static int cpt_process_ccode(struct pci_dev *pdev,
 			 * s/g write length error if number of bytes in gather
 			 * buffer is less than full hmac size.
 			 */
-			if (req->is_trunc_hmac && cpt_status->s.uc_compcode
-			    == ERR_SCATTER_GATHER_WRITE_LENGTH) {
+			if (info->req->is_trunc_hmac &&
+			    uc_ccode == OTX2_CPT_UCC_SG_WRITE_LENGTH) {
 				*res_code = 0;
 				break;
 			}
@@ -429,10 +411,9 @@ static int cpt_process_ccode(struct pci_dev *pdev,
 			dev_err(&pdev->dev,
 				"Request failed with software error code 0x%x\n",
 				cpt_status->s.uc_compcode);
-			otx2_cpt_dump_sg_list(pdev, req);
+			otx2_cpt_dump_sg_list(pdev, info->req);
 			break;
 		}
-
 		/* Request has been processed with success */
 		*res_code = 0;
 		break;
@@ -451,8 +432,8 @@ static inline void process_pending_queue(struct pci_dev *pdev,
 	struct otx2_cpt_pending_entry *resume_pentry = NULL;
 	void (*callback)(int status, void *arg, void *req);
 	struct otx2_cpt_pending_entry *pentry = NULL;
-	struct otx2_cpt_info_buffer *cpt_info = NULL;
 	union otx2_cpt_res_s *cpt_status = NULL;
+	struct otx2_cpt_inst_info *info = NULL;
 	struct otx2_cpt_req_info *req = NULL;
 	struct crypto_async_request *areq;
 	u32 res_code, resume_index;
@@ -477,30 +458,29 @@ static inline void process_pending_queue(struct pci_dev *pdev,
 			goto process_pentry;
 		}
 
-		cpt_info = pentry->info;
-		if (unlikely(!cpt_info)) {
+		info = pentry->info;
+		if (unlikely(!info)) {
 			dev_err(&pdev->dev, "Pending entry post arg NULL\n");
 			goto process_pentry;
 		}
 
-		req = cpt_info->req;
+		req = info->req;
 		if (unlikely(!req)) {
 			dev_err(&pdev->dev, "Request NULL\n");
 			goto process_pentry;
 		}
 
-		cpt_status = (union otx2_cpt_res_s *) pentry->completion_addr;
+		cpt_status = pentry->completion_addr;
 		if (unlikely(!cpt_status)) {
 			dev_err(&pdev->dev, "Completion address NULL\n");
 			goto process_pentry;
 		}
 
-		if (cpt_process_ccode(pdev, cpt_status, cpt_info, req,
-				      &res_code)) {
+		if (cpt_process_ccode(pdev, cpt_status, info, &res_code)) {
 			spin_unlock_bh(&pqueue->lock);
 			return;
 		}
-		cpt_info->pdev = pdev;
+		info->pdev = pdev;
 
 process_pentry:
 		/*
@@ -524,7 +504,7 @@ static inline void process_pending_queue(struct pci_dev *pdev,
 				 * EINPROGRESS is an indication for sending
 				 * side that it can resume sending requests
 				 */
-				callback(-EINPROGRESS, areq, cpt_info);
+				callback(-EINPROGRESS, areq, info);
 				spin_lock_bh(&pqueue->lock);
 			}
 		}
@@ -543,7 +523,7 @@ static inline void process_pending_queue(struct pci_dev *pdev,
 		 * invalid.
 		 */
 		if (callback)
-			callback(res_code, areq, cpt_info);
+			callback(res_code, areq, info);
 	}
 }
 
@@ -552,3 +532,10 @@ void otx2_cpt_post_process(struct otx2_cptlf_wqe *wqe)
 	process_pending_queue(wqe->lfs->pdev,
 			      &wqe->lfs->lf[wqe->lf_num].pqueue);
 }
+
+int otx2_cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev)
+{
+	struct otx2_cptvf_dev *cptvf = pci_get_drvdata(pdev);
+
+	return cptvf->lfs.kcrypto_eng_grp_num;
+}
-- 
2.31.1

