From eb99f34da98ed0ac17c9d45da7901e98196c1b7d Mon Sep 17 00:00:00 2001
From: Sunil Goutham <sgoutham@marvell.com>
Date: Mon, 28 Oct 2019 15:35:46 +0530
Subject: [PATCH 385/767] octeontx2-pf: NAPI handler cleanup

commit d8a3a2d822c226650d7f362c291d3c4ad0a7031e from
git@git.assembla.com:cavium/WindRiver.linux.git

Major changes done
- Segregate Rx and Tx CQ handlers
- Removed costly CQ_OP_STATUS reads, instead check cqe_type
  in cqe_hdr to determine valid CQE.

Change-Id: I14bfcf2dae39178681464b10174fb269a89c46a0
Signed-off-by: Sunil Goutham <sgoutham@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/17954
Reviewed-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
Tested-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 .../marvell/octeontx2/nic/otx2_common.c       |  20 +-
 .../marvell/octeontx2/nic/otx2_common.h       |   6 +-
 .../ethernet/marvell/octeontx2/nic/otx2_pf.c  |  19 +-
 .../marvell/octeontx2/nic/otx2_txrx.c         | 229 ++++++++----------
 .../marvell/octeontx2/nic/otx2_txrx.h         |  19 +-
 5 files changed, 145 insertions(+), 148 deletions(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
index 1df44d96db90..01b5f1765e42 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
@@ -676,8 +676,16 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	int err, pool_id;
 
 	cq = &qset->cq[qidx];
-	cq->cqe_cnt = (qidx < pfvf->hw.rx_queues) ? qset->rqe_cnt
-			: qset->sqe_cnt;
+	cq->cq_idx = qidx;
+	if (qidx < pfvf->hw.rx_queues) {
+		cq->cq_type = CQ_RX;
+		cq->cint_idx = qidx;
+		cq->cqe_cnt = qset->rqe_cnt;
+	} else {
+		cq->cq_type = CQ_TX;
+		cq->cint_idx = qidx - pfvf->hw.rx_queues;
+		cq->cqe_cnt = qset->sqe_cnt;
+	}
 	cq->cqe_size = pfvf->qset.xqe_size;
 
 	/* Allocate memory for CQEs */
@@ -690,11 +698,9 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	/* In case where all RQs auras point to single pool,
 	 * all CQs receive buffer pool also point to same pool.
 	 */
-	pool_id = ((qidx < pfvf->hw.rx_queues) &&
+	pool_id = ((cq->cq_type == CQ_RX) &&
 		   (pfvf->hw.rqpool_cnt != pfvf->hw.rx_queues)) ? 0 : qidx;
 	cq->rbpool = &qset->pool[pool_id];
-
-	cq->cq_idx = qidx;
 	cq->refill_task_sched = false;
 
 	/* Get memory to put this msg */
@@ -706,9 +712,7 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	aq->cq.qsize = Q_SIZE(cq->cqe_cnt, 4);
 	aq->cq.caching = 1;
 	aq->cq.base = cq->cqe->iova;
-	aq->cq.cint_idx = (qidx < pfvf->hw.rx_queues) ? qidx
-				: (qidx - pfvf->hw.rx_queues);
-	cq->cint_idx = aq->cq.cint_idx;
+	aq->cq.cint_idx = cq->cint_idx;
 	aq->cq.cq_err_int_ena = NIX_CQERRINT_BITS;
 	aq->cq.qint_idx = 0;
 	aq->cq.avg_level = 255;
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
index dc7c7c45eae9..d0d75bebae18 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
@@ -620,8 +620,10 @@ int otx2_rxtx_enable(struct otx2_nic *pfvf, bool enable);
 void otx2_ctx_disable(struct mbox *mbox, int type, bool npa);
 int otx2_nix_config_bp(struct otx2_nic *pfvf, bool enable);
 
-int otx2_napi_handler(struct otx2_cq_queue *cq,
-		      struct otx2_nic *pfvf, int budget);
+int otx2_rx_napi_handler(struct otx2_nic *pfvf, struct napi_struct *napi,
+			 struct otx2_cq_queue *cq, int budget);
+int otx2_tx_napi_handler(struct otx2_nic *pfvf,
+			 struct otx2_cq_queue *cq, int budget);
 
 /* RSS configuration APIs*/
 int otx2_rss_init(struct otx2_nic *pfvf);
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
index e685245e0ddf..0177ba1aac61 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
@@ -1412,9 +1412,11 @@ static void otx2_free_hw_resources(struct otx2_nic *pf)
 	struct otx2_qset *qset = &pf->qset;
 	struct nix_lf_free_req *free_req;
 	struct mbox *mbox = &pf->mbox;
+	struct napi_struct *napi;
 	struct otx2_cq_queue *cq;
-	int err, qidx, cqe_count;
 	struct msg_req *req;
+	int qidx, err;
+	u64 cqe_count;
 
 	/* Stop transmission */
 	err = otx2_txschq_stop(pf);
@@ -1437,8 +1439,13 @@ static void otx2_free_hw_resources(struct otx2_nic *pf)
 		cq = &qset->cq[qidx];
 		cqe_count = otx2_read64(pf, NIX_LF_CINTX_CNT(cq->cint_idx));
 		cqe_count &= 0xFFFFFFFF;
-		if (cqe_count)
-			otx2_napi_handler(cq, pf, cqe_count);
+		napi = &qset->napi[cq->cint_idx].napi;
+		if (cqe_count) {
+			if (cq->cq_type == CQ_RX)
+				otx2_rx_napi_handler(pf, napi, cq, cqe_count);
+			else
+				otx2_tx_napi_handler(pf, cq, cqe_count);
+		}
 	}
 
 	/* Free RQ buffer pointers*/
@@ -1558,13 +1565,13 @@ int otx2_open(struct net_device *netdev)
 		 * 'cq_ids[0]' points to RQ's CQ and
 		 * 'cq_ids[1]' points to SQ's CQ and
 		 */
-		cq_poll->cq_ids[0] =
+		cq_poll->cq_ids[CQ_RX] =
 			(qidx <  pf->hw.rx_queues) ? qidx : CINT_INVALID_CQ;
-		cq_poll->cq_ids[1] = (qidx < pf->hw.tx_queues) ?
+		cq_poll->cq_ids[CQ_TX] = (qidx < pf->hw.tx_queues) ?
 				      qidx + pf->hw.rx_queues : CINT_INVALID_CQ;
 		cq_poll->dev = (void *)pf;
 		netif_napi_add(netdev, &cq_poll->napi,
-			       otx2_poll, NAPI_POLL_WEIGHT);
+			       otx2_napi_handler, NAPI_POLL_WEIGHT);
 		napi_enable(&cq_poll->napi);
 	}
 
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
index 75a8789efd32..3a5a879df5de 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
@@ -18,28 +18,26 @@
 #include "otx2_txrx.h"
 #include "otx2_ptp.h"
 
-/* Flush SQE written to LMT to SQB */
-static inline u64 otx2_lmt_flush(uint64_t addr)
-{
-	return atomic64_fetch_xor_relaxed(0, (atomic64_t *)addr);
-}
+#define CQE_ADDR(CQ, idx) ((CQ)->cqe_base + ((CQ)->cqe_size * (idx)))
 
-static inline u64 otx2_nix_cq_op_status(struct otx2_nic *pfvf, int cq_idx)
+static inline struct nix_cqe_hdr_s *otx2_get_next_cqe(struct otx2_cq_queue *cq)
 {
-	u64 incr = (u64)cq_idx << 32;
-	atomic64_t *ptr;
-	u64 status;
+	struct nix_cqe_hdr_s *cqe_hdr;
 
-	ptr = (__force atomic64_t *)otx2_get_regaddr(pfvf, NIX_LF_CQ_OP_STATUS);
+	cqe_hdr = (struct nix_cqe_hdr_s *)CQE_ADDR(cq, cq->cq_head);
+	if (cqe_hdr->cqe_type == NIX_XQE_TYPE_INVALID)
+		return NULL;
 
-	status = atomic64_fetch_add_relaxed(incr, ptr);
+	cq->cq_head++;
+	cq->cq_head &= (cq->cqe_cnt - 1);
 
-	/* Barrier to prevent speculative reads of CQEs and their
-	 * processing before above load of CQ_STATUS returns.
-	 */
-	dma_rmb();
+	return cqe_hdr;
+}
 
-	return status;
+/* Flush SQE written to LMT to SQB */
+static inline u64 otx2_lmt_flush(uint64_t addr)
+{
+	return atomic64_fetch_xor_relaxed(0, (atomic64_t *)addr);
 }
 
 static inline unsigned int frag_num(unsigned int i)
@@ -86,17 +84,17 @@ static void otx2_dma_unmap_skb_frags(struct otx2_nic *pfvf, struct sg_list *sg)
 }
 
 static void otx2_snd_pkt_handler(struct otx2_nic *pfvf,
-				 struct otx2_cq_queue *cq, void *cqe,
+				 struct otx2_cq_queue *cq,
+				 struct otx2_snd_queue *sq,
+				 struct nix_cqe_hdr_s *cqe_hdr,
 				 int budget, int *tx_pkts, int *tx_bytes)
 {
-	struct nix_cqe_hdr_s *cqe_hdr = (struct nix_cqe_hdr_s *)cqe;
 	struct nix_send_comp_s *snd_comp;
 	struct sk_buff *skb = NULL;
-	struct otx2_snd_queue *sq;
 	struct sg_list *sg;
-	int sqe_id;
 
-	snd_comp = (struct nix_send_comp_s *)(cqe + sizeof(*cqe_hdr));
+	snd_comp = (struct nix_send_comp_s *)
+			((void *)cqe_hdr + sizeof(*cqe_hdr));
 	if (snd_comp->status) {
 		/* tx packet error handling*/
 		if (netif_msg_tx_err(pfvf)) {
@@ -108,16 +106,14 @@ static void otx2_snd_pkt_handler(struct otx2_nic *pfvf,
 
 	/* Barrier, so that update to sq by other cpus is visible */
 	smp_mb();
-	sq = &pfvf->qset.sq[cq->cint_idx];
-	sqe_id = snd_comp->sqe_id;
-	sg = &sq->sg[sqe_id];
+	sg = &sq->sg[snd_comp->sqe_id];
 
 	skb = (struct sk_buff *)sg->skb;
 	if (!skb)
 		return;
 
 	if (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {
-		u64 timestamp = ((u64 *)sq->timestamps->base)[sqe_id];
+		u64 timestamp = ((u64 *)sq->timestamps->base)[snd_comp->sqe_id];
 
 		if (timestamp != 1) {
 			u64 tsns;
@@ -222,20 +218,18 @@ static inline void otx2_set_rxtstamp(struct otx2_nic *pfvf, struct sk_buff *skb)
 }
 
 static inline bool otx2_check_rcv_errors(struct otx2_nic *pfvf,
-					 struct otx2_cq_queue *cq, void *cqe)
+					 struct nix_rx_parse_s *parse, int qidx)
 {
 	struct otx2_drv_stats *stats = &pfvf->hw.drv_stats;
-	struct nix_rx_parse_s *parse;
 	struct nix_rx_sg_s *sg;
 	void *start, *end;
 	u64 *iova;
 	int seg;
 
-	parse = (struct nix_rx_parse_s *)(cqe + sizeof(struct nix_cqe_hdr_s));
 	if (netif_msg_rx_err(pfvf))
 		netdev_err(pfvf->netdev,
 			   "RQ%d: Error pkt with errlev:0x%x errcode:0x%x\n",
-			   cq->cq_idx, parse->errlev, parse->errcode);
+			   qidx, parse->errlev, parse->errcode);
 
 	if (parse->errlev == NPC_ERRLVL_RE) {
 		switch (parse->errcode) {
@@ -280,7 +274,7 @@ static inline bool otx2_check_rcv_errors(struct otx2_nic *pfvf,
 		return false;
 	}
 
-	start = cqe + sizeof(struct nix_cqe_hdr_s) + sizeof(*parse);
+	start = (void *)parse + sizeof(*parse);
 	end = start + ((parse->desc_sizem1 + 1) * 16);
 	while ((start + sizeof(*sg)) < end) {
 		sg = (struct nix_rx_sg_s *)start;
@@ -291,7 +285,7 @@ static inline bool otx2_check_rcv_errors(struct otx2_nic *pfvf,
 			return false;
 
 		for (seg = 0; seg < sg->segs; seg++) {
-			otx2_aura_freeptr(pfvf, cq->cq_idx, *iova & ~0x07ULL);
+			otx2_aura_freeptr(pfvf, qidx, *iova & ~0x07ULL);
 			iova++;
 		}
 		if (sg->segs == 1)
@@ -303,10 +297,10 @@ static inline bool otx2_check_rcv_errors(struct otx2_nic *pfvf,
 }
 
 static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
-				 struct otx2_cq_queue *cq, void *cqe)
+				 struct napi_struct *napi,
+				 struct otx2_cq_queue *cq,
+				 struct nix_cqe_hdr_s *cqe_hdr)
 {
-	struct nix_cqe_hdr_s *cqe_hdr = (struct nix_cqe_hdr_s *)cqe;
-	struct otx2_qset *qset = &pfvf->qset;
 	struct nix_rx_parse_s *parse;
 	struct sk_buff *skb = NULL;
 	struct nix_rx_sg_s *sg;
@@ -316,13 +310,13 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 	u64 *iova;
 
 	/* CQE_HDR_S for a Rx pkt is always followed by RX_PARSE_S */
-	parse = (struct nix_rx_parse_s *)(cqe + sizeof(*cqe_hdr));
+	parse = (struct nix_rx_parse_s *)((void *)cqe_hdr + sizeof(*cqe_hdr));
 	if (parse->errlev || parse->errcode) {
-		if (otx2_check_rcv_errors(pfvf, cq, cqe))
+		if (otx2_check_rcv_errors(pfvf, parse, cq->cq_idx))
 			return;
 	}
 
-	start = cqe + sizeof(*cqe_hdr) + sizeof(*parse);
+	start = (void *)parse + sizeof(*parse);
 	end = start + ((parse->desc_sizem1 + 1) * 16);
 
 	/* Run through the each NIX_RX_SG_S subdc and frame the skb */
@@ -366,12 +360,6 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 	if (!skb)
 		return;
 
-	if (netif_msg_pktdata(pfvf) && !skb_is_nonlinear(skb)) {
-		netdev_info(pfvf->netdev, "skb 0x%p, len=%d\n", skb, skb->len);
-		print_hex_dump(KERN_DEBUG, "RX:", DUMP_PREFIX_OFFSET, 16, 1,
-			       skb->data, skb->len, true);
-	}
-
 	otx2_set_rxhash(pfvf, cqe_hdr, skb);
 
 	skb_record_rx_queue(skb, cq->cq_idx);
@@ -391,91 +379,37 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 					       parse->vtag0_tci);
 	}
 
-	napi_gro_receive(&qset->napi[cq->cint_idx].napi, skb);
+	napi_gro_receive(napi, skb);
 }
 
-#define CQE_ADDR(CQ, idx) ((CQ)->cqe_base + ((CQ)->cqe_size * (idx)))
-
-int otx2_napi_handler(struct otx2_cq_queue *cq,
-		      struct otx2_nic *pfvf, int budget)
+int otx2_rx_napi_handler(struct otx2_nic *pfvf,
+			 struct napi_struct *napi,
+			 struct otx2_cq_queue *cq, int budget)
 {
 	struct otx2_pool *rbpool = cq->rbpool;
-	int processed_cqe = 0, workdone = 0;
-	int tx_pkts = 0, tx_bytes = 0;
 	struct nix_cqe_hdr_s *cqe_hdr;
-	struct netdev_queue *txq;
-	u64 cq_status;
+	int processed_cqe = 0;
 	s64 bufptr;
 
-	/* If the pending CQE > 64 skip CQ status read */
-	if (cq->pend_cqe >= budget)
-		goto process_cqe;
-
-	cq_status = otx2_nix_cq_op_status(pfvf, cq->cq_idx);
-	if (cq_status & BIT_ULL(63)) {
-		dev_err(pfvf->dev, "CQ operation error");
-		pfvf->intf_down = true;
-		schedule_work(&pfvf->reset_task);
-		return 0;
-	}
-	if (cq_status & BIT_ULL(46)) {
-		dev_err(pfvf->dev, "CQ stopped due to error");
-		pfvf->intf_down = true;
-		schedule_work(&pfvf->reset_task);
-		return 0;
-	}
-
-	cq->cq_head = (cq_status >> 20) & 0xFFFFF;
-	cq->cq_tail = cq_status & 0xFFFFF;
-
-	/* Since multiple CQs may be mapped to same CINT,
-	 * check if there are valid CQEs in this CQ.
-	 */
-	if (cq->cq_head == cq->cq_tail)
-		return 0;
-process_cqe:
-	cq->pend_cqe = 0;
-	while (cq->cq_head != cq->cq_tail) {
-		if (workdone >= budget) {
-			/* Calculate number of pending CQEs */
-			if (cq->cq_tail < cq->cq_head)
-				cq->pend_cqe = (cq->cqe_cnt - cq->cq_head)
-						+ cq->cq_tail;
-			else
-				cq->pend_cqe = cq->cq_tail - cq->cq_head;
+	/* Make sure HW writes to CQ are done */
+	dma_rmb();
+	while (processed_cqe < budget) {
+		cqe_hdr = otx2_get_next_cqe(cq);
+		if (!cqe_hdr) {
+			if (!processed_cqe)
+				return 0;
 			break;
 		}
+		otx2_rcv_pkt_handler(pfvf, napi, cq, cqe_hdr);
 
-		cqe_hdr = (struct nix_cqe_hdr_s *)CQE_ADDR(cq, cq->cq_head);
-		cq->cq_head++;
-		cq->cq_head &= (cq->cqe_cnt - 1);
-
-		switch (cqe_hdr->cqe_type) {
-		case NIX_XQE_TYPE_RX:
-			/* Receive packet handler*/
-			otx2_rcv_pkt_handler(pfvf, cq, cqe_hdr);
-			workdone++;
-			break;
-		case NIX_XQE_TYPE_SEND:
-			otx2_snd_pkt_handler(pfvf, cq, cqe_hdr, budget,
-					     &tx_pkts, &tx_bytes);
-		}
+		cqe_hdr->cqe_type = NIX_XQE_TYPE_INVALID;
 		processed_cqe++;
 	}
 
+	/* Free CQEs to HW */
 	otx2_write64(pfvf, NIX_LF_CQ_OP_DOOR,
 		     ((u64)cq->cq_idx << 32) | processed_cqe);
 
-	if (tx_pkts) {
-		txq = netdev_get_tx_queue(pfvf->netdev, cq->cint_idx);
-		netdev_tx_completed_queue(txq, tx_pkts, tx_bytes);
-		/* Check if queue was stopped earlier due to ring full */
-		smp_mb();
-		if (netif_carrier_ok(pfvf->netdev) &&
-		    netif_tx_queue_stopped(txq))
-			netif_tx_wake_queue(txq);
-	}
-
 	if (!cq->pool_ptrs)
 		return 0;
 
@@ -501,37 +435,80 @@ int otx2_napi_handler(struct otx2_cq_queue *cq,
 	}
 	otx2_get_page(rbpool);
 
-	return workdone;
+	return processed_cqe;
 }
 
-int otx2_poll(struct napi_struct *napi, int budget)
+int otx2_tx_napi_handler(struct otx2_nic *pfvf,
+			 struct otx2_cq_queue *cq, int budget)
+{
+	struct nix_cqe_hdr_s *cqe_hdr;
+	int tx_pkts = 0, tx_bytes = 0;
+	struct otx2_snd_queue *sq;
+	struct netdev_queue *txq;
+	int processed_cqe = 0;
+
+	sq = &pfvf->qset.sq[cq->cint_idx];
+
+	/* Make sure HW writes to CQ are done */
+	dma_rmb();
+	while (processed_cqe < budget) {
+		cqe_hdr = otx2_get_next_cqe(cq);
+		if (!cqe_hdr) {
+			if (!processed_cqe)
+				return 0;
+			break;
+		}
+		otx2_snd_pkt_handler(pfvf, cq, sq, cqe_hdr, budget,
+				     &tx_pkts, &tx_bytes);
+
+		cqe_hdr->cqe_type = NIX_XQE_TYPE_INVALID;
+		processed_cqe++;
+	}
+
+	/* Free CQEs to HW */
+	otx2_write64(pfvf, NIX_LF_CQ_OP_DOOR,
+		     ((u64)cq->cq_idx << 32) | processed_cqe);
+
+	if (tx_pkts) {
+		txq = netdev_get_tx_queue(pfvf->netdev, cq->cint_idx);
+		netdev_tx_completed_queue(txq, tx_pkts, tx_bytes);
+		/* Check if queue was stopped earlier due to ring full */
+		smp_mb();
+		if (netif_tx_queue_stopped(txq) &&
+		    netif_carrier_ok(pfvf->netdev))
+			netif_tx_wake_queue(txq);
+	}
+	return 0;
+}
+
+int otx2_napi_handler(struct napi_struct *napi, int budget)
 {
 	struct otx2_cq_poll *cq_poll;
 	int workdone = 0, cq_idx, i;
 	struct otx2_cq_queue *cq;
 	struct otx2_qset *qset;
 	struct otx2_nic *pfvf;
-	u64 qcount;
 
 	cq_poll = container_of(napi, struct otx2_cq_poll, napi);
 	pfvf = (struct otx2_nic *)cq_poll->dev;
 	qset = &pfvf->qset;
 
-	for (i = 0; i < MAX_CQS_PER_CNT; i++) {
+	for (i = 0; i < CQS_PER_CINT; i++) {
 		cq_idx = cq_poll->cq_ids[i];
 		if (cq_idx == CINT_INVALID_CQ)
 			continue;
 		cq = &qset->cq[cq_idx];
-		qcount = otx2_read64(pfvf, NIX_LF_CINTX_CNT(cq_poll->cint_idx));
-		qcount = (qcount >> 32) & 0xFFFF;
-		/* If the RQ refill WQ task is running, skip napi
-		 * scheduler for this queue.
-		 */
-		if (cq->refill_task_sched)
-			continue;
-		workdone += otx2_napi_handler(cq, pfvf, budget);
-		if (workdone && qcount == 1)
-			break;
+		if (cq->cq_type == CQ_RX) {
+			/* If the RQ refill WQ task is running, skip napi
+			 * scheduler for this queue.
+			 */
+			if (cq->refill_task_sched)
+				continue;
+			workdone += otx2_rx_napi_handler(pfvf, napi,
+							 cq, budget);
+		} else {
+			workdone += otx2_tx_napi_handler(pfvf, cq, budget);
+		}
 	}
 
 	/* Clear the IRQ */
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h
index 7120ecf436a8..32ddd552e85b 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h
@@ -83,12 +83,17 @@ struct otx2_snd_queue {
 	u64			*sqb_ptrs;
 } ____cacheline_aligned_in_smp;
 
+enum cq_type {
+	CQ_RX,
+	CQ_TX,
+	CQS_PER_CINT = 2, /* RQ + SQ */
+};
+
 struct otx2_cq_poll {
 	void			*dev;
 #define CINT_INVALID_CQ		255
-#define MAX_CQS_PER_CNT		2 /* RQ + SQ */
 	u8			cint_idx;
-	u8			cq_ids[MAX_CQS_PER_CNT];
+	u8			cq_ids[CQS_PER_CINT];
 	struct napi_struct	napi;
 };
 
@@ -101,16 +106,18 @@ struct otx2_pool {
 	struct page		*page;
 };
 
+#define CQ_OP_ERROR	BIT_ULL(63)
+#define CQ_CQ_ERROR	BIT_ULL(46)
+
 struct otx2_cq_queue {
 	u8			cq_idx;
+	u8			cq_type;
 	u8			cint_idx; /* CQ interrupt id */
 	u8			refill_task_sched;
 	u16			cqe_size;
 	u16			pool_ptrs;
 	u32			cqe_cnt;
 	u32			cq_head;
-	u32			cq_tail;
-	u32			pend_cqe;
 	void			*cqe_base;
 	struct qmem		*cqe;
 	struct otx2_pool	*rbpool;
@@ -133,12 +140,12 @@ struct otx2_qset {
 static inline u64 otx2_iova_to_phys(void *iommu_domain, dma_addr_t dma_addr)
 {
 	/* Translation is installed only when IOMMU is present */
-	if (iommu_domain)
+	if (likely(iommu_domain))
 		return iommu_iova_to_phys(iommu_domain, dma_addr);
 	return dma_addr;
 }
 
-int otx2_poll(struct napi_struct *napi, int budget);
+int otx2_napi_handler(struct napi_struct *napi, int budget);
 bool otx2_sq_append_skb(struct net_device *netdev, struct otx2_snd_queue *sq,
 			struct sk_buff *skb, u16 qidx);
 #endif /* OTX2_TXRX_H */
-- 
2.31.1

