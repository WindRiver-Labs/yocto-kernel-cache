From fd2dc2f38d40729e95eff68b963916d46cd09ad1 Mon Sep 17 00:00:00 2001
From: Stefan Chulski <stefanc@marvell.com>
Date: Mon, 12 Oct 2020 10:03:20 -0700
Subject: [PATCH 1043/1921] net: mvpp2: port mvpp2 driver from 4.14

Change-Id: I9345b44089d946e27968bb7281284d941e2b3822
Signed-off-by: Stefan Chulski <stefanc@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/37790
Reviewed-by: Nadav Haklai <nadavh@marvell.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2/mvpp2.h    |  506 ++-
 .../net/ethernet/marvell/mvpp2/mvpp2_cls.c    | 1152 ++---
 .../net/ethernet/marvell/mvpp2/mvpp2_cls.h    |  182 +-
 .../ethernet/marvell/mvpp2/mvpp2_debugfs.c    |  314 +-
 .../net/ethernet/marvell/mvpp2/mvpp2_main.c   | 3901 ++++++++++++-----
 .../net/ethernet/marvell/mvpp2/mvpp2_prs.c    |  107 +-
 .../net/ethernet/marvell/mvpp2/mvpp2_prs.h    |    6 +-
 7 files changed, 3705 insertions(+), 2463 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2.h b/drivers/net/ethernet/marvell/mvpp2/mvpp2.h
index b9c3bf32f1e8..7b4814111337 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2.h
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2.h
@@ -14,7 +14,10 @@
 #include <linux/netdevice.h>
 #include <linux/phy.h>
 #include <linux/phylink.h>
-#include <net/flow_offload.h>
+
+#ifndef CACHE_LINE_MASK
+#define CACHE_LINE_MASK            (~(L1_CACHE_BYTES - 1))
+#endif
 
 /* Fifo Registers */
 #define MVPP2_RX_DATA_FIFO_SIZE_REG(port)	(0x00 + 4 * (port))
@@ -42,10 +45,16 @@
 #define     MVPP2_RXQ_PACKET_OFFSET_OFFS	28
 #define     MVPP2_RXQ_PACKET_OFFSET_MASK	0x70000000
 #define     MVPP2_RXQ_DISABLE_MASK		BIT(31)
+/* Total max number of hw RX queues */
+#define MVPP2_RXQ_MAX_NUM			128
 
 /* Top Registers */
 #define MVPP2_MH_REG(port)			(0x5040 + 4 * (port))
+#define MVPP2_DSA_NON_EXTENDED			BIT(4)
 #define MVPP2_DSA_EXTENDED			BIT(5)
+#define MVPP2_VER_ID_REG			0x50b0
+#define MVPP2_VER_PP22				0x10
+#define MVPP2_VER_PP23				0x11
 
 /* Parser Registers */
 #define MVPP2_PRS_INIT_LOOKUP_REG		0x1000
@@ -102,7 +111,8 @@
 #define MVPP2_CLS_FLOW_TBL1_REG			0x1828
 #define     MVPP2_CLS_FLOW_TBL1_N_FIELDS_MASK	0x7
 #define     MVPP2_CLS_FLOW_TBL1_N_FIELDS(x)	(x)
-#define     MVPP2_CLS_FLOW_TBL1_LU_TYPE(lu)	(((lu) & 0x3f) << 3)
+#define     MVPP2_CLS_FLOW_TBL1_LKP_TYPE_MASK	0x3f
+#define     MVPP2_CLS_FLOW_TBL1_LKP_TYPE(x)	((x) << 3)
 #define     MVPP2_CLS_FLOW_TBL1_PRIO_MASK	0x3f
 #define     MVPP2_CLS_FLOW_TBL1_PRIO(x)		((x) << 9)
 #define     MVPP2_CLS_FLOW_TBL1_SEQ_MASK	0x7
@@ -125,18 +135,17 @@
 #define MVPP22_CLS_C2_TCAM_DATA2		0x1b18
 #define MVPP22_CLS_C2_TCAM_DATA3		0x1b1c
 #define MVPP22_CLS_C2_TCAM_DATA4		0x1b20
-#define     MVPP22_CLS_C2_LU_TYPE(lu)		((lu) & 0x3f)
 #define     MVPP22_CLS_C2_PORT_ID(port)		((port) << 8)
-#define     MVPP22_CLS_C2_PORT_MASK		(0xff << 8)
-#define MVPP22_CLS_C2_TCAM_INV			0x1b24
-#define     MVPP22_CLS_C2_TCAM_INV_BIT		BIT(31)
+#define MVPP2_CLS2_TCAM_INV_REG			0x1b24
+#define     MVPP2_CLS2_TCAM_INV_INVALID		31
+#define     MVPP22_CLS_C2_LKP_TYPE(type)	(type)
+#define     MVPP22_CLS_C2_LKP_TYPE_MASK		(0x3f)
 #define MVPP22_CLS_C2_HIT_CTR			0x1b50
 #define MVPP22_CLS_C2_ACT			0x1b60
 #define     MVPP22_CLS_C2_ACT_RSS_EN(act)	(((act) & 0x3) << 19)
 #define     MVPP22_CLS_C2_ACT_FWD(act)		(((act) & 0x7) << 13)
 #define     MVPP22_CLS_C2_ACT_QHIGH(act)	(((act) & 0x3) << 11)
 #define     MVPP22_CLS_C2_ACT_QLOW(act)		(((act) & 0x3) << 9)
-#define     MVPP22_CLS_C2_ACT_COLOR(act)	((act) & 0x7)
 #define MVPP22_CLS_C2_ATTR0			0x1b64
 #define     MVPP22_CLS_C2_ATTR0_QHIGH(qh)	(((qh) & 0x1f) << 24)
 #define     MVPP22_CLS_C2_ATTR0_QHIGH_MASK	0x1f
@@ -148,8 +157,8 @@
 #define MVPP22_CLS_C2_ATTR2			0x1b6c
 #define     MVPP22_CLS_C2_ATTR2_RSS_EN		BIT(30)
 #define MVPP22_CLS_C2_ATTR3			0x1b70
-#define MVPP22_CLS_C2_TCAM_CTRL			0x1b90
-#define     MVPP22_CLS_C2_TCAM_BYPASS_FIFO	BIT(0)
+#define MVPP2_CLS2_TCAM_CTRL_REG		0x1b90
+#define     MVPP2_CLS2_TCAM_CTRL_BYPASS_FIFO_STAGES	BIT(0)
 
 /* Descriptor Manager Top Registers */
 #define MVPP2_RXQ_NUM_REG			0x2040
@@ -218,6 +227,7 @@
 #define MVPP22_AXI_RXQ_DESCR_WR_ATTR_REG	0x411c
 #define MVPP22_AXI_RX_DATA_WR_ATTR_REG		0x4120
 #define MVPP22_AXI_TX_DATA_RD_ATTR_REG		0x4130
+#define     MVPP22_AXI_TX_DATA_RD_QOS_ATTRIBUTE (0x3 << 4)
 #define MVPP22_AXI_RD_NORMAL_CODE_REG		0x4150
 #define MVPP22_AXI_RD_SNOOP_CODE_REG		0x4154
 #define MVPP22_AXI_WR_NORMAL_CODE_REG		0x4160
@@ -262,8 +272,8 @@
 #define     MVPP2_ISR_ENABLE_INTERRUPT(mask)	((mask) & 0xffff)
 #define     MVPP2_ISR_DISABLE_INTERRUPT(mask)	(((mask) << 16) & 0xffff0000)
 #define MVPP2_ISR_RX_TX_CAUSE_REG(port)		(0x5480 + 4 * (port))
-#define     MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(version) \
-					((version) == MVPP21 ? 0xffff : 0xff)
+#define     MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(variant) \
+			(static_branch_unlikely(&variant) ? 0xffff : 0xff)
 #define     MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK	0xff0000
 #define     MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_OFFSET	16
 #define     MVPP2_CAUSE_RX_FIFO_OVERRUN_MASK	BIT(24)
@@ -278,6 +288,8 @@
 #define     MVPP2_PON_CAUSE_TXP_OCCUP_DESC_ALL_MASK	0x3fc00000
 #define     MVPP2_PON_CAUSE_MISC_SUM_MASK		BIT(31)
 #define MVPP2_ISR_MISC_CAUSE_REG		0x55b0
+#define MVPP2_ISR_RX_ERR_CAUSE_REG(port)	(0x5520 + 4 * (port))
+#define	    MVPP2_ISR_RX_ERR_CAUSE_NONOCC_MASK	0x00ff
 
 /* Buffer Manager registers */
 #define MVPP2_BM_POOL_BASE_REG(pool)		(0x6000 + ((pool) * 4))
@@ -305,6 +317,10 @@
 #define     MVPP2_BM_HIGH_THRESH_MASK		0x7f0000
 #define     MVPP2_BM_HIGH_THRESH_VALUE(val)	((val) << \
 						MVPP2_BM_HIGH_THRESH_OFFS)
+#define     MVPP2_BM_BPPI_HIGH_THRESH		0x1E
+#define     MVPP2_BM_BPPI_LOW_THRESH		0x1C
+#define     MVPP23_BM_BPPI_HIGH_THRESH		0x34
+#define     MVPP23_BM_BPPI_LOW_THRESH		0x28
 #define MVPP2_BM_INTR_CAUSE_REG(pool)		(0x6240 + ((pool) * 4))
 #define     MVPP2_BM_RELEASED_DELAY_MASK	BIT(0)
 #define     MVPP2_BM_ALLOC_FAILED_MASK		BIT(1)
@@ -329,26 +345,12 @@
 #define     MVPP22_BM_ADDR_HIGH_VIRT_RLS_MASK	0xff00
 #define     MVPP22_BM_ADDR_HIGH_VIRT_RLS_SHIFT	8
 
-/* Packet Processor per-port counters */
-#define MVPP2_OVERRUN_ETH_DROP			0x7000
-#define MVPP2_CLS_ETH_DROP			0x7020
+#define MVPP22_BM_POOL_BASE_ADDR_HIGH_REG	0x6310
+#define     MVPP22_BM_POOL_BASE_ADDR_HIGH_MASK	0xff
+#define     MVPP23_BM_8POOL_MODE		BIT(8)
 
 /* Hit counters registers */
 #define MVPP2_CTRS_IDX				0x7040
-#define     MVPP22_CTRS_TX_CTR(port, txq)	((txq) | ((port) << 3) | BIT(7))
-#define MVPP2_TX_DESC_ENQ_CTR			0x7100
-#define MVPP2_TX_DESC_ENQ_TO_DDR_CTR		0x7104
-#define MVPP2_TX_BUFF_ENQ_TO_DDR_CTR		0x7108
-#define MVPP2_TX_DESC_ENQ_HW_FWD_CTR		0x710c
-#define MVPP2_RX_DESC_ENQ_CTR			0x7120
-#define MVPP2_TX_PKTS_DEQ_CTR			0x7130
-#define MVPP2_TX_PKTS_FULL_QUEUE_DROP_CTR	0x7200
-#define MVPP2_TX_PKTS_EARLY_DROP_CTR		0x7204
-#define MVPP2_TX_PKTS_BM_DROP_CTR		0x7208
-#define MVPP2_TX_PKTS_BM_MC_DROP_CTR		0x720c
-#define MVPP2_RX_PKTS_FULL_QUEUE_DROP_CTR	0x7220
-#define MVPP2_RX_PKTS_EARLY_DROP_CTR		0x7224
-#define MVPP2_RX_PKTS_BM_DROP_CTR		0x7228
 #define MVPP2_CLS_DEC_TBL_HIT_CTR		0x7700
 #define MVPP2_CLS_FLOW_TBL_HIT_CTR		0x7704
 
@@ -429,12 +431,15 @@
 #define     MVPP2_GMAC_STATUS0_GMII_SPEED	BIT(1)
 #define     MVPP2_GMAC_STATUS0_MII_SPEED	BIT(2)
 #define     MVPP2_GMAC_STATUS0_FULL_DUPLEX	BIT(3)
-#define     MVPP2_GMAC_STATUS0_RX_PAUSE		BIT(4)
-#define     MVPP2_GMAC_STATUS0_TX_PAUSE		BIT(5)
+#define     MVPP2_GMAC_STATUS0_RX_PAUSE		BIT(6)
+#define     MVPP2_GMAC_STATUS0_TX_PAUSE		BIT(7)
 #define     MVPP2_GMAC_STATUS0_AN_COMPLETE	BIT(11)
+#define MVPP2_GMAC_PORT_FIFO_CFG_0_REG		0x18
+#define     MVPP2_GMAC_TX_FIFO_WM_MASK		0xffff
+#define     MVPP2_GMAC_TX_FIFO_WM_LOW_OFFSET	8
 #define MVPP2_GMAC_PORT_FIFO_CFG_1_REG		0x1c
 #define     MVPP2_GMAC_TX_FIFO_MIN_TH_OFFS	6
-#define     MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK	0x1fc0
+#define     MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK	0x3fc0
 #define     MVPP2_GMAC_TX_FIFO_MIN_TH_MASK(v)	(((v) << 6) & \
 					MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK)
 #define MVPP22_GMAC_INT_STAT			0x20
@@ -451,14 +456,12 @@
 #define MVPP22_GMAC_INT_SUM_MASK		0xa4
 #define     MVPP22_GMAC_INT_SUM_MASK_LINK_STAT	BIT(1)
 
-/* Per-port XGMAC registers. PPv2.2 only, only for GOP port 0,
+/* Per-port XGMAC registers. PPv2.2 and PPv2.3, only for GOP port 0,
  * relative to port->base.
  */
 #define MVPP22_XLG_CTRL0_REG			0x100
 #define     MVPP22_XLG_CTRL0_PORT_EN		BIT(0)
 #define     MVPP22_XLG_CTRL0_MAC_RESET_DIS	BIT(1)
-#define     MVPP22_XLG_CTRL0_FORCE_LINK_DOWN	BIT(2)
-#define     MVPP22_XLG_CTRL0_FORCE_LINK_PASS	BIT(3)
 #define     MVPP22_XLG_CTRL0_RX_FLOW_CTRL_EN	BIT(7)
 #define     MVPP22_XLG_CTRL0_TX_FLOW_CTRL_EN	BIT(8)
 #define     MVPP22_XLG_CTRL0_MIB_CNT_DIS	BIT(14)
@@ -481,10 +484,11 @@
 #define MVPP22_XLG_CTRL4_REG			0x184
 #define     MVPP22_XLG_CTRL4_FWD_FC		BIT(5)
 #define     MVPP22_XLG_CTRL4_FWD_PFC		BIT(6)
+#define     MVPP22_XLG_CTRL4_USE_XPCS		BIT(8)
 #define     MVPP22_XLG_CTRL4_MACMODSELECT_GMAC	BIT(12)
 #define     MVPP22_XLG_CTRL4_EN_IDLE_CHECK	BIT(14)
 
-/* SMI registers. PPv2.2 only, relative to priv->iface_base. */
+/* SMI registers. PPv2.2 and PPv2.3, relative to priv->iface_base. */
 #define MVPP22_SMI_MISC_CFG_REG			0x1204
 #define     MVPP22_SMI_POLLING_EN		BIT(10)
 
@@ -496,7 +500,7 @@
 #define MVPP2_QUEUE_NEXT_DESC(q, index) \
 	(((index) < (q)->last_desc) ? ((index) + 1) : 0)
 
-/* XPCS registers. PPv2.2 only */
+/* XPCS registers.PPv2.2 and PPv2.3 */
 #define MVPP22_MPCS_BASE(port)			(0x7000 + (port) * 0x1000)
 #define MVPP22_MPCS_CTRL			0x14
 #define     MVPP22_MPCS_CTRL_FWD_ERR_CONN	BIT(10)
@@ -507,7 +511,16 @@
 #define     MVPP22_MPCS_CLK_RESET_DIV_RATIO(n)	((n) << 4)
 #define     MVPP22_MPCS_CLK_RESET_DIV_SET	BIT(11)
 
-/* XPCS registers. PPv2.2 only */
+/* FCA registers. PPv2.2 and PPv2.3 */
+#define MVPP22_FCA_BASE(port)			(0x7600 + (port) * 0x1000)
+#define MVPP22_FCA_REG_SIZE			16
+#define MVPP22_FCA_REG_MASK			0xFFFF
+#define MVPP22_FCA_CONTROL_REG			0x0
+#define MVPP22_FCA_ENABLE_PERIODIC		BIT(11)
+#define MVPP22_PERIODIC_COUNTER_LSB_REG		(0x110)
+#define MVPP22_PERIODIC_COUNTER_MSB_REG		(0x114)
+
+/* XPCS registers. PPv2.2 and PPv2.3 */
 #define MVPP22_XPCS_BASE(port)			(0x7400 + (port) * 0x1000)
 #define MVPP22_XPCS_CFG0			0x0
 #define     MVPP22_XPCS_CFG0_RESET_DIS		BIT(0)
@@ -532,11 +545,14 @@
 /* Various constants */
 
 /* Coalescing */
-#define MVPP2_TXDONE_COAL_PKTS_THRESH	64
+#define MVPP2_TXDONE_COAL_PKTS_THRESH	32
 #define MVPP2_TXDONE_HRTIMER_PERIOD_NS	1000000UL
+#define MVPP2_GUARD_TXDONE_HRTIMER_NS	(10 * NSEC_PER_MSEC)
 #define MVPP2_TXDONE_COAL_USEC		1000
 #define MVPP2_RX_COAL_PKTS		32
 #define MVPP2_RX_COAL_USEC		64
+#define MVPP2_TX_BULK_TIME		(50 * NSEC_PER_USEC)
+#define MVPP2_TX_BULK_MAX_PACKETS	(MVPP2_AGGR_TXQ_SIZE / MVPP2_MAX_PORTS)
 
 /* The two bytes Marvell header. Either contains a special value used
  * by Marvell switches when a specific hardware mode is enabled (not
@@ -575,29 +591,41 @@
 /* Maximum number of TXQs used by single port */
 #define MVPP2_MAX_TXQ			8
 
-/* MVPP2_MAX_TSO_SEGS is the maximum number of fragments to allow in the GSO
- * skb. As we need a maxium of two descriptors per fragments (1 header, 1 data),
- * multiply this value by two to count the maximum number of skb descs needed.
+/* SKB/TSO/TX-ring-size/pause-wakeup constatnts depend upon the
+ *  MAX_TSO_SEGS - the max number of fragments to allow in the GSO skb.
+ *  Min-Min requirement for it = maxPacket(64kB)/stdMTU(1500)=44 fragments
+ *  and MVPP2_MAX_TSO_SEGS=max(MVPP2_MAX_TSO_SEGS, MAX_SKB_FRAGS).
+ * MAX_SKB_DESCS: we need 2 descriptors per TSO fragment (1 header, 1 data)
+ *  + per-cpu-reservation MVPP2_CPU_DESC_CHUNK*CPUs for optimization.
+ * TX stop activation threshold (e.g. Queue is full) is MAX_SKB_DESCS
+ * TX stop-to-wake hysteresis is MAX_TSO_SEGS
+ * The Tx ring size cannot be smaller than TSO_SEGS + HYSTERESIS + SKBs
+ * The numbers depend upon num cpus (online) used by the driver
  */
-#define MVPP2_MAX_TSO_SEGS		300
-#define MVPP2_MAX_SKB_DESCS		(MVPP2_MAX_TSO_SEGS * 2 + MAX_SKB_FRAGS)
+#define MVPP2_MAX_TSO_SEGS		44
+#define MVPP2_MAX_SKB_DESCS(ncpus)	(MVPP2_MAX_TSO_SEGS * 2 + \
+					MVPP2_CPU_DESC_CHUNK * ncpus)
+#define MVPP2_TX_PAUSE_HYSTERESIS	(MVPP2_MAX_TSO_SEGS * 2)
 
 /* Max number of RXQs per port */
 #define MVPP2_PORT_MAX_RXQ		32
 
 /* Max number of Rx descriptors */
-#define MVPP2_MAX_RXD_MAX		1024
-#define MVPP2_MAX_RXD_DFLT		128
+#define MVPP2_MAX_RXD_MAX		2048
+#define MVPP2_MAX_RXD_DFLT		MVPP2_MAX_RXD_MAX
 
 /* Max number of Tx descriptors */
 #define MVPP2_MAX_TXD_MAX		2048
-#define MVPP2_MAX_TXD_DFLT		1024
+#define MVPP2_MAX_TXD_DFLT		MVPP2_MAX_TXD_MAX
+#define MVPP2_MIN_TXD(ncpus)	ALIGN(MVPP2_MAX_TSO_SEGS + \
+				      MVPP2_MAX_SKB_DESCS(ncpus) + \
+				      MVPP2_TX_PAUSE_HYSTERESIS, 32)
 
 /* Amount of Tx descriptors that can be reserved at once by CPU */
 #define MVPP2_CPU_DESC_CHUNK		64
 
 /* Max number of Tx descriptors in each aggregated queue */
-#define MVPP2_AGGR_TXQ_SIZE		256
+#define MVPP2_AGGR_TXQ_SIZE		512
 
 /* Descriptor aligned size */
 #define MVPP2_DESC_ALIGNED_SIZE		32
@@ -610,28 +638,53 @@
 #define MVPP2_RX_FIFO_PORT_DATA_SIZE_32KB	0x8000
 #define MVPP2_RX_FIFO_PORT_DATA_SIZE_8KB	0x2000
 #define MVPP2_RX_FIFO_PORT_DATA_SIZE_4KB	0x1000
-#define MVPP2_RX_FIFO_PORT_ATTR_SIZE(data_size)	((data_size) >> 6)
+#define MVPP2_RX_FIFO_PORT_ATTR_SIZE(data_size)	(data_size >> 6)
 #define MVPP2_RX_FIFO_PORT_ATTR_SIZE_4KB	0x40
 #define MVPP2_RX_FIFO_PORT_MIN_PKT		0x80
 
 /* TX FIFO constants */
-#define MVPP22_TX_FIFO_DATA_SIZE_16KB		16
+#define MVPP22_TX_FIFO_DATA_SIZE_18KB		18
 #define MVPP22_TX_FIFO_DATA_SIZE_10KB		10
-#define MVPP22_TX_FIFO_DATA_SIZE_3KB		3
+#define MVPP22_TX_FIFO_DATA_SIZE_1KB		1
+#define MVPP22_TX_FIFO_DATA_SIZE_MIN		3
+#define MVPP22_TX_FIFO_DATA_SIZE_MAX		15
 #define MVPP2_TX_FIFO_THRESHOLD_MIN		256 /* Bytes */
-#define MVPP2_TX_FIFO_THRESHOLD(kb)	\
-		((kb) * 1024 - MVPP2_TX_FIFO_THRESHOLD_MIN)
+#define MVPP2_TX_FIFO_THRESHOLD(kb)		\
+		(kb * 1024 - MVPP2_TX_FIFO_THRESHOLD_MIN)
+#define MVPP22_TX_FIFO_EXTRA_PARAM_MASK		0xFF
+#define MVPP22_TX_FIFO_EXTRA_PARAM_OFFS(port)	(8 * (port))
+#define MVPP22_TX_FIFO_EXTRA_PARAM_SIZE(port, val)		\
+	(((val) >> MVPP22_TX_FIFO_EXTRA_PARAM_OFFS(port)) &	\
+	 MVPP22_TX_FIFO_EXTRA_PARAM_MASK)
+
+/* RX FIFO threshold in 1KB granularity */
+#define MVPP23_PORT0_FIFO_TRSH	(9 * 1024)
+#define MVPP23_PORT1_FIFO_TRSH	(4 * 1024)
+#define MVPP23_PORT2_FIFO_TRSH	(2 * 1024)
+
+/* RX Flow Control Registers */
+#define MVPP2_RX_FC_REG(port)		(0x150 + 4 * (port))
+#define     MVPP2_RX_FC_EN		BIT(24)
+#define     MVPP2_RX_FC_TRSH_OFFS	16
+#define     MVPP2_RX_FC_TRSH_MASK	(0xFF << MVPP2_RX_FC_TRSH_OFFS)
+#define     MVPP2_RX_FC_TRSH_UNIT	256
+
+/* GMAC TX FIFO configuration */
+#define MVPP2_GMAC_TX_FIFO_MIN_TH	\
+	MVPP2_GMAC_TX_FIFO_MIN_TH_MASK(50)
+#define MVPP2_GMAC_TX_FIFO_LOW_WM		75
+#define MVPP2_GMAC_TX_FIFO_HI_WM		77
 
 /* RX buffer constants */
 #define MVPP2_SKB_SHINFO_SIZE \
 	SKB_DATA_ALIGN(sizeof(struct skb_shared_info))
 
+#define MVPP2_MTU_OVERHEAD_SIZE \
+	(MVPP2_MH_SIZE + MVPP2_VLAN_TAG_LEN + ETH_HLEN + ETH_FCS_LEN)
 #define MVPP2_RX_PKT_SIZE(mtu) \
-	ALIGN((mtu) + MVPP2_MH_SIZE + MVPP2_VLAN_TAG_LEN + \
-	      ETH_HLEN + ETH_FCS_LEN, cache_line_size())
+	ALIGN((mtu) + MVPP2_MTU_OVERHEAD_SIZE, cache_line_size())
 
 #define MVPP2_RX_BUF_SIZE(pkt_size)	((pkt_size) + NET_SKB_PAD)
-#define MVPP2_RX_TOTAL_SIZE(buf_size)	((buf_size) + MVPP2_SKB_SHINFO_SIZE)
 #define MVPP2_RX_MAX_PKT_SIZE(total_size) \
 	((total_size) - NET_SKB_PAD - MVPP2_SKB_SHINFO_SIZE)
 
@@ -639,14 +692,7 @@
 #define MVPP2_BIT_TO_WORD(bit)		((bit) / 32)
 #define MVPP2_BIT_IN_WORD(bit)		((bit) % 32)
 
-#define MVPP2_N_PRS_FLOWS		52
-#define MVPP2_N_RFS_ENTRIES_PER_FLOW	4
-
-/* There are 7 supported high-level flows */
-#define MVPP2_N_RFS_RULES		(MVPP2_N_RFS_ENTRIES_PER_FLOW * 7)
-
 /* RSS constants */
-#define MVPP22_N_RSS_TABLES		8
 #define MVPP22_RSS_TABLE_ENTRIES	32
 
 /* IPv6 max L3 address size */
@@ -655,6 +701,9 @@
 /* Port flags */
 #define MVPP2_F_LOOPBACK		BIT(0)
 #define MVPP2_F_DT_COMPAT		BIT(1)
+#define MVPP22_F_IF_MUSDK		BIT(2) /* musdk port */
+/* BIT(1 and 2) are reserved */
+#define MVPP2_F_IF_TX_ON		BIT(3)
 
 /* Marvell tag types */
 enum mvpp2_tag_type {
@@ -680,18 +729,17 @@ enum mvpp2_prs_l3_cast {
 };
 
 /* BM constants */
-#define MVPP2_BM_JUMBO_BUF_NUM		512
-#define MVPP2_BM_LONG_BUF_NUM		1024
+#define MVPP2_BM_JUMBO_BUF_NUM		2048
+#define MVPP2_BM_LONG_BUF_NUM		2048
 #define MVPP2_BM_SHORT_BUF_NUM		2048
 #define MVPP2_BM_POOL_SIZE_MAX		(16*1024 - MVPP2_BM_POOL_PTR_ALIGN/4)
 #define MVPP2_BM_POOL_PTR_ALIGN		128
-#define MVPP2_BM_MAX_POOLS		8
 
 /* BM cookie (32 bits) definition */
 #define MVPP2_BM_COOKIE_POOL_OFFS	8
 #define MVPP2_BM_COOKIE_CPU_OFFS	24
 
-#define MVPP2_BM_SHORT_FRAME_SIZE		512
+#define MVPP2_BM_SHORT_FRAME_SIZE		1024
 #define MVPP2_BM_LONG_FRAME_SIZE		2048
 #define MVPP2_BM_JUMBO_FRAME_SIZE		10240
 /* BM short pool packet size
@@ -734,7 +782,7 @@ enum mvpp2_prs_l3_cast {
 #define MVPP2_MIB_FC_RCVD			0x58
 #define MVPP2_MIB_RX_FIFO_OVERRUN		0x5c
 #define MVPP2_MIB_UNDERSIZE_RCVD		0x60
-#define MVPP2_MIB_FRAGMENTS_RCVD		0x64
+#define MVPP2_MIB_FRAGMENTS_ERR_RCVD		0x64
 #define MVPP2_MIB_OVERSIZE_RCVD			0x68
 #define MVPP2_MIB_JABBER_RCVD			0x6c
 #define MVPP2_MIB_MAC_RCV_ERROR			0x70
@@ -744,30 +792,88 @@ enum mvpp2_prs_l3_cast {
 
 #define MVPP2_MIB_COUNTERS_STATS_DELAY		(1 * HZ)
 
+/* Other counters */
+#define MVPP2_OVERRUN_DROP_REG(port)		(0x7000 + 4 * (port))
+#define MVPP2_CLS_DROP_REG(port)		(0x7020 + 4 * (port))
+#define MVPP2_CNT_IDX_REG			0x7040
+#define MVPP2_TX_PKT_FULLQ_DROP_REG		0x7200
+#define MVPP2_TX_PKT_EARLY_DROP_REG		0x7204
+#define MVPP2_TX_PKT_BM_DROP_REG		0x7208
+#define MVPP2_TX_PKT_BM_MC_DROP_REG		0x720c
+#define MVPP2_RX_PKT_FULLQ_DROP_REG		0x7220
+#define MVPP2_RX_PKT_EARLY_DROP_REG		0x7224
+#define MVPP2_RX_PKT_BM_DROP_REG		0x7228
+
 #define MVPP2_DESC_DMA_MASK	DMA_BIT_MASK(40)
 
-/* Definitions */
-struct mvpp2_dbgfs_entries;
+/* MSS Flow control */
+#define MSS_SRAM_SIZE			0x800
+#define MSS_FC_COM_REG			0
+#define FLOW_CONTROL_ENABLE_BIT		BIT(0)
+#define FLOW_CONTROL_UPDATE_COMMAND_BIT	BIT(31)
+#define FC_QUANTA			0xFFFF
+#define FC_CLK_DIVIDER			0x140
+
+#define MSS_BUF_POOL_BASE		0x40
+#define MSS_BUF_POOL_OFFS		4
+#define MSS_BUF_POOL_REG(id)		(MSS_BUF_POOL_BASE		\
+					+ (id) * MSS_BUF_POOL_OFFS)
+
+#define MSS_BUF_POOL_STOP_MASK		0xFFF
+#define MSS_BUF_POOL_START_MASK		(0xFFF << MSS_BUF_POOL_START_OFFS)
+#define MSS_BUF_POOL_START_OFFS		12
+#define MSS_BUF_POOL_PORTS_MASK		(0xF << MSS_BUF_POOL_PORTS_OFFS)
+#define MSS_BUF_POOL_PORTS_OFFS		24
+#define MSS_BUF_POOL_PORT_OFFS(id)	(0x1 <<				\
+					((id) + MSS_BUF_POOL_PORTS_OFFS))
+
+#define MSS_RXQ_TRESH_BASE		0x200
+#define MSS_RXQ_TRESH_OFFS		4
+#define MSS_RXQ_TRESH_REG(q, fq)	(MSS_RXQ_TRESH_BASE + (((q) + (fq)) \
+					* MSS_RXQ_TRESH_OFFS))
+
+#define MSS_RXQ_TRESH_START_MASK	0xFFFF
+#define MSS_RXQ_TRESH_STOP_MASK		(0xFFFF << MSS_RXQ_TRESH_STOP_OFFS)
+#define MSS_RXQ_TRESH_STOP_OFFS		16
+
+#define MSS_RXQ_ASS_BASE	0x80
+#define MSS_RXQ_ASS_OFFS	4
+#define MSS_RXQ_ASS_PER_REG	4
+#define MSS_RXQ_ASS_PER_OFFS	8
+#define MSS_RXQ_ASS_PORTID_OFFS	0
+#define MSS_RXQ_ASS_PORTID_MASK	0x3
+#define MSS_RXQ_ASS_HOSTID_OFFS	2
+#define MSS_RXQ_ASS_HOSTID_MASK	0x3F
+
+#define MSS_RXQ_ASS_Q_BASE(q, fq) ((((q) + (fq)) % MSS_RXQ_ASS_PER_REG)	 \
+				  * MSS_RXQ_ASS_PER_OFFS)
+#define MSS_RXQ_ASS_PQ_BASE(q, fq) ((((q) + (fq)) / MSS_RXQ_ASS_PER_REG) \
+				   * MSS_RXQ_ASS_OFFS)
+#define MSS_RXQ_ASS_REG(q, fq) (MSS_RXQ_ASS_BASE + MSS_RXQ_ASS_PQ_BASE(q, fq))
+
+#define MSS_THRESHOLD_STOP	768
+#define MSS_THRESHOLD_START	1024
+#define MSS_FC_MAX_TIMEOUT	5000
 
-struct mvpp2_rss_table {
-	u32 indir[MVPP22_RSS_TABLE_ENTRIES];
-};
+/* Definitions */
 
 /* Shared Packet Processor resources */
 struct mvpp2 {
 	/* Shared registers' base addresses */
 	void __iomem *lms_base;
 	void __iomem *iface_base;
+	void __iomem *cm3_base;
 
-	/* On PPv2.2, each "software thread" can access the base
+	/* On PPv2.2 and PPv2.3, each "software thread" can access the base
 	 * register through a separate address space, each 64 KB apart
 	 * from each other. Typically, such address spaces will be
 	 * used per CPU.
 	 */
 	void __iomem *swth_base[MVPP2_MAX_THREADS];
 
-	/* On PPv2.2, some port control registers are located into the system
-	 * controller space. These registers are accessible through a regmap.
+	/* On PPv2.2 and PPv2.3, some port control registers are located into
+	 * the system controller space. These registers are accessible
+	 * through a regmap.
 	 */
 	struct regmap *sysctrl_base;
 
@@ -781,12 +887,9 @@ struct mvpp2 {
 	/* List of pointers to port structures */
 	int port_count;
 	struct mvpp2_port *port_list[MVPP2_MAX_PORTS];
-
 	/* Map of enabled ports */
 	unsigned long port_map;
 
-	struct mvpp2_tai *tai;
-
 	/* Number of Tx threads used */
 	unsigned int nthreads;
 	/* Map of threads needing locking */
@@ -795,11 +898,9 @@ struct mvpp2 {
 	/* Aggregated TXQs */
 	struct mvpp2_tx_queue *aggr_txqs;
 
-	/* Are we using page_pool with per-cpu pools? */
-	int percpu_pools;
-
 	/* BM pools */
 	struct mvpp2_bm_pool *bm_pools;
+	struct mvpp2_bm_pool **pools_pcpu;
 
 	/* PRS shadow table */
 	struct mvpp2_prs_shadow *prs_shadow;
@@ -810,7 +911,7 @@ struct mvpp2 {
 	u32 tclk;
 
 	/* HW version */
-	enum { MVPP21, MVPP22 } hw_version;
+	enum { MVPP21, MVPP22, MVPP23 } hw_version;
 
 	/* Maximum number of RXQs per port */
 	unsigned int max_port_rxqs;
@@ -822,11 +923,16 @@ struct mvpp2 {
 	/* Debugfs root entry */
 	struct dentry *dbgfs_dir;
 
-	/* Debugfs entries private data */
-	struct mvpp2_dbgfs_entries *dbgfs_entries;
+	/* CM3 SRAM pool */
+	struct gen_pool *sram_pool;
+
+	/* Global TX Flow Control config */
+	bool global_tx_fc;
 
-	/* RSS Indirection tables */
-	struct mvpp2_rss_table *rss_tables[MVPP22_N_RSS_TABLES];
+	bool custom_dma_mask;
+
+	/* Spinlocks for CM3 shared memory configuration */
+	spinlock_t mss_spinlock;
 };
 
 struct mvpp2_pcpu_stats {
@@ -839,9 +945,24 @@ struct mvpp2_pcpu_stats {
 
 /* Per-CPU port control */
 struct mvpp2_port_pcpu {
+	/* Timer & Tasklet for bulk-tx optimization */
+	struct hrtimer bulk_timer;
+	bool bulk_timer_scheduled;
+	bool bulk_timer_restart_req;
+	struct tasklet_struct bulk_tasklet;
+
+	/* Timer & Tasklet for egress finalization */
 	struct hrtimer tx_done_timer;
-	struct net_device *dev;
-	bool timer_scheduled;
+	bool tx_done_timer_scheduled;
+	bool guard_timer_scheduled;
+	struct tasklet_struct tx_done_tasklet;
+
+	/* tx-done guard timer fields */
+	struct mvpp2_port *port; /* reference to get from tx_done_timer */
+	bool tx_done_passed;	/* tx-done passed since last guard-check */
+	u8 txq_coal_is_zero_map; /* map tx queues (max=8) forced coal=Zero */
+	u8 txq_busy_suspect_map; /* map suspect txq to be forced */
+	u32 tx_guard_cntr;	/* statistic */
 };
 
 struct mvpp2_queue_vector {
@@ -857,37 +978,6 @@ struct mvpp2_queue_vector {
 	struct cpumask *mask;
 };
 
-/* Internal represention of a Flow Steering rule */
-struct mvpp2_rfs_rule {
-	/* Rule location inside the flow*/
-	int loc;
-
-	/* Flow type, such as TCP_V4_FLOW, IP6_FLOW, etc. */
-	int flow_type;
-
-	/* Index of the C2 TCAM entry handling this rule */
-	int c2_index;
-
-	/* Header fields that needs to be extracted to match this flow */
-	u16 hek_fields;
-
-	/* CLS engine : only c2 is supported for now. */
-	u8 engine;
-
-	/* TCAM key and mask for C2-based steering. These fields should be
-	 * encapsulated in a union should we add more engines.
-	 */
-	u64 c2_tcam;
-	u64 c2_tcam_mask;
-
-	struct flow_rule *flow;
-};
-
-struct mvpp2_ethtool_fs {
-	struct mvpp2_rfs_rule rule;
-	struct ethtool_rxnfc rxnfc;
-};
-
 struct mvpp2_port {
 	u8 id;
 
@@ -958,14 +1048,23 @@ struct mvpp2_port {
 
 	u32 tx_time_coal;
 
-	/* List of steering rules active on that port */
-	struct mvpp2_ethtool_fs *rfs_rules[MVPP2_N_RFS_ENTRIES_PER_FLOW];
-	int n_rfs_rules;
+	/* RSS indirection table */
+	u32 indir[MVPP22_RSS_TABLE_ENTRIES];
 
-	/* Each port has its own view of the rss contexts, so that it can number
-	 * them from 0
-	 */
-	int rss_ctx[MVPP22_N_RSS_TABLES];
+	/* us private storage, allocated/used by User/Kernel mode toggling */
+	void *us_cfg;
+
+	/* Coherency-update for TX-ON from link_status_irq */
+	struct tasklet_struct txqs_on_tasklet;
+
+	/* Firmware TX flow control */
+	bool tx_fc;
+
+	/* Indication, whether port is connected to XLG MAC */
+	bool has_xlg_mac;
+
+	/* Notifier required when the port is connected to the switch */
+	struct notifier_block dsa_notifier;
 };
 
 /* The mvpp2_tx_desc and mvpp2_rx_desc structures describe the
@@ -986,7 +1085,7 @@ struct mvpp2_port {
 
 #define MVPP2_RXD_ERR_SUMMARY		BIT(15)
 #define MVPP2_RXD_ERR_CODE_MASK		(BIT(13) | BIT(14))
-#define MVPP2_RXD_ERR_CRC		0x0
+#define MVPP2_RXD_ERR_MAC		0x0
 #define MVPP2_RXD_ERR_OVERRUN		BIT(13)
 #define MVPP2_RXD_ERR_RESOURCE		(BIT(13) | BIT(14))
 #define MVPP2_RXD_BM_POOL_ID_OFFS	16
@@ -1028,7 +1127,7 @@ struct mvpp21_rx_desc {
 	__le32 reserved8;
 };
 
-/* HW TX descriptor for PPv2.2 */
+/* HW TX descriptor for PPv2.2 and PPv2.3 */
 struct mvpp22_tx_desc {
 	__le32 command;
 	u8  packet_offset;
@@ -1039,7 +1138,7 @@ struct mvpp22_tx_desc {
 	__le64 buf_cookie_misc;
 };
 
-/* HW RX descriptor for PPv2.2 */
+/* HW RX descriptor for PPv2.2 and PPv2.3 */
 struct mvpp22_rx_desc {
 	__le32 status;
 	__le16 reserved1;
@@ -1090,8 +1189,10 @@ struct mvpp2_txq_pcpu {
 	 */
 	int count;
 
-	int wake_threshold;
-	int stop_threshold;
+	u16 wake_threshold;
+	u16 stop_threshold;
+	/* TXQ-number above stop_threshold to be wake-up */
+	u16 stopped_on_txq_id;
 
 	/* Number of Tx DMA descriptors reserved for each CPU */
 	int reserved_num;
@@ -1122,6 +1223,7 @@ struct mvpp2_tx_queue {
 
 	/* Number of currently used Tx DMA descriptor in the descriptor ring */
 	int count;
+	int pending;
 
 	/* Per-CPU control of physical Tx queues */
 	struct mvpp2_txq_pcpu __percpu *pcpu;
@@ -1139,35 +1241,43 @@ struct mvpp2_tx_queue {
 
 	/* Index of the next Tx DMA descriptor to process */
 	int next_desc_to_proc;
-};
+} __aligned(L1_CACHE_BYTES);
 
 struct mvpp2_rx_queue {
+	/* Virtual address of the RX DMA descriptors array */
+	struct mvpp2_rx_desc *descs;
+
+	/* Index of the next-to-process and last RX DMA descriptor */
+	int next_desc_to_proc;
+	int last_desc;
+
 	/* RX queue number, in the range 0-31 for physical RXQs */
 	u8 id;
 
+	/* Port's logic RXQ number to which physical RXQ is mapped */
+	u8 logic_rxq;
+
+	/* Num of RXed packets seen in HW but meanwhile not handled by SW */
+	u16 rx_pending;
+
 	/* Num of rx descriptors in the rx descriptor ring */
 	int size;
 
 	u32 pkts_coal;
 	u32 time_coal;
 
-	/* Virtual address of the RX DMA descriptors array */
-	struct mvpp2_rx_desc *descs;
-
 	/* DMA address of the RX DMA descriptors array */
 	dma_addr_t descs_dma;
 
-	/* Index of the last RX DMA descriptor */
-	int last_desc;
-
-	/* Index of the next RX DMA descriptor to process */
-	int next_desc_to_proc;
-
 	/* ID of port to which physical RXQ is mapped */
 	int port;
 
-	/* Port's logic RXQ number to which physical RXQ is mapped */
-	int logic_rxq;
+} __aligned(L1_CACHE_BYTES);
+
+enum mvpp2_bm_pool_type {
+	MVPP2_BM_SHORT,
+	MVPP2_BM_JUMBO,
+	MVPP2_BM_LONG,
 };
 
 struct mvpp2_bm_pool {
@@ -1186,6 +1296,9 @@ struct mvpp2_bm_pool {
 	int pkt_size;
 	int frag_size;
 
+	/* Pool type (short/long/jumbo) */
+	enum mvpp2_bm_pool_type type;
+
 	/* BPPE virtual base address */
 	u32 *virt_addr;
 	/* BPPE DMA base address */
@@ -1195,19 +1308,120 @@ struct mvpp2_bm_pool {
 	u32 port_map;
 };
 
+#define MVPP2_BM_POOLS_NUM	(recycle ? (2 + num_present_cpus()) : 3)
+#define MVPP2_BM_POOLS_NUM_MAX	8
+
 #define IS_TSO_HEADER(txq_pcpu, addr) \
 	((addr) >= (txq_pcpu)->tso_headers_dma && \
 	 (addr) < (txq_pcpu)->tso_headers_dma + \
 	 (txq_pcpu)->size * TSO_HEADER_SIZE)
+#define TSO_HEADER_MARK		((void *)BIT(0))
 
 #define MVPP2_DRIVER_NAME "mvpp2"
 #define MVPP2_DRIVER_VERSION "1.0"
 
-void mvpp2_write(struct mvpp2 *priv, u32 offset, u32 data);
-u32 mvpp2_read(struct mvpp2 *priv, u32 offset);
+/* Run-time critical Utility/helper methods */
+static inline
+void mvpp2_write(struct mvpp2 *priv, u32 offset, u32 data)
+{
+	writel(data, priv->swth_base[0] + offset);
+}
+
+static inline
+u32 mvpp2_read(struct mvpp2 *priv, u32 offset)
+{
+	return readl(priv->swth_base[0] + offset);
+}
+
+static inline
+u32 mvpp2_read_relaxed(struct mvpp2 *priv, u32 offset)
+{
+	return readl_relaxed(priv->swth_base[0] + offset);
+}
+
+static inline
+u32 mvpp2_cpu_to_thread(struct mvpp2 *priv, int cpu)
+{
+	return cpu % priv->nthreads;
+}
+
+static inline
+void mvpp2_cm3_write(struct mvpp2 *priv, u32 offset, u32 data)
+{
+	writel(data, priv->cm3_base + offset);
+}
+
+static inline
+u32 mvpp2_cm3_read(struct mvpp2 *priv, u32 offset)
+{
+	return readl(priv->cm3_base + offset);
+}
+
+/* These accessors should be used to access:
+ *
+ * - per-thread registers, where each thread has its own copy of the
+ *   register.
+ *
+ *   MVPP2_BM_VIRT_ALLOC_REG
+ *   MVPP2_BM_ADDR_HIGH_ALLOC
+ *   MVPP22_BM_ADDR_HIGH_RLS_REG
+ *   MVPP2_BM_VIRT_RLS_REG
+ *   MVPP2_ISR_RX_TX_CAUSE_REG
+ *   MVPP2_ISR_RX_TX_MASK_REG
+ *   MVPP2_TXQ_NUM_REG
+ *   MVPP2_AGGR_TXQ_UPDATE_REG
+ *   MVPP2_TXQ_RSVD_REQ_REG
+ *   MVPP2_TXQ_RSVD_RSLT_REG
+ *   MVPP2_TXQ_SENT_REG
+ *   MVPP2_RXQ_NUM_REG
+ *
+ * - global registers that must be accessed through a specific thread
+ *   window, because they are related to an access to a per-thread
+ *   register
+ *
+ *   MVPP2_BM_PHY_ALLOC_REG    (related to MVPP2_BM_VIRT_ALLOC_REG)
+ *   MVPP2_BM_PHY_RLS_REG      (related to MVPP2_BM_VIRT_RLS_REG)
+ *   MVPP2_RXQ_THRESH_REG      (related to MVPP2_RXQ_NUM_REG)
+ *   MVPP2_RXQ_DESC_ADDR_REG   (related to MVPP2_RXQ_NUM_REG)
+ *   MVPP2_RXQ_DESC_SIZE_REG   (related to MVPP2_RXQ_NUM_REG)
+ *   MVPP2_RXQ_INDEX_REG       (related to MVPP2_RXQ_NUM_REG)
+ *   MVPP2_TXQ_PENDING_REG     (related to MVPP2_TXQ_NUM_REG)
+ *   MVPP2_TXQ_DESC_ADDR_REG   (related to MVPP2_TXQ_NUM_REG)
+ *   MVPP2_TXQ_DESC_SIZE_REG   (related to MVPP2_TXQ_NUM_REG)
+ *   MVPP2_TXQ_INDEX_REG       (related to MVPP2_TXQ_NUM_REG)
+ *   MVPP2_TXQ_PENDING_REG     (related to MVPP2_TXQ_NUM_REG)
+ *   MVPP2_TXQ_PREF_BUF_REG    (related to MVPP2_TXQ_NUM_REG)
+ *   MVPP2_TXQ_PREF_BUF_REG    (related to MVPP2_TXQ_NUM_REG)
+ */
+static inline
+void mvpp2_thread_write(struct mvpp2 *priv, unsigned int thread,
+			u32 offset, u32 data)
+{
+	writel(data, priv->swth_base[thread] + offset);
+}
+
+static inline
+u32 mvpp2_thread_read(struct mvpp2 *priv, unsigned int thread, u32 offset)
+{
+	return readl(priv->swth_base[thread] + offset);
+}
+
+static inline
+void mvpp2_thread_write_relaxed(struct mvpp2 *priv, unsigned int thread,
+				u32 offset, u32 data)
+{
+	writel_relaxed(data, priv->swth_base[thread] + offset);
+}
+
+static inline
+u32 mvpp2_thread_read_relaxed(struct mvpp2 *priv, unsigned int thread,
+			      u32 offset)
+{
+	return readl_relaxed(priv->swth_base[thread] + offset);
+}
 
 void mvpp2_dbgfs_init(struct mvpp2 *priv, const char *name);
-
 void mvpp2_dbgfs_cleanup(struct mvpp2 *priv);
+void mvpp23_rx_fifo_fc_en(struct mvpp2 *priv, int port, bool en);
 
 #endif
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c b/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
index 6122057d60c0..103c4a4fc8dc 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
@@ -22,302 +22,302 @@
 	}							\
 }
 
-static const struct mvpp2_cls_flow cls_flows[MVPP2_N_PRS_FLOWS] = {
+static struct mvpp2_cls_flow cls_flows[MVPP2_N_FLOWS] = {
 	/* TCP over IPv4 flows, Not fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_NF_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP4_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4 |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_NF_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP4_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OPT |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_NF_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP4_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OTHER |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* TCP over IPv4 flows, Not fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_NF_TAG,
-		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_NF_TAG,
+		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4 | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_NF_TAG,
-		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_NF_TAG,
+		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OPT | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_NF_TAG,
-		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_NF_TAG,
+		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OTHER | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* TCP over IPv4 flows, fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4 |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OPT |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OTHER |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* TCP over IPv4 flows, fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4 | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OPT | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP4, MVPP2_FL_IP4_TCP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V4_FLOW, MVPP2_FL_IP4_TCP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OTHER | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* UDP over IPv4 flows, Not fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_NF_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP4_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4 |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_NF_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP4_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OPT |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_NF_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP4_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OTHER |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* UDP over IPv4 flows, Not fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_NF_TAG,
-		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_NF_TAG,
+		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4 | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_NF_TAG,
-		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_NF_TAG,
+		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OPT | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_NF_TAG,
-		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_NF_TAG,
+		       MVPP22_CLS_HEK_IP4_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OTHER | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* UDP over IPv4 flows, fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4 |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OPT |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OTHER |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* UDP over IPv4 flows, fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4 | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OPT | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP4, MVPP2_FL_IP4_UDP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V4_FLOW, MVPP2_FL_IP4_UDP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OTHER | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* TCP over IPv6 flows, not fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_NF_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP6_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6 |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_NF_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP6_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6_EXT |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* TCP over IPv6 flows, not fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_NF_TAG,
-		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_NF_TAG,
+		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6 | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_NF_TAG,
-		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_NF_TAG,
+		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6_EXT | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* TCP over IPv6 flows, fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP6_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6 |
 		       MVPP2_PRS_RI_IP_FRAG_TRUE | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP6_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6_EXT |
 		       MVPP2_PRS_RI_IP_FRAG_TRUE | MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* TCP over IPv6 flows, fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6 | MVPP2_PRS_RI_IP_FRAG_TRUE |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_TCP6, MVPP2_FL_IP6_TCP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(TCP_V6_FLOW, MVPP2_FL_IP6_TCP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6_EXT | MVPP2_PRS_RI_IP_FRAG_TRUE |
 		       MVPP2_PRS_RI_L4_TCP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* UDP over IPv6 flows, not fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_NF_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP6_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6 |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_NF_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_NF_UNTAG,
 		       MVPP22_CLS_HEK_IP6_5T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6_EXT |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* UDP over IPv6 flows, not fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_NF_TAG,
-		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_NF_TAG,
+		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6 | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_NF_TAG,
-		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_NF_TAG,
+		       MVPP22_CLS_HEK_IP6_5T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6_EXT | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* UDP over IPv6 flows, fragmented, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP6_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6 |
 		       MVPP2_PRS_RI_IP_FRAG_TRUE | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_FRAG_UNTAG,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_FRAG_UNTAG,
 		       MVPP22_CLS_HEK_IP6_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6_EXT |
 		       MVPP2_PRS_RI_IP_FRAG_TRUE | MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK | MVPP2_PRS_RI_VLAN_MASK),
 
 	/* UDP over IPv6 flows, fragmented, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6 | MVPP2_PRS_RI_IP_FRAG_TRUE |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
-	MVPP2_DEF_FLOW(MVPP22_FLOW_UDP6, MVPP2_FL_IP6_UDP_FRAG_TAG,
-		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(UDP_V6_FLOW, MVPP2_FL_IP6_UDP_FRAG_TAG,
+		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6_EXT | MVPP2_PRS_RI_IP_FRAG_TRUE |
 		       MVPP2_PRS_RI_L4_UDP,
 		       MVPP2_PRS_IP_MASK),
 
 	/* IPv4 flows, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP4, MVPP2_FL_IP4_UNTAG,
+	MVPP2_DEF_FLOW(IPV4_FLOW, MVPP2_FL_IP4_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4,
 		       MVPP2_PRS_RI_VLAN_MASK | MVPP2_PRS_RI_L3_PROTO_MASK),
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP4, MVPP2_FL_IP4_UNTAG,
+	MVPP2_DEF_FLOW(IPV4_FLOW, MVPP2_FL_IP4_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OPT,
 		       MVPP2_PRS_RI_VLAN_MASK | MVPP2_PRS_RI_L3_PROTO_MASK),
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP4, MVPP2_FL_IP4_UNTAG,
+	MVPP2_DEF_FLOW(IPV4_FLOW, MVPP2_FL_IP4_UNTAG,
 		       MVPP22_CLS_HEK_IP4_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP4_OTHER,
 		       MVPP2_PRS_RI_VLAN_MASK | MVPP2_PRS_RI_L3_PROTO_MASK),
 
 	/* IPv4 flows, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP4, MVPP2_FL_IP4_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(IPV4_FLOW, MVPP2_FL_IP4_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4,
 		       MVPP2_PRS_RI_L3_PROTO_MASK),
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP4, MVPP2_FL_IP4_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(IPV4_FLOW, MVPP2_FL_IP4_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OPT,
 		       MVPP2_PRS_RI_L3_PROTO_MASK),
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP4, MVPP2_FL_IP4_TAG,
-		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(IPV4_FLOW, MVPP2_FL_IP4_TAG,
+		       MVPP22_CLS_HEK_IP4_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP4_OTHER,
 		       MVPP2_PRS_RI_L3_PROTO_MASK),
 
 	/* IPv6 flows, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP6, MVPP2_FL_IP6_UNTAG,
+	MVPP2_DEF_FLOW(IPV6_FLOW, MVPP2_FL_IP6_UNTAG,
 		       MVPP22_CLS_HEK_IP6_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6,
 		       MVPP2_PRS_RI_VLAN_MASK | MVPP2_PRS_RI_L3_PROTO_MASK),
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP6, MVPP2_FL_IP6_UNTAG,
+	MVPP2_DEF_FLOW(IPV6_FLOW, MVPP2_FL_IP6_UNTAG,
 		       MVPP22_CLS_HEK_IP6_2T,
 		       MVPP2_PRS_RI_VLAN_NONE | MVPP2_PRS_RI_L3_IP6,
 		       MVPP2_PRS_RI_VLAN_MASK | MVPP2_PRS_RI_L3_PROTO_MASK),
 
 	/* IPv6 flows, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP6, MVPP2_FL_IP6_TAG,
-		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(IPV6_FLOW, MVPP2_FL_IP6_TAG,
+		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6,
 		       MVPP2_PRS_RI_L3_PROTO_MASK),
-	MVPP2_DEF_FLOW(MVPP22_FLOW_IP6, MVPP2_FL_IP6_TAG,
-		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_TAGGED,
+	MVPP2_DEF_FLOW(IPV6_FLOW, MVPP2_FL_IP6_TAG,
+		       MVPP22_CLS_HEK_IP6_2T | MVPP22_CLS_HEK_OPT_VLAN,
 		       MVPP2_PRS_RI_L3_IP6,
 		       MVPP2_PRS_RI_L3_PROTO_MASK),
 
 	/* Non IP flow, no vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_ETHERNET, MVPP2_FL_NON_IP_UNTAG,
+	MVPP2_DEF_FLOW(ETHER_FLOW, MVPP2_FL_NON_IP_UNTAG,
 		       0,
 		       MVPP2_PRS_RI_VLAN_NONE,
 		       MVPP2_PRS_RI_VLAN_MASK),
 	/* Non IP flow, with vlan tag */
-	MVPP2_DEF_FLOW(MVPP22_FLOW_ETHERNET, MVPP2_FL_NON_IP_TAG,
+	MVPP2_DEF_FLOW(ETHER_FLOW, MVPP2_FL_NON_IP_TAG,
 		       MVPP22_CLS_HEK_OPT_VLAN,
 		       0, 0),
 };
@@ -344,9 +344,9 @@ static void mvpp2_cls_flow_write(struct mvpp2 *priv,
 				 struct mvpp2_cls_flow_entry *fe)
 {
 	mvpp2_write(priv, MVPP2_CLS_FLOW_INDEX_REG, fe->index);
-	mvpp2_write(priv, MVPP2_CLS_FLOW_TBL0_REG, fe->data[0]);
-	mvpp2_write(priv, MVPP2_CLS_FLOW_TBL1_REG, fe->data[1]);
-	mvpp2_write(priv, MVPP2_CLS_FLOW_TBL2_REG, fe->data[2]);
+	mvpp2_write(priv, MVPP2_CLS_FLOW_TBL0_REG,  fe->data[0]);
+	mvpp2_write(priv, MVPP2_CLS_FLOW_TBL1_REG,  fe->data[1]);
+	mvpp2_write(priv, MVPP2_CLS_FLOW_TBL2_REG,  fe->data[2]);
 }
 
 u32 mvpp2_cls_lookup_hits(struct mvpp2 *priv, int index)
@@ -436,6 +436,19 @@ static void mvpp2_cls_flow_last_set(struct mvpp2_cls_flow_entry *fe,
 	fe->data[0] |= !!is_last;
 }
 
+static bool mvpp2_cls_flow_last_get(struct mvpp2_cls_flow_entry *fe)
+{
+	return (fe->data[0] & MVPP2_CLS_FLOW_TBL0_LAST);
+}
+
+static void mvpp2_cls_flow_lkp_type_set(struct mvpp2_cls_flow_entry *fe,
+					int lkp_type)
+{
+	fe->data[1] &= ~MVPP2_CLS_FLOW_TBL1_LKP_TYPE(
+			MVPP2_CLS_FLOW_TBL1_LKP_TYPE_MASK);
+	fe->data[1] |= MVPP2_CLS_FLOW_TBL1_LKP_TYPE(lkp_type);
+}
+
 static void mvpp2_cls_flow_pri_set(struct mvpp2_cls_flow_entry *fe, int prio)
 {
 	fe->data[1] &= ~MVPP2_CLS_FLOW_TBL1_PRIO(MVPP2_CLS_FLOW_TBL1_PRIO_MASK);
@@ -448,22 +461,14 @@ static void mvpp2_cls_flow_port_add(struct mvpp2_cls_flow_entry *fe,
 	fe->data[0] |= MVPP2_CLS_FLOW_TBL0_PORT_ID(port);
 }
 
-static void mvpp2_cls_flow_port_remove(struct mvpp2_cls_flow_entry *fe,
-				       u32 port)
-{
-	fe->data[0] &= ~MVPP2_CLS_FLOW_TBL0_PORT_ID(port);
-}
-
-static void mvpp2_cls_flow_lu_type_set(struct mvpp2_cls_flow_entry *fe,
-				       u8 lu_type)
+static int mvpp2_cls_flow_port_get(struct mvpp2_cls_flow_entry *fe)
 {
-	fe->data[1] &= ~MVPP2_CLS_FLOW_TBL1_LU_TYPE(MVPP2_CLS_LU_TYPE_MASK);
-	fe->data[1] |= MVPP2_CLS_FLOW_TBL1_LU_TYPE(lu_type);
+	return ((fe->data[0] >> 4) & MVPP2_CLS_FLOW_TBL0_PORT_ID_MASK);
 }
 
 /* Initialize the parser entry for the given flow */
 static void mvpp2_cls_flow_prs_init(struct mvpp2 *priv,
-				    const struct mvpp2_cls_flow *flow)
+				    struct mvpp2_cls_flow *flow)
 {
 	mvpp2_prs_add_flow(priv, flow->flow_id, flow->prs_ri.ri,
 			   flow->prs_ri.ri_mask);
@@ -471,7 +476,7 @@ static void mvpp2_cls_flow_prs_init(struct mvpp2 *priv,
 
 /* Initialize the Lookup Id table entry for the given flow */
 static void mvpp2_cls_flow_lkp_init(struct mvpp2 *priv,
-				    const struct mvpp2_cls_flow *flow)
+				    struct mvpp2_cls_flow *flow)
 {
 	struct mvpp2_cls_lookup_entry le;
 
@@ -484,7 +489,7 @@ static void mvpp2_cls_flow_lkp_init(struct mvpp2 *priv,
 	/* We point on the first lookup in the sequence for the flow, that is
 	 * the C2 lookup.
 	 */
-	le.data |= MVPP2_CLS_LKP_FLOW_PTR(MVPP2_CLS_FLT_FIRST(flow->flow_id));
+	le.data |= MVPP2_CLS_LKP_FLOW_PTR(MVPP2_FLOW_C2_ENTRY(flow->flow_id));
 
 	/* CLS is always enabled, RSS is enabled/disabled in C2 lookup */
 	le.data |= MVPP2_CLS_LKP_TBL_LOOKUP_EN_MASK;
@@ -492,113 +497,21 @@ static void mvpp2_cls_flow_lkp_init(struct mvpp2 *priv,
 	mvpp2_cls_lookup_write(priv, &le);
 }
 
-static void mvpp2_cls_c2_write(struct mvpp2 *priv,
-			       struct mvpp2_cls_c2_entry *c2)
-{
-	u32 val;
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_IDX, c2->index);
-
-	val = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_INV);
-	if (c2->valid)
-		val &= ~MVPP22_CLS_C2_TCAM_INV_BIT;
-	else
-		val |= MVPP22_CLS_C2_TCAM_INV_BIT;
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_INV, val);
-
-	mvpp2_write(priv, MVPP22_CLS_C2_ACT, c2->act);
-
-	mvpp2_write(priv, MVPP22_CLS_C2_ATTR0, c2->attr[0]);
-	mvpp2_write(priv, MVPP22_CLS_C2_ATTR1, c2->attr[1]);
-	mvpp2_write(priv, MVPP22_CLS_C2_ATTR2, c2->attr[2]);
-	mvpp2_write(priv, MVPP22_CLS_C2_ATTR3, c2->attr[3]);
-
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA0, c2->tcam[0]);
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA1, c2->tcam[1]);
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA2, c2->tcam[2]);
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA3, c2->tcam[3]);
-	/* Writing TCAM_DATA4 flushes writes to TCAM_DATA0-4 and INV to HW */
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA4, c2->tcam[4]);
-}
-
-void mvpp2_cls_c2_read(struct mvpp2 *priv, int index,
-		       struct mvpp2_cls_c2_entry *c2)
-{
-	u32 val;
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_IDX, index);
-
-	c2->index = index;
-
-	c2->tcam[0] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA0);
-	c2->tcam[1] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA1);
-	c2->tcam[2] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA2);
-	c2->tcam[3] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA3);
-	c2->tcam[4] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA4);
-
-	c2->act = mvpp2_read(priv, MVPP22_CLS_C2_ACT);
-
-	c2->attr[0] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR0);
-	c2->attr[1] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR1);
-	c2->attr[2] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR2);
-	c2->attr[3] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR3);
-
-	val = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_INV);
-	c2->valid = !(val & MVPP22_CLS_C2_TCAM_INV_BIT);
-}
-
-static int mvpp2_cls_ethtool_flow_to_type(int flow_type)
-{
-	switch (flow_type & ~(FLOW_EXT | FLOW_MAC_EXT | FLOW_RSS)) {
-	case ETHER_FLOW:
-		return MVPP22_FLOW_ETHERNET;
-	case TCP_V4_FLOW:
-		return MVPP22_FLOW_TCP4;
-	case TCP_V6_FLOW:
-		return MVPP22_FLOW_TCP6;
-	case UDP_V4_FLOW:
-		return MVPP22_FLOW_UDP4;
-	case UDP_V6_FLOW:
-		return MVPP22_FLOW_UDP6;
-	case IPV4_FLOW:
-		return MVPP22_FLOW_IP4;
-	case IPV6_FLOW:
-		return MVPP22_FLOW_IP6;
-	default:
-		return -EOPNOTSUPP;
-	}
-}
-
-static int mvpp2_cls_c2_port_flow_index(struct mvpp2_port *port, int loc)
-{
-	return MVPP22_CLS_C2_RFS_LOC(port->id, loc);
-}
-
 /* Initialize the flow table entries for the given flow */
-static void mvpp2_cls_flow_init(struct mvpp2 *priv,
-				const struct mvpp2_cls_flow *flow)
+static void mvpp2_cls_flow_init(struct mvpp2 *priv, struct mvpp2_cls_flow *flow)
 {
 	struct mvpp2_cls_flow_entry fe;
-	int i, pri = 0;
-
-	/* Assign default values to all entries in the flow */
-	for (i = MVPP2_CLS_FLT_FIRST(flow->flow_id);
-	     i <= MVPP2_CLS_FLT_LAST(flow->flow_id); i++) {
-		memset(&fe, 0, sizeof(fe));
-		fe.index = i;
-		mvpp2_cls_flow_pri_set(&fe, pri++);
-
-		if (i == MVPP2_CLS_FLT_LAST(flow->flow_id))
-			mvpp2_cls_flow_last_set(&fe, 1);
-
-		mvpp2_cls_flow_write(priv, &fe);
-	}
+	int i;
 
-	/* RSS config C2 lookup */
-	mvpp2_cls_flow_read(priv, MVPP2_CLS_FLT_C2_RSS_ENTRY(flow->flow_id),
-			    &fe);
+	/* C2 lookup */
+	memset(&fe, 0, sizeof(fe));
+	fe.index = MVPP2_FLOW_C2_ENTRY(flow->flow_id);
 
 	mvpp2_cls_flow_eng_set(&fe, MVPP22_CLS_ENGINE_C2);
 	mvpp2_cls_flow_port_id_sel(&fe, true);
-	mvpp2_cls_flow_lu_type_set(&fe, MVPP22_CLS_LU_TYPE_ALL);
+	mvpp2_cls_flow_last_set(&fe, 0);
+	mvpp2_cls_flow_pri_set(&fe, 0);
+	mvpp2_cls_flow_lkp_type_set(&fe, MVPP2_CLS_LKP_DEFAULT);
 
 	/* Add all ports */
 	for (i = 0; i < MVPP2_MAX_PORTS; i++)
@@ -608,19 +521,22 @@ static void mvpp2_cls_flow_init(struct mvpp2 *priv,
 
 	/* C3Hx lookups */
 	for (i = 0; i < MVPP2_MAX_PORTS; i++) {
-		mvpp2_cls_flow_read(priv,
-				    MVPP2_CLS_FLT_HASH_ENTRY(i, flow->flow_id),
-				    &fe);
+		memset(&fe, 0, sizeof(fe));
+		fe.index = MVPP2_PORT_FLOW_INDEX(i, flow->flow_id);
 
-		/* Set a default engine. Will be overwritten when setting the
-		 * real HEK parameters
-		 */
 		mvpp2_cls_flow_eng_set(&fe, MVPP22_CLS_ENGINE_C3HA);
 		mvpp2_cls_flow_port_id_sel(&fe, true);
+		mvpp2_cls_flow_pri_set(&fe, i + 1);
 		mvpp2_cls_flow_port_add(&fe, BIT(i));
+		mvpp2_cls_flow_lkp_type_set(&fe, MVPP2_CLS_LKP_HASH);
 
 		mvpp2_cls_flow_write(priv, &fe);
 	}
+
+	/* Update the last entry */
+	mvpp2_cls_flow_last_set(&fe, 1);
+
+	mvpp2_cls_flow_write(priv, &fe);
 }
 
 /* Adds a field to the Header Extracted Key generation parameters*/
@@ -639,6 +555,20 @@ static int mvpp2_flow_add_hek_field(struct mvpp2_cls_flow_entry *fe,
 	return 0;
 }
 
+static void mvpp2_cls_c2_inv_set(struct mvpp2 *priv,
+				 int index)
+{
+	/* write index reg */
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_IDX, index);
+
+	/* set invalid bit */
+	mvpp2_write(priv, MVPP2_CLS2_TCAM_INV_REG,
+		    (1 << MVPP2_CLS2_TCAM_INV_INVALID));
+
+	/* trigger */
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA4, 0);
+}
+
 static int mvpp2_flow_set_hek_fields(struct mvpp2_cls_flow_entry *fe,
 				     unsigned long hash_opts)
 {
@@ -651,15 +581,9 @@ static int mvpp2_flow_set_hek_fields(struct mvpp2_cls_flow_entry *fe,
 
 	for_each_set_bit(i, &hash_opts, MVPP22_CLS_HEK_N_FIELDS) {
 		switch (BIT(i)) {
-		case MVPP22_CLS_HEK_OPT_MAC_DA:
-			field_id = MVPP22_CLS_FIELD_MAC_DA;
-			break;
 		case MVPP22_CLS_HEK_OPT_VLAN:
 			field_id = MVPP22_CLS_FIELD_VLAN;
 			break;
-		case MVPP22_CLS_HEK_OPT_VLAN_PRI:
-			field_id = MVPP22_CLS_FIELD_VLAN_PRI;
-			break;
 		case MVPP22_CLS_HEK_OPT_IP4SA:
 			field_id = MVPP22_CLS_FIELD_IP4SA;
 			break;
@@ -688,36 +612,42 @@ static int mvpp2_flow_set_hek_fields(struct mvpp2_cls_flow_entry *fe,
 	return 0;
 }
 
-/* Returns the size, in bits, of the corresponding HEK field */
-static int mvpp2_cls_hek_field_size(u32 field)
+struct mvpp2_cls_flow *mvpp2_cls_flow_get(int flow)
 {
-	switch (field) {
-	case MVPP22_CLS_HEK_OPT_MAC_DA:
-		return 48;
-	case MVPP22_CLS_HEK_OPT_VLAN:
-		return 12;
-	case MVPP22_CLS_HEK_OPT_VLAN_PRI:
-		return 3;
-	case MVPP22_CLS_HEK_OPT_IP4SA:
-	case MVPP22_CLS_HEK_OPT_IP4DA:
-		return 32;
-	case MVPP22_CLS_HEK_OPT_IP6SA:
-	case MVPP22_CLS_HEK_OPT_IP6DA:
-		return 128;
-	case MVPP22_CLS_HEK_OPT_L4SIP:
-	case MVPP22_CLS_HEK_OPT_L4DIP:
-		return 16;
-	default:
-		return -1;
-	}
+	if (flow >= MVPP2_N_FLOWS)
+		return NULL;
+
+	return &cls_flows[flow];
 }
 
-const struct mvpp2_cls_flow *mvpp2_cls_flow_get(int flow)
+int mvpp2_cls_flow_hash_find(struct mvpp2_port *port,
+			     struct mvpp2_cls_flow *flow,
+			     struct mvpp2_cls_flow_entry *fe,
+			     int *flow_index)
 {
-	if (flow >= MVPP2_N_PRS_FLOWS)
-		return NULL;
+	int engine, flow_offset, port_bm, idx = 0, is_last = 0;
 
-	return &cls_flows[flow];
+	flow_offset = 0;
+	do {
+		idx = MVPP2_PORT_FLOW_INDEX(flow_offset, flow->flow_id);
+		if (idx >= MVPP2_CLS_FLOWS_TBL_SIZE)
+			break;
+		mvpp2_cls_flow_read(port->priv, idx, fe);
+		engine = mvpp2_cls_flow_eng_get(fe);
+		port_bm = mvpp2_cls_flow_port_get(fe);
+		is_last = mvpp2_cls_flow_last_get(fe);
+		if ((engine == MVPP22_CLS_ENGINE_C3HA ||
+		     engine == MVPP22_CLS_ENGINE_C3HB) &&
+		    (port_bm & BIT(port->id)))
+			break;
+		flow_offset++;
+	} while (!is_last);
+
+	*flow_index = idx;
+	if (is_last)
+		return -EINVAL;
+
+	return 0;
 }
 
 /* Set the hash generation options for the given traffic flow.
@@ -734,17 +664,21 @@ const struct mvpp2_cls_flow *mvpp2_cls_flow_get(int flow)
 static int mvpp2_port_rss_hash_opts_set(struct mvpp2_port *port, int flow_type,
 					u16 requested_opts)
 {
-	const struct mvpp2_cls_flow *flow;
 	struct mvpp2_cls_flow_entry fe;
+	struct mvpp2_cls_flow *flow;
 	int i, engine, flow_index;
 	u16 hash_opts;
 
-	for_each_cls_flow_id_with_type(i, flow_type) {
+	for (i = 0; i < MVPP2_N_FLOWS; i++) {
 		flow = mvpp2_cls_flow_get(i);
 		if (!flow)
 			return -EINVAL;
 
-		flow_index = MVPP2_CLS_FLT_HASH_ENTRY(port->id, flow->flow_id);
+		if (flow->flow_type != flow_type)
+			continue;
+
+		if (mvpp2_cls_flow_hash_find(port, flow, &fe, &flow_index))
+			return -EINVAL;
 
 		mvpp2_cls_flow_read(port->priv, flow_index, &fe);
 
@@ -786,9 +720,6 @@ u16 mvpp2_flow_get_hek_fields(struct mvpp2_cls_flow_entry *fe)
 		case MVPP22_CLS_FIELD_VLAN:
 			hash_opts |= MVPP22_CLS_HEK_OPT_VLAN;
 			break;
-		case MVPP22_CLS_FIELD_VLAN_PRI:
-			hash_opts |= MVPP22_CLS_HEK_OPT_VLAN_PRI;
-			break;
 		case MVPP22_CLS_FIELD_L3_PROTO:
 			hash_opts |= MVPP22_CLS_HEK_OPT_L3_PROTO;
 			break;
@@ -822,17 +753,21 @@ u16 mvpp2_flow_get_hek_fields(struct mvpp2_cls_flow_entry *fe)
  */
 static u16 mvpp2_port_rss_hash_opts_get(struct mvpp2_port *port, int flow_type)
 {
-	const struct mvpp2_cls_flow *flow;
 	struct mvpp2_cls_flow_entry fe;
+	struct mvpp2_cls_flow *flow;
 	int i, flow_index;
 	u16 hash_opts = 0;
 
-	for_each_cls_flow_id_with_type(i, flow_type) {
+	for (i = 0; i < MVPP2_N_FLOWS; i++) {
 		flow = mvpp2_cls_flow_get(i);
 		if (!flow)
 			return 0;
 
-		flow_index = MVPP2_CLS_FLT_HASH_ENTRY(port->id, flow->flow_id);
+		if (flow->flow_type != flow_type)
+			continue;
+
+		if (mvpp2_cls_flow_hash_find(port, flow, &fe, &flow_index))
+			return 0;
 
 		mvpp2_cls_flow_read(port->priv, flow_index, &fe);
 
@@ -844,10 +779,10 @@ static u16 mvpp2_port_rss_hash_opts_get(struct mvpp2_port *port, int flow_type)
 
 static void mvpp2_cls_port_init_flows(struct mvpp2 *priv)
 {
-	const struct mvpp2_cls_flow *flow;
+	struct mvpp2_cls_flow *flow;
 	int i;
 
-	for (i = 0; i < MVPP2_N_PRS_FLOWS; i++) {
+	for (i = 0; i < MVPP2_N_FLOWS; i++) {
 		flow = mvpp2_cls_flow_get(i);
 		if (!flow)
 			break;
@@ -858,6 +793,51 @@ static void mvpp2_cls_port_init_flows(struct mvpp2 *priv)
 	}
 }
 
+static void mvpp2_cls_c2_write(struct mvpp2 *priv,
+			       struct mvpp2_cls_c2_entry *c2)
+{
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_IDX, c2->index);
+
+	mvpp2_write(priv, MVPP22_CLS_C2_ACT, c2->act);
+
+	mvpp2_write(priv, MVPP22_CLS_C2_ATTR0, c2->attr[0]);
+	mvpp2_write(priv, MVPP22_CLS_C2_ATTR1, c2->attr[1]);
+	mvpp2_write(priv, MVPP22_CLS_C2_ATTR2, c2->attr[2]);
+	mvpp2_write(priv, MVPP22_CLS_C2_ATTR3, c2->attr[3]);
+
+	/* write valid bit*/
+	mvpp2_write(priv, MVPP2_CLS2_TCAM_INV_REG,
+		    (0 << MVPP2_CLS2_TCAM_INV_INVALID));
+
+	/* Write TCAM */
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA0, c2->tcam[0]);
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA1, c2->tcam[1]);
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA2, c2->tcam[2]);
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA3, c2->tcam[3]);
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_DATA4, c2->tcam[4]);
+}
+
+void mvpp2_cls_c2_read(struct mvpp2 *priv, int index,
+		       struct mvpp2_cls_c2_entry *c2)
+{
+	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_IDX, index);
+
+	c2->index = index;
+
+	c2->tcam[0] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA0);
+	c2->tcam[1] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA1);
+	c2->tcam[2] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA2);
+	c2->tcam[3] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA3);
+	c2->tcam[4] = mvpp2_read(priv, MVPP22_CLS_C2_TCAM_DATA4);
+
+	c2->act = mvpp2_read(priv, MVPP22_CLS_C2_ACT);
+
+	c2->attr[0] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR0);
+	c2->attr[1] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR1);
+	c2->attr[2] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR2);
+	c2->attr[3] = mvpp2_read(priv, MVPP22_CLS_C2_ATTR3);
+}
+
 static void mvpp2_port_c2_cls_init(struct mvpp2_port *port)
 {
 	struct mvpp2_cls_c2_entry c2;
@@ -871,9 +851,9 @@ static void mvpp2_port_c2_cls_init(struct mvpp2_port *port)
 	c2.tcam[4] = MVPP22_CLS_C2_PORT_ID(pmap);
 	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_PORT_ID(pmap));
 
-	/* Match on Lookup Type */
-	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_LU_TYPE(MVPP2_CLS_LU_TYPE_MASK));
-	c2.tcam[4] |= MVPP22_CLS_C2_LU_TYPE(MVPP22_CLS_LU_TYPE_ALL);
+	/* Set lkp_type */
+	c2.tcam[4] |= MVPP22_CLS_C2_LKP_TYPE(MVPP2_CLS_LKP_DEFAULT);
+	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_LKP_TYPE_MASK);
 
 	/* Update RSS status after matching this entry */
 	c2.act = MVPP22_CLS_C2_ACT_RSS_EN(MVPP22_C2_UPD_LOCK);
@@ -893,17 +873,27 @@ static void mvpp2_port_c2_cls_init(struct mvpp2_port *port)
 	c2.attr[0] = MVPP22_CLS_C2_ATTR0_QHIGH(qh) |
 		      MVPP22_CLS_C2_ATTR0_QLOW(ql);
 
-	c2.valid = true;
-
 	mvpp2_cls_c2_write(port->priv, &c2);
 }
 
+static void mvpp2_cls_c2_init(struct mvpp2 *priv)
+{
+	int index;
+
+	/* Toggle C2 from Built-In Self-Test mode to Functional mode */
+	mvpp2_write(priv, MVPP2_CLS2_TCAM_CTRL_REG,
+		    MVPP2_CLS2_TCAM_CTRL_BYPASS_FIFO_STAGES);
+
+	/* Invalidate all C2 entries */
+	for (index = 0; index < MVPP22_CLS_C2_MAX_ENTRIES; index++)
+		mvpp2_cls_c2_inv_set(priv, index);
+}
+
 /* Classifier default initialization */
 void mvpp2_cls_init(struct mvpp2 *priv)
 {
 	struct mvpp2_cls_lookup_entry le;
 	struct mvpp2_cls_flow_entry fe;
-	struct mvpp2_cls_c2_entry c2;
 	int index;
 
 	/* Enable classifier */
@@ -927,21 +917,15 @@ void mvpp2_cls_init(struct mvpp2 *priv)
 		mvpp2_cls_lookup_write(priv, &le);
 	}
 
-	/* Clear C2 TCAM engine table */
-	memset(&c2, 0, sizeof(c2));
-	c2.valid = false;
-	for (index = 0; index < MVPP22_CLS_C2_N_ENTRIES; index++) {
-		c2.index = index;
-		mvpp2_cls_c2_write(priv, &c2);
-	}
-
-	/* Disable the FIFO stages in C2 engine, which are only used in BIST
-	 * mode
+	/* Clear CLS_SWFWD_PCTRL register - value of QueueHigh is defined by
+	 * the Classifier
 	 */
-	mvpp2_write(priv, MVPP22_CLS_C2_TCAM_CTRL,
-		    MVPP22_CLS_C2_TCAM_BYPASS_FIFO);
+	mvpp2_write(priv, MVPP2_CLS_SWFWD_PCTRL_REG, 0);
 
 	mvpp2_cls_port_init_flows(priv);
+
+	/* Initialize C2 */
+	mvpp2_cls_c2_init(priv);
 }
 
 void mvpp2_cls_port_config(struct mvpp2_port *port)
@@ -981,22 +965,12 @@ u32 mvpp2_cls_c2_hit_count(struct mvpp2 *priv, int c2_index)
 	return mvpp2_read(priv, MVPP22_CLS_C2_HIT_CTR);
 }
 
-static void mvpp2_rss_port_c2_enable(struct mvpp2_port *port, u32 ctx)
+static void mvpp2_rss_port_c2_enable(struct mvpp2_port *port)
 {
 	struct mvpp2_cls_c2_entry c2;
-	u8 qh, ql;
 
 	mvpp2_cls_c2_read(port->priv, MVPP22_CLS_C2_RSS_ENTRY(port->id), &c2);
 
-	/* The RxQ number is used to select the RSS table. It that case, we set
-	 * it to be the ctx number.
-	 */
-	qh = (ctx >> 3) & MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
-	ql = ctx & MVPP22_CLS_C2_ATTR0_QLOW_MASK;
-
-	c2.attr[0] = MVPP22_CLS_C2_ATTR0_QHIGH(qh) |
-		     MVPP22_CLS_C2_ATTR0_QLOW(ql);
-
 	c2.attr[2] |= MVPP22_CLS_C2_ATTR2_RSS_EN;
 
 	mvpp2_cls_c2_write(port->priv, &c2);
@@ -1005,440 +979,29 @@ static void mvpp2_rss_port_c2_enable(struct mvpp2_port *port, u32 ctx)
 static void mvpp2_rss_port_c2_disable(struct mvpp2_port *port)
 {
 	struct mvpp2_cls_c2_entry c2;
-	u8 qh, ql;
 
 	mvpp2_cls_c2_read(port->priv, MVPP22_CLS_C2_RSS_ENTRY(port->id), &c2);
 
-	/* Reset the default destination RxQ to the port's first rx queue. */
-	qh = (port->first_rxq >> 3) & MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
-	ql = port->first_rxq & MVPP22_CLS_C2_ATTR0_QLOW_MASK;
-
-	c2.attr[0] = MVPP22_CLS_C2_ATTR0_QHIGH(qh) |
-		      MVPP22_CLS_C2_ATTR0_QLOW(ql);
-
 	c2.attr[2] &= ~MVPP22_CLS_C2_ATTR2_RSS_EN;
 
 	mvpp2_cls_c2_write(port->priv, &c2);
 }
 
-static inline int mvpp22_rss_ctx(struct mvpp2_port *port, int port_rss_ctx)
+void mvpp22_rss_enable(struct mvpp2_port *port)
 {
-	return port->rss_ctx[port_rss_ctx];
-}
-
-int mvpp22_port_rss_enable(struct mvpp2_port *port)
-{
-	if (mvpp22_rss_ctx(port, 0) < 0)
-		return -EINVAL;
-
-	mvpp2_rss_port_c2_enable(port, mvpp22_rss_ctx(port, 0));
-
-	return 0;
+	mvpp2_rss_port_c2_enable(port);
 }
 
-int mvpp22_port_rss_disable(struct mvpp2_port *port)
+void mvpp22_rss_disable(struct mvpp2_port *port)
 {
-	if (mvpp22_rss_ctx(port, 0) < 0)
-		return -EINVAL;
-
 	mvpp2_rss_port_c2_disable(port);
-
-	return 0;
-}
-
-static void mvpp22_port_c2_lookup_disable(struct mvpp2_port *port, int entry)
-{
-	struct mvpp2_cls_c2_entry c2;
-
-	mvpp2_cls_c2_read(port->priv, entry, &c2);
-
-	/* Clear the port map so that the entry doesn't match anymore */
-	c2.tcam[4] &= ~(MVPP22_CLS_C2_PORT_ID(BIT(port->id)));
-
-	mvpp2_cls_c2_write(port->priv, &c2);
 }
 
 /* Set CPU queue number for oversize packets */
 void mvpp2_cls_oversize_rxq_set(struct mvpp2_port *port)
 {
-	u32 val;
-
 	mvpp2_write(port->priv, MVPP2_CLS_OVERSIZE_RXQ_LOW_REG(port->id),
 		    port->first_rxq & MVPP2_CLS_OVERSIZE_RXQ_LOW_MASK);
-
-	mvpp2_write(port->priv, MVPP2_CLS_SWFWD_P2HQ_REG(port->id),
-		    (port->first_rxq >> MVPP2_CLS_OVERSIZE_RXQ_LOW_BITS));
-
-	val = mvpp2_read(port->priv, MVPP2_CLS_SWFWD_PCTRL_REG);
-	val &= ~MVPP2_CLS_SWFWD_PCTRL_MASK(port->id);
-	mvpp2_write(port->priv, MVPP2_CLS_SWFWD_PCTRL_REG, val);
-}
-
-static int mvpp2_port_c2_tcam_rule_add(struct mvpp2_port *port,
-				       struct mvpp2_rfs_rule *rule)
-{
-	struct flow_action_entry *act;
-	struct mvpp2_cls_c2_entry c2;
-	u8 qh, ql, pmap;
-	int index, ctx;
-
-	memset(&c2, 0, sizeof(c2));
-
-	index = mvpp2_cls_c2_port_flow_index(port, rule->loc);
-	if (index < 0)
-		return -EINVAL;
-	c2.index = index;
-
-	act = &rule->flow->action.entries[0];
-
-	rule->c2_index = c2.index;
-
-	c2.tcam[3] = (rule->c2_tcam & 0xffff) |
-		     ((rule->c2_tcam_mask & 0xffff) << 16);
-	c2.tcam[2] = ((rule->c2_tcam >> 16) & 0xffff) |
-		     (((rule->c2_tcam_mask >> 16) & 0xffff) << 16);
-	c2.tcam[1] = ((rule->c2_tcam >> 32) & 0xffff) |
-		     (((rule->c2_tcam_mask >> 32) & 0xffff) << 16);
-	c2.tcam[0] = ((rule->c2_tcam >> 48) & 0xffff) |
-		     (((rule->c2_tcam_mask >> 48) & 0xffff) << 16);
-
-	pmap = BIT(port->id);
-	c2.tcam[4] = MVPP22_CLS_C2_PORT_ID(pmap);
-	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_PORT_ID(pmap));
-
-	/* Match on Lookup Type */
-	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_LU_TYPE(MVPP2_CLS_LU_TYPE_MASK));
-	c2.tcam[4] |= MVPP22_CLS_C2_LU_TYPE(rule->loc);
-
-	if (act->id == FLOW_ACTION_DROP) {
-		c2.act = MVPP22_CLS_C2_ACT_COLOR(MVPP22_C2_COL_RED_LOCK);
-	} else {
-		/* We want to keep the default color derived from the Header
-		 * Parser drop entries, for VLAN and MAC filtering. This will
-		 * assign a default color of Green or Red, and we want matches
-		 * with a non-drop action to keep that color.
-		 */
-		c2.act = MVPP22_CLS_C2_ACT_COLOR(MVPP22_C2_COL_NO_UPD_LOCK);
-
-		/* Update RSS status after matching this entry */
-		if (act->queue.ctx)
-			c2.attr[2] |= MVPP22_CLS_C2_ATTR2_RSS_EN;
-
-		/* Always lock the RSS_EN decision. We might have high prio
-		 * rules steering to an RXQ, and a lower one steering to RSS,
-		 * we don't want the low prio RSS rule overwriting this flag.
-		 */
-		c2.act = MVPP22_CLS_C2_ACT_RSS_EN(MVPP22_C2_UPD_LOCK);
-
-		/* Mark packet as "forwarded to software", needed for RSS */
-		c2.act |= MVPP22_CLS_C2_ACT_FWD(MVPP22_C2_FWD_SW_LOCK);
-
-		c2.act |= MVPP22_CLS_C2_ACT_QHIGH(MVPP22_C2_UPD_LOCK) |
-			   MVPP22_CLS_C2_ACT_QLOW(MVPP22_C2_UPD_LOCK);
-
-		if (act->queue.ctx) {
-			/* Get the global ctx number */
-			ctx = mvpp22_rss_ctx(port, act->queue.ctx);
-			if (ctx < 0)
-				return -EINVAL;
-
-			qh = (ctx >> 3) & MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
-			ql = ctx & MVPP22_CLS_C2_ATTR0_QLOW_MASK;
-		} else {
-			qh = ((act->queue.index + port->first_rxq) >> 3) &
-			      MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
-			ql = (act->queue.index + port->first_rxq) &
-			      MVPP22_CLS_C2_ATTR0_QLOW_MASK;
-		}
-
-		c2.attr[0] = MVPP22_CLS_C2_ATTR0_QHIGH(qh) |
-			      MVPP22_CLS_C2_ATTR0_QLOW(ql);
-	}
-
-	c2.valid = true;
-
-	mvpp2_cls_c2_write(port->priv, &c2);
-
-	return 0;
-}
-
-static int mvpp2_port_c2_rfs_rule_insert(struct mvpp2_port *port,
-					 struct mvpp2_rfs_rule *rule)
-{
-	return mvpp2_port_c2_tcam_rule_add(port, rule);
-}
-
-static int mvpp2_port_cls_rfs_rule_remove(struct mvpp2_port *port,
-					  struct mvpp2_rfs_rule *rule)
-{
-	const struct mvpp2_cls_flow *flow;
-	struct mvpp2_cls_flow_entry fe;
-	int index, i;
-
-	for_each_cls_flow_id_containing_type(i, rule->flow_type) {
-		flow = mvpp2_cls_flow_get(i);
-		if (!flow)
-			return 0;
-
-		index = MVPP2_CLS_FLT_C2_RFS(port->id, flow->flow_id, rule->loc);
-
-		mvpp2_cls_flow_read(port->priv, index, &fe);
-		mvpp2_cls_flow_port_remove(&fe, BIT(port->id));
-		mvpp2_cls_flow_write(port->priv, &fe);
-	}
-
-	if (rule->c2_index >= 0)
-		mvpp22_port_c2_lookup_disable(port, rule->c2_index);
-
-	return 0;
-}
-
-static int mvpp2_port_flt_rfs_rule_insert(struct mvpp2_port *port,
-					  struct mvpp2_rfs_rule *rule)
-{
-	const struct mvpp2_cls_flow *flow;
-	struct mvpp2 *priv = port->priv;
-	struct mvpp2_cls_flow_entry fe;
-	int index, ret, i;
-
-	if (rule->engine != MVPP22_CLS_ENGINE_C2)
-		return -EOPNOTSUPP;
-
-	ret = mvpp2_port_c2_rfs_rule_insert(port, rule);
-	if (ret)
-		return ret;
-
-	for_each_cls_flow_id_containing_type(i, rule->flow_type) {
-		flow = mvpp2_cls_flow_get(i);
-		if (!flow)
-			return 0;
-
-		if ((rule->hek_fields & flow->supported_hash_opts) != rule->hek_fields)
-			continue;
-
-		index = MVPP2_CLS_FLT_C2_RFS(port->id, flow->flow_id, rule->loc);
-
-		mvpp2_cls_flow_read(priv, index, &fe);
-		mvpp2_cls_flow_eng_set(&fe, rule->engine);
-		mvpp2_cls_flow_port_id_sel(&fe, true);
-		mvpp2_flow_set_hek_fields(&fe, rule->hek_fields);
-		mvpp2_cls_flow_lu_type_set(&fe, rule->loc);
-		mvpp2_cls_flow_port_add(&fe, 0xf);
-
-		mvpp2_cls_flow_write(priv, &fe);
-	}
-
-	return 0;
-}
-
-static int mvpp2_cls_c2_build_match(struct mvpp2_rfs_rule *rule)
-{
-	struct flow_rule *flow = rule->flow;
-	int offs = 0;
-
-	/* The order of insertion in C2 tcam must match the order in which
-	 * the fields are found in the header
-	 */
-	if (flow_rule_match_key(flow, FLOW_DISSECTOR_KEY_VLAN)) {
-		struct flow_match_vlan match;
-
-		flow_rule_match_vlan(flow, &match);
-		if (match.mask->vlan_id) {
-			rule->hek_fields |= MVPP22_CLS_HEK_OPT_VLAN;
-
-			rule->c2_tcam |= ((u64)match.key->vlan_id) << offs;
-			rule->c2_tcam_mask |= ((u64)match.mask->vlan_id) << offs;
-
-			/* Don't update the offset yet */
-		}
-
-		if (match.mask->vlan_priority) {
-			rule->hek_fields |= MVPP22_CLS_HEK_OPT_VLAN_PRI;
-
-			/* VLAN pri is always at offset 13 relative to the
-			 * current offset
-			 */
-			rule->c2_tcam |= ((u64)match.key->vlan_priority) <<
-				(offs + 13);
-			rule->c2_tcam_mask |= ((u64)match.mask->vlan_priority) <<
-				(offs + 13);
-		}
-
-		if (match.mask->vlan_dei)
-			return -EOPNOTSUPP;
-
-		/* vlan id and prio always seem to take a full 16-bit slot in
-		 * the Header Extracted Key.
-		 */
-		offs += 16;
-	}
-
-	if (flow_rule_match_key(flow, FLOW_DISSECTOR_KEY_PORTS)) {
-		struct flow_match_ports match;
-
-		flow_rule_match_ports(flow, &match);
-		if (match.mask->src) {
-			rule->hek_fields |= MVPP22_CLS_HEK_OPT_L4SIP;
-
-			rule->c2_tcam |= ((u64)ntohs(match.key->src)) << offs;
-			rule->c2_tcam_mask |= ((u64)ntohs(match.mask->src)) << offs;
-			offs += mvpp2_cls_hek_field_size(MVPP22_CLS_HEK_OPT_L4SIP);
-		}
-
-		if (match.mask->dst) {
-			rule->hek_fields |= MVPP22_CLS_HEK_OPT_L4DIP;
-
-			rule->c2_tcam |= ((u64)ntohs(match.key->dst)) << offs;
-			rule->c2_tcam_mask |= ((u64)ntohs(match.mask->dst)) << offs;
-			offs += mvpp2_cls_hek_field_size(MVPP22_CLS_HEK_OPT_L4DIP);
-		}
-	}
-
-	if (hweight16(rule->hek_fields) > MVPP2_FLOW_N_FIELDS)
-		return -EOPNOTSUPP;
-
-	return 0;
-}
-
-static int mvpp2_cls_rfs_parse_rule(struct mvpp2_rfs_rule *rule)
-{
-	struct flow_rule *flow = rule->flow;
-	struct flow_action_entry *act;
-
-	act = &flow->action.entries[0];
-	if (act->id != FLOW_ACTION_QUEUE && act->id != FLOW_ACTION_DROP)
-		return -EOPNOTSUPP;
-
-	/* When both an RSS context and an queue index are set, the index
-	 * is considered as an offset to be added to the indirection table
-	 * entries. We don't support this, so reject this rule.
-	 */
-	if (act->queue.ctx && act->queue.index)
-		return -EOPNOTSUPP;
-
-	/* For now, only use the C2 engine which has a HEK size limited to 64
-	 * bits for TCAM matching.
-	 */
-	rule->engine = MVPP22_CLS_ENGINE_C2;
-
-	if (mvpp2_cls_c2_build_match(rule))
-		return -EINVAL;
-
-	return 0;
-}
-
-int mvpp2_ethtool_cls_rule_get(struct mvpp2_port *port,
-			       struct ethtool_rxnfc *rxnfc)
-{
-	struct mvpp2_ethtool_fs *efs;
-
-	if (rxnfc->fs.location >= MVPP2_N_RFS_ENTRIES_PER_FLOW)
-		return -EINVAL;
-
-	efs = port->rfs_rules[rxnfc->fs.location];
-	if (!efs)
-		return -ENOENT;
-
-	memcpy(rxnfc, &efs->rxnfc, sizeof(efs->rxnfc));
-
-	return 0;
-}
-
-int mvpp2_ethtool_cls_rule_ins(struct mvpp2_port *port,
-			       struct ethtool_rxnfc *info)
-{
-	struct ethtool_rx_flow_spec_input input = {};
-	struct ethtool_rx_flow_rule *ethtool_rule;
-	struct mvpp2_ethtool_fs *efs, *old_efs;
-	int ret = 0;
-
-	if (info->fs.location >= MVPP2_N_RFS_ENTRIES_PER_FLOW)
-		return -EINVAL;
-
-	efs = kzalloc(sizeof(*efs), GFP_KERNEL);
-	if (!efs)
-		return -ENOMEM;
-
-	input.fs = &info->fs;
-
-	/* We need to manually set the rss_ctx, since this info isn't present
-	 * in info->fs
-	 */
-	if (info->fs.flow_type & FLOW_RSS)
-		input.rss_ctx = info->rss_context;
-
-	ethtool_rule = ethtool_rx_flow_rule_create(&input);
-	if (IS_ERR(ethtool_rule)) {
-		ret = PTR_ERR(ethtool_rule);
-		goto clean_rule;
-	}
-
-	efs->rule.flow = ethtool_rule->rule;
-	efs->rule.flow_type = mvpp2_cls_ethtool_flow_to_type(info->fs.flow_type);
-	if (efs->rule.flow_type < 0) {
-		ret = efs->rule.flow_type;
-		goto clean_rule;
-	}
-
-	ret = mvpp2_cls_rfs_parse_rule(&efs->rule);
-	if (ret)
-		goto clean_eth_rule;
-
-	efs->rule.loc = info->fs.location;
-
-	/* Replace an already existing rule */
-	if (port->rfs_rules[efs->rule.loc]) {
-		old_efs = port->rfs_rules[efs->rule.loc];
-		ret = mvpp2_port_cls_rfs_rule_remove(port, &old_efs->rule);
-		if (ret)
-			goto clean_eth_rule;
-		kfree(old_efs);
-		port->n_rfs_rules--;
-	}
-
-	ret = mvpp2_port_flt_rfs_rule_insert(port, &efs->rule);
-	if (ret)
-		goto clean_eth_rule;
-
-	ethtool_rx_flow_rule_destroy(ethtool_rule);
-	efs->rule.flow = NULL;
-
-	memcpy(&efs->rxnfc, info, sizeof(*info));
-	port->rfs_rules[efs->rule.loc] = efs;
-	port->n_rfs_rules++;
-
-	return ret;
-
-clean_eth_rule:
-	ethtool_rx_flow_rule_destroy(ethtool_rule);
-clean_rule:
-	kfree(efs);
-	return ret;
-}
-
-int mvpp2_ethtool_cls_rule_del(struct mvpp2_port *port,
-			       struct ethtool_rxnfc *info)
-{
-	struct mvpp2_ethtool_fs *efs;
-	int ret;
-
-	if (info->fs.location >= MVPP2_N_RFS_ENTRIES_PER_FLOW)
-		return -EINVAL;
-
-	efs = port->rfs_rules[info->fs.location];
-	if (!efs)
-		return -EINVAL;
-
-	/* Remove the rule from the engines. */
-	ret = mvpp2_port_cls_rfs_rule_remove(port, &efs->rule);
-	if (ret)
-		return ret;
-
-	port->n_rfs_rules--;
-	port->rfs_rules[info->fs.location] = NULL;
-	kfree(efs);
-
-	return 0;
 }
 
 static inline u32 mvpp22_rxfh_indir(struct mvpp2_port *port, u32 rxq)
@@ -1460,181 +1023,37 @@ static inline u32 mvpp22_rxfh_indir(struct mvpp2_port *port, u32 rxq)
 	return port->first_rxq + ((rxq * nrxqs + rxq / cpus) % port->nrxqs);
 }
 
-static void mvpp22_rss_fill_table(struct mvpp2_port *port,
-				  struct mvpp2_rss_table *table,
-				  u32 rss_ctx)
+void mvpp22_rss_fill_table(struct mvpp2_port *port, u32 table)
 {
 	struct mvpp2 *priv = port->priv;
 	int i;
 
 	for (i = 0; i < MVPP22_RSS_TABLE_ENTRIES; i++) {
-		u32 sel = MVPP22_RSS_INDEX_TABLE(rss_ctx) |
+		u32 sel = MVPP22_RSS_INDEX_TABLE(table) |
 			  MVPP22_RSS_INDEX_TABLE_ENTRY(i);
 		mvpp2_write(priv, MVPP22_RSS_INDEX, sel);
 
 		mvpp2_write(priv, MVPP22_RSS_TABLE_ENTRY,
-			    mvpp22_rxfh_indir(port, table->indir[i]));
-	}
-}
-
-static int mvpp22_rss_context_create(struct mvpp2_port *port, u32 *rss_ctx)
-{
-	struct mvpp2 *priv = port->priv;
-	u32 ctx;
-
-	/* Find the first free RSS table */
-	for (ctx = 0; ctx < MVPP22_N_RSS_TABLES; ctx++) {
-		if (!priv->rss_tables[ctx])
-			break;
-	}
-
-	if (ctx == MVPP22_N_RSS_TABLES)
-		return -EINVAL;
-
-	priv->rss_tables[ctx] = kzalloc(sizeof(*priv->rss_tables[ctx]),
-					GFP_KERNEL);
-	if (!priv->rss_tables[ctx])
-		return -ENOMEM;
-
-	*rss_ctx = ctx;
-
-	/* Set the table width: replace the whole classifier Rx queue number
-	 * with the ones configured in RSS table entries.
-	 */
-	mvpp2_write(priv, MVPP22_RSS_INDEX, MVPP22_RSS_INDEX_TABLE(ctx));
-	mvpp2_write(priv, MVPP22_RSS_WIDTH, 8);
-
-	mvpp2_write(priv, MVPP22_RSS_INDEX, MVPP22_RSS_INDEX_QUEUE(ctx));
-	mvpp2_write(priv, MVPP22_RXQ2RSS_TABLE, MVPP22_RSS_TABLE_POINTER(ctx));
-
-	return 0;
-}
-
-int mvpp22_port_rss_ctx_create(struct mvpp2_port *port, u32 *port_ctx)
-{
-	u32 rss_ctx;
-	int ret, i;
-
-	ret = mvpp22_rss_context_create(port, &rss_ctx);
-	if (ret)
-		return ret;
-
-	/* Find the first available context number in the port, starting from 1.
-	 * Context 0 on each port is reserved for the default context.
-	 */
-	for (i = 1; i < MVPP22_N_RSS_TABLES; i++) {
-		if (port->rss_ctx[i] < 0)
-			break;
-	}
-
-	if (i == MVPP22_N_RSS_TABLES)
-		return -EINVAL;
-
-	port->rss_ctx[i] = rss_ctx;
-	*port_ctx = i;
-
-	return 0;
-}
-
-static struct mvpp2_rss_table *mvpp22_rss_table_get(struct mvpp2 *priv,
-						    int rss_ctx)
-{
-	if (rss_ctx < 0 || rss_ctx >= MVPP22_N_RSS_TABLES)
-		return NULL;
-
-	return priv->rss_tables[rss_ctx];
-}
-
-int mvpp22_port_rss_ctx_delete(struct mvpp2_port *port, u32 port_ctx)
-{
-	struct mvpp2 *priv = port->priv;
-	struct ethtool_rxnfc *rxnfc;
-	int i, rss_ctx, ret;
-
-	rss_ctx = mvpp22_rss_ctx(port, port_ctx);
-
-	if (rss_ctx < 0 || rss_ctx >= MVPP22_N_RSS_TABLES)
-		return -EINVAL;
-
-	/* Invalidate any active classification rule that use this context */
-	for (i = 0; i < MVPP2_N_RFS_ENTRIES_PER_FLOW; i++) {
-		if (!port->rfs_rules[i])
-			continue;
-
-		rxnfc = &port->rfs_rules[i]->rxnfc;
-		if (!(rxnfc->fs.flow_type & FLOW_RSS) ||
-		    rxnfc->rss_context != port_ctx)
-			continue;
-
-		ret = mvpp2_ethtool_cls_rule_del(port, rxnfc);
-		if (ret) {
-			netdev_warn(port->dev,
-				    "couldn't remove classification rule %d associated to this context",
-				    rxnfc->fs.location);
-		}
+			    mvpp22_rxfh_indir(port, port->indir[i]));
 	}
-
-	kfree(priv->rss_tables[rss_ctx]);
-
-	priv->rss_tables[rss_ctx] = NULL;
-	port->rss_ctx[port_ctx] = -1;
-
-	return 0;
-}
-
-int mvpp22_port_rss_ctx_indir_set(struct mvpp2_port *port, u32 port_ctx,
-				  const u32 *indir)
-{
-	int rss_ctx = mvpp22_rss_ctx(port, port_ctx);
-	struct mvpp2_rss_table *rss_table = mvpp22_rss_table_get(port->priv,
-								 rss_ctx);
-
-	if (!rss_table)
-		return -EINVAL;
-
-	memcpy(rss_table->indir, indir,
-	       MVPP22_RSS_TABLE_ENTRIES * sizeof(rss_table->indir[0]));
-
-	mvpp22_rss_fill_table(port, rss_table, rss_ctx);
-
-	return 0;
-}
-
-int mvpp22_port_rss_ctx_indir_get(struct mvpp2_port *port, u32 port_ctx,
-				  u32 *indir)
-{
-	int rss_ctx =  mvpp22_rss_ctx(port, port_ctx);
-	struct mvpp2_rss_table *rss_table = mvpp22_rss_table_get(port->priv,
-								 rss_ctx);
-
-	if (!rss_table)
-		return -EINVAL;
-
-	memcpy(indir, rss_table->indir,
-	       MVPP22_RSS_TABLE_ENTRIES * sizeof(rss_table->indir[0]));
-
-	return 0;
 }
 
 int mvpp2_ethtool_rxfh_set(struct mvpp2_port *port, struct ethtool_rxnfc *info)
 {
 	u16 hash_opts = 0;
-	u32 flow_type;
 
-	flow_type = mvpp2_cls_ethtool_flow_to_type(info->flow_type);
-
-	switch (flow_type) {
-	case MVPP22_FLOW_TCP4:
-	case MVPP22_FLOW_UDP4:
-	case MVPP22_FLOW_TCP6:
-	case MVPP22_FLOW_UDP6:
+	switch (info->flow_type) {
+	case TCP_V4_FLOW:
+	case UDP_V4_FLOW:
+	case TCP_V6_FLOW:
+	case UDP_V6_FLOW:
 		if (info->data & RXH_L4_B_0_1)
 			hash_opts |= MVPP22_CLS_HEK_OPT_L4SIP;
 		if (info->data & RXH_L4_B_2_3)
 			hash_opts |= MVPP22_CLS_HEK_OPT_L4DIP;
 		/* Fallthrough */
-	case MVPP22_FLOW_IP4:
-	case MVPP22_FLOW_IP6:
+	case IPV4_FLOW:
+	case IPV6_FLOW:
 		if (info->data & RXH_L2DA)
 			hash_opts |= MVPP22_CLS_HEK_OPT_MAC_DA;
 		if (info->data & RXH_VLAN)
@@ -1651,18 +1070,15 @@ int mvpp2_ethtool_rxfh_set(struct mvpp2_port *port, struct ethtool_rxnfc *info)
 	default: return -EOPNOTSUPP;
 	}
 
-	return mvpp2_port_rss_hash_opts_set(port, flow_type, hash_opts);
+	return mvpp2_port_rss_hash_opts_set(port, info->flow_type, hash_opts);
 }
 
 int mvpp2_ethtool_rxfh_get(struct mvpp2_port *port, struct ethtool_rxnfc *info)
 {
 	unsigned long hash_opts;
-	u32 flow_type;
 	int i;
 
-	flow_type = mvpp2_cls_ethtool_flow_to_type(info->flow_type);
-
-	hash_opts = mvpp2_port_rss_hash_opts_get(port, flow_type);
+	hash_opts = mvpp2_port_rss_hash_opts_get(port, info->flow_type);
 	info->data = 0;
 
 	for_each_set_bit(i, &hash_opts, MVPP22_CLS_HEK_N_FIELDS) {
@@ -1697,40 +1113,38 @@ int mvpp2_ethtool_rxfh_get(struct mvpp2_port *port, struct ethtool_rxnfc *info)
 	return 0;
 }
 
-int mvpp22_port_rss_init(struct mvpp2_port *port)
+void mvpp22_rss_port_init(struct mvpp2_port *port)
 {
-	struct mvpp2_rss_table *table;
-	u32 context = 0;
-	int i, ret;
-
-	for (i = 0; i < MVPP22_N_RSS_TABLES; i++)
-		port->rss_ctx[i] = -1;
-
-	ret = mvpp22_rss_context_create(port, &context);
-	if (ret)
-		return ret;
+	struct mvpp2 *priv = port->priv;
+	int i;
 
-	table = mvpp22_rss_table_get(port->priv, context);
-	if (!table)
-		return -EINVAL;
+	/* Set the table width: replace the whole classifier Rx queue number
+	 * with the ones configured in RSS table entries.
+	 */
+	mvpp2_write(priv, MVPP22_RSS_INDEX, MVPP22_RSS_INDEX_TABLE(port->id));
+	mvpp2_write(priv, MVPP22_RSS_WIDTH, 8);
 
-	port->rss_ctx[0] = context;
+	/* The default RxQ is used as a key to select the RSS table to use.
+	 * We use one RSS table per port.
+	 */
+	mvpp2_write(priv, MVPP22_RSS_INDEX,
+		    MVPP22_RSS_INDEX_QUEUE(port->first_rxq));
+	mvpp2_write(priv, MVPP22_RXQ2RSS_TABLE,
+		    MVPP22_RSS_TABLE_POINTER(port->id));
 
 	/* Configure the first table to evenly distribute the packets across
 	 * real Rx Queues. The table entries map a hash to a port Rx Queue.
 	 */
 	for (i = 0; i < MVPP22_RSS_TABLE_ENTRIES; i++)
-		table->indir[i] = ethtool_rxfh_indir_default(i, port->nrxqs);
+		port->indir[i] = ethtool_rxfh_indir_default(i, port->nrxqs);
 
-	mvpp22_rss_fill_table(port, table, mvpp22_rss_ctx(port, 0));
+	mvpp22_rss_fill_table(port, port->id);
 
 	/* Configure default flows */
-	mvpp2_port_rss_hash_opts_set(port, MVPP22_FLOW_IP4, MVPP22_CLS_HEK_IP4_2T);
-	mvpp2_port_rss_hash_opts_set(port, MVPP22_FLOW_IP6, MVPP22_CLS_HEK_IP6_2T);
-	mvpp2_port_rss_hash_opts_set(port, MVPP22_FLOW_TCP4, MVPP22_CLS_HEK_IP4_5T);
-	mvpp2_port_rss_hash_opts_set(port, MVPP22_FLOW_TCP6, MVPP22_CLS_HEK_IP6_5T);
-	mvpp2_port_rss_hash_opts_set(port, MVPP22_FLOW_UDP4, MVPP22_CLS_HEK_IP4_5T);
-	mvpp2_port_rss_hash_opts_set(port, MVPP22_FLOW_UDP6, MVPP22_CLS_HEK_IP6_5T);
-
-	return 0;
+	mvpp2_port_rss_hash_opts_set(port, IPV4_FLOW, MVPP22_CLS_HEK_IP4_2T);
+	mvpp2_port_rss_hash_opts_set(port, IPV6_FLOW, MVPP22_CLS_HEK_IP6_2T);
+	mvpp2_port_rss_hash_opts_set(port, TCP_V4_FLOW, MVPP22_CLS_HEK_IP4_5T);
+	mvpp2_port_rss_hash_opts_set(port, TCP_V6_FLOW, MVPP22_CLS_HEK_IP6_5T);
+	mvpp2_port_rss_hash_opts_set(port, UDP_V4_FLOW, MVPP22_CLS_HEK_IP4_5T);
+	mvpp2_port_rss_hash_opts_set(port, UDP_V6_FLOW, MVPP22_CLS_HEK_IP6_5T);
 }
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.h b/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.h
index 8867f25afab4..e5b7d28abc07 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.h
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.h
@@ -33,16 +33,15 @@ enum mvpp2_cls_engine {
 };
 
 #define MVPP22_CLS_HEK_OPT_MAC_DA	BIT(0)
-#define MVPP22_CLS_HEK_OPT_VLAN_PRI	BIT(1)
-#define MVPP22_CLS_HEK_OPT_VLAN		BIT(2)
-#define MVPP22_CLS_HEK_OPT_L3_PROTO	BIT(3)
-#define MVPP22_CLS_HEK_OPT_IP4SA	BIT(4)
-#define MVPP22_CLS_HEK_OPT_IP4DA	BIT(5)
-#define MVPP22_CLS_HEK_OPT_IP6SA	BIT(6)
-#define MVPP22_CLS_HEK_OPT_IP6DA	BIT(7)
-#define MVPP22_CLS_HEK_OPT_L4SIP	BIT(8)
-#define MVPP22_CLS_HEK_OPT_L4DIP	BIT(9)
-#define MVPP22_CLS_HEK_N_FIELDS		10
+#define MVPP22_CLS_HEK_OPT_VLAN		BIT(1)
+#define MVPP22_CLS_HEK_OPT_L3_PROTO	BIT(2)
+#define MVPP22_CLS_HEK_OPT_IP4SA	BIT(3)
+#define MVPP22_CLS_HEK_OPT_IP4DA	BIT(4)
+#define MVPP22_CLS_HEK_OPT_IP6SA	BIT(5)
+#define MVPP22_CLS_HEK_OPT_IP6DA	BIT(6)
+#define MVPP22_CLS_HEK_OPT_L4SIP	BIT(7)
+#define MVPP22_CLS_HEK_OPT_L4DIP	BIT(8)
+#define MVPP22_CLS_HEK_N_FIELDS		9
 
 #define MVPP22_CLS_HEK_L4_OPTS	(MVPP22_CLS_HEK_OPT_L4SIP | \
 				 MVPP22_CLS_HEK_OPT_L4DIP)
@@ -60,12 +59,8 @@ enum mvpp2_cls_engine {
 #define MVPP22_CLS_HEK_IP6_5T	(MVPP22_CLS_HEK_IP6_2T | \
 				 MVPP22_CLS_HEK_L4_OPTS)
 
-#define MVPP22_CLS_HEK_TAGGED	(MVPP22_CLS_HEK_OPT_VLAN | \
-				 MVPP22_CLS_HEK_OPT_VLAN_PRI)
-
 enum mvpp2_cls_field_id {
 	MVPP22_CLS_FIELD_MAC_DA = 0x03,
-	MVPP22_CLS_FIELD_VLAN_PRI = 0x05,
 	MVPP22_CLS_FIELD_VLAN = 0x06,
 	MVPP22_CLS_FIELD_L3_PROTO = 0x0f,
 	MVPP22_CLS_FIELD_IP4SA = 0x10,
@@ -76,6 +71,19 @@ enum mvpp2_cls_field_id {
 	MVPP22_CLS_FIELD_L4DIP = 0x1e,
 };
 
+enum mvpp2_cls_lkp_type {
+	MVPP2_CLS_LKP_HASH = 0,
+	MVPP2_CLS_LKP_DEFAULT = 3,
+	MVPP2_CLS_LKP_MAX,
+};
+enum mvpp2_cls_flow_seq {
+	MVPP2_CLS_FLOW_SEQ_NORMAL = 0,
+	MVPP2_CLS_FLOW_SEQ_FIRST1,
+	MVPP2_CLS_FLOW_SEQ_FIRST2,
+	MVPP2_CLS_FLOW_SEQ_LAST,
+	MVPP2_CLS_FLOW_SEQ_MIDDLE
+};
+
 /* Classifier C2 engine constants */
 #define MVPP22_CLS_C2_TCAM_EN(data)		((data) << 16)
 
@@ -97,62 +105,40 @@ enum mvpp22_cls_c2_fwd_action {
 	MVPP22_C2_FWD_HW_LOW_LAT_LOCK,
 };
 
-enum mvpp22_cls_c2_color_action {
-	MVPP22_C2_COL_NO_UPD = 0,
-	MVPP22_C2_COL_NO_UPD_LOCK,
-	MVPP22_C2_COL_GREEN,
-	MVPP22_C2_COL_GREEN_LOCK,
-	MVPP22_C2_COL_YELLOW,
-	MVPP22_C2_COL_YELLOW_LOCK,
-	MVPP22_C2_COL_RED,		/* Drop */
-	MVPP22_C2_COL_RED_LOCK,		/* Drop */
-};
-
 #define MVPP2_CLS_C2_TCAM_WORDS			5
 #define MVPP2_CLS_C2_ATTR_WORDS			5
 
 struct mvpp2_cls_c2_entry {
 	u32 index;
-	/* TCAM lookup key */
 	u32 tcam[MVPP2_CLS_C2_TCAM_WORDS];
-	/* Actions to perform upon TCAM match */
 	u32 act;
-	/* Attributes relative to the actions to perform */
 	u32 attr[MVPP2_CLS_C2_ATTR_WORDS];
-	/* Entry validity */
-	u8 valid;
 };
 
-#define MVPP22_FLOW_ETHER_BIT	BIT(0)
-#define MVPP22_FLOW_IP4_BIT	BIT(1)
-#define MVPP22_FLOW_IP6_BIT	BIT(2)
-#define MVPP22_FLOW_TCP_BIT	BIT(3)
-#define MVPP22_FLOW_UDP_BIT	BIT(4)
-
-#define MVPP22_FLOW_TCP4	(MVPP22_FLOW_ETHER_BIT | MVPP22_FLOW_IP4_BIT | MVPP22_FLOW_TCP_BIT)
-#define MVPP22_FLOW_TCP6	(MVPP22_FLOW_ETHER_BIT | MVPP22_FLOW_IP6_BIT | MVPP22_FLOW_TCP_BIT)
-#define MVPP22_FLOW_UDP4	(MVPP22_FLOW_ETHER_BIT | MVPP22_FLOW_IP4_BIT | MVPP22_FLOW_UDP_BIT)
-#define MVPP22_FLOW_UDP6	(MVPP22_FLOW_ETHER_BIT | MVPP22_FLOW_IP6_BIT | MVPP22_FLOW_UDP_BIT)
-#define MVPP22_FLOW_IP4		(MVPP22_FLOW_ETHER_BIT | MVPP22_FLOW_IP4_BIT)
-#define MVPP22_FLOW_IP6		(MVPP22_FLOW_ETHER_BIT | MVPP22_FLOW_IP6_BIT)
-#define MVPP22_FLOW_ETHERNET	(MVPP22_FLOW_ETHER_BIT)
-
 /* Classifier C2 engine entries */
-#define MVPP22_CLS_C2_N_ENTRIES		256
-
-/* Number of per-port dedicated entries in the C2 TCAM */
-#define MVPP22_CLS_C2_PORT_N_FLOWS	MVPP2_N_RFS_ENTRIES_PER_FLOW
+#define MVPP22_CLS_C2_MAX_ENTRIES	256
+#define MVPP22_CLS_C2_RSS_ENTRY(port)	(port)
+#define MVPP22_CLS_C2_N_ENTRIES		MVPP2_MAX_PORTS
 
-/* Each port has oen range per flow type + one entry controling the global RSS
- * setting and the default rx queue
+/* RSS flow entries in the flow table. We have 2 entries per port for RSS.
+ *
+ * The first performs a lookup using the C2 TCAM engine, to tag the
+ * packet for software forwarding (needed for RSS), enable or disable RSS, and
+ * assign the default rx queue.
+ *
+ * The second configures the hash generation, by specifying which fields of the
+ * packet header are used to generate the hash, and specifies the relevant hash
+ * engine to use.
  */
-#define MVPP22_CLS_C2_PORT_RANGE	(MVPP22_CLS_C2_PORT_N_FLOWS + 1)
-#define MVPP22_CLS_C2_PORT_FIRST(p)	((p) * MVPP22_CLS_C2_PORT_RANGE)
-#define MVPP22_CLS_C2_RSS_ENTRY(p)	(MVPP22_CLS_C2_PORT_FIRST((p) + 1) - 1)
-
-#define MVPP22_CLS_C2_PORT_FLOW_FIRST(p)	(MVPP22_CLS_C2_PORT_FIRST(p))
+#define MVPP22_RSS_FLOW_C2_OFFS		0
+#define MVPP22_RSS_FLOW_HASH_OFFS	1
+#define MVPP22_RSS_FLOW_SIZE		(MVPP22_RSS_FLOW_HASH_OFFS + 1)
 
-#define MVPP22_CLS_C2_RFS_LOC(p, loc)	(MVPP22_CLS_C2_PORT_FLOW_FIRST(p) + (loc))
+#define MVPP22_RSS_FLOW_C2(port)	((port) * MVPP22_RSS_FLOW_SIZE + \
+					 MVPP22_RSS_FLOW_C2_OFFS)
+#define MVPP22_RSS_FLOW_HASH(port)	((port) * MVPP22_RSS_FLOW_SIZE + \
+					 MVPP22_RSS_FLOW_HASH_OFFS)
+#define MVPP22_RSS_FLOW_FIRST(port)	MVPP22_RSS_FLOW_C2(port)
 
 /* Packet flow ID */
 enum mvpp2_prs_flow {
@@ -182,16 +168,6 @@ enum mvpp2_prs_flow {
 	MVPP2_FL_LAST,
 };
 
-/* LU Type defined for all engines, and specified in the flow table */
-#define MVPP2_CLS_LU_TYPE_MASK			0x3f
-
-enum mvpp2_cls_lu_type {
-	/* rule->loc is used as a lu-type for the entries 0 - 62. */
-	MVPP22_CLS_LU_TYPE_ALL = 63,
-};
-
-#define MVPP2_N_FLOWS		(MVPP2_FL_LAST - MVPP2_FL_START)
-
 struct mvpp2_cls_flow {
 	/* The L2-L4 traffic flow type */
 	int flow_type;
@@ -206,47 +182,13 @@ struct mvpp2_cls_flow {
 	struct mvpp2_prs_result_info prs_ri;
 };
 
-#define MVPP2_CLS_FLT_ENTRIES_PER_FLOW		(MVPP2_MAX_PORTS + 1 + 16)
-#define MVPP2_CLS_FLT_FIRST(id)			(((id) - MVPP2_FL_START) * \
-						 MVPP2_CLS_FLT_ENTRIES_PER_FLOW)
-
-#define MVPP2_CLS_FLT_C2_RFS(port, id, rfs_n)	(MVPP2_CLS_FLT_FIRST(id) + \
-						 ((port) * MVPP2_MAX_PORTS) + \
-						 (rfs_n))
+#define MVPP2_N_FLOWS	52
 
-#define MVPP2_CLS_FLT_C2_RSS_ENTRY(id)		(MVPP2_CLS_FLT_C2_RFS(MVPP2_MAX_PORTS, id, 0))
-#define MVPP2_CLS_FLT_HASH_ENTRY(port, id)	(MVPP2_CLS_FLT_C2_RSS_ENTRY(id) + 1 + (port))
-#define MVPP2_CLS_FLT_LAST(id)			(MVPP2_CLS_FLT_FIRST(id) + \
-						 MVPP2_CLS_FLT_ENTRIES_PER_FLOW - 1)
-
-/* Iterate on each classifier flow id. Sets 'i' to be the index of the first
- * entry in the cls_flows table for each different flow_id.
- * This relies on entries having the same flow_id in the cls_flows table being
- * contiguous.
- */
-#define for_each_cls_flow_id(i)						      \
-	for ((i) = 0; (i) < MVPP2_N_PRS_FLOWS; (i)++)			      \
-		if ((i) > 0 &&						      \
-		    cls_flows[(i)].flow_id == cls_flows[(i) - 1].flow_id)       \
-			continue;					      \
-		else
-
-/* Iterate on each classifier flow that has a given flow_type. Sets 'i' to be
- * the index of the first entry in the cls_flow table for each different flow_id
- * that has the given flow_type. This allows to operate on all flows that
- * matches a given ethtool flow type.
- */
-#define for_each_cls_flow_id_with_type(i, type)				      \
-	for_each_cls_flow_id((i))					      \
-		if (cls_flows[(i)].flow_type != (type))			      \
-			continue;					      \
-		else
-
-#define for_each_cls_flow_id_containing_type(i, type)			      \
-	for_each_cls_flow_id((i))					      \
-		if ((cls_flows[(i)].flow_type & (type)) != (type))	      \
-			continue;					      \
-		else
+#define MVPP2_ENTRIES_PER_FLOW			(MVPP2_MAX_PORTS + 1)
+#define MVPP2_FLOW_C2_ENTRY(id)			((((id) - MVPP2_FL_START) * \
+						 MVPP2_ENTRIES_PER_FLOW) + 1)
+#define MVPP2_PORT_FLOW_INDEX(offset, id)	(MVPP2_FLOW_C2_ENTRY(id) + \
+						 1 + (offset))
 
 struct mvpp2_cls_flow_entry {
 	u32 index;
@@ -259,18 +201,12 @@ struct mvpp2_cls_lookup_entry {
 	u32 data;
 };
 
-int mvpp22_port_rss_init(struct mvpp2_port *port);
-
-int mvpp22_port_rss_enable(struct mvpp2_port *port);
-int mvpp22_port_rss_disable(struct mvpp2_port *port);
+void mvpp22_rss_fill_table(struct mvpp2_port *port, u32 table);
 
-int mvpp22_port_rss_ctx_create(struct mvpp2_port *port, u32 *rss_ctx);
-int mvpp22_port_rss_ctx_delete(struct mvpp2_port *port, u32 rss_ctx);
+void mvpp22_rss_port_init(struct mvpp2_port *port);
 
-int mvpp22_port_rss_ctx_indir_set(struct mvpp2_port *port, u32 rss_ctx,
-				  const u32 *indir);
-int mvpp22_port_rss_ctx_indir_get(struct mvpp2_port *port, u32 rss_ctx,
-				  u32 *indir);
+void mvpp22_rss_enable(struct mvpp2_port *port);
+void mvpp22_rss_disable(struct mvpp2_port *port);
 
 int mvpp2_ethtool_rxfh_get(struct mvpp2_port *port, struct ethtool_rxnfc *info);
 int mvpp2_ethtool_rxfh_set(struct mvpp2_port *port, struct ethtool_rxnfc *info);
@@ -285,7 +221,7 @@ int mvpp2_cls_flow_eng_get(struct mvpp2_cls_flow_entry *fe);
 
 u16 mvpp2_flow_get_hek_fields(struct mvpp2_cls_flow_entry *fe);
 
-const struct mvpp2_cls_flow *mvpp2_cls_flow_get(int flow);
+struct mvpp2_cls_flow *mvpp2_cls_flow_get(int flow);
 
 u32 mvpp2_cls_flow_hits(struct mvpp2 *priv, int index);
 
@@ -302,13 +238,9 @@ u32 mvpp2_cls_c2_hit_count(struct mvpp2 *priv, int c2_index);
 void mvpp2_cls_c2_read(struct mvpp2 *priv, int index,
 		       struct mvpp2_cls_c2_entry *c2);
 
-int mvpp2_ethtool_cls_rule_get(struct mvpp2_port *port,
-			       struct ethtool_rxnfc *rxnfc);
-
-int mvpp2_ethtool_cls_rule_ins(struct mvpp2_port *port,
-			       struct ethtool_rxnfc *info);
-
-int mvpp2_ethtool_cls_rule_del(struct mvpp2_port *port,
-			       struct ethtool_rxnfc *info);
+int mvpp2_cls_flow_hash_find(struct mvpp2_port *port,
+			     struct mvpp2_cls_flow *flow,
+			     struct mvpp2_cls_flow_entry *fe,
+			     int *flow_index);
 
 #endif
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_debugfs.c b/drivers/net/ethernet/marvell/mvpp2/mvpp2_debugfs.c
index 4a3baa7e0142..d3df19feff5d 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_debugfs.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_debugfs.c
@@ -18,48 +18,22 @@ struct mvpp2_dbgfs_prs_entry {
 	struct mvpp2 *priv;
 };
 
-struct mvpp2_dbgfs_c2_entry {
-	int id;
-	struct mvpp2 *priv;
-};
-
 struct mvpp2_dbgfs_flow_entry {
 	int flow;
 	struct mvpp2 *priv;
 };
 
-struct mvpp2_dbgfs_flow_tbl_entry {
-	int id;
-	struct mvpp2 *priv;
-};
-
 struct mvpp2_dbgfs_port_flow_entry {
 	struct mvpp2_port *port;
 	struct mvpp2_dbgfs_flow_entry *dbg_fe;
 };
 
-struct mvpp2_dbgfs_entries {
-	/* Entries for Header Parser debug info */
-	struct mvpp2_dbgfs_prs_entry prs_entries[MVPP2_PRS_TCAM_SRAM_SIZE];
-
-	/* Entries for Classifier C2 engine debug info */
-	struct mvpp2_dbgfs_c2_entry c2_entries[MVPP22_CLS_C2_N_ENTRIES];
-
-	/* Entries for Classifier Flow Table debug info */
-	struct mvpp2_dbgfs_flow_tbl_entry flt_entries[MVPP2_CLS_FLOWS_TBL_SIZE];
-
-	/* Entries for Classifier flows debug info */
-	struct mvpp2_dbgfs_flow_entry flow_entries[MVPP2_N_PRS_FLOWS];
-
-	/* Entries for per-port flows debug info */
-	struct mvpp2_dbgfs_port_flow_entry port_flow_entries[MVPP2_MAX_PORTS];
-};
-
 static int mvpp2_dbgfs_flow_flt_hits_show(struct seq_file *s, void *unused)
 {
-	struct mvpp2_dbgfs_flow_tbl_entry *entry = s->private;
+	struct mvpp2_dbgfs_flow_entry *entry = s->private;
+	int id = MVPP2_FLOW_C2_ENTRY(entry->flow);
 
-	u32 hits = mvpp2_cls_flow_hits(entry->priv, entry->id);
+	u32 hits = mvpp2_cls_flow_hits(entry->priv, id);
 
 	seq_printf(s, "%u\n", hits);
 
@@ -84,7 +58,7 @@ DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_flow_dec_hits);
 static int mvpp2_dbgfs_flow_type_show(struct seq_file *s, void *unused)
 {
 	struct mvpp2_dbgfs_flow_entry *entry = s->private;
-	const struct mvpp2_cls_flow *f;
+	struct mvpp2_cls_flow *f;
 	const char *flow_name;
 
 	f = mvpp2_cls_flow_get(entry->flow);
@@ -119,12 +93,30 @@ static int mvpp2_dbgfs_flow_type_show(struct seq_file *s, void *unused)
 	return 0;
 }
 
-DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_flow_type);
+static int mvpp2_dbgfs_flow_type_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mvpp2_dbgfs_flow_type_show, inode->i_private);
+}
+
+static int mvpp2_dbgfs_flow_type_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct mvpp2_dbgfs_flow_entry *flow_entry = seq->private;
+
+	kfree(flow_entry);
+	return single_release(inode, file);
+}
+
+static const struct file_operations mvpp2_dbgfs_flow_type_fops = {
+	.open = mvpp2_dbgfs_flow_type_open,
+	.read = seq_read,
+	.release = mvpp2_dbgfs_flow_type_release,
+};
 
 static int mvpp2_dbgfs_flow_id_show(struct seq_file *s, void *unused)
 {
-	const struct mvpp2_dbgfs_flow_entry *entry = s->private;
-	const struct mvpp2_cls_flow *f;
+	struct mvpp2_dbgfs_flow_entry *entry = s->private;
+	struct mvpp2_cls_flow *f;
 
 	f = mvpp2_cls_flow_get(entry->flow);
 	if (!f)
@@ -142,7 +134,7 @@ static int mvpp2_dbgfs_port_flow_hash_opt_show(struct seq_file *s, void *unused)
 	struct mvpp2_dbgfs_port_flow_entry *entry = s->private;
 	struct mvpp2_port *port = entry->port;
 	struct mvpp2_cls_flow_entry fe;
-	const struct mvpp2_cls_flow *f;
+	struct mvpp2_cls_flow *f;
 	int flow_index;
 	u16 hash_opts;
 
@@ -150,7 +142,8 @@ static int mvpp2_dbgfs_port_flow_hash_opt_show(struct seq_file *s, void *unused)
 	if (!f)
 		return -EINVAL;
 
-	flow_index = MVPP2_CLS_FLT_HASH_ENTRY(entry->port->id, f->flow_id);
+	if (mvpp2_cls_flow_hash_find(port, f, &fe, &flow_index))
+		return -EINVAL;
 
 	mvpp2_cls_flow_read(port->priv, flow_index, &fe);
 
@@ -161,21 +154,43 @@ static int mvpp2_dbgfs_port_flow_hash_opt_show(struct seq_file *s, void *unused)
 	return 0;
 }
 
-DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_port_flow_hash_opt);
+static int mvpp2_dbgfs_port_flow_hash_opt_open(struct inode *inode,
+					       struct file *file)
+{
+	return single_open(file, mvpp2_dbgfs_port_flow_hash_opt_show,
+			   inode->i_private);
+}
+
+static int mvpp2_dbgfs_port_flow_hash_opt_release(struct inode *inode,
+						  struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct mvpp2_dbgfs_port_flow_entry *flow_entry = seq->private;
+
+	kfree(flow_entry);
+	return single_release(inode, file);
+}
+
+static const struct file_operations mvpp2_dbgfs_port_flow_hash_opt_fops = {
+	.open = mvpp2_dbgfs_port_flow_hash_opt_open,
+	.read = seq_read,
+	.release = mvpp2_dbgfs_port_flow_hash_opt_release,
+};
 
 static int mvpp2_dbgfs_port_flow_engine_show(struct seq_file *s, void *unused)
 {
 	struct mvpp2_dbgfs_port_flow_entry *entry = s->private;
 	struct mvpp2_port *port = entry->port;
 	struct mvpp2_cls_flow_entry fe;
-	const struct mvpp2_cls_flow *f;
+	struct mvpp2_cls_flow *f;
 	int flow_index, engine;
 
 	f = mvpp2_cls_flow_get(entry->dbg_fe->flow);
 	if (!f)
 		return -EINVAL;
 
-	flow_index = MVPP2_CLS_FLT_HASH_ENTRY(entry->port->id, f->flow_id);
+	if (mvpp2_cls_flow_hash_find(port, f, &fe, &flow_index))
+		return -EINVAL;
 
 	mvpp2_cls_flow_read(port->priv, flow_index, &fe);
 
@@ -190,10 +205,11 @@ DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_port_flow_engine);
 
 static int mvpp2_dbgfs_flow_c2_hits_show(struct seq_file *s, void *unused)
 {
-	struct mvpp2_dbgfs_c2_entry *entry = s->private;
+	struct mvpp2_port *port = s->private;
 	u32 hits;
 
-	hits = mvpp2_cls_c2_hit_count(entry->priv, entry->id);
+	hits = mvpp2_cls_c2_hit_count(port->priv,
+				      MVPP22_CLS_C2_RSS_ENTRY(port->id));
 
 	seq_printf(s, "%u\n", hits);
 
@@ -204,11 +220,11 @@ DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_flow_c2_hits);
 
 static int mvpp2_dbgfs_flow_c2_rxq_show(struct seq_file *s, void *unused)
 {
-	struct mvpp2_dbgfs_c2_entry *entry = s->private;
+	struct mvpp2_port *port = s->private;
 	struct mvpp2_cls_c2_entry c2;
 	u8 qh, ql;
 
-	mvpp2_cls_c2_read(entry->priv, entry->id, &c2);
+	mvpp2_cls_c2_read(port->priv, MVPP22_CLS_C2_RSS_ENTRY(port->id), &c2);
 
 	qh = (c2.attr[0] >> MVPP22_CLS_C2_ATTR0_QHIGH_OFFS) &
 	     MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
@@ -225,11 +241,11 @@ DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_flow_c2_rxq);
 
 static int mvpp2_dbgfs_flow_c2_enable_show(struct seq_file *s, void *unused)
 {
-	struct mvpp2_dbgfs_c2_entry *entry = s->private;
+	struct mvpp2_port *port = s->private;
 	struct mvpp2_cls_c2_entry c2;
 	int enabled;
 
-	mvpp2_cls_c2_read(entry->priv, entry->id, &c2);
+	mvpp2_cls_c2_read(port->priv, MVPP22_CLS_C2_RSS_ENTRY(port->id), &c2);
 
 	enabled = !!(c2.attr[2] & MVPP22_CLS_C2_ATTR2_RSS_EN);
 
@@ -275,6 +291,41 @@ static int mvpp2_dbgfs_port_vid_show(struct seq_file *s, void *unused)
 
 DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_port_vid);
 
+static int mvpp2_prs_hw_hits_dump(struct seq_file *s,
+				  struct mvpp2_prs_entry *pe)
+{
+	struct mvpp2 *priv = ((struct mvpp2_port *)s->private)->priv;
+	unsigned int cnt;
+
+	cnt = mvpp2_prs_hits(priv, pe->index);
+	if (cnt != 0)
+		seq_printf(s, "----- HITS: %d ------\n", cnt);
+	return 0;
+}
+
+static int mvpp2_dbgfs_port_parser_dump(struct seq_file *s,
+					struct mvpp2_prs_entry *pe)
+{
+	int i;
+
+	/* hw entry id */
+	seq_printf(s, "   [%4d] ", pe->index);
+
+	i = MVPP2_PRS_TCAM_WORDS - 1;
+	seq_printf(s, "%1.1x ", pe->tcam[i--] & MVPP2_PRS_LU_MASK);
+
+	while (i >= 0)
+		seq_printf(s, "%4.4x ", (pe->tcam[i--]) & MVPP2_PRS_WORD_MASK);
+
+	seq_printf(s, "| %4.4x %8.8x %8.8x %8.8x\n",
+		   pe->sram[3] & MVPP2_PRS_WORD_MASK,
+		   pe->sram[2],  pe->sram[1],  pe->sram[0]);
+
+	mvpp2_prs_hw_hits_dump(s, pe);
+
+	return 0;
+}
+
 static int mvpp2_dbgfs_port_parser_show(struct seq_file *s, void *unused)
 {
 	struct mvpp2_port *port = s->private;
@@ -288,7 +339,7 @@ static int mvpp2_dbgfs_port_parser_show(struct seq_file *s, void *unused)
 
 		pmap = mvpp2_prs_tcam_port_map_get(&pe);
 		if (priv->prs_shadow[i].valid && test_bit(port->id, &pmap))
-			seq_printf(s, "%03d\n", i);
+			mvpp2_dbgfs_port_parser_dump(s, &pe);
 	}
 
 	return 0;
@@ -442,7 +493,25 @@ static int mvpp2_dbgfs_prs_valid_show(struct seq_file *s, void *unused)
 	return 0;
 }
 
-DEFINE_SHOW_ATTRIBUTE(mvpp2_dbgfs_prs_valid);
+static int mvpp2_dbgfs_prs_valid_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mvpp2_dbgfs_prs_valid_show, inode->i_private);
+}
+
+static int mvpp2_dbgfs_prs_valid_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct mvpp2_dbgfs_prs_entry *entry = seq->private;
+
+	kfree(entry);
+	return single_release(inode, file);
+}
+
+static const struct file_operations mvpp2_dbgfs_prs_valid_fops = {
+	.open = mvpp2_dbgfs_prs_valid_open,
+	.read = seq_read,
+	.release = mvpp2_dbgfs_prs_valid_release,
+};
 
 static int mvpp2_dbgfs_flow_port_init(struct dentry *parent,
 				      struct mvpp2_port *port,
@@ -452,8 +521,13 @@ static int mvpp2_dbgfs_flow_port_init(struct dentry *parent,
 	struct dentry *port_dir;
 
 	port_dir = debugfs_create_dir(port->dev->name, parent);
+	if (IS_ERR(port_dir))
+		return PTR_ERR(port_dir);
 
-	port_entry = &port->priv->dbgfs_entries->port_flow_entries[port->id];
+	/* This will be freed by 'hash_opts' release op */
+	port_entry = kmalloc(sizeof(*port_entry), GFP_KERNEL);
+	if (!port_entry)
+		return -ENOMEM;
 
 	port_entry->port = port;
 	port_entry->dbg_fe = entry;
@@ -478,12 +552,20 @@ static int mvpp2_dbgfs_flow_entry_init(struct dentry *parent,
 	sprintf(flow_entry_name, "%02d", flow);
 
 	flow_entry_dir = debugfs_create_dir(flow_entry_name, parent);
+	if (!flow_entry_dir)
+		return -ENOMEM;
 
-	entry = &priv->dbgfs_entries->flow_entries[flow];
+	/* This will be freed by 'type' release op */
+	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
 
 	entry->flow = flow;
 	entry->priv = priv;
 
+	debugfs_create_file("flow_hits", 0444, flow_entry_dir, entry,
+			    &mvpp2_dbgfs_flow_flt_hits_fops);
+
 	debugfs_create_file("dec_hits", 0444, flow_entry_dir, entry,
 			    &mvpp2_dbgfs_flow_dec_hits_fops);
 
@@ -500,7 +582,6 @@ static int mvpp2_dbgfs_flow_entry_init(struct dentry *parent,
 		if (ret)
 			return ret;
 	}
-
 	return 0;
 }
 
@@ -510,8 +591,10 @@ static int mvpp2_dbgfs_flow_init(struct dentry *parent, struct mvpp2 *priv)
 	int i, ret;
 
 	flow_dir = debugfs_create_dir("flows", parent);
+	if (!flow_dir)
+		return -ENOMEM;
 
-	for (i = 0; i < MVPP2_N_PRS_FLOWS; i++) {
+	for (i = 0; i < MVPP2_N_FLOWS; i++) {
 		ret = mvpp2_dbgfs_flow_entry_init(flow_dir, priv, i);
 		if (ret)
 			return ret;
@@ -533,8 +616,13 @@ static int mvpp2_dbgfs_prs_entry_init(struct dentry *parent,
 	sprintf(prs_entry_name, "%03d", tid);
 
 	prs_entry_dir = debugfs_create_dir(prs_entry_name, parent);
+	if (!prs_entry_dir)
+		return -ENOMEM;
 
-	entry = &priv->dbgfs_entries->prs_entries[tid];
+	/* The 'valid' entry's ops will free that */
+	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
 
 	entry->tid = tid;
 	entry->priv = priv;
@@ -558,9 +646,6 @@ static int mvpp2_dbgfs_prs_entry_init(struct dentry *parent,
 	debugfs_create_file("hits", 0444, prs_entry_dir, entry,
 			    &mvpp2_dbgfs_prs_hits_fops);
 
-	debugfs_create_file("pmap", 0444, prs_entry_dir, entry,
-			     &mvpp2_dbgfs_prs_pmap_fops);
-
 	return 0;
 }
 
@@ -570,6 +655,8 @@ static int mvpp2_dbgfs_prs_init(struct dentry *parent, struct mvpp2 *priv)
 	int i, ret;
 
 	prs_dir = debugfs_create_dir("parser", parent);
+	if (!prs_dir)
+		return -ENOMEM;
 
 	for (i = 0; i < MVPP2_PRS_TCAM_SRAM_SIZE; i++) {
 		ret = mvpp2_dbgfs_prs_entry_init(prs_dir, priv, i);
@@ -580,104 +667,14 @@ static int mvpp2_dbgfs_prs_init(struct dentry *parent, struct mvpp2 *priv)
 	return 0;
 }
 
-static int mvpp2_dbgfs_c2_entry_init(struct dentry *parent,
-				     struct mvpp2 *priv, int id)
-{
-	struct mvpp2_dbgfs_c2_entry *entry;
-	struct dentry *c2_entry_dir;
-	char c2_entry_name[10];
-
-	if (id >= MVPP22_CLS_C2_N_ENTRIES)
-		return -EINVAL;
-
-	sprintf(c2_entry_name, "%03d", id);
-
-	c2_entry_dir = debugfs_create_dir(c2_entry_name, parent);
-	if (!c2_entry_dir)
-		return -ENOMEM;
-
-	entry = &priv->dbgfs_entries->c2_entries[id];
-
-	entry->id = id;
-	entry->priv = priv;
-
-	debugfs_create_file("hits", 0444, c2_entry_dir, entry,
-			    &mvpp2_dbgfs_flow_c2_hits_fops);
-
-	debugfs_create_file("default_rxq", 0444, c2_entry_dir, entry,
-			    &mvpp2_dbgfs_flow_c2_rxq_fops);
-
-	debugfs_create_file("rss_enable", 0444, c2_entry_dir, entry,
-			    &mvpp2_dbgfs_flow_c2_enable_fops);
-
-	return 0;
-}
-
-static int mvpp2_dbgfs_flow_tbl_entry_init(struct dentry *parent,
-					   struct mvpp2 *priv, int id)
-{
-	struct mvpp2_dbgfs_flow_tbl_entry *entry;
-	struct dentry *flow_tbl_entry_dir;
-	char flow_tbl_entry_name[10];
-
-	if (id >= MVPP2_CLS_FLOWS_TBL_SIZE)
-		return -EINVAL;
-
-	sprintf(flow_tbl_entry_name, "%03d", id);
-
-	flow_tbl_entry_dir = debugfs_create_dir(flow_tbl_entry_name, parent);
-	if (!flow_tbl_entry_dir)
-		return -ENOMEM;
-
-	entry = &priv->dbgfs_entries->flt_entries[id];
-
-	entry->id = id;
-	entry->priv = priv;
-
-	debugfs_create_file("hits", 0444, flow_tbl_entry_dir, entry,
-			    &mvpp2_dbgfs_flow_flt_hits_fops);
-
-	return 0;
-}
-
-static int mvpp2_dbgfs_cls_init(struct dentry *parent, struct mvpp2 *priv)
-{
-	struct dentry *cls_dir, *c2_dir, *flow_tbl_dir;
-	int i, ret;
-
-	cls_dir = debugfs_create_dir("classifier", parent);
-	if (!cls_dir)
-		return -ENOMEM;
-
-	c2_dir = debugfs_create_dir("c2", cls_dir);
-	if (!c2_dir)
-		return -ENOMEM;
-
-	for (i = 0; i < MVPP22_CLS_C2_N_ENTRIES; i++) {
-		ret = mvpp2_dbgfs_c2_entry_init(c2_dir, priv, i);
-		if (ret)
-			return ret;
-	}
-
-	flow_tbl_dir = debugfs_create_dir("flow_table", cls_dir);
-	if (!flow_tbl_dir)
-		return -ENOMEM;
-
-	for (i = 0; i < MVPP2_CLS_FLOWS_TBL_SIZE; i++) {
-		ret = mvpp2_dbgfs_flow_tbl_entry_init(flow_tbl_dir, priv, i);
-		if (ret)
-			return ret;
-	}
-
-	return 0;
-}
-
 static int mvpp2_dbgfs_port_init(struct dentry *parent,
 				 struct mvpp2_port *port)
 {
 	struct dentry *port_dir;
 
 	port_dir = debugfs_create_dir(port->dev->name, parent);
+	if (IS_ERR(port_dir))
+		return PTR_ERR(port_dir);
 
 	debugfs_create_file("parser_entries", 0444, port_dir, port,
 			    &mvpp2_dbgfs_port_parser_fops);
@@ -688,14 +685,21 @@ static int mvpp2_dbgfs_port_init(struct dentry *parent,
 	debugfs_create_file("vid_filter", 0444, port_dir, port,
 			    &mvpp2_dbgfs_port_vid_fops);
 
+	debugfs_create_file("c2_hits", 0444, port_dir, port,
+			    &mvpp2_dbgfs_flow_c2_hits_fops);
+
+	debugfs_create_file("default_rxq", 0444, port_dir, port,
+			    &mvpp2_dbgfs_flow_c2_rxq_fops);
+
+	debugfs_create_file("rss_enable", 0444, port_dir, port,
+			    &mvpp2_dbgfs_flow_c2_enable_fops);
+
 	return 0;
 }
 
 void mvpp2_dbgfs_cleanup(struct mvpp2 *priv)
 {
 	debugfs_remove_recursive(priv->dbgfs_dir);
-
-	kfree(priv->dbgfs_entries);
 }
 
 void mvpp2_dbgfs_init(struct mvpp2 *priv, const char *name)
@@ -704,24 +708,22 @@ void mvpp2_dbgfs_init(struct mvpp2 *priv, const char *name)
 	int ret, i;
 
 	mvpp2_root = debugfs_lookup(MVPP2_DRIVER_NAME, NULL);
-	if (!mvpp2_root)
+	if (!mvpp2_root) {
 		mvpp2_root = debugfs_create_dir(MVPP2_DRIVER_NAME, NULL);
+		if (IS_ERR(mvpp2_root))
+			return;
+	}
 
 	mvpp2_dir = debugfs_create_dir(name, mvpp2_root);
+	if (IS_ERR(mvpp2_dir))
+		return;
 
 	priv->dbgfs_dir = mvpp2_dir;
-	priv->dbgfs_entries = kzalloc(sizeof(*priv->dbgfs_entries), GFP_KERNEL);
-	if (!priv->dbgfs_entries)
-		goto err;
 
 	ret = mvpp2_dbgfs_prs_init(mvpp2_dir, priv);
 	if (ret)
 		goto err;
 
-	ret = mvpp2_dbgfs_cls_init(mvpp2_dir, priv);
-	if (ret)
-		goto err;
-
 	for (i = 0; i < priv->port_count; i++) {
 		ret = mvpp2_dbgfs_port_init(mvpp2_dir, priv->port_list[i]);
 		if (ret)
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
index 6223740f9b57..3bc39c0f2f54 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
@@ -8,6 +8,7 @@
  */
 
 #include <linux/acpi.h>
+#include <linux/dma-direct.h>
 #include <linux/kernel.h>
 #include <linux/netdevice.h>
 #include <linux/etherdevice.h>
@@ -17,6 +18,7 @@
 #include <linux/mbus.h>
 #include <linux/module.h>
 #include <linux/mfd/syscon.h>
+#include <linux/if_vlan.h>
 #include <linux/interrupt.h>
 #include <linux/cpumask.h>
 #include <linux/of.h>
@@ -25,6 +27,8 @@
 #include <linux/of_net.h>
 #include <linux/of_address.h>
 #include <linux/of_device.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/genalloc.h>
 #include <linux/phy.h>
 #include <linux/phylink.h>
 #include <linux/phy/phy.h>
@@ -33,25 +37,76 @@
 #include <linux/ktime.h>
 #include <linux/regmap.h>
 #include <uapi/linux/ppp_defs.h>
+#include <net/dsa.h>
 #include <net/ip.h>
 #include <net/ipv6.h>
 #include <net/tso.h>
+#include <net/busy_poll.h>
 
 #include "mvpp2.h"
 #include "mvpp2_prs.h"
 #include "mvpp2_cls.h"
 
-enum mvpp2_bm_pool_log_num {
-	MVPP2_BM_SHORT,
-	MVPP2_BM_LONG,
-	MVPP2_BM_JUMBO,
-	MVPP2_BM_POOLS_NUM
+/* RX-TX fast-forwarding path optimization */
+#define MVPP2_RXTX_HASH			0xbac0
+#define MVPP2_RXTX_HASH_CONST_MASK	0xfff0
+#define MVPP2_RXTX_HASH_BMID_MASK	0xf
+/* HashBits[31..16] contain skb->head[22..7], the head is aligned and [6..0]=0,
+ * so skb->head is shifted left for (16-7) bits.
+ * This hash permits to detect 2 non-recyclable cases:
+ * - new skb with old hash inside
+ * - same skb but NET-stack has replaced the data-buffer with another one
+ */
+#define MVPP2_HEAD_HASH_SHIFT	(16 - 7)
+#define MVPP2_RXTX_HASH_GENER(skb, bm_pool_id) \
+	(((u32)(phys_addr_t)skb->head << MVPP2_HEAD_HASH_SHIFT) | \
+					MVPP2_RXTX_HASH | bm_pool_id)
+#define MVPP2_RXTX_HASH_IS_OK(skb, hash) \
+	(MVPP2_RXTX_HASH_GENER(skb, 0) == (hash & ~MVPP2_RXTX_HASH_BMID_MASK))
+#define MVPP2_RXTX_HASH_IS_OK_TX(skb, hash) \
+	(((((u32)(phys_addr_t)skb->head << MVPP2_HEAD_HASH_SHIFT) | \
+			MVPP2_RXTX_HASH) ^ hash) <= MVPP2_RXTX_HASH_BMID_MASK)
+
+/* The recycle pool size should be "effectively big" but limited (to eliminate
+ * memory-wasting on TX-pick). It should be >8 (Net-stack-forwarding-buffer)
+ * and >pkt-coalescing. For "effective" >=NAPI_POLL_WEIGHT.
+ * For 4 ports we need more buffers but not x4, statistically it is enough x3.
+ * SKB-pool is shared for Small/Large/Jumbo buffers so we need more SKBs,
+ * statistically it is enough x5.
+ */
+#define MVPP2_RECYCLE_FULL	(NAPI_POLL_WEIGHT * 3)
+#define MVPP2_RECYCLE_FULL_SKB	(NAPI_POLL_WEIGHT * 5)
+
+struct mvpp2_recycle_pool {
+	void *pbuf[MVPP2_RECYCLE_FULL_SKB];
 };
 
-static struct {
-	int pkt_size;
-	int buf_num;
-} mvpp2_pools[MVPP2_BM_POOLS_NUM];
+struct mvpp2_recycle_pcpu {
+	/* All pool-indexes are in 1 cache-line */
+	short int idx[MVPP2_BM_POOLS_NUM_MAX];
+	/* BM/SKB-buffer pools */
+	struct mvpp2_recycle_pool pool[MVPP2_BM_POOLS_NUM_MAX];
+} __aligned(L1_CACHE_BYTES);
+
+struct mvpp2_share {
+	struct mvpp2_recycle_pcpu *recycle;
+	void *recycle_base;
+
+	/* Counters set by Probe/Init/Open */
+	int num_open_ports;
+} __aligned(L1_CACHE_BYTES);
+
+struct mvpp2_share mvpp2_share;
+
+static inline void mvpp2_recycle_put(struct mvpp2_port *port,
+				     struct mvpp2_txq_pcpu *txq_pcpu,
+				     struct mvpp2_txq_pcpu_buf *tx_buf);
+
+static void mvpp2_tx_done_guard_force_irq(struct mvpp2_port *port,
+					  int sw_thread, u8 to_zero_map);
+static inline void mvpp2_tx_done_guard_timer_set(struct mvpp2_port *port,
+						 int sw_thread);
+static u32 mvpp2_tx_done_guard_get_stats(struct mvpp2_port *port, int cpu);
 
 /* The prototype is added here to be used in start_dev when using ACPI. This
  * will be removed once phylink is used for all modes (dt+ACPI).
@@ -61,101 +116,39 @@ static void mvpp2_mac_config(struct phylink_config *config, unsigned int mode,
 static void mvpp2_mac_link_up(struct phylink_config *config, unsigned int mode,
 			      phy_interface_t interface, struct phy_device *phy);
 
+/* Branch prediction switches */
+DEFINE_STATIC_KEY_FALSE(mvpp21_variant);
+DEFINE_STATIC_KEY_FALSE(mvpp2_recycle_ena);
+
 /* Queue modes */
 #define MVPP2_QDIST_SINGLE_MODE	0
 #define MVPP2_QDIST_MULTI_MODE	1
 
 static int queue_mode = MVPP2_QDIST_MULTI_MODE;
+static int tx_fifo_protection;
+static int bm_underrun_protect = 1;
+static int recycle;
+static u32 tx_fifo_map;
 
 module_param(queue_mode, int, 0444);
 MODULE_PARM_DESC(queue_mode, "Set queue_mode (single=0, multi=1)");
 
-/* Utility/helper methods */
+module_param(tx_fifo_protection, int, 0444);
+MODULE_PARM_DESC(tx_fifo_protection, "Set tx_fifo_protection (off=0, on=1)");
 
-void mvpp2_write(struct mvpp2 *priv, u32 offset, u32 data)
-{
-	writel(data, priv->swth_base[0] + offset);
-}
+module_param(bm_underrun_protect, int, 0444);
+MODULE_PARM_DESC(bm_underrun_protect, "Set BM underrun protect feature (0-1), def=1");
 
-u32 mvpp2_read(struct mvpp2 *priv, u32 offset)
-{
-	return readl(priv->swth_base[0] + offset);
-}
-
-static u32 mvpp2_read_relaxed(struct mvpp2 *priv, u32 offset)
-{
-	return readl_relaxed(priv->swth_base[0] + offset);
-}
-
-static inline u32 mvpp2_cpu_to_thread(struct mvpp2 *priv, int cpu)
-{
-	return cpu % priv->nthreads;
-}
-
-/* These accessors should be used to access:
- *
- * - per-thread registers, where each thread has its own copy of the
- *   register.
- *
- *   MVPP2_BM_VIRT_ALLOC_REG
- *   MVPP2_BM_ADDR_HIGH_ALLOC
- *   MVPP22_BM_ADDR_HIGH_RLS_REG
- *   MVPP2_BM_VIRT_RLS_REG
- *   MVPP2_ISR_RX_TX_CAUSE_REG
- *   MVPP2_ISR_RX_TX_MASK_REG
- *   MVPP2_TXQ_NUM_REG
- *   MVPP2_AGGR_TXQ_UPDATE_REG
- *   MVPP2_TXQ_RSVD_REQ_REG
- *   MVPP2_TXQ_RSVD_RSLT_REG
- *   MVPP2_TXQ_SENT_REG
- *   MVPP2_RXQ_NUM_REG
- *
- * - global registers that must be accessed through a specific thread
- *   window, because they are related to an access to a per-thread
- *   register
- *
- *   MVPP2_BM_PHY_ALLOC_REG    (related to MVPP2_BM_VIRT_ALLOC_REG)
- *   MVPP2_BM_PHY_RLS_REG      (related to MVPP2_BM_VIRT_RLS_REG)
- *   MVPP2_RXQ_THRESH_REG      (related to MVPP2_RXQ_NUM_REG)
- *   MVPP2_RXQ_DESC_ADDR_REG   (related to MVPP2_RXQ_NUM_REG)
- *   MVPP2_RXQ_DESC_SIZE_REG   (related to MVPP2_RXQ_NUM_REG)
- *   MVPP2_RXQ_INDEX_REG       (related to MVPP2_RXQ_NUM_REG)
- *   MVPP2_TXQ_PENDING_REG     (related to MVPP2_TXQ_NUM_REG)
- *   MVPP2_TXQ_DESC_ADDR_REG   (related to MVPP2_TXQ_NUM_REG)
- *   MVPP2_TXQ_DESC_SIZE_REG   (related to MVPP2_TXQ_NUM_REG)
- *   MVPP2_TXQ_INDEX_REG       (related to MVPP2_TXQ_NUM_REG)
- *   MVPP2_TXQ_PENDING_REG     (related to MVPP2_TXQ_NUM_REG)
- *   MVPP2_TXQ_PREF_BUF_REG    (related to MVPP2_TXQ_NUM_REG)
- *   MVPP2_TXQ_PREF_BUF_REG    (related to MVPP2_TXQ_NUM_REG)
- */
-static void mvpp2_thread_write(struct mvpp2 *priv, unsigned int thread,
-			       u32 offset, u32 data)
-{
-	writel(data, priv->swth_base[thread] + offset);
-}
-
-static u32 mvpp2_thread_read(struct mvpp2 *priv, unsigned int thread,
-			     u32 offset)
-{
-	return readl(priv->swth_base[thread] + offset);
-}
-
-static void mvpp2_thread_write_relaxed(struct mvpp2 *priv, unsigned int thread,
-				       u32 offset, u32 data)
-{
-	writel_relaxed(data, priv->swth_base[thread] + offset);
-}
+module_param(recycle, int, 0444);
+MODULE_PARM_DESC(recycle, "Recycle: 0:disable(default), >=1:enable");
 
-static u32 mvpp2_thread_read_relaxed(struct mvpp2 *priv, unsigned int thread,
-				     u32 offset)
-{
-	return readl_relaxed(priv->swth_base[thread] + offset);
-}
+module_param(tx_fifo_map, uint, 0444);
+MODULE_PARM_DESC(tx_fifo_map, "Set PPv2 TX FIFO ports map");
 
 static dma_addr_t mvpp2_txdesc_dma_addr_get(struct mvpp2_port *port,
 					    struct mvpp2_tx_desc *tx_desc)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		return le32_to_cpu(tx_desc->pp21.buf_dma_addr);
 	else
 		return le64_to_cpu(tx_desc->pp22.buf_dma_addr_ptp) &
@@ -171,7 +164,7 @@ static void mvpp2_txdesc_dma_addr_set(struct mvpp2_port *port,
 	addr = dma_addr & ~MVPP2_TX_DESC_ALIGN;
 	offset = dma_addr & MVPP2_TX_DESC_ALIGN;
 
-	if (port->priv->hw_version == MVPP21) {
+	if (static_branch_unlikely(&mvpp21_variant)) {
 		tx_desc->pp21.buf_dma_addr = cpu_to_le32(addr);
 		tx_desc->pp21.packet_offset = offset;
 	} else {
@@ -186,7 +179,7 @@ static void mvpp2_txdesc_dma_addr_set(struct mvpp2_port *port,
 static size_t mvpp2_txdesc_size_get(struct mvpp2_port *port,
 				    struct mvpp2_tx_desc *tx_desc)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		return le16_to_cpu(tx_desc->pp21.data_size);
 	else
 		return le16_to_cpu(tx_desc->pp22.data_size);
@@ -196,7 +189,7 @@ static void mvpp2_txdesc_size_set(struct mvpp2_port *port,
 				  struct mvpp2_tx_desc *tx_desc,
 				  size_t size)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		tx_desc->pp21.data_size = cpu_to_le16(size);
 	else
 		tx_desc->pp22.data_size = cpu_to_le16(size);
@@ -206,7 +199,7 @@ static void mvpp2_txdesc_txq_set(struct mvpp2_port *port,
 				 struct mvpp2_tx_desc *tx_desc,
 				 unsigned int txq)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		tx_desc->pp21.phys_txq = txq;
 	else
 		tx_desc->pp22.phys_txq = txq;
@@ -216,7 +209,7 @@ static void mvpp2_txdesc_cmd_set(struct mvpp2_port *port,
 				 struct mvpp2_tx_desc *tx_desc,
 				 unsigned int command)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		tx_desc->pp21.command = cpu_to_le32(command);
 	else
 		tx_desc->pp22.command = cpu_to_le32(command);
@@ -225,7 +218,7 @@ static void mvpp2_txdesc_cmd_set(struct mvpp2_port *port,
 static unsigned int mvpp2_txdesc_offset_get(struct mvpp2_port *port,
 					    struct mvpp2_tx_desc *tx_desc)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		return tx_desc->pp21.packet_offset;
 	else
 		return tx_desc->pp22.packet_offset;
@@ -234,27 +227,17 @@ static unsigned int mvpp2_txdesc_offset_get(struct mvpp2_port *port,
 static dma_addr_t mvpp2_rxdesc_dma_addr_get(struct mvpp2_port *port,
 					    struct mvpp2_rx_desc *rx_desc)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		return le32_to_cpu(rx_desc->pp21.buf_dma_addr);
 	else
 		return le64_to_cpu(rx_desc->pp22.buf_dma_addr_key_hash) &
 		       MVPP2_DESC_DMA_MASK;
 }
 
-static unsigned long mvpp2_rxdesc_cookie_get(struct mvpp2_port *port,
-					     struct mvpp2_rx_desc *rx_desc)
-{
-	if (port->priv->hw_version == MVPP21)
-		return le32_to_cpu(rx_desc->pp21.buf_cookie);
-	else
-		return le64_to_cpu(rx_desc->pp22.buf_cookie_misc) &
-		       MVPP2_DESC_DMA_MASK;
-}
-
 static size_t mvpp2_rxdesc_size_get(struct mvpp2_port *port,
 				    struct mvpp2_rx_desc *rx_desc)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		return le16_to_cpu(rx_desc->pp21.data_size);
 	else
 		return le16_to_cpu(rx_desc->pp22.data_size);
@@ -263,7 +246,7 @@ static size_t mvpp2_rxdesc_size_get(struct mvpp2_port *port,
 static u32 mvpp2_rxdesc_status_get(struct mvpp2_port *port,
 				   struct mvpp2_rx_desc *rx_desc)
 {
-	if (port->priv->hw_version == MVPP21)
+	if (static_branch_unlikely(&mvpp21_variant))
 		return le32_to_cpu(rx_desc->pp21.status);
 	else
 		return le32_to_cpu(rx_desc->pp22.status);
@@ -278,12 +261,12 @@ static void mvpp2_txq_inc_get(struct mvpp2_txq_pcpu *txq_pcpu)
 
 static void mvpp2_txq_inc_put(struct mvpp2_port *port,
 			      struct mvpp2_txq_pcpu *txq_pcpu,
-			      struct sk_buff *skb,
+			      void *skb_or_tso_mark,
 			      struct mvpp2_tx_desc *tx_desc)
 {
 	struct mvpp2_txq_pcpu_buf *tx_buf =
 		txq_pcpu->buffs + txq_pcpu->txq_put_index;
-	tx_buf->skb = skb;
+	tx_buf->skb = (struct sk_buff *)skb_or_tso_mark;
 	tx_buf->size = mvpp2_txdesc_size_get(port, tx_desc);
 	tx_buf->dma = mvpp2_txdesc_dma_addr_get(port, tx_desc) +
 		mvpp2_txdesc_offset_get(port, tx_desc);
@@ -292,26 +275,6 @@ static void mvpp2_txq_inc_put(struct mvpp2_port *port,
 		txq_pcpu->txq_put_index = 0;
 }
 
-/* Get number of maximum RXQ */
-static int mvpp2_get_nrxqs(struct mvpp2 *priv)
-{
-	unsigned int nrxqs;
-
-	if (priv->hw_version == MVPP22 && queue_mode == MVPP2_QDIST_SINGLE_MODE)
-		return 1;
-
-	/* According to the PPv2.2 datasheet and our experiments on
-	 * PPv2.1, RX queues have an allocation granularity of 4 (when
-	 * more than a single one on PPv2.2).
-	 * Round up to nearest multiple of 4.
-	 */
-	nrxqs = (num_possible_cpus() + 3) & ~0x3;
-	if (nrxqs > MVPP2_PORT_MAX_RXQ)
-		nrxqs = MVPP2_PORT_MAX_RXQ;
-
-	return nrxqs;
-}
-
 /* Get number of physical egress port */
 static inline int mvpp2_egress_port(struct mvpp2_port *port)
 {
@@ -342,8 +305,85 @@ static void mvpp2_frag_free(const struct mvpp2_bm_pool *pool, void *data)
 
 /* Buffer Manager configuration routines */
 
+/* Get default packet size for given BM pool type */
+static int mvpp2_bm_pool_default_pkt_size(enum mvpp2_bm_pool_type bm_pool_type)
+{
+	switch (bm_pool_type) {
+	case MVPP2_BM_SHORT:
+		return MVPP2_BM_SHORT_PKT_SIZE;
+	case MVPP2_BM_JUMBO:
+		return MVPP2_BM_JUMBO_PKT_SIZE;
+	case MVPP2_BM_LONG:
+		return MVPP2_BM_LONG_PKT_SIZE;
+	default:
+		return -EINVAL;
+	}
+}
+
+/* Get default buffer count for given BM pool type */
+static int mvpp2_bm_pool_default_buf_num(enum mvpp2_bm_pool_type bm_pool_type)
+{
+	switch (bm_pool_type) {
+	case MVPP2_BM_SHORT:
+		return MVPP2_BM_SHORT_BUF_NUM;
+	case MVPP2_BM_JUMBO:
+		return MVPP2_BM_JUMBO_BUF_NUM;
+	case MVPP2_BM_LONG:
+		return MVPP2_BM_LONG_BUF_NUM;
+	default:
+		return -EINVAL;
+	}
+}
+
+/* Get BM pool type mapping - return the hardware Buffer Manager pools
+ * type according to the mapping to its ID:
+ * POOL#0 - short packets
+ * POOL#1 - jumbo packets
+ * POOL#2 - long packets
+ * In case the KS recycling feature is enabled, ID = 2 is
+ * the first (CPU#0) out of the per-CPU pools for long packets.
+ */
+static enum mvpp2_bm_pool_type mvpp2_bm_pool_get_type(int id)
+{
+	switch (id) {
+	case 0:
+		return MVPP2_BM_SHORT;
+	case 1:
+		return MVPP2_BM_JUMBO;
+	case 2:
+		return MVPP2_BM_LONG;
+	default:
+		if (recycle)
+			return MVPP2_BM_LONG;
+		return -EINVAL;
+	}
+}
+
+/* Get BM pool ID mapping - return the hardware Buffer Manager pools
+ * ID according to the mapping to its type:
+ * Short packets - POOL#0
+ * Jumbo packets - POOL#1
+ * Long packets - POOL#2
+ * In case the KS recycling feature is enabled, ID = 2 is
+ * the first (CPU#0) out of the per-CPU pools for long packets.
+ */
+static int mvpp2_bm_pool_get_id(enum mvpp2_bm_pool_type bm_pool_type)
+{
+	switch (bm_pool_type) {
+	case MVPP2_BM_SHORT:
+		return 0;
+	case MVPP2_BM_JUMBO:
+		return 1;
+	case MVPP2_BM_LONG:
+		return 2;
+	default:
+		return -EINVAL;
+	}
+}
+
 /* Create pool */
-static int mvpp2_bm_pool_create(struct device *dev, struct mvpp2 *priv,
+static int mvpp2_bm_pool_create(struct platform_device *pdev,
+				struct mvpp2 *priv,
 				struct mvpp2_bm_pool *bm_pool, int size)
 {
 	u32 val;
@@ -354,7 +394,7 @@ static int mvpp2_bm_pool_create(struct device *dev, struct mvpp2 *priv,
 	if (!IS_ALIGNED(size, 16))
 		return -EINVAL;
 
-	/* PPv2.1 needs 8 bytes per buffer pointer, PPv2.2 needs 16
+	/* PPv2.1 needs 8 bytes per buffer pointer, PPv2.2 and PPv2.3 needs 16
 	 * bytes per buffer pointer
 	 */
 	if (priv->hw_version == MVPP21)
@@ -362,7 +402,7 @@ static int mvpp2_bm_pool_create(struct device *dev, struct mvpp2 *priv,
 	else
 		bm_pool->size_bytes = 2 * sizeof(u64) * size;
 
-	bm_pool->virt_addr = dma_alloc_coherent(dev, bm_pool->size_bytes,
+	bm_pool->virt_addr = dma_alloc_coherent(&pdev->dev, bm_pool->size_bytes,
 						&bm_pool->dma_addr,
 						GFP_KERNEL);
 	if (!bm_pool->virt_addr)
@@ -370,9 +410,9 @@ static int mvpp2_bm_pool_create(struct device *dev, struct mvpp2 *priv,
 
 	if (!IS_ALIGNED((unsigned long)bm_pool->virt_addr,
 			MVPP2_BM_POOL_PTR_ALIGN)) {
-		dma_free_coherent(dev, bm_pool->size_bytes,
+		dma_free_coherent(&pdev->dev, bm_pool->size_bytes,
 				  bm_pool->virt_addr, bm_pool->dma_addr);
-		dev_err(dev, "BM pool %d is not %d bytes aligned\n",
+		dev_err(&pdev->dev, "BM pool %d is not %d bytes aligned\n",
 			bm_pool->id, MVPP2_BM_POOL_PTR_ALIGN);
 		return -ENOMEM;
 	}
@@ -383,11 +423,27 @@ static int mvpp2_bm_pool_create(struct device *dev, struct mvpp2 *priv,
 
 	val = mvpp2_read(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id));
 	val |= MVPP2_BM_START_MASK;
+
+	val &= ~MVPP2_BM_LOW_THRESH_MASK;
+	val &= ~MVPP2_BM_HIGH_THRESH_MASK;
+
+	/* Set 8 Pools BPPI threshold if BM underrun protection feature
+	 * were enabled
+	 */
+	if (priv->hw_version == MVPP23 && bm_underrun_protect) {
+		val |= MVPP2_BM_LOW_THRESH_VALUE(MVPP23_BM_BPPI_LOW_THRESH);
+		val |= MVPP2_BM_HIGH_THRESH_VALUE(MVPP23_BM_BPPI_HIGH_THRESH);
+	} else {
+		val |= MVPP2_BM_LOW_THRESH_VALUE(MVPP2_BM_BPPI_LOW_THRESH);
+		val |= MVPP2_BM_HIGH_THRESH_VALUE(MVPP2_BM_BPPI_HIGH_THRESH);
+	}
+
 	mvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);
 
 	bm_pool->size = size;
 	bm_pool->pkt_size = 0;
 	bm_pool->buf_num = 0;
+	bm_pool->type = mvpp2_bm_pool_get_type(bm_pool->id);
 
 	return 0;
 }
@@ -414,23 +470,16 @@ static void mvpp2_bm_bufs_get_addrs(struct device *dev, struct mvpp2 *priv,
 
 	*dma_addr = mvpp2_thread_read(priv, thread,
 				      MVPP2_BM_PHY_ALLOC_REG(bm_pool->id));
-	*phys_addr = mvpp2_thread_read(priv, thread, MVPP2_BM_VIRT_ALLOC_REG);
 
-	if (priv->hw_version == MVPP22) {
+	if (priv->hw_version != MVPP21 && sizeof(dma_addr_t) == 8) {
 		u32 val;
-		u32 dma_addr_highbits, phys_addr_highbits;
+		u32 dma_addr_highbits;
 
 		val = mvpp2_thread_read(priv, thread, MVPP22_BM_ADDR_HIGH_ALLOC);
 		dma_addr_highbits = (val & MVPP22_BM_ADDR_HIGH_PHYS_MASK);
-		phys_addr_highbits = (val & MVPP22_BM_ADDR_HIGH_VIRT_MASK) >>
-			MVPP22_BM_ADDR_HIGH_VIRT_SHIFT;
-
-		if (sizeof(dma_addr_t) == 8)
-			*dma_addr |= (u64)dma_addr_highbits << 32;
-
-		if (sizeof(phys_addr_t) == 8)
-			*phys_addr |= (u64)phys_addr_highbits << 32;
+		*dma_addr |= (u64)dma_addr_highbits << 32;
 	}
+	*phys_addr = dma_to_phys(dev, *dma_addr);
 
 	put_cpu();
 }
@@ -487,14 +536,15 @@ static int mvpp2_check_hw_buf_num(struct mvpp2 *priv, struct mvpp2_bm_pool *bm_p
 }
 
 /* Cleanup pool */
-static int mvpp2_bm_pool_destroy(struct device *dev, struct mvpp2 *priv,
+static int mvpp2_bm_pool_destroy(struct platform_device *pdev,
+				 struct mvpp2 *priv,
 				 struct mvpp2_bm_pool *bm_pool)
 {
 	int buf_num;
 	u32 val;
 
 	buf_num = mvpp2_check_hw_buf_num(priv, bm_pool);
-	mvpp2_bm_bufs_free(dev, priv, bm_pool, buf_num);
+	mvpp2_bm_bufs_free(&pdev->dev, priv, bm_pool, buf_num);
 
 	/* Check buffer counters after free */
 	buf_num = mvpp2_check_hw_buf_num(priv, bm_pool);
@@ -508,26 +558,37 @@ static int mvpp2_bm_pool_destroy(struct device *dev, struct mvpp2 *priv,
 	val |= MVPP2_BM_STOP_MASK;
 	mvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);
 
-	dma_free_coherent(dev, bm_pool->size_bytes,
+	dma_free_coherent(&pdev->dev, bm_pool->size_bytes,
 			  bm_pool->virt_addr,
 			  bm_pool->dma_addr);
 	return 0;
 }
 
-static int mvpp2_bm_pools_init(struct device *dev, struct mvpp2 *priv)
+static int mvpp2_bm_pools_init(struct platform_device *pdev,
+			       struct mvpp2 *priv)
 {
-	int i, err, size, poolnum = MVPP2_BM_POOLS_NUM;
+	int i, err, size, cpu;
 	struct mvpp2_bm_pool *bm_pool;
 
-	if (priv->percpu_pools)
-		poolnum = mvpp2_get_nrxqs(priv) * 2;
+	if (recycle) {
+		/* Allocate per-CPU long pools array */
+		priv->pools_pcpu = devm_kcalloc(&pdev->dev, num_present_cpus(),
+						sizeof(*priv->pools_pcpu),
+						GFP_KERNEL);
+		if (!priv->pools_pcpu)
+			return -ENOMEM;
+	}
+
+	/* Initialize Virtual with 0x0 */
+	for_each_present_cpu(cpu)
+		mvpp2_thread_write(priv, cpu, MVPP2_BM_VIRT_RLS_REG, 0x0);
 
 	/* Create all pools with maximum size */
 	size = MVPP2_BM_POOL_SIZE_MAX;
-	for (i = 0; i < poolnum; i++) {
+	for (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {
 		bm_pool = &priv->bm_pools[i];
 		bm_pool->id = i;
-		err = mvpp2_bm_pool_create(dev, priv, bm_pool, size);
+		err = mvpp2_bm_pool_create(pdev, priv, bm_pool, size);
 		if (err)
 			goto err_unroll_pools;
 		mvpp2_bm_pool_bufsize_set(priv, bm_pool, 0);
@@ -535,56 +596,69 @@ static int mvpp2_bm_pools_init(struct device *dev, struct mvpp2 *priv)
 	return 0;
 
 err_unroll_pools:
-	dev_err(dev, "failed to create BM pool %d, size %d\n", i, size);
+	dev_err(&pdev->dev, "failed to create BM pool %d, size %d\n", i, size);
 	for (i = i - 1; i >= 0; i--)
-		mvpp2_bm_pool_destroy(dev, priv, &priv->bm_pools[i]);
+		mvpp2_bm_pool_destroy(pdev, priv, &priv->bm_pools[i]);
 	return err;
 }
 
-static int mvpp2_bm_init(struct device *dev, struct mvpp2 *priv)
+/* Routine enable PPv23 8 pool mode */
+static void mvpp23_bm_set_8pool_mode(struct mvpp2 *priv)
 {
-	int i, err, poolnum = MVPP2_BM_POOLS_NUM;
+	int val;
+
+	val = mvpp2_read(priv, MVPP22_BM_POOL_BASE_ADDR_HIGH_REG);
+	val |= MVPP23_BM_8POOL_MODE;
+	mvpp2_write(priv, MVPP22_BM_POOL_BASE_ADDR_HIGH_REG, val);
+}
 
-	if (priv->percpu_pools)
-		poolnum = mvpp2_get_nrxqs(priv) * 2;
+/* Cleanup pool before actual initialization in the OS */
+static void mvpp2_bm_pool_cleanup(struct mvpp2 *priv, int pool_id)
+{
+	u32 val;
+	int i;
 
-	dev_info(dev, "using %d %s buffers\n", poolnum,
-		 priv->percpu_pools ? "per-cpu" : "shared");
+	/* Drain the BM from all possible residues left by firmware */
+	for (i = 0; i < MVPP2_BM_POOL_SIZE_MAX; i++)
+		mvpp2_read(priv, MVPP2_BM_PHY_ALLOC_REG(pool_id));
 
-	for (i = 0; i < poolnum; i++) {
-		/* Mask BM all interrupts */
-		mvpp2_write(priv, MVPP2_BM_INTR_MASK_REG(i), 0);
-		/* Clear BM cause register */
-		mvpp2_write(priv, MVPP2_BM_INTR_CAUSE_REG(i), 0);
+	/* Stop the BM pool */
+	val = mvpp2_read(priv, MVPP2_BM_POOL_CTRL_REG(pool_id));
+	val |= MVPP2_BM_STOP_MASK;
+	mvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(pool_id), val);
+
+	/* Mask BM all interrupts */
+	mvpp2_write(priv, MVPP2_BM_INTR_MASK_REG(pool_id), 0);
+	/* Clear BM cause register */
+	mvpp2_write(priv, MVPP2_BM_INTR_CAUSE_REG(pool_id), 0);
+}
+
+static int mvpp2_bm_init(struct platform_device *pdev, struct mvpp2 *priv)
+{
+	int i, err;
+
+	for (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {
+		/* Make sure about the pool state in case it was
+		 * used by firmware.
+		 */
+		mvpp2_bm_pool_cleanup(priv, i);
 	}
 
 	/* Allocate and initialize BM pools */
-	priv->bm_pools = devm_kcalloc(dev, poolnum,
+	priv->bm_pools = devm_kcalloc(&pdev->dev, MVPP2_BM_POOLS_NUM,
 				      sizeof(*priv->bm_pools), GFP_KERNEL);
 	if (!priv->bm_pools)
 		return -ENOMEM;
 
-	err = mvpp2_bm_pools_init(dev, priv);
+	if (priv->hw_version == MVPP23 && bm_underrun_protect)
+		mvpp23_bm_set_8pool_mode(priv);
+
+	err = mvpp2_bm_pools_init(pdev, priv);
 	if (err < 0)
 		return err;
 	return 0;
 }
 
-static void mvpp2_setup_bm_pool(void)
-{
-	/* Short pool */
-	mvpp2_pools[MVPP2_BM_SHORT].buf_num  = MVPP2_BM_SHORT_BUF_NUM;
-	mvpp2_pools[MVPP2_BM_SHORT].pkt_size = MVPP2_BM_SHORT_PKT_SIZE;
-
-	/* Long pool */
-	mvpp2_pools[MVPP2_BM_LONG].buf_num  = MVPP2_BM_LONG_BUF_NUM;
-	mvpp2_pools[MVPP2_BM_LONG].pkt_size = MVPP2_BM_LONG_PKT_SIZE;
-
-	/* Jumbo pool */
-	mvpp2_pools[MVPP2_BM_JUMBO].buf_num  = MVPP2_BM_JUMBO_BUF_NUM;
-	mvpp2_pools[MVPP2_BM_JUMBO].pkt_size = MVPP2_BM_JUMBO_PKT_SIZE;
-}
-
 /* Attach long pool to rxq */
 static void mvpp2_rxq_long_pool_set(struct mvpp2_port *port,
 				    int lrxq, int long_pool)
@@ -627,36 +701,226 @@ static void mvpp2_rxq_short_pool_set(struct mvpp2_port *port,
 	mvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);
 }
 
-static void *mvpp2_buf_alloc(struct mvpp2_port *port,
-			     struct mvpp2_bm_pool *bm_pool,
-			     dma_addr_t *buf_dma_addr,
-			     phys_addr_t *buf_phys_addr,
-			     gfp_t gfp_mask)
+static dma_addr_t mvpp2_buf_alloc(struct mvpp2_port *port,
+				  struct mvpp2_bm_pool *bm_pool,
+				  gfp_t gfp_mask)
 {
 	dma_addr_t dma_addr;
 	void *data;
 
 	data = mvpp2_frag_alloc(bm_pool);
 	if (!data)
-		return NULL;
+		return (dma_addr_t)data;
 
 	dma_addr = dma_map_single(port->dev->dev.parent, data,
-				  MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),
-				  DMA_FROM_DEVICE);
+				  bm_pool->buf_size, DMA_FROM_DEVICE);
 	if (unlikely(dma_mapping_error(port->dev->dev.parent, dma_addr))) {
 		mvpp2_frag_free(bm_pool, data);
-		return NULL;
+		dma_addr = 0;
+	}
+	return dma_addr;
+}
+
+/* Routine calculate single queue shares address space */
+static int mvpp22_calc_shared_addr_space(struct mvpp2_port *port)
+{
+	/* If number of CPU's greater than number of threads, return last
+	 * address space
+	 */
+	if (num_active_cpus() >= MVPP2_MAX_THREADS)
+		return MVPP2_MAX_THREADS - 1;
+
+	return num_active_cpus();
+}
+
+/* Routine enable flow control for RXQs conditon */
+void mvpp2_rxq_enable_fc(struct mvpp2_port *port)
+{
+	int val, cm3_state, host_id, q;
+	int fq = port->first_rxq;
+	unsigned long flags;
+
+	spin_lock_irqsave(&port->priv->mss_spinlock, flags);
+
+	/* Remove Flow control enable bit to prevent race between FW and Kernel
+	 * If Flow control were enabled, it would be re-enabled.
+	 */
+	val = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);
+	cm3_state = (val & FLOW_CONTROL_ENABLE_BIT);
+	val &= ~FLOW_CONTROL_ENABLE_BIT;
+	mvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);
+
+	/* Set same Flow control for all RXQs */
+	for (q = 0; q < port->nrxqs; q++) {
+		/* Set stop and start Flow control RXQ thresholds */
+		val = MSS_THRESHOLD_START;
+		val |= (MSS_THRESHOLD_STOP << MSS_RXQ_TRESH_STOP_OFFS);
+		mvpp2_cm3_write(port->priv, MSS_RXQ_TRESH_REG(q, fq), val);
+
+		val = mvpp2_cm3_read(port->priv, MSS_RXQ_ASS_REG(q, fq));
+		/* Set RXQ port ID */
+		val &= ~(MSS_RXQ_ASS_PORTID_MASK << MSS_RXQ_ASS_Q_BASE(q, fq));
+		val |= (port->id << MSS_RXQ_ASS_Q_BASE(q, fq));
+		val &= ~(MSS_RXQ_ASS_HOSTID_MASK << (MSS_RXQ_ASS_Q_BASE(q, fq)
+			+ MSS_RXQ_ASS_HOSTID_OFFS));
+
+		/* Calculate RXQ host ID:
+		 * In Single queue mode: Host ID equal to Host ID used for
+		 *			 shared RX interrupt
+		 * In Multi queue mode: Host ID equal to number of
+		 *			RXQ ID / number of CoS queues
+		 * In Single resource mode: Host ID always equal to 0
+		 */
+		if (queue_mode == MVPP2_QDIST_SINGLE_MODE)
+			host_id = mvpp22_calc_shared_addr_space(port);
+		else if (queue_mode == MVPP2_QDIST_MULTI_MODE)
+			host_id = q;
+		else
+			host_id = 0;
+
+		/* Set RXQ host ID */
+		val |= (host_id << (MSS_RXQ_ASS_Q_BASE(q, fq)
+			+ MSS_RXQ_ASS_HOSTID_OFFS));
+
+		mvpp2_cm3_write(port->priv, MSS_RXQ_ASS_REG(q, fq), val);
+	}
+
+	/* Notify Firmware that Flow control config space ready for update */
+	val = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);
+	val |= FLOW_CONTROL_UPDATE_COMMAND_BIT;
+	val |= cm3_state;
+	mvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);
+
+	spin_unlock_irqrestore(&port->priv->mss_spinlock, flags);
+}
+
+/* Routine disable flow control for RXQs conditon */
+void mvpp2_rxq_disable_fc(struct mvpp2_port *port)
+{
+	int val, cm3_state, q;
+	unsigned long flags;
+	int fq = port->first_rxq;
+
+	spin_lock_irqsave(&port->priv->mss_spinlock, flags);
+
+	/* Remove Flow control enable bit to prevent race between FW and Kernel
+	 * If Flow control were enabled, it would be re-enabled.
+	 */
+	val = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);
+	cm3_state = (val & FLOW_CONTROL_ENABLE_BIT);
+	val &= ~FLOW_CONTROL_ENABLE_BIT;
+	mvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);
+
+	/* Disable Flow control for all RXQs */
+	for (q = 0; q < port->nrxqs; q++) {
+		/* Set threshold 0 to disable Flow control */
+		val = 0;
+		val |= (0 << MSS_RXQ_TRESH_STOP_OFFS);
+		mvpp2_cm3_write(port->priv, MSS_RXQ_TRESH_REG(q, fq), val);
+
+		val = mvpp2_cm3_read(port->priv, MSS_RXQ_ASS_REG(q, fq));
+
+		val &= ~(MSS_RXQ_ASS_PORTID_MASK << MSS_RXQ_ASS_Q_BASE(q, fq));
+
+		val &= ~(MSS_RXQ_ASS_HOSTID_MASK << (MSS_RXQ_ASS_Q_BASE(q, fq)
+			+ MSS_RXQ_ASS_HOSTID_OFFS));
+
+		mvpp2_cm3_write(port->priv, MSS_RXQ_ASS_REG(q, fq), val);
+	}
+
+	/* Notify Firmware that Flow control config space ready for update */
+	val = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);
+	val |= FLOW_CONTROL_UPDATE_COMMAND_BIT;
+	val |= cm3_state;
+	mvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);
+
+	spin_unlock_irqrestore(&port->priv->mss_spinlock, flags);
+}
+
+/* Routine disable/enable flow control for BM pool conditon */
+void mvpp2_bm_pool_update_fc(struct mvpp2_port *port,
+			     struct mvpp2_bm_pool *pool,
+			     bool en)
+{
+	int val, cm3_state;
+	unsigned long flags;
+
+	spin_lock_irqsave(&port->priv->mss_spinlock, flags);
+
+	/* Remove Flow control enable bit to prevent race between FW and Kernel
+	 * If Flow control were enabled, it would be re-enabled.
+	 */
+	val = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);
+	cm3_state = (val & FLOW_CONTROL_ENABLE_BIT);
+	val &= ~FLOW_CONTROL_ENABLE_BIT;
+	mvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);
+
+	/* Check if BM pool should be enabled/disable */
+	if (en) {
+		/* Set BM pool start and stop thresholds per port */
+		val = mvpp2_cm3_read(port->priv, MSS_BUF_POOL_REG(pool->id));
+		val |= MSS_BUF_POOL_PORT_OFFS(port->id);
+		val &= ~MSS_BUF_POOL_START_MASK;
+		val |= (MSS_THRESHOLD_START << MSS_BUF_POOL_START_OFFS);
+		val &= ~MSS_BUF_POOL_STOP_MASK;
+		val |= MSS_THRESHOLD_STOP;
+		mvpp2_cm3_write(port->priv, MSS_BUF_POOL_REG(pool->id), val);
+	} else {
+		/* Remove BM pool from the port */
+		val = mvpp2_cm3_read(port->priv, MSS_BUF_POOL_REG(pool->id));
+		val &= ~MSS_BUF_POOL_PORT_OFFS(port->id);
+
+		/* Zero BM pool start and stop thresholds to disable pool
+		 * flow control if pool empty (not used by any port)
+		 */
+		if (!pool->buf_num) {
+			val &= ~MSS_BUF_POOL_START_MASK;
+			val &= ~MSS_BUF_POOL_STOP_MASK;
+		}
+
+		mvpp2_cm3_write(port->priv, MSS_BUF_POOL_REG(pool->id), val);
+	}
+
+	/* Notify Firmware that Flow control config space ready for update */
+	val = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);
+	val |= FLOW_CONTROL_UPDATE_COMMAND_BIT;
+	val |= cm3_state;
+	mvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);
+
+	spin_unlock_irqrestore(&port->priv->mss_spinlock, flags);
+}
+
+static int mvpp2_enable_global_fc(struct mvpp2 *priv)
+{
+	int val, timeout = 0;
+
+	/* Enable global flow control. In this stage global
+	 * flow control enabled, but still disabled per port.
+	 */
+	val = mvpp2_cm3_read(priv, MSS_FC_COM_REG);
+	val |= FLOW_CONTROL_ENABLE_BIT;
+	mvpp2_cm3_write(priv, MSS_FC_COM_REG, val);
+
+	/* Check if Firmware running and disable FC if not*/
+	val |= FLOW_CONTROL_UPDATE_COMMAND_BIT;
+	mvpp2_cm3_write(priv, MSS_FC_COM_REG, val);
+
+	while (timeout < MSS_FC_MAX_TIMEOUT) {
+		val = mvpp2_cm3_read(priv, MSS_FC_COM_REG);
+
+		if (!(val & FLOW_CONTROL_UPDATE_COMMAND_BIT))
+			return 0;
+		usleep_range(10, 20);
+		timeout++;
 	}
-	*buf_dma_addr = dma_addr;
-	*buf_phys_addr = virt_to_phys(data);
 
-	return data;
+	priv->global_tx_fc = false;
+	return -ENOTSUPP;
 }
 
 /* Release buffer to BM */
 static inline void mvpp2_bm_pool_put(struct mvpp2_port *port, int pool,
-				     dma_addr_t buf_dma_addr,
-				     phys_addr_t buf_phys_addr)
+				     dma_addr_t buf_dma_addr)
 {
 	unsigned int thread = mvpp2_cpu_to_thread(port->priv, get_cpu());
 	unsigned long flags = 0;
@@ -664,29 +928,21 @@ static inline void mvpp2_bm_pool_put(struct mvpp2_port *port, int pool,
 	if (test_bit(thread, &port->priv->lock_map))
 		spin_lock_irqsave(&port->bm_lock[thread], flags);
 
-	if (port->priv->hw_version == MVPP22) {
-		u32 val = 0;
-
-		if (sizeof(dma_addr_t) == 8)
-			val |= upper_32_bits(buf_dma_addr) &
+	/* MVPP2_BM_VIRT_RLS_REG is not interpreted by HW, and simply
+	 * returned in the "cookie" field of the RX descriptor.
+	 * For performance reasons don't store VA|PA and don't use "cookie".
+	 * VA/PA obtained faster from dma_to_phys(dma-addr) and phys_to_virt.
+	 */
+#if defined(CONFIG_ARCH_DMA_ADDR_T_64BIT) && defined(CONFIG_PHYS_ADDR_T_64BIT)
+	if (!static_branch_unlikely(&mvpp21_variant)) {
+		u32 val = upper_32_bits(buf_dma_addr) &
 				MVPP22_BM_ADDR_HIGH_PHYS_RLS_MASK;
 
-		if (sizeof(phys_addr_t) == 8)
-			val |= (upper_32_bits(buf_phys_addr)
-				<< MVPP22_BM_ADDR_HIGH_VIRT_RLS_SHIFT) &
-				MVPP22_BM_ADDR_HIGH_VIRT_RLS_MASK;
-
 		mvpp2_thread_write_relaxed(port->priv, thread,
 					   MVPP22_BM_ADDR_HIGH_RLS_REG, val);
 	}
+#endif
 
-	/* MVPP2_BM_VIRT_RLS_REG is not interpreted by HW, and simply
-	 * returned in the "cookie" field of the RX
-	 * descriptor. Instead of storing the virtual address, we
-	 * store the physical address
-	 */
-	mvpp2_thread_write_relaxed(port->priv, thread,
-				   MVPP2_BM_VIRT_RLS_REG, buf_phys_addr);
 	mvpp2_thread_write_relaxed(port->priv, thread,
 				   MVPP2_BM_PHY_RLS_REG(pool), buf_dma_addr);
 
@@ -700,20 +956,8 @@ static inline void mvpp2_bm_pool_put(struct mvpp2_port *port, int pool,
 static int mvpp2_bm_bufs_add(struct mvpp2_port *port,
 			     struct mvpp2_bm_pool *bm_pool, int buf_num)
 {
-	int i, buf_size, total_size;
+	int i;
 	dma_addr_t dma_addr;
-	phys_addr_t phys_addr;
-	void *buf;
-
-	if (port->priv->percpu_pools &&
-	    bm_pool->pkt_size > MVPP2_BM_LONG_PKT_SIZE) {
-		netdev_err(port->dev,
-			   "attempted to use jumbo frames with per-cpu pools");
-		return 0;
-	}
-
-	buf_size = MVPP2_RX_BUF_SIZE(bm_pool->pkt_size);
-	total_size = MVPP2_RX_TOTAL_SIZE(buf_size);
 
 	if (buf_num < 0 ||
 	    (buf_num + bm_pool->buf_num > bm_pool->size)) {
@@ -724,13 +968,11 @@ static int mvpp2_bm_bufs_add(struct mvpp2_port *port,
 	}
 
 	for (i = 0; i < buf_num; i++) {
-		buf = mvpp2_buf_alloc(port, bm_pool, &dma_addr,
-				      &phys_addr, GFP_KERNEL);
-		if (!buf)
+		dma_addr = mvpp2_buf_alloc(port, bm_pool, GFP_KERNEL);
+		if (!dma_addr)
 			break;
 
-		mvpp2_bm_pool_put(port, bm_pool->id, dma_addr,
-				  phys_addr);
+		mvpp2_bm_pool_put(port, bm_pool->id, dma_addr);
 	}
 
 	/* Update BM driver with number of buffers added to pool */
@@ -738,7 +980,9 @@ static int mvpp2_bm_bufs_add(struct mvpp2_port *port,
 
 	netdev_dbg(port->dev,
 		   "pool %d: pkt_size=%4d, buf_size=%4d, total_size=%4d\n",
-		   bm_pool->id, bm_pool->pkt_size, buf_size, total_size);
+		   bm_pool->id, bm_pool->pkt_size,
+		   MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),
+		   bm_pool->frag_size);
 
 	netdev_dbg(port->dev,
 		   "pool %d: %d of %d buffers added\n",
@@ -753,10 +997,10 @@ static struct mvpp2_bm_pool *
 mvpp2_bm_pool_use(struct mvpp2_port *port, unsigned pool, int pkt_size)
 {
 	struct mvpp2_bm_pool *new_pool = &port->priv->bm_pools[pool];
+	enum mvpp2_bm_pool_type pool_type = mvpp2_bm_pool_get_type(pool);
 	int num;
 
-	if ((port->priv->percpu_pools && pool > mvpp2_get_nrxqs(port->priv) * 2) ||
-	    (!port->priv->percpu_pools && pool >= MVPP2_BM_POOLS_NUM)) {
+	if (pool >= MVPP2_BM_POOLS_NUM) {
 		netdev_err(port->dev, "Invalid pool %d\n", pool);
 		return NULL;
 	}
@@ -772,14 +1016,9 @@ mvpp2_bm_pool_use(struct mvpp2_port *port, unsigned pool, int pkt_size)
 		 */
 		pkts_num = new_pool->buf_num;
 		if (pkts_num == 0) {
-			if (port->priv->percpu_pools) {
-				if (pool < port->nrxqs)
-					pkts_num = mvpp2_pools[MVPP2_BM_SHORT].buf_num;
-				else
-					pkts_num = mvpp2_pools[MVPP2_BM_LONG].buf_num;
-			} else {
-				pkts_num = mvpp2_pools[pool].buf_num;
-			}
+			pkts_num = mvpp2_bm_pool_default_buf_num(pool_type);
+			if (pkts_num < 0)
+				return NULL;
 		} else {
 			mvpp2_bm_bufs_free(port->dev->dev.parent,
 					   port->priv, new_pool, pkts_num);
@@ -787,8 +1026,9 @@ mvpp2_bm_pool_use(struct mvpp2_port *port, unsigned pool, int pkt_size)
 
 		new_pool->pkt_size = pkt_size;
 		new_pool->frag_size =
-			SKB_DATA_ALIGN(MVPP2_RX_BUF_SIZE(pkt_size)) +
-			MVPP2_SKB_SHINFO_SIZE;
+			SKB_DATA_ALIGN(MVPP2_RX_BUF_SIZE(pkt_size +
+						MVPP2_VLAN_TAG_EDSA_LEN)) +
+						MVPP2_SKB_SHINFO_SIZE;
 
 		/* Allocate buffers for this pool */
 		num = mvpp2_bm_bufs_add(port, new_pool, pkts_num);
@@ -805,76 +1045,57 @@ mvpp2_bm_pool_use(struct mvpp2_port *port, unsigned pool, int pkt_size)
 	return new_pool;
 }
 
-static struct mvpp2_bm_pool *
-mvpp2_bm_pool_use_percpu(struct mvpp2_port *port, int type,
-			 unsigned int pool, int pkt_size)
+/* Create long pool per-CPU */
+static void mvpp2_bm_pool_pcpu_use(void *arg)
 {
-	struct mvpp2_bm_pool *new_pool = &port->priv->bm_pools[pool];
-	int num;
-
-	if (pool > port->nrxqs * 2) {
-		netdev_err(port->dev, "Invalid pool %d\n", pool);
-		return NULL;
-	}
-
-	/* Allocate buffers in case BM pool is used as long pool, but packet
-	 * size doesn't match MTU or BM pool hasn't being used yet
-	 */
-	if (new_pool->pkt_size == 0) {
-		int pkts_num;
-
-		/* Set default buffer number or free all the buffers in case
-		 * the pool is not empty
-		 */
-		pkts_num = new_pool->buf_num;
-		if (pkts_num == 0)
-			pkts_num = mvpp2_pools[type].buf_num;
-		else
-			mvpp2_bm_bufs_free(port->dev->dev.parent,
-					   port->priv, new_pool, pkts_num);
-
-		new_pool->pkt_size = pkt_size;
-		new_pool->frag_size =
-			SKB_DATA_ALIGN(MVPP2_RX_BUF_SIZE(pkt_size)) +
-			MVPP2_SKB_SHINFO_SIZE;
+	struct mvpp2_port *port = arg;
+	struct mvpp2_bm_pool **pools_pcpu = port->priv->pools_pcpu;
+	int cpu = smp_processor_id();
+	int pool_id, pkt_size;
 
-		/* Allocate buffers for this pool */
-		num = mvpp2_bm_bufs_add(port, new_pool, pkts_num);
-		if (num != pkts_num) {
-			WARN(1, "pool %d: %d of %d allocated\n",
-			     new_pool->id, num, pkts_num);
-			return NULL;
-		}
-	}
+	if (pools_pcpu[cpu])
+		return;
 
-	mvpp2_bm_pool_bufsize_set(port->priv, new_pool,
-				  MVPP2_RX_BUF_SIZE(new_pool->pkt_size));
+	pool_id = mvpp2_bm_pool_get_id(MVPP2_BM_LONG) + cpu,
+	pkt_size = mvpp2_bm_pool_default_pkt_size(MVPP2_BM_LONG);
 
-	return new_pool;
+	pools_pcpu[cpu] = mvpp2_bm_pool_use(port, pool_id, pkt_size);
 }
 
-/* Initialize pools for swf, shared buffers variant */
-static int mvpp2_swf_bm_pool_init_shared(struct mvpp2_port *port)
+/* Initialize pools for swf */
+static int mvpp2_swf_bm_pool_pcpu_init(struct mvpp2_port *port)
 {
-	enum mvpp2_bm_pool_log_num long_log_pool, short_log_pool;
-	int rxq;
+	enum mvpp2_bm_pool_type long_pool_type, short_pool_type;
+	int rxq, pkt_size, pool_id, cpu;
 
 	/* If port pkt_size is higher than 1518B:
 	 * HW Long pool - SW Jumbo pool, HW Short pool - SW Long pool
 	 * else: HW Long pool - SW Long pool, HW Short pool - SW Short pool
 	 */
 	if (port->pkt_size > MVPP2_BM_LONG_PKT_SIZE) {
-		long_log_pool = MVPP2_BM_JUMBO;
-		short_log_pool = MVPP2_BM_LONG;
+		long_pool_type = MVPP2_BM_JUMBO;
+		short_pool_type = MVPP2_BM_LONG;
 	} else {
-		long_log_pool = MVPP2_BM_LONG;
-		short_log_pool = MVPP2_BM_SHORT;
+		long_pool_type = MVPP2_BM_LONG;
+		short_pool_type = MVPP2_BM_SHORT;
 	}
 
-	if (!port->pool_long) {
-		port->pool_long =
-			mvpp2_bm_pool_use(port, long_log_pool,
-					  mvpp2_pools[long_log_pool].pkt_size);
+	/* First handle the per-CPU long pools,
+	 * as they are used in both cases.
+	 */
+	on_each_cpu(mvpp2_bm_pool_pcpu_use, port, 1);
+	/* Sanity check */
+	for_each_present_cpu(cpu) {
+		if (!port->priv->pools_pcpu[cpu])
+			return -ENOMEM;
+	}
+
+	if (!port->pool_long && long_pool_type == MVPP2_BM_JUMBO) {
+		/* HW Long pool - SW Jumbo pool */
+		pool_id = mvpp2_bm_pool_get_id(long_pool_type);
+		pkt_size = mvpp2_bm_pool_default_pkt_size(long_pool_type);
+
+		port->pool_long = mvpp2_bm_pool_use(port, pool_id, pkt_size);
 		if (!port->pool_long)
 			return -ENOMEM;
 
@@ -882,12 +1103,27 @@ static int mvpp2_swf_bm_pool_init_shared(struct mvpp2_port *port)
 
 		for (rxq = 0; rxq < port->nrxqs; rxq++)
 			mvpp2_rxq_long_pool_set(port, rxq, port->pool_long->id);
+
+		/* HW Short pool - SW Long pool (per-CPU) */
+		port->pool_short = port->priv->pools_pcpu[0];
+		for (rxq = 0; rxq < port->nrxqs; rxq++)
+			mvpp2_rxq_short_pool_set(port, rxq,
+						 port->pool_short->id + rxq);
+
+	} else if (!port->pool_long) {
+		/* HW Long pool - SW Long pool (per-CPU) */
+		port->pool_long = port->priv->pools_pcpu[0];
+		for (rxq = 0; rxq < port->nrxqs; rxq++)
+			mvpp2_rxq_long_pool_set(port, rxq,
+						port->pool_long->id + rxq);
 	}
 
 	if (!port->pool_short) {
-		port->pool_short =
-			mvpp2_bm_pool_use(port, short_log_pool,
-					  mvpp2_pools[short_log_pool].pkt_size);
+		/* HW Short pool - SW Short pool */
+		pool_id = mvpp2_bm_pool_get_id(short_pool_type);
+		pkt_size = mvpp2_bm_pool_default_pkt_size(short_pool_type);
+
+		port->pool_short = mvpp2_bm_pool_use(port, pool_id, pkt_size);
 		if (!port->pool_short)
 			return -ENOMEM;
 
@@ -898,109 +1134,141 @@ static int mvpp2_swf_bm_pool_init_shared(struct mvpp2_port *port)
 						 port->pool_short->id);
 	}
 
+	/* Fill per-CPU Long pools' port map */
+	for_each_present_cpu(cpu)
+		port->priv->pools_pcpu[cpu]->port_map |= BIT(port->id);
+
 	return 0;
 }
 
-/* Initialize pools for swf, percpu buffers variant */
-static int mvpp2_swf_bm_pool_init_percpu(struct mvpp2_port *port)
+/* Initialize pools for swf */
+static int mvpp2_swf_bm_pool_init(struct mvpp2_port *port)
 {
-	struct mvpp2_bm_pool *p;
-	int i;
-
-	for (i = 0; i < port->nrxqs; i++) {
-		p = mvpp2_bm_pool_use_percpu(port, MVPP2_BM_SHORT, i,
-					     mvpp2_pools[MVPP2_BM_SHORT].pkt_size);
-		if (!p)
-			return -ENOMEM;
+	enum mvpp2_bm_pool_type long_pool_type, short_pool_type;
+	int rxq;
 
-		port->priv->bm_pools[i].port_map |= BIT(port->id);
-		mvpp2_rxq_short_pool_set(port, i, port->priv->bm_pools[i].id);
+	/* If port pkt_size is higher than 1518B:
+	 * HW Long pool - SW Jumbo pool, HW Short pool - SW Long pool
+	 * else: HW Long pool - SW Long pool, HW Short pool - SW Short pool
+	 */
+	if (port->pkt_size > MVPP2_BM_LONG_PKT_SIZE) {
+		long_pool_type = MVPP2_BM_JUMBO;
+		short_pool_type = MVPP2_BM_LONG;
+	} else {
+		long_pool_type = MVPP2_BM_LONG;
+		short_pool_type = MVPP2_BM_SHORT;
 	}
 
-	for (i = 0; i < port->nrxqs; i++) {
-		p = mvpp2_bm_pool_use_percpu(port, MVPP2_BM_LONG, i + port->nrxqs,
-					     mvpp2_pools[MVPP2_BM_LONG].pkt_size);
-		if (!p)
+	if (!port->pool_long) {
+		port->pool_long =
+			mvpp2_bm_pool_use(port,
+					  mvpp2_bm_pool_get_id(long_pool_type),
+				mvpp2_bm_pool_default_pkt_size(long_pool_type));
+		if (!port->pool_long)
 			return -ENOMEM;
 
-		port->priv->bm_pools[i + port->nrxqs].port_map |= BIT(port->id);
-		mvpp2_rxq_long_pool_set(port, i,
-					port->priv->bm_pools[i + port->nrxqs].id);
-	}
-
-	port->pool_long = NULL;
-	port->pool_short = NULL;
+		port->pool_long->port_map |= BIT(port->id);
 
-	return 0;
-}
+		for (rxq = 0; rxq < port->nrxqs; rxq++)
+			mvpp2_rxq_long_pool_set(port, rxq, port->pool_long->id);
+	}
 
-static int mvpp2_swf_bm_pool_init(struct mvpp2_port *port)
-{
-	if (port->priv->percpu_pools)
-		return mvpp2_swf_bm_pool_init_percpu(port);
-	else
-		return mvpp2_swf_bm_pool_init_shared(port);
-}
+	if (!port->pool_short) {
+		port->pool_short =
+			mvpp2_bm_pool_use(port,
+					  mvpp2_bm_pool_get_id(short_pool_type),
+			       mvpp2_bm_pool_default_pkt_size(short_pool_type));
+		if (!port->pool_short)
+			return -ENOMEM;
 
-static void mvpp2_set_hw_csum(struct mvpp2_port *port,
-			      enum mvpp2_bm_pool_log_num new_long_pool)
-{
-	const netdev_features_t csums = NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+		port->pool_short->port_map |= BIT(port->id);
 
-	/* Update L4 checksum when jumbo enable/disable on port.
-	 * Only port 0 supports hardware checksum offload due to
-	 * the Tx FIFO size limitation.
-	 * Also, don't set NETIF_F_HW_CSUM because L3_offset in TX descriptor
-	 * has 7 bits, so the maximum L3 offset is 128.
-	 */
-	if (new_long_pool == MVPP2_BM_JUMBO && port->id != 0) {
-		port->dev->features &= ~csums;
-		port->dev->hw_features &= ~csums;
-	} else {
-		port->dev->features |= csums;
-		port->dev->hw_features |= csums;
+		for (rxq = 0; rxq < port->nrxqs; rxq++)
+			mvpp2_rxq_short_pool_set(port, rxq,
+						 port->pool_short->id);
 	}
+
+	return 0;
 }
 
 static int mvpp2_bm_update_mtu(struct net_device *dev, int mtu)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	enum mvpp2_bm_pool_log_num new_long_pool;
+	enum mvpp2_bm_pool_type new_long_pool_type;
+	struct mvpp2_bm_pool **pools_pcpu = port->priv->pools_pcpu;
 	int pkt_size = MVPP2_RX_PKT_SIZE(mtu);
-
-	if (port->priv->percpu_pools)
-		goto out_set;
+	int err, cpu;
 
 	/* If port MTU is higher than 1518B:
 	 * HW Long pool - SW Jumbo pool, HW Short pool - SW Long pool
 	 * else: HW Long pool - SW Long pool, HW Short pool - SW Short pool
 	 */
 	if (pkt_size > MVPP2_BM_LONG_PKT_SIZE)
-		new_long_pool = MVPP2_BM_JUMBO;
+		new_long_pool_type = MVPP2_BM_JUMBO;
 	else
-		new_long_pool = MVPP2_BM_LONG;
+		new_long_pool_type = MVPP2_BM_LONG;
+
+	if (new_long_pool_type != port->pool_long->type) {
+		if (port->tx_fc) {
+			if (recycle) {
+				for_each_present_cpu(cpu)
+					mvpp2_bm_pool_update_fc(port,
+								pools_pcpu[cpu],
+								false);
+			} else if (pkt_size > MVPP2_BM_LONG_PKT_SIZE)
+				mvpp2_bm_pool_update_fc(port,
+							port->pool_short,
+							false);
+			else
+				mvpp2_bm_pool_update_fc(port, port->pool_long,
+							false);
+		}
 
-	if (new_long_pool != port->pool_long->id) {
 		/* Remove port from old short & long pool */
-		port->pool_long = mvpp2_bm_pool_use(port, port->pool_long->id,
-						    port->pool_long->pkt_size);
 		port->pool_long->port_map &= ~BIT(port->id);
 		port->pool_long = NULL;
 
-		port->pool_short = mvpp2_bm_pool_use(port, port->pool_short->id,
-						     port->pool_short->pkt_size);
 		port->pool_short->port_map &= ~BIT(port->id);
 		port->pool_short = NULL;
 
 		port->pkt_size =  pkt_size;
 
 		/* Add port to new short & long pool */
-		mvpp2_swf_bm_pool_init(port);
+		if (recycle) {
+			for_each_present_cpu(cpu)
+				pools_pcpu[cpu]->port_map &= ~BIT(port->id);
+			err = mvpp2_swf_bm_pool_pcpu_init(port);
+		} else {
+			err = mvpp2_swf_bm_pool_init(port);
+		}
+		if (err)
+			return err;
+
+		if (port->tx_fc) {
+			if (recycle) {
+				for_each_present_cpu(cpu)
+					mvpp2_bm_pool_update_fc(port,
+								pools_pcpu[cpu],
+								false);
+			} else if (pkt_size > MVPP2_BM_LONG_PKT_SIZE)
+				mvpp2_bm_pool_update_fc(port, port->pool_long,
+							true);
+			else
+				mvpp2_bm_pool_update_fc(port, port->pool_short,
+							true);
+		}
 
-		mvpp2_set_hw_csum(port, new_long_pool);
+		/* Update L4 checksum when jumbo enable/disable on port */
+		if (new_long_pool_type == MVPP2_BM_JUMBO && port->id != 0) {
+			dev->features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
+			dev->hw_features &= ~(NETIF_F_IP_CSUM |
+					      NETIF_F_IPV6_CSUM);
+		} else {
+			dev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+			dev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+		}
 	}
 
-out_set:
 	dev->mtu = mtu;
 	dev->wanted_features = dev->features;
 
@@ -1061,6 +1329,9 @@ static void mvpp2_interrupts_mask(void *arg)
 	mvpp2_thread_write(port->priv,
 			   mvpp2_cpu_to_thread(port->priv, smp_processor_id()),
 			   MVPP2_ISR_RX_TX_MASK_REG(port->id), 0);
+	mvpp2_thread_write(port->priv,
+			   mvpp2_cpu_to_thread(port->priv, smp_processor_id()),
+			   MVPP2_ISR_RX_ERR_CAUSE_REG(port->id), 0);
 }
 
 /* Unmask the current thread's Rx/Tx interrupts.
@@ -1076,14 +1347,20 @@ static void mvpp2_interrupts_unmask(void *arg)
 	if (smp_processor_id() > port->priv->nthreads)
 		return;
 
-	val = MVPP2_CAUSE_MISC_SUM_MASK |
-		MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(port->priv->hw_version);
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
+	val = MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(mvpp21_variant);
 	if (port->has_tx_irqs)
 		val |= MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;
 
 	mvpp2_thread_write(port->priv,
 			   mvpp2_cpu_to_thread(port->priv, smp_processor_id()),
 			   MVPP2_ISR_RX_TX_MASK_REG(port->id), val);
+	mvpp2_thread_write(port->priv,
+			   mvpp2_cpu_to_thread(port->priv, smp_processor_id()),
+			   MVPP2_ISR_RX_ERR_CAUSE_REG(port->id),
+			   MVPP2_ISR_RX_ERR_CAUSE_NONOCC_MASK);
 }
 
 static void
@@ -1092,13 +1369,13 @@ mvpp2_shared_interrupt_mask_unmask(struct mvpp2_port *port, bool mask)
 	u32 val;
 	int i;
 
-	if (port->priv->hw_version != MVPP22)
+	if (port->priv->hw_version == MVPP21)
 		return;
 
 	if (mask)
 		val = 0;
 	else
-		val = MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(MVPP22);
+		val = MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(mvpp21_variant);
 
 	for (i = 0; i < port->nqvecs; i++) {
 		struct mvpp2_queue_vector *v = port->qvecs + i;
@@ -1108,15 +1385,13 @@ mvpp2_shared_interrupt_mask_unmask(struct mvpp2_port *port, bool mask)
 
 		mvpp2_thread_write(port->priv, v->sw_thread_id,
 				   MVPP2_ISR_RX_TX_MASK_REG(port->id), val);
+		mvpp2_thread_write(port->priv, v->sw_thread_id,
+				   MVPP2_ISR_RX_ERR_CAUSE_REG(port->id),
+				   MVPP2_ISR_RX_ERR_CAUSE_NONOCC_MASK);
 	}
 }
 
 /* Port configuration routines */
-static bool mvpp2_is_xlg(phy_interface_t interface)
-{
-	return interface == PHY_INTERFACE_MODE_10GKR ||
-	       interface == PHY_INTERFACE_MODE_XAUI;
-}
 
 static void mvpp22_gop_init_rgmii(struct mvpp2_port *port)
 {
@@ -1161,11 +1436,21 @@ static void mvpp22_gop_init_xpcs(struct mvpp2_port *port)
 	void __iomem *xpcs = priv->iface_base + MVPP22_XPCS_BASE(port->gop_id);
 	u32 val;
 
+	/* Reset the XPCS when reconfiguring the lanes */
+	val = readl(xpcs + MVPP22_XPCS_CFG0);
+	writel(val & ~MVPP22_XPCS_CFG0_RESET_DIS, xpcs + MVPP22_XPCS_CFG0);
+
+	/* XPCS */
 	val = readl(xpcs + MVPP22_XPCS_CFG0);
 	val &= ~(MVPP22_XPCS_CFG0_PCS_MODE(0x3) |
 		 MVPP22_XPCS_CFG0_ACTIVE_LANE(0x3));
 	val |= MVPP22_XPCS_CFG0_ACTIVE_LANE(2);
 	writel(val, xpcs + MVPP22_XPCS_CFG0);
+
+	/* Release lanes from reset */
+	val = readl(xpcs + MVPP22_XPCS_CFG0);
+	writel(val | MVPP22_XPCS_CFG0_RESET_DIS, xpcs + MVPP22_XPCS_CFG0);
+
 }
 
 static void mvpp22_gop_init_mpcs(struct mvpp2_port *port)
@@ -1174,14 +1459,63 @@ static void mvpp22_gop_init_mpcs(struct mvpp2_port *port)
 	void __iomem *mpcs = priv->iface_base + MVPP22_MPCS_BASE(port->gop_id);
 	u32 val;
 
+	/* MPCS */
 	val = readl(mpcs + MVPP22_MPCS_CTRL);
 	val &= ~MVPP22_MPCS_CTRL_FWD_ERR_CONN;
 	writel(val, mpcs + MVPP22_MPCS_CTRL);
 
 	val = readl(mpcs + MVPP22_MPCS_CLK_RESET);
-	val &= ~MVPP22_MPCS_CLK_RESET_DIV_RATIO(0x7);
+	val &= ~(MVPP22_MPCS_CLK_RESET_DIV_RATIO(0x7) | MAC_CLK_RESET_MAC |
+		 MAC_CLK_RESET_SD_RX | MAC_CLK_RESET_SD_TX);
 	val |= MVPP22_MPCS_CLK_RESET_DIV_RATIO(1);
 	writel(val, mpcs + MVPP22_MPCS_CLK_RESET);
+
+	val &= ~MVPP22_MPCS_CLK_RESET_DIV_SET;
+	val |= MAC_CLK_RESET_MAC | MAC_CLK_RESET_SD_RX | MAC_CLK_RESET_SD_TX;
+	writel(val, mpcs + MVPP22_MPCS_CLK_RESET);
+}
+
+static void mvpp22_gop_fca_enable_periodic(struct mvpp2_port *port, bool en)
+{
+	struct mvpp2 *priv = port->priv;
+	void __iomem *fca = priv->iface_base + MVPP22_FCA_BASE(port->gop_id);
+	u32 val;
+
+	val = readl(fca + MVPP22_FCA_CONTROL_REG);
+	val &= ~MVPP22_FCA_ENABLE_PERIODIC;
+	if (en)
+		val |= MVPP22_FCA_ENABLE_PERIODIC;
+	writel(val, fca + MVPP22_FCA_CONTROL_REG);
+}
+
+static void mvpp22_gop_fca_set_timer(struct mvpp2_port *port, u32 timer)
+{
+	struct mvpp2 *priv = port->priv;
+	void __iomem *fca = priv->iface_base + MVPP22_FCA_BASE(port->gop_id);
+	u32 lsb, msb;
+
+	lsb = timer & MVPP22_FCA_REG_MASK;
+	msb = timer >> MVPP22_FCA_REG_SIZE;
+
+	writel(lsb, fca + MVPP22_PERIODIC_COUNTER_LSB_REG);
+	writel(msb, fca + MVPP22_PERIODIC_COUNTER_MSB_REG);
+}
+
+/* Set Flow Control timer x140 faster than pause quanta to ensure that link
+ * partner won't send taffic if port in XOFF mode.
+ */
+static void mvpp22_gop_fca_set_periodic_timer(struct mvpp2_port *port)
+{
+	u32 timer;
+
+	timer = (port->priv->tclk / (USEC_PER_SEC * FC_CLK_DIVIDER))
+		* FC_QUANTA;
+
+	mvpp22_gop_fca_enable_periodic(port, false);
+
+	mvpp22_gop_fca_set_timer(port, timer);
+
+	mvpp22_gop_fca_enable_periodic(port, true);
 }
 
 static int mvpp22_gop_init(struct mvpp2_port *port)
@@ -1204,18 +1538,22 @@ static int mvpp22_gop_init(struct mvpp2_port *port)
 	case PHY_INTERFACE_MODE_SGMII:
 	case PHY_INTERFACE_MODE_1000BASEX:
 	case PHY_INTERFACE_MODE_2500BASEX:
+	case PHY_INTERFACE_MODE_2500BASET:
 		mvpp22_gop_init_sgmii(port);
 		break;
-	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
 		if (port->gop_id != 0)
 			goto invalid_conf;
 		mvpp22_gop_init_xpcs(port);
 		break;
 	case PHY_INTERFACE_MODE_10GKR:
-		if (port->gop_id != 0)
+	case PHY_INTERFACE_MODE_5GKR:
+		if (!port->has_xlg_mac)
 			goto invalid_conf;
 		mvpp22_gop_init_mpcs(port);
 		break;
+	case PHY_INTERFACE_MODE_INTERNAL:
+		return 0;
 	default:
 		goto unsupported_conf;
 	}
@@ -1233,6 +1571,8 @@ static int mvpp22_gop_init(struct mvpp2_port *port)
 	val |= GENCONF_SOFT_RESET1_GOP;
 	regmap_write(priv->sysctrl_base, GENCONF_SOFT_RESET1, val);
 
+	mvpp22_gop_fca_set_periodic_timer(port);
+
 unsupported_conf:
 	return 0;
 
@@ -1247,17 +1587,20 @@ static void mvpp22_gop_unmask_irq(struct mvpp2_port *port)
 
 	if (phy_interface_mode_is_rgmii(port->phy_interface) ||
 	    phy_interface_mode_is_8023z(port->phy_interface) ||
-	    port->phy_interface == PHY_INTERFACE_MODE_SGMII) {
+	    port->phy_interface == PHY_INTERFACE_MODE_SGMII ||
+	    port->phy_interface == PHY_INTERFACE_MODE_2500BASET) {
 		/* Enable the GMAC link status irq for this port */
 		val = readl(port->base + MVPP22_GMAC_INT_SUM_MASK);
 		val |= MVPP22_GMAC_INT_SUM_MASK_LINK_STAT;
 		writel(val, port->base + MVPP22_GMAC_INT_SUM_MASK);
 	}
 
-	if (port->gop_id == 0) {
+	if (port->has_xlg_mac) {
 		/* Enable the XLG/GIG irqs for this port */
 		val = readl(port->base + MVPP22_XLG_EXT_INT_MASK);
-		if (mvpp2_is_xlg(port->phy_interface))
+		if (port->phy_interface == PHY_INTERFACE_MODE_10GKR ||
+		    port->phy_interface == PHY_INTERFACE_MODE_5GKR ||
+		    port->phy_interface == PHY_INTERFACE_MODE_INTERNAL)
 			val |= MVPP22_XLG_EXT_INT_MASK_XLG;
 		else
 			val |= MVPP22_XLG_EXT_INT_MASK_GIG;
@@ -1269,7 +1612,7 @@ static void mvpp22_gop_mask_irq(struct mvpp2_port *port)
 {
 	u32 val;
 
-	if (port->gop_id == 0) {
+	if (port->has_xlg_mac) {
 		val = readl(port->base + MVPP22_XLG_EXT_INT_MASK);
 		val &= ~(MVPP22_XLG_EXT_INT_MASK_XLG |
 			 MVPP22_XLG_EXT_INT_MASK_GIG);
@@ -1278,7 +1621,8 @@ static void mvpp22_gop_mask_irq(struct mvpp2_port *port)
 
 	if (phy_interface_mode_is_rgmii(port->phy_interface) ||
 	    phy_interface_mode_is_8023z(port->phy_interface) ||
-	    port->phy_interface == PHY_INTERFACE_MODE_SGMII) {
+	    port->phy_interface == PHY_INTERFACE_MODE_SGMII ||
+	    port->phy_interface == PHY_INTERFACE_MODE_2500BASET) {
 		val = readl(port->base + MVPP22_GMAC_INT_SUM_MASK);
 		val &= ~MVPP22_GMAC_INT_SUM_MASK_LINK_STAT;
 		writel(val, port->base + MVPP22_GMAC_INT_SUM_MASK);
@@ -1298,7 +1642,7 @@ static void mvpp22_gop_setup_irq(struct mvpp2_port *port)
 		writel(val, port->base + MVPP22_GMAC_INT_MASK);
 	}
 
-	if (port->gop_id == 0) {
+	if (port->has_xlg_mac) {
 		val = readl(port->base + MVPP22_XLG_INT_MASK);
 		val |= MVPP22_XLG_INT_MASK_LINK;
 		writel(val, port->base + MVPP22_XLG_INT_MASK);
@@ -1336,10 +1680,16 @@ static void mvpp2_port_enable(struct mvpp2_port *port)
 {
 	u32 val;
 
-	/* Only GOP port 0 has an XLG MAC */
-	if (port->gop_id == 0 && mvpp2_is_xlg(port->phy_interface)) {
+	if (port->phy_interface == PHY_INTERFACE_MODE_INTERNAL)
+		return;
+
+	if (port->has_xlg_mac &&
+	    (port->phy_interface == PHY_INTERFACE_MODE_RXAUI ||
+	     port->phy_interface == PHY_INTERFACE_MODE_10GKR ||
+	     port->phy_interface == PHY_INTERFACE_MODE_5GKR)) {
 		val = readl(port->base + MVPP22_XLG_CTRL0_REG);
-		val |= MVPP22_XLG_CTRL0_PORT_EN;
+		val |= MVPP22_XLG_CTRL0_PORT_EN |
+		       MVPP22_XLG_CTRL0_MAC_RESET_DIS;
 		val &= ~MVPP22_XLG_CTRL0_MIB_CNT_DIS;
 		writel(val, port->base + MVPP22_XLG_CTRL0_REG);
 	} else {
@@ -1354,16 +1704,21 @@ static void mvpp2_port_disable(struct mvpp2_port *port)
 {
 	u32 val;
 
-	/* Only GOP port 0 has an XLG MAC */
-	if (port->gop_id == 0 && mvpp2_is_xlg(port->phy_interface)) {
+	if (port->phy_interface == PHY_INTERFACE_MODE_INTERNAL)
+		return;
+
+	if (port->has_xlg_mac &&
+	    (port->phy_interface == PHY_INTERFACE_MODE_RXAUI ||
+	     port->phy_interface == PHY_INTERFACE_MODE_10GKR ||
+	     port->phy_interface == PHY_INTERFACE_MODE_5GKR)) {
 		val = readl(port->base + MVPP22_XLG_CTRL0_REG);
 		val &= ~MVPP22_XLG_CTRL0_PORT_EN;
 		writel(val, port->base + MVPP22_XLG_CTRL0_REG);
+	} else {
+		val = readl(port->base + MVPP2_GMAC_CTRL_0_REG);
+		val &= ~(MVPP2_GMAC_PORT_EN_MASK);
+		writel(val, port->base + MVPP2_GMAC_CTRL_0_REG);
 	}
-
-	val = readl(port->base + MVPP2_GMAC_CTRL_0_REG);
-	val &= ~(MVPP2_GMAC_PORT_EN_MASK);
-	writel(val, port->base + MVPP2_GMAC_CTRL_0_REG);
 }
 
 /* Set IEEE 802.3x Flow Control Xon Packet Transmission Mode */
@@ -1416,17 +1771,6 @@ static u64 mvpp2_read_count(struct mvpp2_port *port,
 	return val;
 }
 
-/* Some counters are accessed indirectly by first writing an index to
- * MVPP2_CTRS_IDX. The index can represent various resources depending on the
- * register we access, it can be a hit counter for some classification tables,
- * a counter specific to a rxq, a txq or a buffer pool.
- */
-static u32 mvpp2_read_index(struct mvpp2 *priv, u32 index, u32 reg)
-{
-	mvpp2_write(priv, MVPP2_CTRS_IDX, index);
-	return mvpp2_read(priv, reg);
-}
-
 /* Due to the fact that software statistics and hardware statistics are, by
  * design, incremented at different moments in the chain of packet processing,
  * it is very likely that incoming packets could have been dropped after being
@@ -1436,7 +1780,7 @@ static u32 mvpp2_read_index(struct mvpp2 *priv, u32 index, u32 reg)
  * Hence, statistics gathered from userspace with ifconfig (software) and
  * ethtool (hardware) cannot be compared.
  */
-static const struct mvpp2_ethtool_counter mvpp2_ethtool_mib_regs[] = {
+static const struct mvpp2_ethtool_counter mvpp2_ethtool_regs[] = {
 	{ MVPP2_MIB_GOOD_OCTETS_RCVD, "good_octets_received", true },
 	{ MVPP2_MIB_BAD_OCTETS_RCVD, "bad_octets_received" },
 	{ MVPP2_MIB_CRC_ERRORS_SENT, "crc_errors_sent" },
@@ -1457,112 +1801,149 @@ static const struct mvpp2_ethtool_counter mvpp2_ethtool_mib_regs[] = {
 	{ MVPP2_MIB_FC_RCVD, "fc_received" },
 	{ MVPP2_MIB_RX_FIFO_OVERRUN, "rx_fifo_overrun" },
 	{ MVPP2_MIB_UNDERSIZE_RCVD, "undersize_received" },
-	{ MVPP2_MIB_FRAGMENTS_RCVD, "fragments_received" },
+	{ MVPP2_MIB_FRAGMENTS_ERR_RCVD, "fragments_err_received" },
 	{ MVPP2_MIB_OVERSIZE_RCVD, "oversize_received" },
 	{ MVPP2_MIB_JABBER_RCVD, "jabber_received" },
 	{ MVPP2_MIB_MAC_RCV_ERROR, "mac_receive_error" },
 	{ MVPP2_MIB_BAD_CRC_EVENT, "bad_crc_event" },
 	{ MVPP2_MIB_COLLISION, "collision" },
 	{ MVPP2_MIB_LATE_COLLISION, "late_collision" },
+#define MVPP2_LAST_MIB		MVPP2_MIB_LATE_COLLISION
+
+	/* Extend counters */
+	{ MVPP2_OVERRUN_DROP_REG(0),	"rx_ppv2_overrun" },
+	{ MVPP2_CLS_DROP_REG(0),	"rx_cls_drop" },
+	{ MVPP2_RX_PKT_FULLQ_DROP_REG,	"rx_fullq_drop" },
+	{ MVPP2_RX_PKT_EARLY_DROP_REG,	"rx_early_drop" },
+	{ MVPP2_RX_PKT_BM_DROP_REG,	"rx_bm_drop" },
+
+	/* Extend SW counters (not registers) */
+#define MVPP2_FIRST_CNT_SW		0xf000
+#define MVPP2_TX_GUARD_CNT(cpu)	(MVPP2_FIRST_CNT_SW + cpu)
+	{ MVPP2_TX_GUARD_CNT(0),	"tx-guard-cpu0" },
+	{ MVPP2_TX_GUARD_CNT(1),	"tx-guard-cpu1" },
+	{ MVPP2_TX_GUARD_CNT(2),	"tx-guard-cpu2" },
+	{ MVPP2_TX_GUARD_CNT(3),	"tx-guard-cpu3" },
 };
 
-static const struct mvpp2_ethtool_counter mvpp2_ethtool_port_regs[] = {
-	{ MVPP2_OVERRUN_ETH_DROP, "rx_fifo_or_parser_overrun_drops" },
-	{ MVPP2_CLS_ETH_DROP, "rx_classifier_drops" },
+static const char mvpp22_priv_flags_strings[][ETH_GSTRING_LEN] = {
+	"musdk",
 };
 
-static const struct mvpp2_ethtool_counter mvpp2_ethtool_txq_regs[] = {
-	{ MVPP2_TX_DESC_ENQ_CTR, "txq_%d_desc_enqueue" },
-	{ MVPP2_TX_DESC_ENQ_TO_DDR_CTR, "txq_%d_desc_enqueue_to_ddr" },
-	{ MVPP2_TX_BUFF_ENQ_TO_DDR_CTR, "txq_%d_buff_euqueue_to_ddr" },
-	{ MVPP2_TX_DESC_ENQ_HW_FWD_CTR, "txq_%d_desc_hardware_forwarded" },
-	{ MVPP2_TX_PKTS_DEQ_CTR, "txq_%d_packets_dequeued" },
-	{ MVPP2_TX_PKTS_FULL_QUEUE_DROP_CTR, "txq_%d_queue_full_drops" },
-	{ MVPP2_TX_PKTS_EARLY_DROP_CTR, "txq_%d_packets_early_drops" },
-	{ MVPP2_TX_PKTS_BM_DROP_CTR, "txq_%d_packets_bm_drops" },
-	{ MVPP2_TX_PKTS_BM_MC_DROP_CTR, "txq_%d_packets_rep_bm_drops" },
-};
+#define MVPP22_F_IF_MUSDK_PRIV	BIT(0)
 
-static const struct mvpp2_ethtool_counter mvpp2_ethtool_rxq_regs[] = {
-	{ MVPP2_RX_DESC_ENQ_CTR, "rxq_%d_desc_enqueue" },
-	{ MVPP2_RX_PKTS_FULL_QUEUE_DROP_CTR, "rxq_%d_queue_full_drops" },
-	{ MVPP2_RX_PKTS_EARLY_DROP_CTR, "rxq_%d_packets_early_drops" },
-	{ MVPP2_RX_PKTS_BM_DROP_CTR, "rxq_%d_packets_bm_drops" },
-};
+static int mvpp2_ethtool_get_mib_cntr_size(void)
+{
+	int i = 0;
 
-#define MVPP2_N_ETHTOOL_STATS(ntxqs, nrxqs)	(ARRAY_SIZE(mvpp2_ethtool_mib_regs) + \
-						 ARRAY_SIZE(mvpp2_ethtool_port_regs) + \
-						 (ARRAY_SIZE(mvpp2_ethtool_txq_regs) * (ntxqs)) + \
-						 (ARRAY_SIZE(mvpp2_ethtool_rxq_regs) * (nrxqs)))
+	while (i < ARRAY_SIZE(mvpp2_ethtool_regs)) {
+		if (mvpp2_ethtool_regs[i++].offset == MVPP2_LAST_MIB)
+			break;
+	}
+	return i; /* mib_size */
+}
 
-static void mvpp2_ethtool_get_strings(struct net_device *netdev, u32 sset,
-				      u8 *data)
+static int mvpp2_ethtool_get_cntr_index(u32 offset)
 {
-	struct mvpp2_port *port = netdev_priv(netdev);
-	int i, q;
-
-	if (sset != ETH_SS_STATS)
-		return;
+	int i = 0;
 
-	for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_mib_regs); i++) {
-		strscpy(data, mvpp2_ethtool_mib_regs[i].string,
-			ETH_GSTRING_LEN);
-		data += ETH_GSTRING_LEN;
+	while (i < ARRAY_SIZE(mvpp2_ethtool_regs)) {
+		if (mvpp2_ethtool_regs[i].offset == offset)
+			break;
+		i++;
 	}
+	return i;
+}
 
-	for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_port_regs); i++) {
-		strscpy(data, mvpp2_ethtool_port_regs[i].string,
-			ETH_GSTRING_LEN);
-		data += ETH_GSTRING_LEN;
-	}
+/* hw_get_stats - update the ethtool_stats accumulator from HW-registers
+ * The HW-registers/counters are cleared on read.
+ */
+static void mvpp2_hw_get_stats(struct mvpp2_port *port, u64 *pstats)
+{
+	int i, mib_size, queue, cpu;
+	unsigned int reg_offs;
+	u32 val, cls_drops;
+	u64 *ptmp;
 
-	for (q = 0; q < port->ntxqs; q++) {
-		for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_txq_regs); i++) {
-			snprintf(data, ETH_GSTRING_LEN,
-				 mvpp2_ethtool_txq_regs[i].string, q);
-			data += ETH_GSTRING_LEN;
-		}
-	}
+	mib_size = mvpp2_ethtool_get_mib_cntr_size();
 
-	for (q = 0; q < port->nrxqs; q++) {
-		for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_rxq_regs); i++) {
-			snprintf(data, ETH_GSTRING_LEN,
-				 mvpp2_ethtool_rxq_regs[i].string,
-				 q);
-			data += ETH_GSTRING_LEN;
+	cls_drops = mvpp2_read(port->priv, MVPP2_OVERRUN_DROP_REG(port->id));
+
+	for (i = 0; i < mib_size; i++) {
+		if (mvpp2_ethtool_regs[i].offset == MVPP2_MIB_COLLISION) {
+			val = mvpp2_read_count(port, &mvpp2_ethtool_regs[i]);
+			port->dev->stats.collisions += val;
+			*pstats++ += val;
+			continue;
+		}
+		*pstats++ += mvpp2_read_count(port, &mvpp2_ethtool_regs[i]);
+	}
+
+	/* Extend HW counters */
+	*pstats++ += cls_drops;
+	*pstats++ += mvpp2_read(port->priv, MVPP2_CLS_DROP_REG(port->id));
+	ptmp = pstats;
+	queue = port->first_rxq;
+	while (queue < (port->first_rxq + port->nrxqs)) {
+		mvpp2_write(port->priv, MVPP2_CNT_IDX_REG, queue++);
+		pstats = ptmp;
+		i = mib_size + 2;
+		while (i < ARRAY_SIZE(mvpp2_ethtool_regs)) {
+			reg_offs = mvpp2_ethtool_regs[i++].offset;
+			if (reg_offs == MVPP2_FIRST_CNT_SW)
+				break;
+			*pstats++ += mvpp2_read(port->priv, reg_offs);
 		}
 	}
+
+	/* Extend SW counters (i=MVPP2_FIRST_CNT_SW) */
+	for_each_present_cpu(cpu)
+		*pstats++ = mvpp2_tx_done_guard_get_stats(port, cpu);
 }
 
-static void mvpp2_read_stats(struct mvpp2_port *port)
+static void mvpp2_hw_clear_stats(struct mvpp2_port *port)
 {
-	u64 *pstats;
-	int i, q;
+	int i, mib_size, queue;
+	unsigned int reg_offs;
 
-	pstats = port->ethtool_stats;
+	mib_size = mvpp2_ethtool_get_mib_cntr_size();
 
-	for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_mib_regs); i++)
-		*pstats++ += mvpp2_read_count(port, &mvpp2_ethtool_mib_regs[i]);
+	for (i = 0; i < mib_size; i++)
+		mvpp2_read_count(port, &mvpp2_ethtool_regs[i]);
 
-	for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_port_regs); i++)
-		*pstats++ += mvpp2_read(port->priv,
-					mvpp2_ethtool_port_regs[i].offset +
-					4 * port->id);
+	/* Extend counters */
+	mvpp2_read(port->priv, MVPP2_OVERRUN_DROP_REG(port->id));
+	mvpp2_read(port->priv, MVPP2_CLS_DROP_REG(port->id));
+	queue = port->first_rxq;
+	while (queue < (port->first_rxq + port->nrxqs)) {
+		mvpp2_write(port->priv, MVPP2_CNT_IDX_REG, queue++);
+		i = mib_size + 2;
+		while (i < ARRAY_SIZE(mvpp2_ethtool_regs)) {
+			reg_offs = mvpp2_ethtool_regs[i++].offset;
+			if (reg_offs == MVPP2_FIRST_CNT_SW)
+				break;
+			mvpp2_read(port->priv, reg_offs);
+		}
+	}
+	/* Extend SW counters (i=MVPP2_FIRST_CNT_SW) */
+	/* no clear */
+}
 
-	for (q = 0; q < port->ntxqs; q++)
-		for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_txq_regs); i++)
-			*pstats++ += mvpp2_read_index(port->priv,
-						      MVPP22_CTRS_TX_CTR(port->id, q),
-						      mvpp2_ethtool_txq_regs[i].offset);
+static void mvpp2_ethtool_get_strings(struct net_device *netdev, u32 sset,
+				      u8 *data)
+{
+	int i;
 
-	/* Rxqs are numbered from 0 from the user standpoint, but not from the
-	 * driver's. We need to add the  port->first_rxq offset.
-	 */
-	for (q = 0; q < port->nrxqs; q++)
-		for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_rxq_regs); i++)
-			*pstats++ += mvpp2_read_index(port->priv,
-						      port->first_rxq + q,
-						      mvpp2_ethtool_rxq_regs[i].offset);
+	switch (sset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_regs); i++)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       &mvpp2_ethtool_regs[i].string, ETH_GSTRING_LEN);
+		break;
+	case ETH_SS_PRIV_FLAGS:
+		memcpy(data, mvpp22_priv_flags_strings,
+		       ARRAY_SIZE(mvpp22_priv_flags_strings) * ETH_GSTRING_LEN);
+	}
 }
 
 static void mvpp2_gather_hw_statistics(struct work_struct *work)
@@ -1571,109 +1952,69 @@ static void mvpp2_gather_hw_statistics(struct work_struct *work)
 	struct mvpp2_port *port = container_of(del_work, struct mvpp2_port,
 					       stats_work);
 
+	/* Update the statistic buffer by q-work only, not by ethtool-S */
 	mutex_lock(&port->gather_stats_lock);
-
-	mvpp2_read_stats(port);
-
-	/* No need to read again the counters right after this function if it
-	 * was called asynchronously by the user (ie. use of ethtool).
-	 */
-	cancel_delayed_work(&port->stats_work);
+	mvpp2_hw_get_stats(port, port->ethtool_stats);
+	mutex_unlock(&port->gather_stats_lock);
 	queue_delayed_work(port->priv->stats_queue, &port->stats_work,
 			   MVPP2_MIB_COUNTERS_STATS_DELAY);
-
-	mutex_unlock(&port->gather_stats_lock);
 }
 
 static void mvpp2_ethtool_get_stats(struct net_device *dev,
 				    struct ethtool_stats *stats, u64 *data)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
+	int cls_drp, fc_rcv;
 
-	/* Update statistics for the given port, then take the lock to avoid
-	 * concurrent accesses on the ethtool_stats structure during its copy.
+	/* Use statistic already accumulated in ethtool_stats by q-work
+	 * and copy under mutex-lock it into given ethtool-data-buffer.
 	 */
-	mvpp2_gather_hw_statistics(&port->stats_work.work);
-
 	mutex_lock(&port->gather_stats_lock);
 	memcpy(data, port->ethtool_stats,
-	       sizeof(u64) * MVPP2_N_ETHTOOL_STATS(port->ntxqs, port->nrxqs));
+	       sizeof(u64) * ARRAY_SIZE(mvpp2_ethtool_regs));
 	mutex_unlock(&port->gather_stats_lock);
+
+	/* Do not count flow control receive frames as classifier drops */
+	cls_drp = mvpp2_ethtool_get_cntr_index(MVPP2_CLS_DROP_REG(0));
+	fc_rcv = mvpp2_ethtool_get_cntr_index(MVPP2_MIB_FC_RCVD);
+	data[cls_drp] =
+		data[fc_rcv] > data[cls_drp] ? 0 : data[cls_drp] - data[fc_rcv];
 }
 
 static int mvpp2_ethtool_get_sset_count(struct net_device *dev, int sset)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
 
-	if (sset == ETH_SS_STATS)
-		return MVPP2_N_ETHTOOL_STATS(port->ntxqs, port->nrxqs);
-
+	switch (sset) {
+	case ETH_SS_STATS:
+		return ARRAY_SIZE(mvpp2_ethtool_regs);
+	case ETH_SS_PRIV_FLAGS:
+		return (port->priv->hw_version == MVPP21) ?
+			0 : ARRAY_SIZE(mvpp22_priv_flags_strings);
+	}
 	return -EOPNOTSUPP;
 }
 
-static void mvpp2_mac_reset_assert(struct mvpp2_port *port)
+static void mvpp2_port_reset(struct mvpp2_port *port)
 {
 	u32 val;
 
+	/* Read the GOP statistics to reset the hardware counters */
+	mvpp2_hw_clear_stats(port);
+
 	val = readl(port->base + MVPP2_GMAC_CTRL_2_REG) |
 	      MVPP2_GMAC_PORT_RESET_MASK;
 	writel(val, port->base + MVPP2_GMAC_CTRL_2_REG);
 
-	if (port->priv->hw_version == MVPP22 && port->gop_id == 0) {
+	if (port->has_xlg_mac) {
+		/* Set the XLG MAC in reset */
 		val = readl(port->base + MVPP22_XLG_CTRL0_REG) &
 		      ~MVPP22_XLG_CTRL0_MAC_RESET_DIS;
 		writel(val, port->base + MVPP22_XLG_CTRL0_REG);
-	}
-}
-
-static void mvpp22_pcs_reset_assert(struct mvpp2_port *port)
-{
-	struct mvpp2 *priv = port->priv;
-	void __iomem *mpcs, *xpcs;
-	u32 val;
-
-	if (port->priv->hw_version != MVPP22 || port->gop_id != 0)
-		return;
-
-	mpcs = priv->iface_base + MVPP22_MPCS_BASE(port->gop_id);
-	xpcs = priv->iface_base + MVPP22_XPCS_BASE(port->gop_id);
-
-	val = readl(mpcs + MVPP22_MPCS_CLK_RESET);
-	val &= ~(MAC_CLK_RESET_MAC | MAC_CLK_RESET_SD_RX | MAC_CLK_RESET_SD_TX);
-	val |= MVPP22_MPCS_CLK_RESET_DIV_SET;
-	writel(val, mpcs + MVPP22_MPCS_CLK_RESET);
-
-	val = readl(xpcs + MVPP22_XPCS_CFG0);
-	writel(val & ~MVPP22_XPCS_CFG0_RESET_DIS, xpcs + MVPP22_XPCS_CFG0);
-}
-
-static void mvpp22_pcs_reset_deassert(struct mvpp2_port *port)
-{
-	struct mvpp2 *priv = port->priv;
-	void __iomem *mpcs, *xpcs;
-	u32 val;
-
-	if (port->priv->hw_version != MVPP22 || port->gop_id != 0)
-		return;
-
-	mpcs = priv->iface_base + MVPP22_MPCS_BASE(port->gop_id);
-	xpcs = priv->iface_base + MVPP22_XPCS_BASE(port->gop_id);
 
-	switch (port->phy_interface) {
-	case PHY_INTERFACE_MODE_10GKR:
-		val = readl(mpcs + MVPP22_MPCS_CLK_RESET);
-		val |= MAC_CLK_RESET_MAC | MAC_CLK_RESET_SD_RX |
-		       MAC_CLK_RESET_SD_TX;
-		val &= ~MVPP22_MPCS_CLK_RESET_DIV_SET;
-		writel(val, mpcs + MVPP22_MPCS_CLK_RESET);
-		break;
-	case PHY_INTERFACE_MODE_XAUI:
-	case PHY_INTERFACE_MODE_RXAUI:
-		val = readl(xpcs + MVPP22_XPCS_CFG0);
-		writel(val | MVPP22_XPCS_CFG0_RESET_DIS, xpcs + MVPP22_XPCS_CFG0);
-		break;
-	default:
-		break;
+		while (readl(port->base + MVPP22_XLG_CTRL0_REG) &
+		       MVPP22_XLG_CTRL0_MAC_RESET_DIS)
+			continue;
 	}
 }
 
@@ -1682,6 +2023,9 @@ static inline void mvpp2_gmac_max_rx_size_set(struct mvpp2_port *port)
 {
 	u32 val;
 
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
 	val = readl(port->base + MVPP2_GMAC_CTRL_0_REG);
 	val &= ~MVPP2_GMAC_MAX_RX_SIZE_MASK;
 	val |= (((port->pkt_size - MVPP2_MH_SIZE) / 2) <<
@@ -1694,6 +2038,9 @@ static inline void mvpp2_xlg_max_rx_size_set(struct mvpp2_port *port)
 {
 	u32 val;
 
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
 	val =  readl(port->base + MVPP22_XLG_CTRL1_REG);
 	val &= ~MVPP22_XLG_CTRL1_FRAMESIZELIMIT_MASK;
 	val |= ((port->pkt_size - MVPP2_MH_SIZE) / 2) <<
@@ -1701,19 +2048,41 @@ static inline void mvpp2_xlg_max_rx_size_set(struct mvpp2_port *port)
 	writel(val, port->base + MVPP22_XLG_CTRL1_REG);
 }
 
+static void mvpp2_gmac_tx_fifo_configure(struct mvpp2_port *port)
+{
+	u32 val, tx_fifo_min_th;
+	u8 low_wm, hi_wm;
+
+	tx_fifo_min_th = MVPP2_GMAC_TX_FIFO_MIN_TH;
+	low_wm = MVPP2_GMAC_TX_FIFO_LOW_WM;
+	hi_wm = MVPP2_GMAC_TX_FIFO_HI_WM;
+
+	/* Update TX FIFO MIN Threshold */
+	val = readl(port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);
+	val &= ~MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK;
+	val |= tx_fifo_min_th;
+	writel(val, port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);
+
+	/* Update TX FIFO levels of assertion/deassertion
+	 * of p2mem_ready_signal, which indicates readiness
+	 * for fetching the data from DRAM.
+	 */
+	val = readl(port->base + MVPP2_GMAC_PORT_FIFO_CFG_0_REG);
+	val &= ~MVPP2_GMAC_TX_FIFO_WM_MASK;
+	val |= (low_wm << MVPP2_GMAC_TX_FIFO_WM_LOW_OFFSET) | hi_wm;
+	writel(val, port->base + MVPP2_GMAC_PORT_FIFO_CFG_0_REG);
+}
+
 /* Set defaults to the MVPP2 port */
 static void mvpp2_defaults_set(struct mvpp2_port *port)
 {
-	int tx_port_num, val, queue, lrxq;
+	int tx_port_num, val, queue, ptxq, lrxq;
 
-	if (port->priv->hw_version == MVPP21) {
-		/* Update TX FIFO MIN Threshold */
-		val = readl(port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);
-		val &= ~MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK;
-		/* Min. TX threshold must be less than minimal packet length */
-		val |= MVPP2_GMAC_TX_FIFO_MIN_TH_MASK(64 - 4 - 2);
-		writel(val, port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);
-	}
+	if (phy_interface_mode_is_rgmii(port->phy_interface) ||
+	    port->phy_interface == PHY_INTERFACE_MODE_SGMII ||
+	    port->phy_interface == PHY_INTERFACE_MODE_1000BASEX ||
+	    port->phy_interface == PHY_INTERFACE_MODE_2500BASEX)
+		mvpp2_gmac_tx_fifo_configure(port);
 
 	/* Disable Legacy WRR, Disable EJP, Release from reset */
 	tx_port_num = mvpp2_egress_port(port);
@@ -1725,9 +2094,11 @@ static void mvpp2_defaults_set(struct mvpp2_port *port)
 	mvpp2_write(port->priv, MVPP2_TXP_SCHED_FIXED_PRIO_REG, 0);
 
 	/* Close bandwidth for all queues */
-	for (queue = 0; queue < MVPP2_MAX_TXQ; queue++)
+	for (queue = 0; queue < MVPP2_MAX_TXQ; queue++) {
+		ptxq = mvpp2_txq_phys(port->id, queue);
 		mvpp2_write(port->priv,
-			    MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(queue), 0);
+			    MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(ptxq), 0);
+	}
 
 	/* Set refill period to 1 usec, refill tokens
 	 * and bucket size to maximum
@@ -1796,6 +2167,9 @@ static void mvpp2_egress_enable(struct mvpp2_port *port)
 	int queue;
 	int tx_port_num = mvpp2_egress_port(port);
 
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
 	/* Enable all initialized TXs. */
 	qmap = 0;
 	for (queue = 0; queue < port->ntxqs; queue++) {
@@ -1818,6 +2192,9 @@ static void mvpp2_egress_disable(struct mvpp2_port *port)
 	int delay;
 	int tx_port_num = mvpp2_egress_port(port);
 
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
 	/* Issue stop command for active channels only */
 	mvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);
 	reg_data = (mvpp2_read(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG)) &
@@ -1920,9 +2297,13 @@ mvpp2_txq_next_desc_get(struct mvpp2_tx_queue *txq)
  */
 static void mvpp2_aggr_txq_pend_desc_add(struct mvpp2_port *port, int pending)
 {
+	int cpu = smp_processor_id();
+
+	mvpp2_tx_done_guard_timer_set(port, cpu);
+
 	/* aggregated access - relevant TXQ number is written in TX desc */
 	mvpp2_thread_write(port->priv,
-			   mvpp2_cpu_to_thread(port->priv, smp_processor_id()),
+			   mvpp2_cpu_to_thread(port->priv, cpu),
 			   MVPP2_AGGR_TXQ_UPDATE_REG, pending);
 }
 
@@ -1979,8 +2360,9 @@ static int mvpp2_txq_reserved_desc_num_proc(struct mvpp2_port *port,
 					    struct mvpp2_txq_pcpu *txq_pcpu,
 					    int num)
 {
-	int req, desc_count;
 	unsigned int thread;
+	int req, desc_count;
+	struct mvpp2_txq_pcpu *txq_pcpu_aux;
 
 	if (txq_pcpu->reserved_num >= num)
 		return 0;
@@ -1988,27 +2370,24 @@ static int mvpp2_txq_reserved_desc_num_proc(struct mvpp2_port *port,
 	/* Not enough descriptors reserved! Update the reserved descriptor
 	 * count and check again.
 	 */
-
-	desc_count = 0;
-	/* Compute total of used descriptors */
-	for (thread = 0; thread < port->priv->nthreads; thread++) {
-		struct mvpp2_txq_pcpu *txq_pcpu_aux;
-
-		txq_pcpu_aux = per_cpu_ptr(txq->pcpu, thread);
-		desc_count += txq_pcpu_aux->count;
-		desc_count += txq_pcpu_aux->reserved_num;
+	if (num <= MAX_SKB_FRAGS) {
+		req = MVPP2_CPU_DESC_CHUNK;
+	} else {
+		/* Compute total of used descriptors */
+		desc_count = 0;
+		for (thread = 0; thread < port->priv->nthreads; thread++) {
+			txq_pcpu_aux = per_cpu_ptr(txq->pcpu, thread);
+			desc_count += txq_pcpu_aux->reserved_num;
+		}
+		req = max(MVPP2_CPU_DESC_CHUNK, num - txq_pcpu->reserved_num);
+		/* Check the reservation is possible */
+		if ((desc_count + req) > txq->size)
+			return -ENOMEM;
 	}
 
-	req = max(MVPP2_CPU_DESC_CHUNK, num - txq_pcpu->reserved_num);
-	desc_count += req;
-
-	if (desc_count >
-	   (txq->size - (MVPP2_MAX_THREADS * MVPP2_CPU_DESC_CHUNK)))
-		return -ENOMEM;
-
 	txq_pcpu->reserved_num += mvpp2_txq_alloc_reserved_desc(port, txq, req);
 
-	/* OK, the descriptor could have been updated: check again. */
+	/* Check the resulting reservation is enough */
 	if (txq_pcpu->reserved_num < num)
 		return -ENOMEM;
 	return 0;
@@ -2101,6 +2480,107 @@ static void mvpp2_txq_sent_counter_clear(void *arg)
 	}
 }
 
+/* Avoid wrong tx_done calling for netif_tx_wake at time of
+ * dev-stop or linkDown processing by flag MVPP2_F_IF_TX_ON.
+ * Set/clear it on each cpu.
+ */
+static inline bool mvpp2_tx_stopped(struct mvpp2_port *port)
+{
+	return !(port->flags & MVPP2_F_IF_TX_ON);
+}
+
+static void mvpp2_txqs_on(void *arg)
+{
+	((struct mvpp2_port *)arg)->flags |= MVPP2_F_IF_TX_ON;
+}
+
+static void mvpp2_txqs_off(void *arg)
+{
+	((struct mvpp2_port *)arg)->flags &= ~MVPP2_F_IF_TX_ON;
+}
+
+static void mvpp2_txqs_on_tasklet_cb(unsigned long data)
+{
+	/* Activated/runs on 1 cpu only (with link_status_irq)
+	 * to update/guarantee TX_ON coherency on other cpus
+	 */
+	struct mvpp2_port *port = (struct mvpp2_port *)data;
+
+	if (mvpp2_tx_stopped(port))
+		on_each_cpu(mvpp2_txqs_off, port, 1);
+	else
+		on_each_cpu(mvpp2_txqs_on, port, 1);
+}
+
+static void mvpp2_txqs_on_tasklet_init(struct mvpp2_port *port)
+{
+	/* Init called only for port with link_status_isr */
+	tasklet_init(&port->txqs_on_tasklet,
+		     mvpp2_txqs_on_tasklet_cb,
+		     (unsigned long)port);
+}
+
+static void mvpp2_txqs_on_tasklet_kill(struct mvpp2_port *port)
+{
+	if (port->txqs_on_tasklet.func)
+		tasklet_kill(&port->txqs_on_tasklet);
+}
+
+/* Use mvpp2 APIs instead of netif_TX_ALL:
+ *  netif_tx_start_all_queues -> mvpp2_tx_start_all_queues
+ *  netif_tx_wake_all_queues  -> mvpp2_tx_wake_all_queues
+ *  netif_tx_stop_all_queues  -> mvpp2_tx_stop_all_queues
+ * But keep using per-queue APIs netif_tx_wake_queue,
+ *  netif_tx_stop_queue and netif_tx_queue_stopped.
+ */
+static void mvpp2_tx_start_all_queues(struct net_device *dev)
+{
+	struct mvpp2_port *port = netdev_priv(dev);
+
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
+	/* Never called from IRQ. Update all cpus directly */
+	on_each_cpu(mvpp2_txqs_on, port, 1);
+	netif_tx_start_all_queues(dev);
+}
+
+static void mvpp2_tx_wake_all_queues(struct net_device *dev)
+{
+	struct mvpp2_port *port = netdev_priv(dev);
+
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
+	if (irqs_disabled()) {
+		/* Link-status IRQ context (also ACPI).
+		 * Set for THIS cpu, update other cpus over tasklet
+		 */
+		mvpp2_txqs_on((void *)port);
+		tasklet_schedule(&port->txqs_on_tasklet);
+	} else {
+		on_each_cpu(mvpp2_txqs_on, port, 1);
+	}
+	netif_tx_wake_all_queues(dev);
+}
+
+static void mvpp2_tx_stop_all_queues(struct net_device *dev)
+{
+	struct mvpp2_port *port = netdev_priv(dev);
+
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		return;
+
+	if (irqs_disabled()) {
+		/* IRQ context. Set for THIS, update other cpus over tasklet */
+		mvpp2_txqs_off((void *)port);
+		tasklet_schedule(&port->txqs_on_tasklet);
+	} else {
+		on_each_cpu(mvpp2_txqs_off, port, 1);
+	}
+	netif_tx_stop_all_queues(dev);
+}
+
 /* Set max sizes for Tx queues */
 static void mvpp2_txp_max_tx_size_set(struct mvpp2_port *port)
 {
@@ -2150,6 +2630,22 @@ static void mvpp2_txp_max_tx_size_set(struct mvpp2_port *port)
 	}
 }
 
+/* Routine set the number of non-occupied descriptors threshold that change
+ * interrupt error cause polled by FW Flow Control
+ */
+void mvpp2_set_rxq_free_tresh(struct mvpp2_port *port,
+			      struct mvpp2_rx_queue *rxq)
+{
+	u32 val;
+
+	mvpp2_write(port->priv, MVPP2_RXQ_NUM_REG, rxq->id);
+
+	val = mvpp2_read(port->priv, MVPP2_RXQ_THRESH_REG);
+	val &= ~MVPP2_RXQ_NON_OCCUPIED_MASK;
+	val |= MSS_THRESHOLD_STOP << MVPP2_RXQ_NON_OCCUPIED_OFFSET;
+	mvpp2_write(port->priv, MVPP2_RXQ_THRESH_REG, val);
+}
+
 /* Set the number of packets that will be received before Rx interrupt
  * will be generated by HW.
  */
@@ -2168,22 +2664,42 @@ static void mvpp2_rx_pkts_coal_set(struct mvpp2_port *port,
 	put_cpu();
 }
 
-/* For some reason in the LSP this is done on each CPU. Why ? */
-static void mvpp2_tx_pkts_coal_set(struct mvpp2_port *port,
-				   struct mvpp2_tx_queue *txq)
+/* Set pkts-coalescing HW with ZERO or configured VALUE
+ * The same should be set for all TXQs and all for all CPUs.
+ * Setting ZERO causes for immediate flush into tx-done handler.
+ */
+static inline void mvpp2_tx_pkts_coal_set_txqs(struct mvpp2_port *port,
+					       int cpu, u32 val)
+{
+	struct mvpp2_tx_queue *txq;
+	int queue;
+
+	val <<= MVPP2_TXQ_THRESH_OFFSET;
+
+	for (queue = 0; queue < port->ntxqs; queue++) {
+		txq = port->txqs[queue];
+		mvpp2_thread_write(port->priv, cpu, MVPP2_TXQ_NUM_REG,
+				   txq->id);
+		mvpp2_thread_write(port->priv, cpu, MVPP2_TXQ_THRESH_REG, val);
+	}
+}
+
+static void mvpp2_tx_pkts_coal_set(struct mvpp2_port *port)
 {
-	unsigned int thread;
-	u32 val;
+	struct mvpp2_tx_queue *txq = port->txqs[0];
+	u32 cfg_val = txq->done_pkts_coal;
+	int cpu;
 
-	if (txq->done_pkts_coal > MVPP2_TXQ_THRESH_MASK)
-		txq->done_pkts_coal = MVPP2_TXQ_THRESH_MASK;
+	for_each_present_cpu(cpu)
+		mvpp2_tx_pkts_coal_set_txqs(port, cpu, cfg_val);
+}
 
-	val = (txq->done_pkts_coal << MVPP2_TXQ_THRESH_OFFSET);
-	/* PKT-coalescing registers are per-queue + per-thread */
-	for (thread = 0; thread < MVPP2_MAX_THREADS; thread++) {
-		mvpp2_thread_write(port->priv, thread, MVPP2_TXQ_NUM_REG, txq->id);
-		mvpp2_thread_write(port->priv, thread, MVPP2_TXQ_THRESH_REG, val);
-	}
+/* Set ZERO value on on_each_cpu IRQ-context for 1 cpu only */
+static void mvpp2_tx_pkts_coal_set_zero_pcpu(void *arg)
+{
+	struct mvpp2_port *port = arg;
+
+	mvpp2_tx_pkts_coal_set_txqs(port, smp_processor_id(), 0);
 }
 
 static u32 mvpp2_usec_to_cycles(u32 usec, unsigned long clk_hz)
@@ -2249,12 +2765,22 @@ static void mvpp2_txq_bufs_free(struct mvpp2_port *port,
 		struct mvpp2_txq_pcpu_buf *tx_buf =
 			txq_pcpu->buffs + txq_pcpu->txq_get_index;
 
-		if (!IS_TSO_HEADER(txq_pcpu, tx_buf->dma))
+		if (!tx_buf->skb) {
 			dma_unmap_single(port->dev->dev.parent, tx_buf->dma,
 					 tx_buf->size, DMA_TO_DEVICE);
-		if (tx_buf->skb)
-			dev_kfree_skb_any(tx_buf->skb);
-
+		} else if (tx_buf->skb != TSO_HEADER_MARK) {
+			dma_unmap_single(port->dev->dev.parent, tx_buf->dma,
+					 tx_buf->size, DMA_TO_DEVICE);
+			if (static_branch_unlikely(&mvpp2_recycle_ena)) {
+				mvpp2_recycle_put(port, txq_pcpu, tx_buf);
+				/* sets tx_buf->skb=NULL if put to recycle */
+				if (tx_buf->skb)
+					dev_kfree_skb_any(tx_buf->skb);
+			} else {
+				dev_kfree_skb_any(tx_buf->skb);
+			}
+		}
+		/* else: no action, tx_buf->skb always overwritten in xmit */
 		mvpp2_txq_inc_get(txq_pcpu);
 	}
 }
@@ -2292,9 +2818,15 @@ static void mvpp2_txq_done(struct mvpp2_port *port, struct mvpp2_tx_queue *txq,
 
 	txq_pcpu->count -= tx_done;
 
-	if (netif_tx_queue_stopped(nq))
-		if (txq_pcpu->count <= txq_pcpu->wake_threshold)
+	if (netif_tx_queue_stopped(nq) && !mvpp2_tx_stopped(port)) {
+		/* Wake if netif_tx_queue_stopped on same txq->log_id */
+		if (txq_pcpu->stopped_on_txq_id == txq->log_id &&
+		    txq_pcpu->count <= txq_pcpu->wake_threshold) {
+			txq_pcpu->stopped_on_txq_id = MVPP2_MAX_TXQ;
+			nq = netdev_get_tx_queue(port->dev, txq->log_id);
 			netif_tx_wake_queue(nq);
+		}
+	}
 }
 
 static unsigned int mvpp2_tx_done(struct mvpp2_port *port, u32 cause,
@@ -2304,6 +2836,9 @@ static unsigned int mvpp2_tx_done(struct mvpp2_port *port, u32 cause,
 	struct mvpp2_txq_pcpu *txq_pcpu;
 	unsigned int tx_todo = 0;
 
+	/* Set/Restore "no-force" */
+	mvpp2_tx_done_guard_force_irq(port, thread, 0);
+
 	while (cause) {
 		txq = mvpp2_get_tx_queue(port, cause);
 		if (!txq)
@@ -2332,8 +2867,8 @@ static int mvpp2_aggr_txq_init(struct platform_device *pdev,
 
 	/* Allocate memory for TX descriptors */
 	aggr_txq->descs = dma_alloc_coherent(&pdev->dev,
-					     MVPP2_AGGR_TXQ_SIZE * MVPP2_DESC_ALIGNED_SIZE,
-					     &aggr_txq->descs_dma, GFP_KERNEL);
+				MVPP2_AGGR_TXQ_SIZE * MVPP2_DESC_ALIGNED_SIZE,
+				&aggr_txq->descs_dma, GFP_KERNEL);
 	if (!aggr_txq->descs)
 		return -ENOMEM;
 
@@ -2377,6 +2912,7 @@ static int mvpp2_rxq_init(struct mvpp2_port *port,
 		return -ENOMEM;
 
 	rxq->last_desc = rxq->size - 1;
+	rxq->rx_pending = 0;
 
 	/* Zero occupied and non-occupied counters - direct access */
 	mvpp2_write(port->priv, MVPP2_RXQ_STATUS_REG(rxq->id), 0);
@@ -2400,6 +2936,9 @@ static int mvpp2_rxq_init(struct mvpp2_port *port,
 	mvpp2_rx_pkts_coal_set(port, rxq);
 	mvpp2_rx_time_coal_set(port, rxq);
 
+	/* Set the number of non occupied descriptors threshold */
+	mvpp2_set_rxq_free_tresh(port, rxq);
+
 	/* Add number of descriptors ready for receiving packets */
 	mvpp2_rxq_status_update(port, rxq->id, 0, rxq->size);
 
@@ -2412,6 +2951,7 @@ static void mvpp2_rxq_drop_pkts(struct mvpp2_port *port,
 {
 	int rx_received, i;
 
+	rxq->rx_pending = 0;
 	rx_received = mvpp2_rxq_received(port, rxq->id);
 	if (!rx_received)
 		return;
@@ -2425,8 +2965,7 @@ static void mvpp2_rxq_drop_pkts(struct mvpp2_port *port,
 			MVPP2_RXD_BM_POOL_ID_OFFS;
 
 		mvpp2_bm_pool_put(port, pool,
-				  mvpp2_rxdesc_dma_addr_get(port, rx_desc),
-				  mvpp2_rxdesc_cookie_get(port, rx_desc));
+				  mvpp2_rxdesc_dma_addr_get(port, rx_desc));
 	}
 	mvpp2_rxq_status_update(port, rxq->id, rx_received, rx_received);
 }
@@ -2461,6 +3000,19 @@ static void mvpp2_rxq_deinit(struct mvpp2_port *port,
 	put_cpu();
 }
 
+/* Disable all rx/ingress queues, called by mvpp2_init */
+static void mvpp2_rxq_disable_all(struct mvpp2 *priv)
+{
+	int i;
+	u32 val;
+
+	for (i = 0; i < MVPP2_RXQ_MAX_NUM; i++) {
+		val = mvpp2_read(priv, MVPP2_RXQ_CONFIG_REG(i));
+		val |= MVPP2_RXQ_DISABLE_MASK;
+		mvpp2_write(priv, MVPP2_RXQ_CONFIG_REG(i), val);
+	}
+}
+
 /* Create and initialize a Tx queue */
 static int mvpp2_txq_init(struct mvpp2_port *port,
 			  struct mvpp2_tx_queue *txq)
@@ -2538,8 +3090,11 @@ static int mvpp2_txq_init(struct mvpp2_port *port,
 		txq_pcpu->txq_get_index = 0;
 		txq_pcpu->tso_headers = NULL;
 
-		txq_pcpu->stop_threshold = txq->size - MVPP2_MAX_SKB_DESCS;
-		txq_pcpu->wake_threshold = txq_pcpu->stop_threshold / 2;
+		txq_pcpu->stop_threshold = txq->size -
+				MVPP2_MAX_SKB_DESCS(num_present_cpus());
+		txq_pcpu->wake_threshold = txq_pcpu->stop_threshold -
+						MVPP2_TX_PAUSE_HYSTERESIS;
+		txq_pcpu->stopped_on_txq_id = MVPP2_MAX_TXQ;
 
 		txq_pcpu->tso_headers =
 			dma_alloc_coherent(port->dev->dev.parent,
@@ -2584,7 +3139,7 @@ static void mvpp2_txq_deinit(struct mvpp2_port *port,
 	txq->descs_dma         = 0;
 
 	/* Set minimum bandwidth for disabled TXQs */
-	mvpp2_write(port->priv, MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(txq->log_id), 0);
+	mvpp2_write(port->priv, MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(txq->id), 0);
 
 	/* Set Tx descriptors queue starting address and size */
 	thread = mvpp2_cpu_to_thread(port->priv, get_cpu());
@@ -2607,6 +3162,11 @@ static void mvpp2_txq_clean(struct mvpp2_port *port, struct mvpp2_tx_queue *txq)
 	val |= MVPP2_TXQ_DRAIN_EN_MASK;
 	mvpp2_thread_write(port->priv, thread, MVPP2_TXQ_PREF_BUF_REG, val);
 
+	/* Temporarily enable egress for the port.
+	 * It is required for releasing all remaining packets.
+	 */
+	mvpp2_egress_enable(port);
+
 	/* The napi queue has been stopped so wait for all packets
 	 * to be transmitted.
 	 */
@@ -2626,6 +3186,8 @@ static void mvpp2_txq_clean(struct mvpp2_port *port, struct mvpp2_tx_queue *txq)
 		pending &= MVPP2_TXQ_PENDING_MASK;
 	} while (pending);
 
+	mvpp2_egress_disable(port);
+
 	val &= ~MVPP2_TXQ_DRAIN_EN_MASK;
 	mvpp2_thread_write(port->priv, thread, MVPP2_TXQ_PREF_BUF_REG, val);
 	put_cpu();
@@ -2675,6 +3237,9 @@ static void mvpp2_cleanup_rxqs(struct mvpp2_port *port)
 
 	for (queue = 0; queue < port->nrxqs; queue++)
 		mvpp2_rxq_deinit(port, port->rxqs[queue]);
+
+	if (port->tx_fc)
+		mvpp2_rxq_disable_fc(port);
 }
 
 /* Init all Rx queues for port */
@@ -2687,6 +3252,10 @@ static int mvpp2_setup_rxqs(struct mvpp2_port *port)
 		if (err)
 			goto err_cleanup;
 	}
+
+	if (port->tx_fc)
+		mvpp2_rxq_enable_fc(port);
+
 	return 0;
 
 err_cleanup:
@@ -2698,25 +3267,18 @@ static int mvpp2_setup_rxqs(struct mvpp2_port *port)
 static int mvpp2_setup_txqs(struct mvpp2_port *port)
 {
 	struct mvpp2_tx_queue *txq;
-	int queue, err, cpu;
+	int queue, err;
 
 	for (queue = 0; queue < port->ntxqs; queue++) {
 		txq = port->txqs[queue];
 		err = mvpp2_txq_init(port, txq);
 		if (err)
 			goto err_cleanup;
-
-		/* Assign this queue to a CPU */
-		cpu = queue % num_present_cpus();
-		netif_set_xps_queue(port->dev, cpumask_of(cpu), queue);
 	}
 
 	if (port->has_tx_irqs) {
+		/* Download time-coal. The pkts-coal done in start_dev */
 		mvpp2_tx_time_coal_set(port);
-		for (queue = 0; queue < port->ntxqs; queue++) {
-			txq = port->txqs[queue];
-			mvpp2_tx_pkts_coal_set(port, txq);
-		}
 	}
 
 	on_each_cpu(mvpp2_txq_sent_counter_clear, port, 1);
@@ -2749,7 +3311,11 @@ static irqreturn_t mvpp2_link_status_isr(int irq, void *dev_id)
 
 	mvpp22_gop_mask_irq(port);
 
-	if (port->gop_id == 0 && mvpp2_is_xlg(port->phy_interface)) {
+	if (port->has_xlg_mac &&
+	    (port->phy_interface == PHY_INTERFACE_MODE_RXAUI ||
+	     port->phy_interface == PHY_INTERFACE_MODE_10GKR ||
+	     port->phy_interface == PHY_INTERFACE_MODE_5GKR ||
+	     port->phy_interface == PHY_INTERFACE_MODE_INTERNAL)) {
 		val = readl(port->base + MVPP22_XLG_INT_STAT);
 		if (val & MVPP22_XLG_INT_STAT_LINK) {
 			event = true;
@@ -2769,23 +3335,23 @@ static irqreturn_t mvpp2_link_status_isr(int irq, void *dev_id)
 		}
 	}
 
+	if (!netif_running(dev) || !event)
+		goto handled;
+
 	if (port->phylink) {
 		phylink_mac_change(port->phylink, link);
 		goto handled;
 	}
 
-	if (!netif_running(dev) || !event)
-		goto handled;
-
 	if (link) {
 		mvpp2_interrupts_enable(port);
 
 		mvpp2_egress_enable(port);
 		mvpp2_ingress_enable(port);
 		netif_carrier_on(dev);
-		netif_tx_wake_all_queues(dev);
+		mvpp2_tx_wake_all_queues(dev);
 	} else {
-		netif_tx_stop_all_queues(dev);
+		mvpp2_tx_stop_all_queues(dev);
 		netif_carrier_off(dev);
 		mvpp2_ingress_disable(port);
 		mvpp2_egress_disable(port);
@@ -2798,21 +3364,31 @@ static irqreturn_t mvpp2_link_status_isr(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
-static enum hrtimer_restart mvpp2_hr_timer_cb(struct hrtimer *timer)
+static void mvpp2_tx_done_timer_set(struct mvpp2_port_pcpu *port_pcpu)
 {
-	struct net_device *dev;
-	struct mvpp2_port *port;
+	ktime_t interval;
+
+	if (!port_pcpu->tx_done_timer_scheduled) {
+		port_pcpu->tx_done_timer_scheduled = true;
+		interval = MVPP2_TXDONE_HRTIMER_PERIOD_NS;
+		hrtimer_start(&port_pcpu->tx_done_timer, interval,
+			      HRTIMER_MODE_REL_PINNED);
+	}
+}
+
+static void mvpp2_tx_done_proc_cb(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *)data;
+	struct mvpp2_port *port = netdev_priv(dev);
 	struct mvpp2_port_pcpu *port_pcpu;
 	unsigned int tx_todo, cause;
 
-	port_pcpu = container_of(timer, struct mvpp2_port_pcpu, tx_done_timer);
-	dev = port_pcpu->dev;
+	port_pcpu = per_cpu_ptr(port->pcpu,
+				mvpp2_cpu_to_thread(port->priv, smp_processor_id()));
 
 	if (!netif_running(dev))
-		return HRTIMER_NORESTART;
-
-	port_pcpu->timer_scheduled = false;
-	port = netdev_priv(dev);
+		return;
+	port_pcpu->tx_done_timer_scheduled = false;
 
 	/* Process all the Tx queues */
 	cause = (1 << port->ntxqs) - 1;
@@ -2820,16 +3396,318 @@ static enum hrtimer_restart mvpp2_hr_timer_cb(struct hrtimer *timer)
 				mvpp2_cpu_to_thread(port->priv, smp_processor_id()));
 
 	/* Set the timer in case not all the packets were processed */
-	if (tx_todo && !port_pcpu->timer_scheduled) {
-		port_pcpu->timer_scheduled = true;
-		hrtimer_forward_now(&port_pcpu->tx_done_timer,
-				    MVPP2_TXDONE_HRTIMER_PERIOD_NS);
+	if (tx_todo)
+		mvpp2_tx_done_timer_set(port_pcpu);
+}
+
+static enum hrtimer_restart mvpp2_tx_done_timer_cb(struct hrtimer *timer)
+{
+	struct mvpp2_port_pcpu *port_pcpu = container_of(timer,
+							 struct mvpp2_port_pcpu,
+							 tx_done_timer);
+
+	tasklet_schedule(&port_pcpu->tx_done_tasklet);
+
+	return HRTIMER_NORESTART;
+}
+
+/* Bulk-timer could be started/restarted by XMIT, timer-cb or Tasklet.
+ *  XMIT calls bulk-restart() which is CONDITIONAL (restart vs request).
+ *  Timer-cb has own condition-logic, calls hrtimer_forward().
+ *  Tasklet has own condition-logic, calls unconditional bulk-start().
+ *  The flags scheduled::restart_req are used in the state-logic.
+ */
+static inline void mvpp2_bulk_timer_restart(struct mvpp2_port_pcpu *port_pcpu)
+{
+	if (!port_pcpu->bulk_timer_scheduled) {
+		port_pcpu->bulk_timer_scheduled = true;
+		hrtimer_start(&port_pcpu->bulk_timer, MVPP2_TX_BULK_TIME,
+			      HRTIMER_MODE_REL_PINNED);
+	} else {
+		port_pcpu->bulk_timer_restart_req = true;
+	}
+}
+
+static void mvpp2_bulk_timer_start(struct mvpp2_port_pcpu *port_pcpu)
+{
+	port_pcpu->bulk_timer_scheduled = true;
+	port_pcpu->bulk_timer_restart_req = false;
+	hrtimer_start(&port_pcpu->bulk_timer, MVPP2_TX_BULK_TIME,
+		      HRTIMER_MODE_REL_PINNED);
+}
+
+static enum hrtimer_restart mvpp2_bulk_timer_cb(struct hrtimer *timer)
+{
+	/* ISR context */
+	struct mvpp2_port_pcpu *port_pcpu =
+		container_of(timer, struct mvpp2_port_pcpu, bulk_timer);
 
+	if (!port_pcpu->bulk_timer_scheduled) {
+		/* All pending are already flushed by xmit */
+		return HRTIMER_NORESTART;
+	}
+	if (port_pcpu->bulk_timer_restart_req) {
+		/* Not flushed but restart requested by xmit */
+		port_pcpu->bulk_timer_scheduled = true;
+		port_pcpu->bulk_timer_restart_req = false;
+		hrtimer_forward_now(timer, MVPP2_TX_BULK_TIME);
 		return HRTIMER_RESTART;
 	}
+	/* Expired and need the flush for pending */
+	tasklet_schedule(&port_pcpu->bulk_tasklet);
 	return HRTIMER_NORESTART;
 }
 
+static void mvpp2_bulk_tasklet_cb(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *)data;
+	struct mvpp2_port *port = netdev_priv(dev);
+	struct mvpp2_port_pcpu *port_pcpu;
+	struct mvpp2_tx_queue *aggr_txq;
+	int frags;
+	int cpu = smp_processor_id();
+
+	port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+
+	if (!port_pcpu->bulk_timer_scheduled) {
+		/* Flushed by xmit-softirq since timer-irq */
+		return;
+	}
+	port_pcpu->bulk_timer_scheduled = false;
+	if (port_pcpu->bulk_timer_restart_req) {
+		/* Restart requested by xmit-softirq since timer-irq */
+		mvpp2_bulk_timer_start(port_pcpu);
+		return;
+	}
+
+	/* Full time expired. Flush pending packets here */
+	aggr_txq = &port->priv->aggr_txqs[cpu];
+	frags = aggr_txq->pending;
+	if (!frags)
+		return; /* Flushed by xmit */
+	aggr_txq->pending -= frags;
+	mvpp2_aggr_txq_pend_desc_add(port, frags);
+}
+
+/* Guard timer, tasklet, fixer utilities */
+
+/* The Guard fixer, called for 2 opposite actions:
+ *  Activate fix by set frame-coalescing to Zero (according to_zero_map)
+ *     which forces the tx-done IRQ. Called by guard tasklet.
+ *  Deactivate fixer ~ restore the coal-configration (to_zero_map=0)
+ *    when/by tx-done activated.
+ */
+static void mvpp2_tx_done_guard_force_irq(struct mvpp2_port *port,
+					  int sw_thread, u8 to_zero_map)
+{
+	int q;
+	u32 val, coal, qmask, xor;
+	struct mvpp2_port_pcpu *port_pcpu = per_cpu_ptr(port->pcpu, sw_thread);
+
+	if (port_pcpu->txq_coal_is_zero_map == to_zero_map)
+		return; /* all current & requested are already the same */
+
+	xor = port_pcpu->txq_coal_is_zero_map ^ to_zero_map;
+	/* Configuration num-of-frames coalescing is the same for all queues */
+	coal = port->txqs[0]->done_pkts_coal << MVPP2_TXQ_THRESH_OFFSET;
+
+	for (q = 0; q < port->ntxqs; q++) {
+		qmask = 1 << q;
+		if (!(xor & qmask))
+			continue;
+		if (to_zero_map & qmask)
+			val = 0; /* Set ZERO forcing the Interrupt */
+		else
+			val = coal; /* Set/restore configured threshold */
+		mvpp2_thread_write(port->priv, sw_thread,
+				   MVPP2_TXQ_NUM_REG, port->txqs[q]->id);
+		mvpp2_thread_write(port->priv, sw_thread,
+				   MVPP2_TXQ_THRESH_REG, val);
+	}
+	port_pcpu->txq_coal_is_zero_map = to_zero_map;
+}
+
+static inline void mvpp2_tx_done_guard_timer_set(struct mvpp2_port *port,
+						 int sw_thread)
+{
+	struct mvpp2_port_pcpu *port_pcpu = per_cpu_ptr(port->pcpu,
+							sw_thread);
+
+	if (!port_pcpu->guard_timer_scheduled) {
+		port_pcpu->guard_timer_scheduled = true;
+		hrtimer_start(&port_pcpu->tx_done_timer,
+			      MVPP2_GUARD_TXDONE_HRTIMER_NS,
+			      HRTIMER_MODE_REL_PINNED);
+	}
+}
+
+/* Guard timer and tasklet callbacks making check logic upon flags
+ *    guard_timer_scheduled, tx_done_passed,
+ *    txq_coal_is_zero_map, txq_busy_suspect_map
+ */
+static enum hrtimer_restart mvpp2_guard_timer_cb(struct hrtimer *timer)
+{
+	struct mvpp2_port_pcpu *port_pcpu = container_of(timer,
+			 struct mvpp2_port_pcpu, tx_done_timer);
+	struct mvpp2_port *port = port_pcpu->port;
+	struct mvpp2_tx_queue *txq;
+	struct mvpp2_txq_pcpu *txq_pcpu;
+	u8 txq_nonempty_map = 0;
+	int q, cpu;
+	ktime_t time;
+
+	if (port_pcpu->tx_done_passed) {
+		/* ok, tx-done was active since last checking */
+		port_pcpu->tx_done_passed = false;
+		time = MVPP2_GUARD_TXDONE_HRTIMER_NS; /* regular long */
+		goto timer_restart;
+	}
+
+	cpu = smp_processor_id(); /* timer is per-cpu */
+
+	for (q = 0; q < port->ntxqs; q++) {
+		txq = port->txqs[q];
+		txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
+		if (txq_pcpu->count)
+			txq_nonempty_map |= 1 << q;
+	}
+
+	if (!txq_nonempty_map || mvpp2_tx_stopped(port)) {
+		/* All queues are empty, guard-timer may be stopped now
+		 * It would be started again on new transmit.
+		 */
+		port_pcpu->guard_timer_scheduled = false;
+		return HRTIMER_NORESTART;
+	}
+
+	if (port_pcpu->txq_busy_suspect_map) {
+		/* Second-hit ~~ tx-done is really stalled.
+		 * Activate the tasklet to fix.
+		 * Keep guard_timer_scheduled=TRUE
+		 */
+		tasklet_schedule(&port_pcpu->tx_done_tasklet);
+		return HRTIMER_NORESTART;
+	}
+
+	/* First-hit ~~ tx-done seems stalled. Schedule re-check with SHORT time
+	 * bigger a bit than HW-coal-time-usec (1024=2^10 vs NSEC_PER_USEC)
+	 */
+	time = ktime_set(0, port->tx_time_coal << 10);
+	port_pcpu->txq_busy_suspect_map |= txq_nonempty_map;
+
+timer_restart:
+	/* Keep guard_timer_scheduled=TRUE but set new expiration time */
+	hrtimer_forward_now(timer, time);
+	return HRTIMER_RESTART;
+}
+
+static void mvpp2_tx_done_guard_tasklet_cb(unsigned long data)
+{
+	struct mvpp2_port *port = (void *)data;
+	struct mvpp2_port_pcpu *port_pcpu;
+	int cpu;
+
+	 /* stop_dev() has permanent setting for coal=0 */
+	if (mvpp2_tx_stopped(port))
+		return;
+
+	cpu = get_cpu();
+	port_pcpu = per_cpu_ptr(port->pcpu, cpu); /* tasklet is per-cpu */
+
+	if (port_pcpu->tx_done_passed) {
+		port_pcpu->tx_done_passed = false;
+	} else { /* Force IRQ */
+		mvpp2_tx_done_guard_force_irq(port, cpu,
+					      port_pcpu->txq_busy_suspect_map);
+		port_pcpu->tx_guard_cntr++;
+	}
+	port_pcpu->txq_busy_suspect_map = 0;
+
+	/* guard_timer_scheduled is already TRUE, just start the timer */
+	hrtimer_start(&port_pcpu->tx_done_timer,
+		      MVPP2_GUARD_TXDONE_HRTIMER_NS,
+		      HRTIMER_MODE_REL_PINNED);
+	put_cpu();
+}
+
+static u32 mvpp2_tx_done_guard_get_stats(struct mvpp2_port *port, int cpu)
+{
+	return per_cpu_ptr(port->pcpu, cpu)->tx_guard_cntr;
+}
+
+static void mvpp2_tx_done_init_on_open(struct mvpp2_port *port, bool open)
+{
+	struct mvpp2_port_pcpu *port_pcpu;
+	int cpu;
+
+	if (port->flags & MVPP2_F_LOOPBACK)
+		return;
+
+	if (!open)
+		goto close;
+
+	/* Init tx-done tasklets and variables */
+	for_each_present_cpu(cpu) {
+		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+
+		/* Timer works in tx-done or Guard mode. To eliminate per-packet
+		 * mode checking each mode has own "_scheduled" flag.
+		 * Set scheduled=FALSE for active mode and TRUE for inactive, so
+		 * timer would never be started in inactive mode.
+		 */
+		if (port->has_tx_irqs) { /* guard-mode */
+			port_pcpu->txq_coal_is_zero_map = 0;
+			port_pcpu->txq_busy_suspect_map = 0;
+			port_pcpu->tx_done_passed = false;
+
+			/* "true" is never started */
+			port_pcpu->tx_done_timer_scheduled = true;
+			port_pcpu->guard_timer_scheduled = false;
+			tasklet_init(&port_pcpu->tx_done_tasklet,
+				     mvpp2_tx_done_guard_tasklet_cb,
+				     (unsigned long)port);
+		} else {
+			port_pcpu->tx_done_timer_scheduled = false;
+			/* "true" is never started */
+			port_pcpu->guard_timer_scheduled = true;
+			tasklet_init(&port_pcpu->tx_done_tasklet,
+				     mvpp2_tx_done_proc_cb,
+				     (unsigned long)port->dev);
+		}
+	}
+	return;
+close:
+	/* Kill tx-done timers and tasklets */
+	for_each_present_cpu(cpu) {
+		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+		/* Say "scheduled=true" is never started on XMIT */
+		port_pcpu->tx_done_timer_scheduled = true;
+		port_pcpu->guard_timer_scheduled = true;
+		hrtimer_cancel(&port_pcpu->tx_done_timer);
+		tasklet_kill(&port_pcpu->tx_done_tasklet);
+	}
+}
+
+static void mvpp2_tx_done_init_on_probe(struct platform_device *pdev,
+					struct mvpp2_port *port)
+{
+	struct mvpp2_port_pcpu *port_pcpu;
+	int cpu;
+	bool guard_mode = port->has_tx_irqs;
+
+	if (port->flags & MVPP2_F_LOOPBACK)
+		return;
+
+	for_each_present_cpu(cpu) {
+		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+		port_pcpu->port = port;
+		hrtimer_init(&port_pcpu->tx_done_timer, CLOCK_MONOTONIC,
+			     HRTIMER_MODE_REL_PINNED);
+		port_pcpu->tx_done_timer.function = (guard_mode) ?
+				mvpp2_guard_timer_cb : mvpp2_tx_done_timer_cb;
+	}
+}
+
 /* Main RX/TX processing routines */
 
 /* Display more error info */
@@ -2841,8 +3719,8 @@ static void mvpp2_rx_error(struct mvpp2_port *port,
 	char *err_str = NULL;
 
 	switch (status & MVPP2_RXD_ERR_CODE_MASK) {
-	case MVPP2_RXD_ERR_CRC:
-		err_str = "crc";
+	case MVPP2_RXD_ERR_MAC:
+		err_str = "MAC";
 		break;
 	case MVPP2_RXD_ERR_OVERRUN:
 		err_str = "overrun";
@@ -2852,78 +3730,395 @@ static void mvpp2_rx_error(struct mvpp2_port *port,
 		break;
 	}
 	if (err_str && net_ratelimit())
-		netdev_err(port->dev,
+		netdev_dbg(port->dev,
 			   "bad rx status %08x (%s error), size=%zu\n",
 			   status, err_str, sz);
 }
 
-/* Handle RX checksum offload */
-static void mvpp2_rx_csum(struct mvpp2_port *port, u32 status,
-			  struct sk_buff *skb)
+/* Handle RX checksum offload */
+static void mvpp2_rx_csum(struct mvpp2_port *port, u32 status,
+			  struct sk_buff *skb)
+{
+	if (((status & MVPP2_RXD_L3_IP4) &&
+	     !(status & MVPP2_RXD_IP4_HEADER_ERR)) ||
+	    (status & MVPP2_RXD_L3_IP6))
+		if (((status & MVPP2_RXD_L4_UDP) ||
+		     (status & MVPP2_RXD_L4_TCP)) &&
+		     (status & MVPP2_RXD_L4_CSUM_OK)) {
+			skb->csum = 0;
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			return;
+		}
+
+	skb->ip_summed = CHECKSUM_NONE;
+}
+
+/* Allocate a new skb and add it to BM pool */
+static inline int mvpp2_rx_refill(struct mvpp2_port *port,
+				  struct mvpp2_bm_pool *bm_pool, int pool)
+{
+	dma_addr_t dma_addr = mvpp2_buf_alloc(port, bm_pool, GFP_ATOMIC);
+
+	if (!dma_addr)
+		return -ENOMEM;
+
+	mvpp2_bm_pool_put(port, pool, dma_addr);
+
+	return 0;
+}
+
+/* Handle tx checksum */
+static u32 mvpp2_skb_tx_csum(struct mvpp2_port *port, struct sk_buff *skb)
+{
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		int ip_hdr_len = 0;
+		u8 l4_proto;
+		__be16 l3_proto = vlan_get_protocol(skb);
+
+		if (l3_proto == htons(ETH_P_IP)) {
+			struct iphdr *ip4h = ip_hdr(skb);
+
+			/* Calculate IPv4 checksum and L4 checksum */
+			ip_hdr_len = ip4h->ihl;
+			l4_proto = ip4h->protocol;
+		} else if (l3_proto == htons(ETH_P_IPV6)) {
+			struct ipv6hdr *ip6h = ipv6_hdr(skb);
+
+			/* Read l4_protocol from one of IPv6 extra headers */
+			if (skb_network_header_len(skb) > 0)
+				ip_hdr_len = (skb_network_header_len(skb) >> 2);
+			l4_proto = ip6h->nexthdr;
+		} else {
+			return MVPP2_TXD_L4_CSUM_NOT;
+		}
+
+		return mvpp2_txq_desc_csum(skb_network_offset(skb),
+					   l3_proto, ip_hdr_len, l4_proto);
+	}
+
+	return MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE;
+}
+
+void mvpp2_recycle_stats(void)
+{
+	int cpu;
+	int pl_id;
+	struct mvpp2_recycle_pcpu *pcpu;
+
+	pr_info("Recycle-stats: %d open ports (on all CP110s)\n",
+		mvpp2_share.num_open_ports);
+	if (!mvpp2_share.recycle_base)
+		return;
+	pcpu = mvpp2_share.recycle;
+	for_each_online_cpu(cpu) {
+		for (pl_id = 0; pl_id < MVPP2_BM_POOLS_NUM; pl_id++) {
+			pr_info("| cpu[%d].pool_%d: idx=%d\n",
+				cpu, pl_id, pcpu->idx[pl_id]);
+		}
+		pr_info("| ___[%d].skb_____idx=%d__\n",
+			cpu, pcpu->idx[MVPP2_BM_POOLS_NUM]);
+		pcpu++;
+	}
+}
+
+static int mvpp2_recycle_open(void)
+{
+	int cpu, pl_id, size;
+	struct mvpp2_recycle_pcpu *pcpu;
+	phys_addr_t addr;
+
+	mvpp2_share.num_open_ports++;
+	wmb(); /* for num_open_ports */
+
+	if (mvpp2_share.recycle_base)
+		return 0;
+
+	/* Allocate pool-tree */
+	size = sizeof(*pcpu) * num_online_cpus() + L1_CACHE_BYTES;
+	mvpp2_share.recycle_base = kzalloc(size, GFP_KERNEL);
+	if (!mvpp2_share.recycle_base)
+		goto err;
+	/* Use Address aligned to L1_CACHE_BYTES */
+	addr = (phys_addr_t)mvpp2_share.recycle_base + (L1_CACHE_BYTES - 1);
+	addr &= ~(L1_CACHE_BYTES - 1);
+	mvpp2_share.recycle = (void *)addr;
+
+	pcpu = mvpp2_share.recycle;
+	for_each_online_cpu(cpu) {
+		for (pl_id = 0; pl_id <= MVPP2_BM_POOLS_NUM; pl_id++)
+			pcpu->idx[pl_id] = -1;
+		pcpu++;
+	}
+	return 0;
+err:
+	pr_err("mvpp2 error: cannot allocate recycle pool\n");
+	return -ENOMEM;
+}
+
+static void mvpp2_recycle_close(void)
+{
+	int cpu, pl_id, i;
+	struct mvpp2_recycle_pcpu *pcpu;
+	struct mvpp2_recycle_pool *pool;
+
+	mvpp2_share.num_open_ports--;
+	wmb(); /* for num_open_ports */
+
+	/* Do nothing if recycle is not used at all or in use by port/ports */
+	if (mvpp2_share.num_open_ports || !mvpp2_share.recycle_base)
+		return;
+
+	/* Usable (recycle_base!=NULL), but last port gone down
+	 * Let's free all accumulated buffers.
+	 */
+	pcpu = mvpp2_share.recycle;
+	for_each_online_cpu(cpu) {
+		for (pl_id = 0; pl_id <= MVPP2_BM_POOLS_NUM; pl_id++) {
+			pool = &pcpu->pool[pl_id];
+			for (i = 0; i <= pcpu->idx[pl_id]; i++) {
+				if (!pool->pbuf[i])
+					continue;
+				if (pl_id < MVPP2_BM_POOLS_NUM)
+					kfree(pool->pbuf[i]);
+				else
+					kmem_cache_free(skbuff_head_cache,
+							pool->pbuf[i]);
+			}
+		}
+		pcpu++;
+	}
+	kfree(mvpp2_share.recycle_base);
+	mvpp2_share.recycle_base = NULL;
+}
+
+static int mvpp2_recycle_get_bm_id(struct sk_buff *skb)
+{
+	u32 hash;
+
+	/* Keep checking ordering for performance */
+	hash = skb_get_hash_raw(skb);
+	/* Check hash */
+	if (!MVPP2_RXTX_HASH_IS_OK(skb, hash))
+		return -1;
+	/* Check if skb could be free */
+	/* Use skb->cloned but not skb_cloned(), skb_header_cloned() */
+	if (skb_shared(skb) || skb->cloned)
+		return -1;
+	/* ipsec: sp/secpath, _skb_refdst ... */
+	if (!skb_irq_freeable(skb))
+		return -1;
+	if (skb_shinfo(skb)->tx_flags & SKBTX_ZEROCOPY_FRAG)
+		return -1;
+
+	/* Get bm-pool-id */
+	hash &= MVPP2_RXTX_HASH_BMID_MASK;
+	if (hash >= MVPP2_BM_POOLS_NUM)
+		return -1;
+
+	return (int)hash;
+}
+
+static inline void mvpp2_recycle_put(struct mvpp2_port *port,
+				     struct mvpp2_txq_pcpu *txq_pcpu,
+				     struct mvpp2_txq_pcpu_buf *tx_buf)
 {
-	if (((status & MVPP2_RXD_L3_IP4) &&
-	     !(status & MVPP2_RXD_IP4_HEADER_ERR)) ||
-	    (status & MVPP2_RXD_L3_IP6))
-		if (((status & MVPP2_RXD_L4_UDP) ||
-		     (status & MVPP2_RXD_L4_TCP)) &&
-		     (status & MVPP2_RXD_L4_CSUM_OK)) {
-			skb->csum = 0;
-			skb->ip_summed = CHECKSUM_UNNECESSARY;
-			return;
-		}
+	struct mvpp2_recycle_pcpu *pcpu;
+	struct mvpp2_recycle_pool *pool;
+	short int idx, pool_id;
+	struct sk_buff *skb = tx_buf->skb;
+	struct mvpp2_bm_pool *bm_pool;
 
-	skb->ip_summed = CHECKSUM_NONE;
+	/* tx_buf->skb is not NULL */
+	pool_id = mvpp2_recycle_get_bm_id(skb);
+	if (pool_id < 0)
+		return; /* non-recyclable */
+
+	bm_pool = &port->priv->bm_pools[pool_id];
+	if (skb_end_offset(skb) < (bm_pool->frag_size - MVPP2_SKB_SHINFO_SIZE))
+		return; /* shrank -> non-recyclable */
+
+	/* This skb could be destroyed. Put into recycle */
+	pcpu = mvpp2_share.recycle + txq_pcpu->thread;
+	idx = pcpu->idx[pool_id];
+	if (idx < (MVPP2_RECYCLE_FULL - 1)) {
+		pool = &pcpu->pool[pool_id];
+		pool->pbuf[++idx] = skb->head; /* pre-increment */
+		pcpu->idx[pool_id] = idx;
+		skb->head = NULL;
+	}
+	idx = pcpu->idx[MVPP2_BM_POOLS_NUM];
+	if (idx < (MVPP2_RECYCLE_FULL_SKB - 1)) {
+		pool = &pcpu->pool[MVPP2_BM_POOLS_NUM];
+		pool->pbuf[++idx] = skb;
+		pcpu->idx[MVPP2_BM_POOLS_NUM] = idx;
+		if (skb->head) {
+			if (bm_pool->frag_size <= PAGE_SIZE)
+				skb_free_frag(skb->head);
+			else
+				kfree(skb->head);
+		}
+		tx_buf->skb = NULL;
+	}
 }
 
-/* Reuse skb if possible, or allocate a new skb and add it to BM pool */
-static int mvpp2_rx_refill(struct mvpp2_port *port,
-			   struct mvpp2_bm_pool *bm_pool, int pool)
+static struct sk_buff *mvpp2_recycle_get(struct mvpp2_port *port,
+					 struct mvpp2_bm_pool *bm_pool)
 {
+	int cpu;
+	struct mvpp2_recycle_pcpu *pcpu;
+	struct mvpp2_recycle_pool *pool;
+	short int idx;
+	void *frag;
+	struct sk_buff *skb;
 	dma_addr_t dma_addr;
-	phys_addr_t phys_addr;
-	void *buf;
 
-	/* No recycle or too many buffers are in use, so allocate a new skb */
-	buf = mvpp2_buf_alloc(port, bm_pool, &dma_addr, &phys_addr,
-			      GFP_ATOMIC);
-	if (!buf)
-		return -ENOMEM;
+	cpu = smp_processor_id();
+	pcpu = mvpp2_share.recycle + cpu;
 
-	mvpp2_bm_pool_put(port, pool, dma_addr, phys_addr);
+	/* GET bm buffer */
+	idx = pcpu->idx[bm_pool->id];
+	pool = &pcpu->pool[bm_pool->id];
 
-	return 0;
+	if (idx >= 0) {
+		frag = pool->pbuf[idx];
+		pcpu->idx[bm_pool->id]--; /* post-decrement */
+	} else {
+		/* Allocate 2 buffers, put 1, use another now */
+		pcpu->idx[bm_pool->id] = 0;
+		pool->pbuf[0] = mvpp2_frag_alloc(bm_pool);
+		frag = NULL;
+	}
+	if (!frag)
+		frag = mvpp2_frag_alloc(bm_pool);
+
+	/* refill the buffer into BM */
+	dma_addr = dma_map_single(port->dev->dev.parent, frag,
+				  bm_pool->buf_size, DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(port->dev->dev.parent, dma_addr))) {
+		pcpu->idx[bm_pool->id]++; /* Return back to recycle */
+		netdev_err(port->dev, "failed to refill BM pool-%d (%d:%p)\n",
+			   bm_pool->id, pcpu->idx[bm_pool->id], frag);
+		return NULL;
+	}
+
+	/* GET skb buffer */
+	idx = pcpu->idx[MVPP2_BM_POOLS_NUM];
+	if (idx >= 0) {
+		pool = &pcpu->pool[MVPP2_BM_POOLS_NUM];
+		skb = pool->pbuf[idx];
+		pcpu->idx[MVPP2_BM_POOLS_NUM]--;
+	} else {
+		skb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);
+	}
+
+	if (unlikely(!skb)) {
+		dma_unmap_single(port->dev->dev.parent, dma_addr,
+				 bm_pool->buf_size, DMA_FROM_DEVICE);
+		mvpp2_frag_free(bm_pool, frag);
+		return NULL;
+	}
+	mvpp2_bm_pool_put(port, bm_pool->id, dma_addr);
+	return skb;
 }
 
-/* Handle tx checksum */
-static u32 mvpp2_skb_tx_csum(struct mvpp2_port *port, struct sk_buff *skb)
+/* SKB and BM-buff alloc/refill like mvpp2_recycle_get but without recycle */
+static inline
+struct sk_buff *mvpp2_bm_refill_skb_get(struct mvpp2_port *port,
+					struct mvpp2_bm_pool *bm_pool)
 {
-	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		int ip_hdr_len = 0;
-		u8 l4_proto;
-		__be16 l3_proto = vlan_get_protocol(skb);
+	void *frag;
+	struct sk_buff *skb;
+	dma_addr_t dma_addr;
 
-		if (l3_proto == htons(ETH_P_IP)) {
-			struct iphdr *ip4h = ip_hdr(skb);
+	/* GET bm buffer, refill into BM */
+	frag = mvpp2_frag_alloc(bm_pool);
+	dma_addr = dma_map_single(port->dev->dev.parent, frag,
+				  bm_pool->buf_size, DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(port->dev->dev.parent, dma_addr))) {
+		netdev_err(port->dev, "failed to refill BM pool-%d\n",
+			   bm_pool->id);
+		return NULL;
+	}
 
-			/* Calculate IPv4 checksum and L4 checksum */
-			ip_hdr_len = ip4h->ihl;
-			l4_proto = ip4h->protocol;
-		} else if (l3_proto == htons(ETH_P_IPV6)) {
-			struct ipv6hdr *ip6h = ipv6_hdr(skb);
+	/* GET skb buffer */
+	skb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);
+	if (unlikely(!skb)) {
+		dma_unmap_single(port->dev->dev.parent, dma_addr,
+				 bm_pool->buf_size, DMA_FROM_DEVICE);
+		mvpp2_frag_free(bm_pool, frag);
+		return NULL;
+	}
+	mvpp2_bm_pool_put(port, bm_pool->id, dma_addr);
+	return skb;
+}
 
-			/* Read l4_protocol from one of IPv6 extra headers */
-			if (skb_network_header_len(skb) > 0)
-				ip_hdr_len = (skb_network_header_len(skb) >> 2);
-			l4_proto = ip6h->nexthdr;
-		} else {
-			return MVPP2_TXD_L4_CSUM_NOT;
-		}
+static inline void mvpp2_skb_set_extra(struct sk_buff *skb,
+				       struct napi_struct *napi,
+				       u32 status,
+				       u8 rxq_id,
+				       struct mvpp2_bm_pool *bm_pool)
+{
+	u32 hash;
+	enum pkt_hash_types hash_type;
 
-		return mvpp2_txq_desc_csum(skb_network_offset(skb),
-					   l3_proto, ip_hdr_len, l4_proto);
+	/* Improve performance and set identification for RX-TX fast-forward */
+	hash = MVPP2_RXTX_HASH_GENER(skb, bm_pool->id);
+	hash_type = (status & (MVPP2_RXD_L4_UDP | MVPP2_RXD_L4_TCP)) ?
+		PKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3;
+	skb_set_hash(skb, hash, hash_type);
+	skb_mark_napi_id(skb, napi);
+	skb_record_rx_queue(skb, (u16)rxq_id);
+}
+
+/* This is "fast inline" clone of __build_skb+build_skb,
+ * and also with setting mv-extra information
+ */
+static inline
+struct sk_buff *mvpp2_build_skb(void *data, unsigned int frag_size,
+				struct napi_struct *napi,
+				struct mvpp2_port *port,
+				u32 rx_status,
+				u8 rxq_id,
+				struct mvpp2_bm_pool *bm_pool)
+{
+	struct skb_shared_info *shinfo;
+	struct sk_buff *skb;
+	unsigned int size = frag_size ? : ksize(data);
+
+	if (static_branch_unlikely(&mvpp2_recycle_ena))
+		skb = mvpp2_recycle_get(port, bm_pool);
+	else
+		skb = mvpp2_bm_refill_skb_get(port, bm_pool);
+	if (unlikely(!skb))
+		return NULL;
+
+	size -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->truesize = SKB_TRUESIZE(size);
+	refcount_set(&skb->users, 1);
+	skb->head = data;
+	skb->data = data;
+	skb_reset_tail_pointer(skb);
+	skb->end = skb->tail + size;
+	skb->mac_header = (typeof(skb->mac_header))~0U;
+	skb->transport_header = (typeof(skb->transport_header))~0U;
+
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+
+	/* From build_skb wrapper */
+	if (frag_size) {
+		skb->head_frag = 1;
+		if (page_is_pfmemalloc(virt_to_head_page(data)))
+			skb->pfmemalloc = 1;
 	}
 
-	return MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE;
+	mvpp2_skb_set_extra(skb, napi, rx_status, rxq_id, bm_pool);
+
+	return skb;
 }
 
 /* Main rx processing */
@@ -2933,13 +4128,23 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 	struct net_device *dev = port->dev;
 	int rx_received;
 	int rx_done = 0;
-	u32 rcvd_pkts = 0;
+	u32 rcvd_pkts = 0, i = 0;
 	u32 rcvd_bytes = 0;
+	struct sk_buff *skb_all[64];
 
-	/* Get number of received packets and clamp the to-do */
-	rx_received = mvpp2_rxq_received(port, rxq->id);
-	if (rx_todo > rx_received)
-		rx_todo = rx_received;
+	if (rxq->rx_pending >= rx_todo) {
+		rx_received = rx_todo;
+		rxq->rx_pending -= rx_todo;
+	} else {
+		/* Get number of received packets and clamp the to-do */
+		rx_received = mvpp2_rxq_received(port, rxq->id);
+		if (rx_received < rx_todo) {
+			rx_todo = rx_received;
+			rxq->rx_pending = 0;
+		} else {
+			rxq->rx_pending = rx_received - rx_todo;
+		}
+	}
 
 	while (rx_done < rx_todo) {
 		struct mvpp2_rx_desc *rx_desc = mvpp2_rxq_next_desc_get(rxq);
@@ -2949,7 +4154,7 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 		dma_addr_t dma_addr;
 		phys_addr_t phys_addr;
 		u32 rx_status;
-		int pool, rx_bytes, err;
+		int pool, rx_bytes;
 		void *data;
 
 		rx_done++;
@@ -2957,7 +4162,7 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 		rx_bytes = mvpp2_rxdesc_size_get(port, rx_desc);
 		rx_bytes -= MVPP2_MH_SIZE;
 		dma_addr = mvpp2_rxdesc_dma_addr_get(port, rx_desc);
-		phys_addr = mvpp2_rxdesc_cookie_get(port, rx_desc);
+		phys_addr = dma_to_phys(port->dev->dev.parent, dma_addr);
 		data = (void *)phys_to_virt(phys_addr);
 
 		pool = (rx_status & MVPP2_RXD_BM_POOL_ID_MASK) >>
@@ -2974,7 +4179,7 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 			dev->stats.rx_errors++;
 			mvpp2_rx_error(port, rx_desc);
 			/* Return the buffer to the pool */
-			mvpp2_bm_pool_put(port, pool, dma_addr, phys_addr);
+			mvpp2_bm_pool_put(port, pool, dma_addr);
 			continue;
 		}
 
@@ -2983,32 +4188,38 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 		else
 			frag_size = bm_pool->frag_size;
 
-		skb = build_skb(data, frag_size);
+		/* _sync_ for coherency (_unmap_ is asynchroneous).
+		 * _sync_ should be done for the SAME size as in map/unmap.
+		 * The prefetch is for CPU and should be after unmap ~ mapToCPU
+		 */
+		if (rx_todo == 1)
+			dma_sync_single_for_cpu(dev->dev.parent, dma_addr,
+						bm_pool->buf_size,
+						DMA_FROM_DEVICE);
+		dma_unmap_single(dev->dev.parent, dma_addr,
+				 bm_pool->buf_size, DMA_FROM_DEVICE);
+
+		prefetch(data + NET_SKB_PAD); /* packet header */
+
+		skb = mvpp2_build_skb(data, frag_size,
+				      napi, port, rx_status, rxq->id, bm_pool);
 		if (!skb) {
 			netdev_warn(port->dev, "skb build failed\n");
 			goto err_drop_frame;
 		}
 
-		err = mvpp2_rx_refill(port, bm_pool, pool);
-		if (err) {
-			netdev_err(port->dev, "failed to refill BM pools\n");
-			goto err_drop_frame;
-		}
-
-		dma_unmap_single(dev->dev.parent, dma_addr,
-				 bm_pool->buf_size, DMA_FROM_DEVICE);
-
-		rcvd_pkts++;
-		rcvd_bytes += rx_bytes;
-
 		skb_reserve(skb, MVPP2_MH_SIZE + NET_SKB_PAD);
 		skb_put(skb, rx_bytes);
 		skb->protocol = eth_type_trans(skb, dev);
 		mvpp2_rx_csum(port, rx_status, skb);
 
-		napi_gro_receive(napi, skb);
+		skb_all[rcvd_pkts++] = skb;
+		rcvd_bytes += rx_bytes;
 	}
 
+	while (i < rcvd_pkts)
+		napi_gro_receive(napi, skb_all[i++]);
+
 	if (rcvd_pkts) {
 		struct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);
 
@@ -3018,8 +4229,7 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 		u64_stats_update_end(&stats->syncp);
 	}
 
-	/* Update Rx queue management counters */
-	wmb();
+	/* Update HW Rx queue management counters with RX-done */
 	mvpp2_rxq_status_update(port, rxq->id, rx_done, rx_done);
 
 	return rx_todo;
@@ -3062,7 +4272,7 @@ static int mvpp2_tx_frag_process(struct mvpp2_port *port, struct sk_buff *skb,
 		mvpp2_txdesc_size_set(port, tx_desc, frag->size);
 
 		buf_dma_addr = dma_map_single(port->dev->dev.parent, addr,
-					      frag->size, DMA_TO_DEVICE);
+					      skb_frag_size(frag), DMA_TO_DEVICE);
 		if (dma_mapping_error(port->dev->dev.parent, buf_dma_addr)) {
 			mvpp2_txq_desc_put(txq);
 			goto cleanup;
@@ -3116,7 +4326,7 @@ static inline void mvpp2_tso_put_hdr(struct sk_buff *skb,
 	mvpp2_txdesc_cmd_set(port, tx_desc, mvpp2_skb_tx_csum(port, skb) |
 					    MVPP2_TXD_F_DESC |
 					    MVPP2_TXD_PADDING_DISABLE);
-	mvpp2_txq_inc_put(port, txq_pcpu, NULL, tx_desc);
+	mvpp2_txq_inc_put(port, txq_pcpu, TSO_HEADER_MARK, tx_desc);
 }
 
 static inline int mvpp2_tso_put_data(struct sk_buff *skb,
@@ -3164,14 +4374,17 @@ static int mvpp2_tx_tso(struct sk_buff *skb, struct net_device *dev,
 	struct mvpp2_port *port = netdev_priv(dev);
 	struct tso_t tso;
 	int hdr_sz = skb_transport_offset(skb) + tcp_hdrlen(skb);
-	int i, len, descs = 0;
+	int i, len, descs = tso_count_descs(skb);
 
-	/* Check number of available descriptors */
-	if (mvpp2_aggr_desc_num_check(port, aggr_txq, tso_count_descs(skb)) ||
-	    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu,
-					     tso_count_descs(skb)))
+	/* Check enough free-space in txq and
+	 * number of available aggr/reserved descriptors
+	 */
+	if (((txq_pcpu->size - txq_pcpu->count) < descs) ||
+	    mvpp2_aggr_desc_num_check(port, aggr_txq, descs) ||
+	    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu, descs))
 		return 0;
 
+	descs = 0; /* real descs <= tso_count_descs() */
 	tso_start(skb, &tso);
 	len = skb->len - hdr_sz;
 	while (len > 0) {
@@ -3237,8 +4450,11 @@ static netdev_tx_t mvpp2_tx(struct sk_buff *skb, struct net_device *dev)
 	}
 	frags = skb_shinfo(skb)->nr_frags + 1;
 
-	/* Check number of available descriptors */
-	if (mvpp2_aggr_desc_num_check(port, aggr_txq, frags) ||
+	/* Check enough free-space in txq and
+	 * number of available aggr/reserved descriptors
+	 */
+	if (((txq_pcpu->size - txq_pcpu->count) < frags) ||
+	    mvpp2_aggr_desc_num_check(port, aggr_txq, frags) ||
 	    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu, frags)) {
 		frags = 0;
 		goto out;
@@ -3282,19 +4498,41 @@ static netdev_tx_t mvpp2_tx(struct sk_buff *skb, struct net_device *dev)
 out:
 	if (frags > 0) {
 		struct mvpp2_pcpu_stats *stats = per_cpu_ptr(port->stats, thread);
-		struct netdev_queue *nq = netdev_get_tx_queue(dev, txq_id);
+		struct mvpp2_port_pcpu *port_pcpu = this_cpu_ptr(port->pcpu);
+		struct netdev_queue *nq;
+		bool deferred_tx;
 
 		txq_pcpu->reserved_num -= frags;
 		txq_pcpu->count += frags;
 		aggr_txq->count += frags;
 
-		/* Enable transmit */
-		wmb();
-		mvpp2_aggr_txq_pend_desc_add(port, frags);
+		/* Enable transmit; RX-to-TX may be deferred with Bulk-timer */
+		deferred_tx = (frags == 1) &&
+			MVPP2_RXTX_HASH_IS_OK_TX(skb, skb_get_hash_raw(skb)) &&
+			(aggr_txq->pending < min(MVPP2_TX_BULK_MAX_PACKETS,
+					       (int)(txq->done_pkts_coal / 2)));
 
-		if (txq_pcpu->count >= txq_pcpu->stop_threshold)
-			netif_tx_stop_queue(nq);
+		if (deferred_tx) {
+			aggr_txq->pending += frags;
+			mvpp2_bulk_timer_restart(port_pcpu);
+		} else {
+			port_pcpu->bulk_timer_scheduled = false;
+			port_pcpu->bulk_timer_restart_req = false;
+			frags += aggr_txq->pending;
+			aggr_txq->pending = 0;
+			mvpp2_aggr_txq_pend_desc_add(port, frags);
+		}
 
+		if (unlikely(txq_pcpu->count >= txq_pcpu->stop_threshold)) {
+			nq = netdev_get_tx_queue(dev, txq_id);
+			/* txq_id may differ from thread/cpu and come from more
+			 * than one txq_pcpu. Save only the first for wakeup.
+			 */
+			if (unlikely(!netif_tx_queue_stopped(nq))) {
+				txq_pcpu->stopped_on_txq_id = txq_id;
+				netif_tx_stop_queue(nq);
+			}
+		}
 		u64_stats_update_begin(&stats->syncp);
 		stats->tx_packets++;
 		stats->tx_bytes += skb->len;
@@ -3313,12 +4551,7 @@ static netdev_tx_t mvpp2_tx(struct sk_buff *skb, struct net_device *dev)
 	    txq_pcpu->count > 0) {
 		struct mvpp2_port_pcpu *port_pcpu = per_cpu_ptr(port->pcpu, thread);
 
-		if (!port_pcpu->timer_scheduled) {
-			port_pcpu->timer_scheduled = true;
-			hrtimer_start(&port_pcpu->tx_done_timer,
-				      MVPP2_TXDONE_HRTIMER_PERIOD_NS,
-				      HRTIMER_MODE_REL_PINNED_SOFT);
-		}
+		mvpp2_tx_done_timer_set(port_pcpu);
 	}
 
 	if (test_bit(thread, &port->priv->lock_map))
@@ -3327,23 +4560,12 @@ static netdev_tx_t mvpp2_tx(struct sk_buff *skb, struct net_device *dev)
 	return NETDEV_TX_OK;
 }
 
-static inline void mvpp2_cause_error(struct net_device *dev, int cause)
-{
-	if (cause & MVPP2_CAUSE_FCS_ERR_MASK)
-		netdev_err(dev, "FCS error\n");
-	if (cause & MVPP2_CAUSE_RX_FIFO_OVERRUN_MASK)
-		netdev_err(dev, "rx fifo overrun error\n");
-	if (cause & MVPP2_CAUSE_TX_FIFO_UNDERRUN_MASK)
-		netdev_err(dev, "tx fifo underrun error\n");
-}
-
 static int mvpp2_poll(struct napi_struct *napi, int budget)
 {
-	u32 cause_rx_tx, cause_rx, cause_tx, cause_misc;
+	u32 cause_rx_tx, cause_rx, cause_tx;
 	int rx_done = 0;
 	struct mvpp2_port *port = netdev_priv(napi->dev);
 	struct mvpp2_queue_vector *qv;
-	unsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());
 
 	qv = container_of(napi, struct mvpp2_queue_vector, napi);
 
@@ -3360,20 +4582,11 @@ static int mvpp2_poll(struct napi_struct *napi, int budget)
 	cause_rx_tx = mvpp2_thread_read_relaxed(port->priv, qv->sw_thread_id,
 						MVPP2_ISR_RX_TX_CAUSE_REG(port->id));
 
-	cause_misc = cause_rx_tx & MVPP2_CAUSE_MISC_SUM_MASK;
-	if (cause_misc) {
-		mvpp2_cause_error(port->dev, cause_misc);
-
-		/* Clear the cause register */
-		mvpp2_write(port->priv, MVPP2_ISR_MISC_CAUSE_REG, 0);
-		mvpp2_thread_write(port->priv, thread,
-				   MVPP2_ISR_RX_TX_CAUSE_REG(port->id),
-				   cause_rx_tx & ~MVPP2_CAUSE_MISC_SUM_MASK);
-	}
-
 	if (port->has_tx_irqs) {
 		cause_tx = cause_rx_tx & MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;
 		if (cause_tx) {
+			per_cpu_ptr(port->pcpu,
+				    qv->sw_thread_id)->tx_done_passed =	true;
 			cause_tx >>= MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_OFFSET;
 			mvpp2_tx_done(port, cause_tx, qv->sw_thread_id);
 		}
@@ -3381,7 +4594,7 @@ static int mvpp2_poll(struct napi_struct *napi, int budget)
 
 	/* Process RX packets */
 	cause_rx = cause_rx_tx &
-		   MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(port->priv->hw_version);
+		   MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(mvpp21_variant);
 	cause_rx <<= qv->first_rxq;
 	cause_rx |= qv->pending_cause_rx;
 	while (cause_rx && budget > 0) {
@@ -3418,26 +4631,22 @@ static void mvpp22_mode_reconfigure(struct mvpp2_port *port)
 {
 	u32 ctrl3;
 
-	/* Set the GMAC & XLG MAC in reset */
-	mvpp2_mac_reset_assert(port);
-
-	/* Set the MPCS and XPCS in reset */
-	mvpp22_pcs_reset_assert(port);
-
 	/* comphy reconfiguration */
 	mvpp22_comphy_init(port);
 
 	/* gop reconfiguration */
 	mvpp22_gop_init(port);
 
-	mvpp22_pcs_reset_deassert(port);
+	if  (port->phy_interface == PHY_INTERFACE_MODE_INTERNAL)
+		return;
 
-	/* Only GOP port 0 has an XLG MAC */
-	if (port->gop_id == 0) {
+	if (port->has_xlg_mac) {
 		ctrl3 = readl(port->base + MVPP22_XLG_CTRL3_REG);
 		ctrl3 &= ~MVPP22_XLG_CTRL3_MACMODESELECT_MASK;
 
-		if (mvpp2_is_xlg(port->phy_interface))
+		if (port->phy_interface == PHY_INTERFACE_MODE_RXAUI ||
+		    port->phy_interface == PHY_INTERFACE_MODE_10GKR ||
+		    port->phy_interface == PHY_INTERFACE_MODE_5GKR)
 			ctrl3 |= MVPP22_XLG_CTRL3_MACMODESELECT_10G;
 		else
 			ctrl3 |= MVPP22_XLG_CTRL3_MACMODESELECT_GMAC;
@@ -3445,7 +4654,10 @@ static void mvpp22_mode_reconfigure(struct mvpp2_port *port)
 		writel(ctrl3, port->base + MVPP22_XLG_CTRL3_REG);
 	}
 
-	if (port->gop_id == 0 && mvpp2_is_xlg(port->phy_interface))
+	if (port->has_xlg_mac &&
+	    (port->phy_interface == PHY_INTERFACE_MODE_RXAUI ||
+	     port->phy_interface == PHY_INTERFACE_MODE_10GKR ||
+	     port->phy_interface == PHY_INTERFACE_MODE_5GKR))
 		mvpp2_xlg_max_rx_size_set(port);
 	else
 		mvpp2_gmac_max_rx_size_set(port);
@@ -3458,13 +4670,17 @@ static void mvpp2_start_dev(struct mvpp2_port *port)
 
 	mvpp2_txp_max_tx_size_set(port);
 
+	/* stop_dev() sets Coal to ZERO. Care to restore it now */
+	if (port->has_tx_irqs)
+		mvpp2_tx_pkts_coal_set(port);
+
 	for (i = 0; i < port->nqvecs; i++)
 		napi_enable(&port->qvecs[i].napi);
 
 	/* Enable interrupts on all threads */
 	mvpp2_interrupts_enable(port);
 
-	if (port->priv->hw_version == MVPP22)
+	if (port->priv->hw_version != MVPP21)
 		mvpp22_mode_reconfigure(port);
 
 	if (port->phylink) {
@@ -3482,7 +4698,7 @@ static void mvpp2_start_dev(struct mvpp2_port *port)
 				  port->phy_interface, NULL);
 	}
 
-	netif_tx_start_all_queues(port->dev);
+	mvpp2_tx_start_all_queues(port->dev);
 }
 
 /* Set hw internals when stopping port */
@@ -3490,6 +4706,21 @@ static void mvpp2_stop_dev(struct mvpp2_port *port)
 {
 	int i;
 
+	/* Stop-dev called by ifconfig but also by ethtool-features.
+	 * Under active traffic the BM/RX and TX PP2-HW could be non-empty.
+	 * Stop asap new packets ariving from both RX and TX directions,
+	 * but do NOT disable egress free/send-out and interrupts tx-done,
+	 * yeild and msleep this context for gracefull finishing.
+	 * Flush all tx-done by forcing pkts-coal to ZERO
+	 */
+	mvpp2_tx_stop_all_queues(port->dev);
+	mvpp2_ingress_disable(port);
+	if (port->has_tx_irqs)
+		on_each_cpu(mvpp2_tx_pkts_coal_set_zero_pcpu, port, 1);
+
+	msleep(40);
+	mvpp2_egress_disable(port);
+
 	/* Disable interrupts on all threads */
 	mvpp2_interrupts_disable(port);
 
@@ -3520,11 +4751,8 @@ static int mvpp2_check_ringparam_valid(struct net_device *dev,
 	else if (!IS_ALIGNED(ring->tx_pending, 32))
 		new_tx_pending = ALIGN(ring->tx_pending, 32);
 
-	/* The Tx ring size cannot be smaller than the minimum number of
-	 * descriptors needed for TSO.
-	 */
-	if (new_tx_pending < MVPP2_MAX_SKB_DESCS)
-		new_tx_pending = ALIGN(MVPP2_MAX_SKB_DESCS, 32);
+	if (new_tx_pending < MVPP2_MIN_TXD(num_present_cpus()))
+		new_tx_pending = MVPP2_MIN_TXD(num_present_cpus());
 
 	if (ring->rx_pending != new_rx_pending) {
 		netdev_info(dev, "illegal Rx ring size value %d, round to %d\n",
@@ -3619,25 +4847,32 @@ static void mvpp2_irqs_deinit(struct mvpp2_port *port)
 	}
 }
 
-static bool mvpp22_rss_is_supported(void)
+static bool mvpp22_rss_is_supported(struct mvpp2_port *port)
 {
-	return queue_mode == MVPP2_QDIST_MULTI_MODE;
+	return (queue_mode == MVPP2_QDIST_MULTI_MODE) &&
+		!(port->flags & MVPP2_F_LOOPBACK) &&
+		!(port->flags & MVPP22_F_IF_MUSDK);
 }
 
 static int mvpp2_open(struct net_device *dev)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
 	struct mvpp2 *priv = port->priv;
+	struct mvpp2_port_pcpu *port_pcpu;
 	unsigned char mac_bcast[ETH_ALEN] = {
 			0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
 	bool valid = false;
-	int err;
+	int err, cpu;
 
 	err = mvpp2_prs_mac_da_accept(port, mac_bcast, true);
 	if (err) {
 		netdev_err(dev, "mvpp2_prs_mac_da_accept BC failed\n");
 		return err;
 	}
+
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		goto skip_musdk_parser;
+
 	err = mvpp2_prs_mac_da_accept(port, dev->dev_addr, true);
 	if (err) {
 		netdev_err(dev, "mvpp2_prs_mac_da_accept own addr failed\n");
@@ -3654,6 +4889,7 @@ static int mvpp2_open(struct net_device *dev)
 		return err;
 	}
 
+skip_musdk_parser:
 	/* Allocate the Rx/Tx queues */
 	err = mvpp2_setup_rxqs(port);
 	if (err) {
@@ -3667,6 +4903,9 @@ static int mvpp2_open(struct net_device *dev)
 		goto err_cleanup_rxqs;
 	}
 
+	/* Recycle buffer pool for performance optimization */
+	mvpp2_recycle_open();
+
 	err = mvpp2_irqs_init(port);
 	if (err) {
 		netdev_err(port->dev, "cannot init IRQs\n");
@@ -3685,7 +4924,9 @@ static int mvpp2_open(struct net_device *dev)
 		valid = true;
 	}
 
-	if (priv->hw_version == MVPP22 && port->link_irq) {
+	if (priv->hw_version != MVPP21 && port->link_irq &&
+	    (!port->phylink || !port->has_phy)) {
+		mvpp2_txqs_on_tasklet_init(port);
 		err = request_irq(port->link_irq, mvpp2_link_status_isr, 0,
 				  dev->name, port);
 		if (err) {
@@ -3711,10 +4952,19 @@ static int mvpp2_open(struct net_device *dev)
 		goto err_free_irq;
 	}
 
+	/* Init bulk-transmit timer */
+	for_each_present_cpu(cpu) {
+		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+		port_pcpu->bulk_timer_scheduled = false;
+		port_pcpu->bulk_timer_restart_req = false;
+	}
+
 	/* Unmask interrupts on all CPUs */
 	on_each_cpu(mvpp2_interrupts_unmask, port, 1);
 	mvpp2_shared_interrupt_mask_unmask(port, false);
 
+	mvpp2_tx_done_init_on_open(port, true);
+
 	mvpp2_start_dev(port);
 
 	/* Start hardware statistics gathering */
@@ -3737,6 +4987,7 @@ static int mvpp2_stop(struct net_device *dev)
 	struct mvpp2_port *port = netdev_priv(dev);
 	struct mvpp2_port_pcpu *port_pcpu;
 	unsigned int thread;
+	int cpu;
 
 	mvpp2_stop_dev(port);
 
@@ -3755,16 +5006,24 @@ static int mvpp2_stop(struct net_device *dev)
 			port_pcpu = per_cpu_ptr(port->pcpu, thread);
 
 			hrtimer_cancel(&port_pcpu->tx_done_timer);
-			port_pcpu->timer_scheduled = false;
+			port_pcpu->tx_done_timer_scheduled = false;
+			tasklet_kill(&port_pcpu->tx_done_tasklet);
 		}
 	}
+	/* Cancel bulk tasklet and timer */
+	for_each_present_cpu(cpu) {
+		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+		hrtimer_cancel(&port_pcpu->bulk_timer);
+		tasklet_kill(&port_pcpu->bulk_tasklet);
+	}
+	mvpp2_tx_done_init_on_open(port, false);
+	mvpp2_txqs_on_tasklet_kill(port);
 	mvpp2_cleanup_rxqs(port);
 	mvpp2_cleanup_txqs(port);
 
 	cancel_delayed_work_sync(&port->stats_work);
 
-	mvpp2_mac_reset_assert(port);
-	mvpp22_pcs_reset_assert(port);
+	mvpp2_recycle_close();
 
 	return 0;
 }
@@ -3846,99 +5105,50 @@ static int mvpp2_set_mac_address(struct net_device *dev, void *p)
 	return err;
 }
 
-/* Shut down all the ports, reconfigure the pools as percpu or shared,
- * then bring up again all ports.
- */
-static int mvpp2_bm_switch_buffers(struct mvpp2 *priv, bool percpu)
-{
-	int numbufs = MVPP2_BM_POOLS_NUM, i;
-	struct mvpp2_port *port = NULL;
-	bool status[MVPP2_MAX_PORTS];
-
-	for (i = 0; i < priv->port_count; i++) {
-		port = priv->port_list[i];
-		status[i] = netif_running(port->dev);
-		if (status[i])
-			mvpp2_stop(port->dev);
-	}
-
-	/* nrxqs is the same for all ports */
-	if (priv->percpu_pools)
-		numbufs = port->nrxqs * 2;
-
-	for (i = 0; i < numbufs; i++)
-		mvpp2_bm_pool_destroy(port->dev->dev.parent, priv, &priv->bm_pools[i]);
-
-	devm_kfree(port->dev->dev.parent, priv->bm_pools);
-	priv->percpu_pools = percpu;
-	mvpp2_bm_init(port->dev->dev.parent, priv);
-
-	for (i = 0; i < priv->port_count; i++) {
-		port = priv->port_list[i];
-		mvpp2_swf_bm_pool_init(port);
-		if (status[i])
-			mvpp2_open(port->dev);
-	}
-
-	return 0;
-}
-
 static int mvpp2_change_mtu(struct net_device *dev, int mtu)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	bool running = netif_running(dev);
-	struct mvpp2 *priv = port->priv;
 	int err;
 
-	if (!IS_ALIGNED(MVPP2_RX_PKT_SIZE(mtu), 8)) {
-		netdev_info(dev, "illegal MTU value %d, round to %d\n", mtu,
-			    ALIGN(MVPP2_RX_PKT_SIZE(mtu), 8));
-		mtu = ALIGN(MVPP2_RX_PKT_SIZE(mtu), 8);
+	if (port->flags & MVPP22_F_IF_MUSDK) {
+		netdev_err(dev, "MTU cannot be modified in MUSDK mode\n");
+		return -EPERM;
 	}
 
-	if (MVPP2_RX_PKT_SIZE(mtu) > MVPP2_BM_LONG_PKT_SIZE) {
-		if (priv->percpu_pools) {
-			netdev_warn(dev, "mtu %d too high, switching to shared buffers", mtu);
-			mvpp2_bm_switch_buffers(priv, false);
+	if (!netif_running(dev)) {
+		err = mvpp2_bm_update_mtu(dev, mtu);
+		if (!err) {
+			port->pkt_size =  MVPP2_RX_PKT_SIZE(mtu);
+			return 0;
 		}
-	} else {
-		bool jumbo = false;
-		int i;
-
-		for (i = 0; i < priv->port_count; i++)
-			if (priv->port_list[i] != port &&
-			    MVPP2_RX_PKT_SIZE(priv->port_list[i]->dev->mtu) >
-			    MVPP2_BM_LONG_PKT_SIZE) {
-				jumbo = true;
-				break;
-			}
 
-		/* No port is using jumbo frames */
-		if (!jumbo) {
-			dev_info(port->dev->dev.parent,
-				 "all ports have a low MTU, switching to per-cpu buffers");
-			mvpp2_bm_switch_buffers(priv, true);
-		}
+		/* Reconfigure BM to the original MTU */
+		err = mvpp2_bm_update_mtu(dev, dev->mtu);
+		if (err)
+			goto log_error;
 	}
 
-	if (running)
-		mvpp2_stop_dev(port);
+	mvpp2_stop_dev(port);
 
 	err = mvpp2_bm_update_mtu(dev, mtu);
-	if (err) {
-		netdev_err(dev, "failed to change MTU\n");
-		/* Reconfigure BM to the original MTU */
-		mvpp2_bm_update_mtu(dev, dev->mtu);
-	} else {
+	if (!err) {
 		port->pkt_size =  MVPP2_RX_PKT_SIZE(mtu);
+		goto out_start;
 	}
 
-	if (running) {
-		mvpp2_start_dev(port);
-		mvpp2_egress_enable(port);
-		mvpp2_ingress_enable(port);
-	}
+	/* Reconfigure BM to the original MTU */
+	err = mvpp2_bm_update_mtu(dev, dev->mtu);
+	if (err)
+		goto log_error;
 
+out_start:
+	mvpp2_start_dev(port);
+	mvpp2_egress_enable(port);
+	mvpp2_ingress_enable(port);
+
+	return 0;
+log_error:
+	netdev_err(dev, "failed to change MTU\n");
 	return err;
 }
 
@@ -3974,6 +5184,7 @@ mvpp2_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
 	stats->rx_errors	= dev->stats.rx_errors;
 	stats->rx_dropped	= dev->stats.rx_dropped;
 	stats->tx_dropped	= dev->stats.tx_dropped;
+	stats->collisions	= dev->stats.collisions;
 }
 
 static int mvpp2_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
@@ -4027,9 +5238,9 @@ static int mvpp2_set_features(struct net_device *dev,
 
 	if (changed & NETIF_F_RXHASH) {
 		if (features & NETIF_F_RXHASH)
-			mvpp22_port_rss_enable(port);
+			mvpp22_rss_enable(port);
 		else
-			mvpp22_port_rss_disable(port);
+			mvpp22_rss_disable(port);
 	}
 
 	return 0;
@@ -4052,6 +5263,7 @@ static int mvpp2_ethtool_set_coalesce(struct net_device *dev,
 				      struct ethtool_coalesce *c)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
+	struct mvpp2_tx_queue *txq;
 	int queue;
 
 	for (queue = 0; queue < port->nrxqs; queue++) {
@@ -4063,18 +5275,22 @@ static int mvpp2_ethtool_set_coalesce(struct net_device *dev,
 		mvpp2_rx_time_coal_set(port, rxq);
 	}
 
-	if (port->has_tx_irqs) {
+	/* Set TX time and pkts coalescing configuration */
+	if (port->has_tx_irqs)
 		port->tx_time_coal = c->tx_coalesce_usecs;
-		mvpp2_tx_time_coal_set(port);
-	}
 
 	for (queue = 0; queue < port->ntxqs; queue++) {
-		struct mvpp2_tx_queue *txq = port->txqs[queue];
-
+		txq = port->txqs[queue];
 		txq->done_pkts_coal = c->tx_max_coalesced_frames;
+		if (port->has_tx_irqs &&
+		    txq->done_pkts_coal > MVPP2_TXQ_THRESH_MASK)
+			txq->done_pkts_coal = MVPP2_TXQ_THRESH_MASK;
+	}
 
-		if (port->has_tx_irqs)
-			mvpp2_tx_pkts_coal_set(port, txq);
+	if (port->has_tx_irqs) {
+		/* Download configured values into MVPP2 HW */
+		mvpp2_tx_time_coal_set(port);
+		mvpp2_tx_pkts_coal_set(port);
 	}
 
 	return 0;
@@ -4096,12 +5312,16 @@ static int mvpp2_ethtool_get_coalesce(struct net_device *dev,
 static void mvpp2_ethtool_get_drvinfo(struct net_device *dev,
 				      struct ethtool_drvinfo *drvinfo)
 {
+	struct mvpp2_port *port = netdev_priv(dev);
+
 	strlcpy(drvinfo->driver, MVPP2_DRIVER_NAME,
 		sizeof(drvinfo->driver));
 	strlcpy(drvinfo->version, MVPP2_DRIVER_VERSION,
 		sizeof(drvinfo->version));
 	strlcpy(drvinfo->bus_info, dev_name(&dev->dev),
 		sizeof(drvinfo->bus_info));
+	drvinfo->n_priv_flags = (port->priv->hw_version == MVPP21) ?
+			0 : ARRAY_SIZE(mvpp22_priv_flags_strings);
 }
 
 static void mvpp2_ethtool_get_ringparam(struct net_device *dev,
@@ -4127,6 +5347,15 @@ static int mvpp2_ethtool_set_ringparam(struct net_device *dev,
 	if (err)
 		return err;
 
+	if (ring->rx_pending < MSS_THRESHOLD_START && port->tx_fc) {
+		netdev_warn(dev, "TX FC disabled. Ring size is less than %d\n",
+			    MSS_THRESHOLD_START);
+		port->tx_fc = false;
+		mvpp2_rxq_disable_fc(port);
+		if (port->priv->hw_version == MVPP23)
+			mvpp23_rx_fifo_fc_en(port->priv, port->id, false);
+	}
+
 	if (!netif_running(dev)) {
 		port->rx_ring_size = ring->rx_pending;
 		port->tx_ring_size = ring->tx_pending;
@@ -4186,11 +5415,52 @@ static void mvpp2_ethtool_get_pause_param(struct net_device *dev,
 	phylink_ethtool_get_pauseparam(port->phylink, pause);
 }
 
+static void mvpp2_reconfigure_fc(struct mvpp2_port *port)
+{
+	struct mvpp2_bm_pool **pools_pcpu = port->priv->pools_pcpu;
+	int cpu;
+
+	if (recycle) {
+		for_each_present_cpu(cpu)
+			mvpp2_bm_pool_update_fc(port, pools_pcpu[cpu],
+						port->tx_fc);
+		if (port->pool_long->type == MVPP2_BM_JUMBO)
+			mvpp2_bm_pool_update_fc(port,
+						port->pool_long, port->tx_fc);
+		else
+			mvpp2_bm_pool_update_fc(port,
+						port->pool_short, port->tx_fc);
+	} else {
+		mvpp2_bm_pool_update_fc(port, port->pool_long, port->tx_fc);
+		mvpp2_bm_pool_update_fc(port, port->pool_short, port->tx_fc);
+	}
+	if (port->priv->hw_version == MVPP23)
+		mvpp23_rx_fifo_fc_en(port->priv, port->id, port->tx_fc);
+}
+
 static int mvpp2_ethtool_set_pause_param(struct net_device *dev,
 					 struct ethtool_pauseparam *pause)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
 
+	if (pause->tx_pause && port->priv->global_tx_fc &&
+	    bm_underrun_protect) {
+		if (port->rx_ring_size < MSS_THRESHOLD_START) {
+			netdev_err(dev, "TX FC cannot be supported.");
+			netdev_err(dev, "Ring size is less than %d\n",
+				   MSS_THRESHOLD_START);
+			return -EINVAL;
+		}
+
+		port->tx_fc = true;
+		mvpp2_rxq_enable_fc(port);
+		mvpp2_reconfigure_fc(port);
+	} else if (port->priv->global_tx_fc) {
+		port->tx_fc = false;
+		mvpp2_rxq_disable_fc(port);
+		mvpp2_reconfigure_fc(port);
+	}
+
 	if (!port->phylink)
 		return -ENOTSUPP;
 
@@ -4223,9 +5493,9 @@ static int mvpp2_ethtool_get_rxnfc(struct net_device *dev,
 				   struct ethtool_rxnfc *info, u32 *rules)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	int ret = 0, i, loc = 0;
+	int ret = 0;
 
-	if (!mvpp22_rss_is_supported())
+	if (!mvpp22_rss_is_supported(port))
 		return -EOPNOTSUPP;
 
 	switch (info->cmd) {
@@ -4235,18 +5505,6 @@ static int mvpp2_ethtool_get_rxnfc(struct net_device *dev,
 	case ETHTOOL_GRXRINGS:
 		info->data = port->nrxqs;
 		break;
-	case ETHTOOL_GRXCLSRLCNT:
-		info->rule_cnt = port->n_rfs_rules;
-		break;
-	case ETHTOOL_GRXCLSRULE:
-		ret = mvpp2_ethtool_cls_rule_get(port, info);
-		break;
-	case ETHTOOL_GRXCLSRLALL:
-		for (i = 0; i < MVPP2_N_RFS_ENTRIES_PER_FLOW; i++) {
-			if (port->rfs_rules[i])
-				rules[loc++] = i;
-		}
-		break;
 	default:
 		return -ENOTSUPP;
 	}
@@ -4260,19 +5518,13 @@ static int mvpp2_ethtool_set_rxnfc(struct net_device *dev,
 	struct mvpp2_port *port = netdev_priv(dev);
 	int ret = 0;
 
-	if (!mvpp22_rss_is_supported())
+	if (!mvpp22_rss_is_supported(port))
 		return -EOPNOTSUPP;
 
 	switch (info->cmd) {
 	case ETHTOOL_SRXFH:
 		ret = mvpp2_ethtool_rxfh_set(port, info);
 		break;
-	case ETHTOOL_SRXCLSRLINS:
-		ret = mvpp2_ethtool_cls_rule_ins(port, info);
-		break;
-	case ETHTOOL_SRXCLSRLDEL:
-		ret = mvpp2_ethtool_cls_rule_del(port, info);
-		break;
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -4281,34 +5533,35 @@ static int mvpp2_ethtool_set_rxnfc(struct net_device *dev,
 
 static u32 mvpp2_ethtool_get_rxfh_indir_size(struct net_device *dev)
 {
-	return mvpp22_rss_is_supported() ? MVPP22_RSS_TABLE_ENTRIES : 0;
+	struct mvpp2_port *port = netdev_priv(dev);
+
+	return mvpp22_rss_is_supported(port) ? MVPP22_RSS_TABLE_ENTRIES : 0;
 }
 
 static int mvpp2_ethtool_get_rxfh(struct net_device *dev, u32 *indir, u8 *key,
 				  u8 *hfunc)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	int ret = 0;
 
-	if (!mvpp22_rss_is_supported())
+	if (!mvpp22_rss_is_supported(port))
 		return -EOPNOTSUPP;
 
 	if (indir)
-		ret = mvpp22_port_rss_ctx_indir_get(port, 0, indir);
+		memcpy(indir, port->indir,
+		       ARRAY_SIZE(port->indir) * sizeof(port->indir[0]));
 
 	if (hfunc)
 		*hfunc = ETH_RSS_HASH_CRC32;
 
-	return ret;
+	return 0;
 }
 
 static int mvpp2_ethtool_set_rxfh(struct net_device *dev, const u32 *indir,
 				  const u8 *key, const u8 hfunc)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	int ret = 0;
 
-	if (!mvpp22_rss_is_supported())
+	if (!mvpp22_rss_is_supported(port))
 		return -EOPNOTSUPP;
 
 	if (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_CRC32)
@@ -4317,60 +5570,139 @@ static int mvpp2_ethtool_set_rxfh(struct net_device *dev, const u32 *indir,
 	if (key)
 		return -EOPNOTSUPP;
 
-	if (indir)
-		ret = mvpp22_port_rss_ctx_indir_set(port, 0, indir);
+	if (indir) {
+		memcpy(port->indir, indir,
+		       ARRAY_SIZE(port->indir) * sizeof(port->indir[0]));
+		mvpp22_rss_fill_table(port, port->id);
+	}
 
-	return ret;
+	return 0;
 }
 
-static int mvpp2_ethtool_get_rxfh_context(struct net_device *dev, u32 *indir,
-					  u8 *key, u8 *hfunc, u32 rss_context)
+static u32 mvpp22_get_priv_flags(struct net_device *dev)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	int ret = 0;
+	u32 priv_flags = 0;
 
-	if (!mvpp22_rss_is_supported())
-		return -EOPNOTSUPP;
-	if (rss_context >= MVPP22_N_RSS_TABLES)
-		return -EINVAL;
+	if (port->flags & MVPP22_F_IF_MUSDK)
+		priv_flags |= MVPP22_F_IF_MUSDK_PRIV;
+	return priv_flags;
+}
 
-	if (hfunc)
-		*hfunc = ETH_RSS_HASH_CRC32;
+static int mvpp2_port_musdk_cfg(struct net_device *dev, bool ena)
+{
+	struct mvpp2_port_us_cfg {
+		unsigned int nqvecs;
+		unsigned int nrxqs;
+		unsigned int ntxqs;
+		int mtu;
+		bool rxhash_en;
+		u8 rss_en;
+	} *us;
+
+	struct mvpp2_port *port = netdev_priv(dev);
+	int rxq;
+
+	if (ena) {
+		/* Disable Queues and IntVec allocations for MUSDK,
+		 * but save original values.
+		 */
+		us = kzalloc(sizeof(*us), GFP_KERNEL);
+		if (!us)
+			return -ENOMEM;
+		port->us_cfg = (void *)us;
+		us->nqvecs = port->nqvecs;
+		us->nrxqs  = port->nrxqs;
+		us->ntxqs = port->ntxqs;
+		us->mtu = dev->mtu;
+		us->rxhash_en = !!(dev->hw_features & NETIF_F_RXHASH);
+
+		port->nqvecs = 0;
+		port->nrxqs  = 0;
+		port->ntxqs  = 0;
+		if (us->rxhash_en) {
+			dev->hw_features &= ~NETIF_F_RXHASH;
+			netdev_update_features(dev);
+		}
+	} else {
+		/* Back to Kernel mode */
+		us = port->us_cfg;
+		port->nqvecs = us->nqvecs;
+		port->nrxqs  = us->nrxqs;
+		port->ntxqs  = us->ntxqs;
+		if (us->rxhash_en) {
+			dev->hw_features |= NETIF_F_RXHASH;
+			netdev_update_features(dev);
+		}
+		kfree(us);
+		port->us_cfg = NULL;
+
+		/* Restore RxQ/pool association */
+		for (rxq = 0; rxq < port->nrxqs; rxq++) {
+			mvpp2_rxq_long_pool_set(port, rxq, port->pool_long->id);
+			mvpp2_rxq_short_pool_set(port, rxq,
+						 port->pool_short->id);
+		}
+	}
+	return 0;
+}
+
+static int mvpp2_port_musdk_set(struct net_device *dev, bool ena)
+{
+	struct mvpp2_port *port = netdev_priv(dev);
+	bool running = netif_running(dev);
+	int err;
+
+	/* This procedure is called by ethtool change or by Module-remove.
+	 * For "remove" do anything only if we are in musdk-mode
+	 * and toggling back to Kernel-mode is really required.
+	 */
+	if (!ena && !port->us_cfg)
+		return 0;
+
+	if (running)
+		mvpp2_stop(dev);
+
+	if (ena) {
+		err = mvpp2_port_musdk_cfg(dev, ena);
+		port->flags |= MVPP22_F_IF_MUSDK;
+	} else {
+		err = mvpp2_port_musdk_cfg(dev, ena);
+		port->flags &= ~MVPP22_F_IF_MUSDK;
+	}
+
+	if (err) {
+		netdev_err(dev, "musdk set=%d: error=%d\n", ena, err);
+		if (err)
+			return err;
+		/* print Error message but continue */
+	}
 
-	if (indir)
-		ret = mvpp22_port_rss_ctx_indir_get(port, rss_context, indir);
+	if (running)
+		mvpp2_open(dev);
 
-	return ret;
+	return 0;
 }
 
-static int mvpp2_ethtool_set_rxfh_context(struct net_device *dev,
-					  const u32 *indir, const u8 *key,
-					  const u8 hfunc, u32 *rss_context,
-					  bool delete)
+static int mvpp22_set_priv_flags(struct net_device *dev, u32 priv_flags)
 {
 	struct mvpp2_port *port = netdev_priv(dev);
-	int ret;
+	bool f_old, f_new;
+	int err = 0;
 
-	if (!mvpp22_rss_is_supported())
+	if (recycle && (priv_flags & MVPP22_F_IF_MUSDK_PRIV)) {
+		WARN(1, "Fail to enable MUSDK. KS recycling feature enabled.");
 		return -EOPNOTSUPP;
-
-	if (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_CRC32)
-		return -EOPNOTSUPP;
-
-	if (key)
-		return -EOPNOTSUPP;
-
-	if (delete)
-		return mvpp22_port_rss_ctx_delete(port, *rss_context);
-
-	if (*rss_context == ETH_RXFH_CONTEXT_ALLOC) {
-		ret = mvpp22_port_rss_ctx_create(port, rss_context);
-		if (ret)
-			return ret;
 	}
 
-	return mvpp22_port_rss_ctx_indir_set(port, *rss_context, indir);
+	f_old = port->flags & MVPP22_F_IF_MUSDK;
+	f_new = priv_flags & MVPP22_F_IF_MUSDK_PRIV;
+	if (f_old != f_new)
+		err = mvpp2_port_musdk_set(dev, f_new);
+
+	return err;
 }
+
 /* Device ops */
 
 static const struct net_device_ops mvpp2_netdev_ops = {
@@ -4407,8 +5739,8 @@ static const struct ethtool_ops mvpp2_eth_tool_ops = {
 	.get_rxfh_indir_size	= mvpp2_ethtool_get_rxfh_indir_size,
 	.get_rxfh		= mvpp2_ethtool_get_rxfh,
 	.set_rxfh		= mvpp2_ethtool_set_rxfh,
-	.get_rxfh_context	= mvpp2_ethtool_get_rxfh_context,
-	.set_rxfh_context	= mvpp2_ethtool_set_rxfh_context,
+	.get_priv_flags		= mvpp22_get_priv_flags,
+	.set_priv_flags		= mvpp22_set_priv_flags,
 };
 
 /* Used for PPv2.1, or PPv2.2 with the old Device Tree binding that
@@ -4531,7 +5863,7 @@ static void mvpp2_rx_irqs_setup(struct mvpp2_port *port)
 		return;
 	}
 
-	/* Handle the more complicated PPv2.2 case */
+	/* Handle the more complicated PPv2.2 and PPv2.3 case */
 	for (i = 0; i < port->nqvecs; i++) {
 		struct mvpp2_queue_vector *qv = port->qvecs + i;
 
@@ -4555,7 +5887,7 @@ static int mvpp2_port_init(struct mvpp2_port *port)
 	struct mvpp2 *priv = port->priv;
 	struct mvpp2_txq_pcpu *txq_pcpu;
 	unsigned int thread;
-	int queue, err, val;
+	int queue, err;
 
 	/* Checks for hardware constraints */
 	if (port->first_rxq + port->nrxqs >
@@ -4569,18 +5901,6 @@ static int mvpp2_port_init(struct mvpp2_port *port)
 	mvpp2_egress_disable(port);
 	mvpp2_port_disable(port);
 
-	if (mvpp2_is_xlg(port->phy_interface)) {
-		val = readl(port->base + MVPP22_XLG_CTRL0_REG);
-		val &= ~MVPP22_XLG_CTRL0_FORCE_LINK_PASS;
-		val |= MVPP22_XLG_CTRL0_FORCE_LINK_DOWN;
-		writel(val, port->base + MVPP22_XLG_CTRL0_REG);
-	} else {
-		val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
-		val &= ~MVPP2_GMAC_FORCE_LINK_PASS;
-		val |= MVPP2_GMAC_FORCE_LINK_DOWN;
-		writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
-	}
-
 	port->tx_time_coal = MVPP2_TXDONE_COAL_USEC;
 
 	port->txqs = devm_kcalloc(dev, port->ntxqs, sizeof(*port->txqs),
@@ -4638,7 +5958,7 @@ static int mvpp2_port_init(struct mvpp2_port *port)
 		/* Map this Rx queue to a physical queue */
 		rxq->id = port->first_rxq + queue;
 		rxq->port = port->id;
-		rxq->logic_rxq = queue;
+		rxq->logic_rxq = (u8)queue;
 
 		port->rxqs[queue] = rxq;
 	}
@@ -4663,22 +5983,20 @@ static int mvpp2_port_init(struct mvpp2_port *port)
 	mvpp2_cls_oversize_rxq_set(port);
 	mvpp2_cls_port_config(port);
 
-	if (mvpp22_rss_is_supported())
-		mvpp22_port_rss_init(port);
+	if (mvpp22_rss_is_supported(port))
+		mvpp22_rss_port_init(port);
 
 	/* Provide an initial Rx packet size */
 	port->pkt_size = MVPP2_RX_PKT_SIZE(port->dev->mtu);
 
 	/* Initialize pools for swf */
-	err = mvpp2_swf_bm_pool_init(port);
+	if (recycle)
+		err = mvpp2_swf_bm_pool_pcpu_init(port);
+	else
+		err = mvpp2_swf_bm_pool_init(port);
 	if (err)
 		goto err_free_percpu;
 
-	/* Clear all port stats */
-	mvpp2_read_stats(port);
-	memset(port->ethtool_stats, 0,
-	       MVPP2_N_ETHTOOL_STATS(port->ntxqs, port->nrxqs) * sizeof(u64));
-
 	return 0;
 
 err_free_percpu:
@@ -4708,7 +6026,7 @@ static bool mvpp22_port_has_legacy_tx_irqs(struct device_node *port_node,
 
 /* Checks if the port dt description has the required Tx interrupts:
  * - PPv2.1: there are no such interrupts.
- * - PPv2.2:
+ * - PPv2.2 and PPv2.3:
  *   - The old DTs have: "rx-shared", "tx-cpuX" with X in [0...3]
  *   - The new ones have: "hifX" with X in [0..8]
  *
@@ -4777,21 +6095,28 @@ static void mvpp2_phylink_validate(struct phylink_config *config,
 				   unsigned long *supported,
 				   struct phylink_link_state *state)
 {
-	struct mvpp2_port *port = mvpp2_phylink_to_port(config);
+	struct net_device *dev = to_net_dev(config->dev);
+	struct mvpp2_port *port = netdev_priv(dev);
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
 
 	/* Invalid combinations */
 	switch (state->interface) {
 	case PHY_INTERFACE_MODE_10GKR:
-	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_5GKR:
+	case PHY_INTERFACE_MODE_INTERNAL:
+		if (!port->has_xlg_mac)
+			goto empty_set;
+		break;
+	case PHY_INTERFACE_MODE_RXAUI:
 		if (port->gop_id != 0)
 			goto empty_set;
 		break;
+	case PHY_INTERFACE_MODE_GMII:
 	case PHY_INTERFACE_MODE_RGMII:
 	case PHY_INTERFACE_MODE_RGMII_ID:
 	case PHY_INTERFACE_MODE_RGMII_RXID:
 	case PHY_INTERFACE_MODE_RGMII_TXID:
-		if (port->priv->hw_version == MVPP22 && port->gop_id == 0)
+		if (port->priv->hw_version != MVPP21 && port->gop_id == 0)
 			goto empty_set;
 		break;
 	default:
@@ -4803,9 +6128,10 @@ static void mvpp2_phylink_validate(struct phylink_config *config,
 
 	switch (state->interface) {
 	case PHY_INTERFACE_MODE_10GKR:
-	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
 	case PHY_INTERFACE_MODE_NA:
-		if (port->gop_id == 0) {
+	case PHY_INTERFACE_MODE_INTERNAL:
+		if (port->has_xlg_mac) {
 			phylink_set(mask, 10000baseT_Full);
 			phylink_set(mask, 10000baseCR_Full);
 			phylink_set(mask, 10000baseSR_Full);
@@ -4815,6 +6141,14 @@ static void mvpp2_phylink_validate(struct phylink_config *config,
 			phylink_set(mask, 10000baseKR_Full);
 		}
 		/* Fall-through */
+	case PHY_INTERFACE_MODE_5GKR:
+		if (port->has_xlg_mac)
+			phylink_set(mask, 5000baseT_Full);
+		/* Fall-through */
+	case PHY_INTERFACE_MODE_2500BASET:
+		phylink_set(mask, 2500baseT_Full);
+		/* Fall-through */
+	case PHY_INTERFACE_MODE_GMII:
 	case PHY_INTERFACE_MODE_RGMII:
 	case PHY_INTERFACE_MODE_RGMII_ID:
 	case PHY_INTERFACE_MODE_RGMII_RXID:
@@ -4824,12 +6158,11 @@ static void mvpp2_phylink_validate(struct phylink_config *config,
 		phylink_set(mask, 10baseT_Full);
 		phylink_set(mask, 100baseT_Half);
 		phylink_set(mask, 100baseT_Full);
-		/* Fall-through */
+		phylink_set(mask, 1000baseT_Full);
+		break;
 	case PHY_INTERFACE_MODE_1000BASEX:
 	case PHY_INTERFACE_MODE_2500BASEX:
-		phylink_set(mask, 1000baseT_Full);
 		phylink_set(mask, 1000baseX_Full);
-		phylink_set(mask, 2500baseT_Full);
 		phylink_set(mask, 2500baseX_Full);
 		break;
 	default:
@@ -4850,7 +6183,11 @@ static void mvpp22_xlg_link_state(struct mvpp2_port *port,
 {
 	u32 val;
 
-	state->speed = SPEED_10000;
+	if (state->interface == PHY_INTERFACE_MODE_5GKR)
+		state->speed = SPEED_5000;
+	else
+		state->speed = SPEED_10000;
+
 	state->duplex = 1;
 	state->an_complete = 1;
 
@@ -4881,6 +6218,7 @@ static void mvpp2_gmac_link_state(struct mvpp2_port *port,
 		state->speed = SPEED_1000;
 		break;
 	case PHY_INTERFACE_MODE_2500BASEX:
+	case PHY_INTERFACE_MODE_2500BASET:
 		state->speed = SPEED_2500;
 		break;
 	default:
@@ -4902,9 +6240,10 @@ static void mvpp2_gmac_link_state(struct mvpp2_port *port,
 static int mvpp2_phylink_mac_link_state(struct phylink_config *config,
 					struct phylink_link_state *state)
 {
-	struct mvpp2_port *port = mvpp2_phylink_to_port(config);
+	struct net_device *dev = to_net_dev(config->dev);
+	struct mvpp2_port *port = netdev_priv(dev);
 
-	if (port->priv->hw_version == MVPP22 && port->gop_id == 0) {
+	if (port->has_xlg_mac) {
 		u32 mode = readl(port->base + MVPP22_XLG_CTRL3_REG);
 		mode &= MVPP22_XLG_CTRL3_MACMODESELECT_MASK;
 
@@ -4920,7 +6259,8 @@ static int mvpp2_phylink_mac_link_state(struct phylink_config *config,
 
 static void mvpp2_mac_an_restart(struct phylink_config *config)
 {
-	struct mvpp2_port *port = mvpp2_phylink_to_port(config);
+	struct net_device *dev = to_net_dev(config->dev);
+	struct mvpp2_port *port = netdev_priv(dev);
 	u32 val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
 
 	writel(val | MVPP2_GMAC_IN_BAND_RESTART_AN,
@@ -4932,11 +6272,10 @@ static void mvpp2_mac_an_restart(struct phylink_config *config)
 static void mvpp2_xlg_config(struct mvpp2_port *port, unsigned int mode,
 			     const struct phylink_link_state *state)
 {
-	u32 old_ctrl0, ctrl0;
-	u32 old_ctrl4, ctrl4;
+	u32 ctrl0, ctrl4;
 
-	old_ctrl0 = ctrl0 = readl(port->base + MVPP22_XLG_CTRL0_REG);
-	old_ctrl4 = ctrl4 = readl(port->base + MVPP22_XLG_CTRL4_REG);
+	ctrl0 = readl(port->base + MVPP22_XLG_CTRL0_REG);
+	ctrl4 = readl(port->base + MVPP22_XLG_CTRL4_REG);
 
 	ctrl0 |= MVPP22_XLG_CTRL0_MAC_RESET_DIS;
 
@@ -4950,20 +6289,13 @@ static void mvpp2_xlg_config(struct mvpp2_port *port, unsigned int mode,
 	else
 		ctrl0 &= ~MVPP22_XLG_CTRL0_RX_FLOW_CTRL_EN;
 
-	ctrl4 &= ~(MVPP22_XLG_CTRL4_MACMODSELECT_GMAC |
-		   MVPP22_XLG_CTRL4_EN_IDLE_CHECK);
-	ctrl4 |= MVPP22_XLG_CTRL4_FWD_FC | MVPP22_XLG_CTRL4_FWD_PFC;
+	ctrl4 &= ~MVPP22_XLG_CTRL4_MACMODSELECT_GMAC;
 
-	if (old_ctrl0 != ctrl0)
-		writel(ctrl0, port->base + MVPP22_XLG_CTRL0_REG);
-	if (old_ctrl4 != ctrl4)
-		writel(ctrl4, port->base + MVPP22_XLG_CTRL4_REG);
+	if (state->interface == PHY_INTERFACE_MODE_RXAUI)
+		ctrl4 |= MVPP22_XLG_CTRL4_USE_XPCS;
 
-	if (!(old_ctrl0 & MVPP22_XLG_CTRL0_MAC_RESET_DIS)) {
-		while (!(readl(port->base + MVPP22_XLG_CTRL0_REG) &
-			 MVPP22_XLG_CTRL0_MAC_RESET_DIS))
-			continue;
-	}
+	writel(ctrl0, port->base + MVPP22_XLG_CTRL0_REG);
+	writel(ctrl4, port->base + MVPP22_XLG_CTRL4_REG);
 }
 
 static void mvpp2_gmac_config(struct mvpp2_port *port, unsigned int mode,
@@ -4996,7 +6328,8 @@ static void mvpp2_gmac_config(struct mvpp2_port *port, unsigned int mode,
 		ctrl4 |= MVPP22_CTRL4_SYNC_BYPASS_DIS |
 			 MVPP22_CTRL4_DP_CLK_SEL |
 			 MVPP22_CTRL4_QSGMII_BYPASS_ACTIVE;
-	} else if (state->interface == PHY_INTERFACE_MODE_SGMII) {
+	} else if (state->interface == PHY_INTERFACE_MODE_SGMII ||
+		   state->interface == PHY_INTERFACE_MODE_2500BASET) {
 		ctrl2 |= MVPP2_GMAC_PCS_ENABLE_MASK | MVPP2_GMAC_INBAND_AN_MASK;
 		ctrl4 &= ~MVPP22_CTRL4_EXT_PIN_GMII_SEL;
 		ctrl4 |= MVPP22_CTRL4_SYNC_BYPASS_DIS |
@@ -5015,6 +6348,8 @@ static void mvpp2_gmac_config(struct mvpp2_port *port, unsigned int mode,
 	if (phylink_test(state->advertising, Asym_Pause))
 		an |= MVPP2_GMAC_FC_ADV_ASM_EN;
 
+	ctrl4 &= ~(MVPP22_CTRL4_RX_FC_EN | MVPP22_CTRL4_TX_FC_EN);
+
 	/* Configure negotiation style */
 	if (!phylink_autoneg_inband(mode)) {
 		/* Phy or fixed speed - no in-band AN */
@@ -5026,44 +6361,40 @@ static void mvpp2_gmac_config(struct mvpp2_port *port, unsigned int mode,
 		else if (state->speed == SPEED_100)
 			an |= MVPP2_GMAC_CONFIG_MII_SPEED;
 
-		if (state->pause & MLO_PAUSE_TX)
-			ctrl4 |= MVPP22_CTRL4_TX_FC_EN;
-		if (state->pause & MLO_PAUSE_RX)
-			ctrl4 |= MVPP22_CTRL4_RX_FC_EN;
 	} else if (state->interface == PHY_INTERFACE_MODE_SGMII) {
 		/* SGMII in-band mode receives the speed and duplex from
-		 * the PHY. Flow control information is not received. */
-		an &= ~(MVPP2_GMAC_FORCE_LINK_DOWN | MVPP2_GMAC_FORCE_LINK_PASS);
+		 * the PHY. Flow control information is not received.
+		 */
+		an &= ~(MVPP2_GMAC_FORCE_LINK_DOWN |
+			MVPP2_GMAC_FORCE_LINK_PASS);
 		an |= MVPP2_GMAC_IN_BAND_AUTONEG |
 		      MVPP2_GMAC_AN_SPEED_EN |
 		      MVPP2_GMAC_AN_DUPLEX_EN;
 
-		if (state->pause & MLO_PAUSE_TX)
-			ctrl4 |= MVPP22_CTRL4_TX_FC_EN;
-		if (state->pause & MLO_PAUSE_RX)
-			ctrl4 |= MVPP22_CTRL4_RX_FC_EN;
-	} else if (phy_interface_mode_is_8023z(state->interface)) {
+	} else if (phy_interface_mode_is_8023z(state->interface) ||
+		   state->interface == PHY_INTERFACE_MODE_2500BASET) {
 		/* 1000BaseX and 2500BaseX ports cannot negotiate speed nor can
 		 * they negotiate duplex: they are always operating with a fixed
 		 * speed of 1000/2500Mbps in full duplex, so force 1000/2500
 		 * speed and full duplex here.
 		 */
 		ctrl0 |= MVPP2_GMAC_PORT_TYPE_MASK;
-		an &= ~(MVPP2_GMAC_FORCE_LINK_DOWN | MVPP2_GMAC_FORCE_LINK_PASS);
+		an &= ~(MVPP2_GMAC_FORCE_LINK_DOWN |
+			MVPP2_GMAC_FORCE_LINK_PASS);
 		an |= MVPP2_GMAC_IN_BAND_AUTONEG |
+		      MVPP2_GMAC_IN_BAND_AUTONEG_BYPASS |
 		      MVPP2_GMAC_CONFIG_GMII_SPEED |
 		      MVPP2_GMAC_CONFIG_FULL_DUPLEX;
 
-		if (state->pause & MLO_PAUSE_AN && state->an_enabled) {
+		if (state->pause & MLO_PAUSE_AN && state->an_enabled)
 			an |= MVPP2_GMAC_FLOW_CTRL_AUTONEG;
-		} else {
-			if (state->pause & MLO_PAUSE_TX)
-				ctrl4 |= MVPP22_CTRL4_TX_FC_EN;
-			if (state->pause & MLO_PAUSE_RX)
-				ctrl4 |= MVPP22_CTRL4_RX_FC_EN;
-		}
 	}
 
+	if (state->pause & MLO_PAUSE_TX)
+		ctrl4 |= MVPP22_CTRL4_TX_FC_EN;
+	if (state->pause & MLO_PAUSE_RX)
+		ctrl4 |= MVPP22_CTRL4_RX_FC_EN;
+
 /* Some fields of the auto-negotiation register require the port to be down when
  * their value is updated.
  */
@@ -5109,18 +6440,34 @@ static void mvpp2_mac_config(struct phylink_config *config, unsigned int mode,
 			     const struct phylink_link_state *state)
 {
 	struct mvpp2_port *port = mvpp2_phylink_to_port(config);
+	struct net_device *dev = port->dev;
 	bool change_interface = port->phy_interface != state->interface;
 
 	/* Check for invalid configuration */
-	if (mvpp2_is_xlg(state->interface) && port->gop_id != 0) {
-		netdev_err(port->dev, "Invalid mode on %s\n", port->dev->name);
-		return;
-	}
+	switch (state->interface) {
+	case PHY_INTERFACE_MODE_10GKR:
+	case PHY_INTERFACE_MODE_5GKR:
+		if (!port->has_xlg_mac) {
+			netdev_err(dev, "Invalid mode %s on %s\n",
+				   phy_modes(port->phy_interface), dev->name);
+			return;
+		}
+		break;
+	case PHY_INTERFACE_MODE_RXAUI:
+		if (port->id != 0) {
+			netdev_err(dev, "Invalid mode %s on %s\n",
+				   phy_modes(port->phy_interface), dev->name);
+			return;
+		}
+	default:
+		break;
+	};
 
-	/* Make sure the port is disabled when reconfiguring the mode */
-	mvpp2_port_disable(port);
+	if (port->priv->hw_version != MVPP21 && change_interface) {
+		/* Make sure the port is disabled when reconfiguring the mode */
+		mvpp2_tx_stop_all_queues(port->dev);
+		mvpp2_port_disable(port);
 
-	if (port->priv->hw_version == MVPP22 && change_interface) {
 		mvpp22_gop_mask_irq(port);
 
 		port->phy_interface = state->interface;
@@ -5128,23 +6475,26 @@ static void mvpp2_mac_config(struct phylink_config *config, unsigned int mode,
 		/* Reconfigure the serdes lanes */
 		phy_power_off(port->comphy);
 		mvpp22_mode_reconfigure(port);
+
+		mvpp2_tx_wake_all_queues(dev);
+		mvpp2_port_enable(port);
 	}
 
 	/* mac (re)configuration */
-	if (mvpp2_is_xlg(state->interface))
+	if (state->interface == PHY_INTERFACE_MODE_RXAUI ||
+	    state->interface == PHY_INTERFACE_MODE_10GKR ||
+	    state->interface == PHY_INTERFACE_MODE_5GKR) {
 		mvpp2_xlg_config(port, mode, state);
-	else if (phy_interface_mode_is_rgmii(state->interface) ||
-		 phy_interface_mode_is_8023z(state->interface) ||
-		 state->interface == PHY_INTERFACE_MODE_SGMII)
+	} else {
 		mvpp2_gmac_config(port, mode, state);
+		mvpp2_gmac_tx_fifo_configure(port);
+	}
 
 	if (port->priv->hw_version == MVPP21 && port->flags & MVPP2_F_LOOPBACK)
 		mvpp2_port_loopback_set(port, state);
 
-	if (port->priv->hw_version == MVPP22 && change_interface)
+	if (port->priv->hw_version != MVPP21 && change_interface)
 		mvpp22_gop_unmask_irq(port);
-
-	mvpp2_port_enable(port);
 }
 
 static void mvpp2_mac_link_up(struct phylink_config *config, unsigned int mode,
@@ -5153,48 +6503,40 @@ static void mvpp2_mac_link_up(struct phylink_config *config, unsigned int mode,
 	struct mvpp2_port *port = mvpp2_phylink_to_port(config);
 	u32 val;
 
-	if (!phylink_autoneg_inband(mode)) {
-		if (mvpp2_is_xlg(interface)) {
-			val = readl(port->base + MVPP22_XLG_CTRL0_REG);
-			val &= ~MVPP22_XLG_CTRL0_FORCE_LINK_DOWN;
-			val |= MVPP22_XLG_CTRL0_FORCE_LINK_PASS;
-			writel(val, port->base + MVPP22_XLG_CTRL0_REG);
-		} else {
-			val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
-			val &= ~MVPP2_GMAC_FORCE_LINK_DOWN;
-			val |= MVPP2_GMAC_FORCE_LINK_PASS;
-			writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
-		}
+	if (!phylink_autoneg_inband(mode) &&
+	    interface != PHY_INTERFACE_MODE_RXAUI &&
+	    interface != PHY_INTERFACE_MODE_10GKR &&
+	    interface != PHY_INTERFACE_MODE_5GKR) {
+		val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+		val &= ~MVPP2_GMAC_FORCE_LINK_DOWN;
+		val |= MVPP2_GMAC_FORCE_LINK_PASS;
+		writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
 	}
 
 	mvpp2_port_enable(port);
 
 	mvpp2_egress_enable(port);
 	mvpp2_ingress_enable(port);
-	netif_tx_wake_all_queues(port->dev);
+	mvpp2_tx_wake_all_queues(port->dev);
 }
 
-static void mvpp2_mac_link_down(struct phylink_config *config,
-				unsigned int mode, phy_interface_t interface)
+static void mvpp2_mac_link_down(struct phylink_config *config, unsigned int mode,
+				phy_interface_t interface)
 {
 	struct mvpp2_port *port = mvpp2_phylink_to_port(config);
 	u32 val;
 
-	if (!phylink_autoneg_inband(mode)) {
-		if (mvpp2_is_xlg(interface)) {
-			val = readl(port->base + MVPP22_XLG_CTRL0_REG);
-			val &= ~MVPP22_XLG_CTRL0_FORCE_LINK_PASS;
-			val |= MVPP22_XLG_CTRL0_FORCE_LINK_DOWN;
-			writel(val, port->base + MVPP22_XLG_CTRL0_REG);
-		} else {
-			val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
-			val &= ~MVPP2_GMAC_FORCE_LINK_PASS;
-			val |= MVPP2_GMAC_FORCE_LINK_DOWN;
-			writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
-		}
+	if (!phylink_autoneg_inband(mode) &&
+	    interface != PHY_INTERFACE_MODE_RXAUI &&
+	    interface != PHY_INTERFACE_MODE_10GKR &&
+	    interface != PHY_INTERFACE_MODE_5GKR) {
+		val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+		val &= ~MVPP2_GMAC_FORCE_LINK_PASS;
+		val |= MVPP2_GMAC_FORCE_LINK_DOWN;
+		writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
 	}
 
-	netif_tx_stop_all_queues(port->dev);
+	mvpp2_tx_stop_all_queues(port->dev);
 	mvpp2_egress_disable(port);
 	mvpp2_ingress_disable(port);
 
@@ -5210,6 +6552,34 @@ static const struct phylink_mac_ops mvpp2_phylink_ops = {
 	.mac_link_down = mvpp2_mac_link_down,
 };
 
+/* DSA notifier */
+static void mvpp2_dsa_port_register(struct net_device *dev)
+{
+	struct mvpp2_port *port = netdev_priv(dev);
+	struct mvpp2 *priv = port->priv;
+	u32 reg;
+
+	/* For switch port enable non-extended DSA tags and make sure
+	 * the extended DSA tag usage is disabled as those
+	 * two options cannot coexist.
+	 */
+	reg = mvpp2_read(priv, MVPP2_MH_REG(port->id));
+	reg &= ~MVPP2_DSA_EXTENDED;
+	reg |= MVPP2_DSA_NON_EXTENDED;
+	mvpp2_write(priv, MVPP2_MH_REG(port->id), reg);
+}
+
+static int mvpp2_dsa_notifier(struct notifier_block *unused,
+			      unsigned long event, void *ptr)
+{
+	struct dsa_notifier_register_info *info = ptr;
+
+	if (event == DSA_PORT_REGISTER)
+		mvpp2_dsa_port_register(info->master);
+
+	return NOTIFY_DONE;
+}
+
 /* Ports initialization */
 static int mvpp2_port_probe(struct platform_device *pdev,
 			    struct fwnode_handle *port_fwnode,
@@ -5219,16 +6589,19 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 	struct mvpp2_port *port;
 	struct mvpp2_port_pcpu *port_pcpu;
 	struct device_node *port_node = to_of_node(port_fwnode);
-	netdev_features_t features;
 	struct net_device *dev;
+	struct resource *res;
 	struct phylink *phylink;
 	char *mac_from = "";
-	unsigned int ntxqs, nrxqs, thread;
+	unsigned int ntxqs, nrxqs;
 	unsigned long flags = 0;
 	bool has_tx_irqs;
+	dma_addr_t p;
 	u32 id;
+	int features;
 	int phy_mode;
 	int err, i;
+	int cpu;
 
 	has_tx_irqs = mvpp2_port_has_irqs(priv, port_node, &flags);
 	if (!has_tx_irqs && queue_mode == MVPP2_QDIST_MULTI_MODE) {
@@ -5238,12 +6611,28 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 	}
 
 	ntxqs = MVPP2_MAX_TXQ;
-	nrxqs = mvpp2_get_nrxqs(priv);
+	if (priv->hw_version != MVPP21 && queue_mode ==
+	    MVPP2_QDIST_SINGLE_MODE) {
+		nrxqs = 1;
+	} else {
+		/* According to the PPv2.2 datasheet and our experiments on
+		 * PPv2.1, RX queues have an allocation granularity of 4 (when
+		 * more than a single one on PPv2.2).
+		 * Round up to nearest multiple of 4.
+		 */
+		nrxqs = (num_possible_cpus() + 3) & ~0x3;
+		if (nrxqs > MVPP2_PORT_MAX_RXQ)
+			nrxqs = MVPP2_PORT_MAX_RXQ;
+	}
 
 	dev = alloc_etherdev_mqs(sizeof(*port), ntxqs, nrxqs);
 	if (!dev)
 		return -ENOMEM;
 
+	/* XPS mapping queues to 0..N cpus (may be less than ntxqs) */
+	for_each_online_cpu(cpu)
+		netif_set_xps_queue(dev, cpumask_of(cpu), cpu);
+
 	phy_mode = fwnode_get_phy_mode(port_fwnode);
 	if (phy_mode < 0) {
 		dev_err(&pdev->dev, "incorrect phy mode\n");
@@ -5277,6 +6666,12 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 	port->dev = dev;
 	port->fwnode = port_fwnode;
 	port->has_phy = !!of_find_property(port_node, "phy", NULL);
+	if (port->has_phy && phy_mode == PHY_INTERFACE_MODE_INTERNAL) {
+		err = -EINVAL;
+		dev_err(&pdev->dev, "internal mode doesn't work with phy\n");
+		goto err_free_netdev;
+	}
+
 	port->ntxqs = ntxqs;
 	port->nrxqs = nrxqs;
 	port->priv = priv;
@@ -5312,8 +6707,13 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 	port->phy_interface = phy_mode;
 	port->comphy = comphy;
 
+	if ((port->id == 0 && port->priv->hw_version != MVPP21) ||
+	    (port->id == 1 && port->priv->hw_version == MVPP23))
+		port->has_xlg_mac = true;
+
 	if (priv->hw_version == MVPP21) {
-		port->base = devm_platform_ioremap_resource(pdev, 2 + id);
+		res = platform_get_resource(pdev, IORESOURCE_MEM, 2 + id);
+		port->base = devm_ioremap_resource(&pdev->dev, res);
 		if (IS_ERR(port->base)) {
 			err = PTR_ERR(port->base);
 			goto err_free_irq;
@@ -5343,13 +6743,16 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 		goto err_free_irq;
 	}
 
-	port->ethtool_stats = devm_kcalloc(&pdev->dev,
-					   MVPP2_N_ETHTOOL_STATS(ntxqs, nrxqs),
-					   sizeof(u64), GFP_KERNEL);
-	if (!port->ethtool_stats) {
+	p = (dma_addr_t)devm_kcalloc(&pdev->dev,
+				     ARRAY_SIZE(mvpp2_ethtool_regs) +
+				     L1_CACHE_BYTES,
+				     sizeof(u64), GFP_KERNEL);
+	if (!p) {
 		err = -ENOMEM;
 		goto err_free_stats;
 	}
+	p = (p + ~CACHE_LINE_MASK) & CACHE_LINE_MASK;
+	port->ethtool_stats = (void *)p;
 
 	mutex_init(&port->gather_stats_lock);
 	INIT_DELAYED_WORK(&port->stats_work, mvpp2_gather_hw_statistics);
@@ -5368,8 +6771,7 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 
 	mvpp2_port_periodic_xon_disable(port);
 
-	mvpp2_mac_reset_assert(port);
-	mvpp22_pcs_reset_assert(port);
+	mvpp2_port_reset(port);
 
 	port->pcpu = alloc_percpu(struct mvpp2_port_pcpu);
 	if (!port->pcpu) {
@@ -5377,16 +6779,17 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 		goto err_free_txq_pcpu;
 	}
 
-	if (!port->has_tx_irqs) {
-		for (thread = 0; thread < priv->nthreads; thread++) {
-			port_pcpu = per_cpu_ptr(port->pcpu, thread);
+	/* Init tx-done/guard timer and tasklet */
+	 mvpp2_tx_done_init_on_probe(pdev, port);
 
-			hrtimer_init(&port_pcpu->tx_done_timer, CLOCK_MONOTONIC,
-				     HRTIMER_MODE_REL_PINNED_SOFT);
-			port_pcpu->tx_done_timer.function = mvpp2_hr_timer_cb;
-			port_pcpu->timer_scheduled = false;
-			port_pcpu->dev = dev;
-		}
+	/* Init bulk timer and tasklet */
+	for_each_present_cpu(cpu) {
+		port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+		hrtimer_init(&port_pcpu->bulk_timer,
+			     CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);
+		port_pcpu->bulk_timer.function = mvpp2_bulk_timer_cb;
+		tasklet_init(&port_pcpu->bulk_tasklet,
+			     mvpp2_bulk_tasklet_cb, (unsigned long)dev);
 	}
 
 	features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
@@ -5395,13 +6798,13 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 	dev->hw_features |= features | NETIF_F_RXCSUM | NETIF_F_GRO |
 			    NETIF_F_HW_VLAN_CTAG_FILTER;
 
-	if (mvpp22_rss_is_supported()) {
+	if (mvpp22_rss_is_supported(port))
 		dev->hw_features |= NETIF_F_RXHASH;
-		dev->features |= NETIF_F_NTUPLE;
-	}
 
-	if (!port->priv->percpu_pools)
-		mvpp2_set_hw_csum(port, port->pool_long->id);
+	if (port->pool_long->id == MVPP2_BM_JUMBO && port->id != 0) {
+		dev->features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
+		dev->hw_features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
+	}
 
 	dev->vlan_features |= features;
 	dev->gso_max_segs = MVPP2_MAX_TSO_SEGS;
@@ -5409,8 +6812,8 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 
 	/* MTU range: 68 - 9704 */
 	dev->min_mtu = ETH_MIN_MTU;
-	/* 9704 == 9728 - 20 and rounding to 8 */
-	dev->max_mtu = MVPP2_BM_JUMBO_PKT_SIZE;
+	/* 9704 == 9728 - 24 (no rounding for MTU but for frag_size) */
+	dev->max_mtu = MVPP2_BM_JUMBO_PKT_SIZE - MVPP2_MTU_OVERHEAD_SIZE;
 	dev->dev.of_node = port_node;
 
 	/* Phylink isn't used w/ ACPI as of now */
@@ -5438,6 +6841,37 @@ static int mvpp2_port_probe(struct platform_device *pdev,
 
 	priv->port_list[priv->port_count++] = port;
 
+	/* Port may be configured by Uboot to transmit IDLE, so a remote side
+	 * feels the link as UP. Stop TX in same way as in mvpp2_open/stop.
+	 */
+	if (port->of_node && port->phylink) {
+		if (rtnl_is_locked()) {
+			if (!phylink_of_phy_connect(port->phylink,
+						    port->of_node, 0))
+				phylink_disconnect_phy(port->phylink);
+		} else {
+			rtnl_lock();
+			if (!phylink_of_phy_connect(port->phylink,
+						    port->of_node, 0))
+				phylink_disconnect_phy(port->phylink);
+			rtnl_unlock();
+		}
+	}
+
+	/* Init TX locks and bm locks */
+	for (i = 0; i < MVPP2_MAX_THREADS; i++) {
+		spin_lock_init(&port->bm_lock[i]);
+		spin_lock_init(&port->tx_lock[i]);
+	}
+
+	/* Register DSA notifier */
+	port->dsa_notifier.notifier_call = mvpp2_dsa_notifier;
+	err = register_dsa_notifier(&port->dsa_notifier);
+	if (err) {
+		dev_err(&pdev->dev, "failed to register DSA notifier\n");
+		goto err_phylink;
+	}
+
 	return 0;
 
 err_phylink:
@@ -5465,7 +6899,9 @@ static void mvpp2_port_remove(struct mvpp2_port *port)
 {
 	int i;
 
+	mvpp2_port_musdk_set(port->dev, false);
 	unregister_netdev(port->dev);
+	unregister_dsa_notifier(&port->dsa_notifier);
 	if (port->phylink)
 		phylink_destroy(port->phylink);
 	free_percpu(port->pcpu);
@@ -5536,7 +6972,7 @@ static void mvpp22_rx_fifo_set_hw(struct mvpp2 *priv, int port, int data_size)
 	mvpp2_write(priv, MVPP2_RX_ATTR_FIFO_SIZE_REG(port), attr_size);
 }
 
-/* Initialize TX FIFO's: the total FIFO size is 48kB on PPv2.2.
+/* Initialize TX FIFO's: the total FIFO size is 48kB on PPv2.2 and PPv2.3.
  * 4kB fixed space must be assigned for the loopback port.
  * Redistribute remaining avialable 44kB space among all active ports.
  * Guarantee minimum 32kB for 10G port and 8kB for port 1, capable of 2.5G
@@ -5544,10 +6980,10 @@ static void mvpp22_rx_fifo_set_hw(struct mvpp2 *priv, int port, int data_size)
  */
 static void mvpp22_rx_fifo_init(struct mvpp2 *priv)
 {
-	int remaining_ports_count;
+	int port, size;
 	unsigned long port_map;
+	int remaining_ports_count;
 	int size_remainder;
-	int port, size;
 
 	/* The loopback requires fixed 4kB of the FIFO space assignment. */
 	mvpp22_rx_fifo_set_hw(priv, MVPP2_LOOPBACK_PORT_INDEX,
@@ -5585,6 +7021,55 @@ static void mvpp22_rx_fifo_init(struct mvpp2 *priv)
 	mvpp2_write(priv, MVPP2_RX_FIFO_INIT_REG, 0x1);
 }
 
+/* Configure Rx FIFO Flow control thresholds */
+static void mvpp23_rx_fifo_fc_set_tresh(struct mvpp2 *priv)
+{
+	int port, val;
+
+	/* Port 0: maximum speed -10Gb/s port
+	 *	   required by spec RX FIFO threshold 9KB
+	 * Port 1: maximum speed -5Gb/s port
+	 *	   required by spec RX FIFO threshold 4KB
+	 * Port 2: maximum speed -1Gb/s port
+	 *	   required by spec RX FIFO threshold 2KB
+	 */
+
+	/* Without loopback port */
+	for (port = 0; port < (MVPP2_MAX_PORTS - 1); port++) {
+		if (port == 0) {
+			val = (MVPP23_PORT0_FIFO_TRSH / MVPP2_RX_FC_TRSH_UNIT)
+				<< MVPP2_RX_FC_TRSH_OFFS;
+			val &= MVPP2_RX_FC_TRSH_MASK;
+			mvpp2_write(priv, MVPP2_RX_FC_REG(port), val);
+		} else if (port == 1) {
+			val = (MVPP23_PORT1_FIFO_TRSH / MVPP2_RX_FC_TRSH_UNIT)
+				<< MVPP2_RX_FC_TRSH_OFFS;
+			val &= MVPP2_RX_FC_TRSH_MASK;
+			mvpp2_write(priv, MVPP2_RX_FC_REG(port), val);
+		} else {
+			val = (MVPP23_PORT2_FIFO_TRSH / MVPP2_RX_FC_TRSH_UNIT)
+				<< MVPP2_RX_FC_TRSH_OFFS;
+			val &= MVPP2_RX_FC_TRSH_MASK;
+			mvpp2_write(priv, MVPP2_RX_FC_REG(port), val);
+		}
+	}
+}
+
+/* Configure Rx FIFO Flow control thresholds */
+void mvpp23_rx_fifo_fc_en(struct mvpp2 *priv, int port, bool en)
+{
+	int val;
+
+	val = mvpp2_read(priv, MVPP2_RX_FC_REG(port));
+
+	if (en)
+		val |= MVPP2_RX_FC_EN;
+	else
+		val &= ~MVPP2_RX_FC_EN;
+
+	mvpp2_write(priv, MVPP2_RX_FC_REG(port), val);
+}
+
 static void mvpp22_tx_fifo_set_hw(struct mvpp2 *priv, int port, int size)
 {
 	int threshold = MVPP2_TX_FIFO_THRESHOLD(size);
@@ -5593,22 +7078,22 @@ static void mvpp22_tx_fifo_set_hw(struct mvpp2 *priv, int port, int size)
 	mvpp2_write(priv, MVPP22_TX_FIFO_THRESH_REG(port), threshold);
 }
 
-/* Initialize TX FIFO's: the total FIFO size is 19kB on PPv2.2.
- * 3kB fixed space must be assigned for the loopback port.
- * Redistribute remaining avialable 16kB space among all active ports.
+/* Initialize TX FIFO's: the total FIFO size is 19kB on PPv2.2 and PPv2.3.
+ * 1kB fixed space must be assigned for the loopback port.
+ * Redistribute remaining avialable 18kB space among all active ports.
  * The 10G interface should use 10kB (which is maximum possible size
  * per single port).
  */
-static void mvpp22_tx_fifo_init(struct mvpp2 *priv)
+static void mvpp22_tx_fifo_init_default(struct mvpp2 *priv)
 {
-	int remaining_ports_count;
+	int port, size;
 	unsigned long port_map;
+	int remaining_ports_count;
 	int size_remainder;
-	int port, size;
 
-	/* The loopback requires fixed 3kB of the FIFO space assignment. */
+	/* The loopback requires fixed 1kB of the FIFO space assignment. */
 	mvpp22_tx_fifo_set_hw(priv, MVPP2_LOOPBACK_PORT_INDEX,
-			      MVPP22_TX_FIFO_DATA_SIZE_3KB);
+			      MVPP22_TX_FIFO_DATA_SIZE_1KB);
 	port_map = priv->port_map & ~BIT(MVPP2_LOOPBACK_PORT_INDEX);
 
 	/* Set TX FIFO size to 0 for inactive ports. */
@@ -5616,7 +7101,7 @@ static void mvpp22_tx_fifo_init(struct mvpp2 *priv)
 		mvpp22_tx_fifo_set_hw(priv, port, 0);
 
 	/* Assign remaining TX FIFO space among all active ports. */
-	size_remainder = MVPP22_TX_FIFO_DATA_SIZE_16KB;
+	size_remainder = MVPP22_TX_FIFO_DATA_SIZE_18KB;
 	remaining_ports_count = hweight_long(port_map);
 
 	for_each_set_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX) {
@@ -5635,6 +7120,53 @@ static void mvpp22_tx_fifo_init(struct mvpp2 *priv)
 	}
 }
 
+static void mvpp22_tx_fifo_init_param(struct platform_device *pdev,
+				      struct mvpp2 *priv)
+{
+	unsigned long port_map;
+	int size_remainder;
+	int port, size;
+
+	/* The loopback requires fixed 1kB of the FIFO space assignment. */
+	mvpp22_tx_fifo_set_hw(priv, MVPP2_LOOPBACK_PORT_INDEX,
+			      MVPP22_TX_FIFO_DATA_SIZE_1KB);
+	port_map = priv->port_map & ~BIT(MVPP2_LOOPBACK_PORT_INDEX);
+
+	/* Set TX FIFO size to 0 for inactive ports. */
+	for_each_clear_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX) {
+		mvpp22_tx_fifo_set_hw(priv, port, 0);
+		if (MVPP22_TX_FIFO_EXTRA_PARAM_SIZE(port, tx_fifo_map))
+			goto error;
+	}
+
+	/* The physical port requires minimum 3kB */
+	for_each_set_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX) {
+		size = MVPP22_TX_FIFO_EXTRA_PARAM_SIZE(port, tx_fifo_map);
+		if (size < MVPP22_TX_FIFO_DATA_SIZE_MIN ||
+		    size > MVPP22_TX_FIFO_DATA_SIZE_MAX)
+			goto error;
+	}
+
+	/* Assign remaining TX FIFO space among all active ports. */
+	size_remainder = MVPP22_TX_FIFO_DATA_SIZE_18KB;
+	for (port = 0; port < MVPP2_LOOPBACK_PORT_INDEX; port++) {
+		size = MVPP22_TX_FIFO_EXTRA_PARAM_SIZE(port, tx_fifo_map);
+		if (!size)
+			continue;
+		size_remainder -= size;
+		mvpp22_tx_fifo_set_hw(priv, port, size);
+	}
+
+	if (size_remainder)
+		goto error;
+
+	return;
+
+error:
+	dev_warn(&pdev->dev, "Fail to set TX FIFO from module_param, fallback to default\n");
+	mvpp22_tx_fifo_init_default(priv);
+}
+
 static void mvpp2_axi_init(struct mvpp2 *priv)
 {
 	u32 val, rdval, wrval;
@@ -5664,6 +7196,10 @@ static void mvpp2_axi_init(struct mvpp2 *priv)
 	mvpp2_write(priv, MVPP22_AXI_RXQ_DESCR_WR_ATTR_REG, wrval);
 
 	/* Buffer Data */
+	/* Force TX FIFO transactions priority on the AXI QOS bus */
+	if (tx_fifo_protection)
+		rdval |= MVPP22_AXI_TX_DATA_RD_QOS_ATTRIBUTE;
+
 	mvpp2_write(priv, MVPP22_AXI_TX_DATA_RD_ATTR_REG, rdval);
 	mvpp2_write(priv, MVPP22_AXI_RX_DATA_WR_ATTR_REG, wrval);
 
@@ -5695,13 +7231,14 @@ static int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)
 	const struct mbus_dram_target_info *dram_target_info;
 	int err, i;
 	u32 val;
+	dma_addr_t p;
 
 	/* MBUS windows configuration */
 	dram_target_info = mv_mbus_dram_info();
 	if (dram_target_info)
 		mvpp2_conf_mbus_windows(dram_target_info, priv);
 
-	if (priv->hw_version == MVPP22)
+	if (priv->hw_version != MVPP21)
 		mvpp2_axi_init(priv);
 
 	/* Disable HW PHY polling */
@@ -5715,12 +7252,16 @@ static int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)
 		writel(val, priv->iface_base + MVPP22_SMI_MISC_CFG_REG);
 	}
 
-	/* Allocate and initialize aggregated TXQs */
-	priv->aggr_txqs = devm_kcalloc(&pdev->dev, MVPP2_MAX_THREADS,
-				       sizeof(*priv->aggr_txqs),
-				       GFP_KERNEL);
-	if (!priv->aggr_txqs)
+	/* Allocate and initialize aggregated TXQs
+	 * The aggr_txqs[per-cpu] entry should be aligned onto cache.
+	 * So allocate more than needed and round-up the pointer.
+	 */
+	val = sizeof(*priv->aggr_txqs) * MVPP2_MAX_THREADS + L1_CACHE_BYTES;
+	p = (dma_addr_t)devm_kzalloc(&pdev->dev, val, GFP_KERNEL);
+	if (!p)
 		return -ENOMEM;
+	p = (p + ~CACHE_LINE_MASK) & CACHE_LINE_MASK;
+	priv->aggr_txqs = (struct mvpp2_tx_queue *)p;
 
 	for (i = 0; i < MVPP2_MAX_THREADS; i++) {
 		priv->aggr_txqs[i].id = i;
@@ -5735,7 +7276,12 @@ static int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)
 		mvpp2_rx_fifo_init(priv);
 	} else {
 		mvpp22_rx_fifo_init(priv);
-		mvpp22_tx_fifo_init(priv);
+		if (tx_fifo_map)
+			mvpp22_tx_fifo_init_param(pdev, priv);
+		else
+			mvpp22_tx_fifo_init_default(priv);
+		if (priv->hw_version == MVPP23)
+			mvpp23_rx_fifo_fc_set_tresh(priv);
 	}
 
 	if (priv->hw_version == MVPP21)
@@ -5746,7 +7292,7 @@ static int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)
 	mvpp2_write(priv, MVPP2_TX_SNOOP_REG, 0x1);
 
 	/* Buffer Manager initialization */
-	err = mvpp2_bm_init(&pdev->dev, priv);
+	err = mvpp2_bm_init(pdev, priv);
 	if (err < 0)
 		return err;
 
@@ -5758,6 +7304,38 @@ static int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)
 	/* Classifier default initialization */
 	mvpp2_cls_init(priv);
 
+	/* Disable all ingress queues */
+	mvpp2_rxq_disable_all(priv);
+
+	return 0;
+}
+
+static int mvpp2_get_sram(struct platform_device *pdev,
+			  struct mvpp2 *priv)
+{
+	struct device_node *dn = pdev->dev.of_node;
+	struct resource *res;
+
+	if (has_acpi_companion(&pdev->dev)) {
+		res = platform_get_resource(pdev, IORESOURCE_MEM, 2);
+		if (!res) {
+			dev_warn(&pdev->dev, "ACPI is too old, TX FC disabled\n");
+			return 0;
+		}
+		priv->cm3_base = devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(priv->cm3_base))
+			return PTR_ERR(priv->cm3_base);
+	} else {
+		priv->sram_pool = of_gen_pool_get(dn, "cm3-mem", 0);
+		if (!priv->sram_pool) {
+			dev_warn(&pdev->dev, "DT is too old, TX FC disabled\n");
+			return 0;
+		}
+		priv->cm3_base = (void __iomem *)gen_pool_alloc(priv->sram_pool,
+								MSS_SRAM_SIZE);
+		if (!priv->cm3_base)
+			return -ENOMEM;
+	}
 	return 0;
 }
 
@@ -5779,8 +7357,6 @@ static int mvpp2_probe(struct platform_device *pdev)
 	if (has_acpi_companion(&pdev->dev)) {
 		acpi_id = acpi_match_device(pdev->dev.driver->acpi_match_table,
 					    &pdev->dev);
-		if (!acpi_id)
-			return -EINVAL;
 		priv->hw_version = (unsigned long)acpi_id->driver_data;
 	} else {
 		priv->hw_version =
@@ -5793,12 +7369,14 @@ static int mvpp2_probe(struct platform_device *pdev)
 	if (priv->hw_version == MVPP21)
 		queue_mode = MVPP2_QDIST_SINGLE_MODE;
 
-	base = devm_platform_ioremap_resource(pdev, 0);
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	base = devm_ioremap_resource(&pdev->dev, res);
 	if (IS_ERR(base))
 		return PTR_ERR(base);
 
 	if (priv->hw_version == MVPP21) {
-		priv->lms_base = devm_platform_ioremap_resource(pdev, 1);
+		res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+		priv->lms_base = devm_ioremap_resource(&pdev->dev, res);
 		if (IS_ERR(priv->lms_base))
 			return PTR_ERR(priv->lms_base);
 	} else {
@@ -5817,9 +7395,18 @@ static int mvpp2_probe(struct platform_device *pdev)
 		priv->iface_base = devm_ioremap_resource(&pdev->dev, res);
 		if (IS_ERR(priv->iface_base))
 			return PTR_ERR(priv->iface_base);
+
+		/* Map CM3 SRAM */
+		err = mvpp2_get_sram(pdev, priv);
+		if (err)
+			dev_warn(&pdev->dev, "Fail to alloc CM3 SRAM\n");
+
+		/* Enable global Flow Control only if hanler to SRAM not NULL */
+		if (priv->cm3_base)
+			priv->global_tx_fc = true;
 	}
 
-	if (priv->hw_version == MVPP22 && dev_of_node(&pdev->dev)) {
+	if (priv->hw_version != MVPP21 && dev_of_node(&pdev->dev)) {
 		priv->sysctrl_base =
 			syscon_regmap_lookup_by_phandle(pdev->dev.of_node,
 							"marvell,system-controller");
@@ -5832,13 +7419,6 @@ static int mvpp2_probe(struct platform_device *pdev)
 			priv->sysctrl_base = NULL;
 	}
 
-	if (priv->hw_version == MVPP22 &&
-	    mvpp2_get_nrxqs(priv) * 2 <= MVPP2_BM_MAX_POOLS)
-		priv->percpu_pools = 1;
-
-	mvpp2_setup_bm_pool();
-
-
 	priv->nthreads = min_t(unsigned int, num_present_cpus(),
 			       MVPP2_MAX_THREADS);
 
@@ -5855,18 +7435,15 @@ static int mvpp2_probe(struct platform_device *pdev)
 		priv->swth_base[i] = base + i * addr_space_sz;
 	}
 
-	if (priv->hw_version == MVPP21)
-		priv->max_port_rxqs = 8;
-	else
-		priv->max_port_rxqs = 32;
-
 	if (dev_of_node(&pdev->dev)) {
 		priv->pp_clk = devm_clk_get(&pdev->dev, "pp_clk");
-		if (IS_ERR(priv->pp_clk))
-			return PTR_ERR(priv->pp_clk);
+		if (IS_ERR(priv->pp_clk)) {
+			err = PTR_ERR(priv->pp_clk);
+			goto err_cm3;
+		}
 		err = clk_prepare_enable(priv->pp_clk);
 		if (err < 0)
-			return err;
+			goto err_cm3;
 
 		priv->gop_clk = devm_clk_get(&pdev->dev, "gop_clk");
 		if (IS_ERR(priv->gop_clk)) {
@@ -5877,7 +7454,7 @@ static int mvpp2_probe(struct platform_device *pdev)
 		if (err < 0)
 			goto err_pp_clk;
 
-		if (priv->hw_version == MVPP22) {
+		if (priv->hw_version != MVPP21) {
 			priv->mg_clk = devm_clk_get(&pdev->dev, "mg_clk");
 			if (IS_ERR(priv->mg_clk)) {
 				err = PTR_ERR(priv->mg_clk);
@@ -5918,10 +7495,39 @@ static int mvpp2_probe(struct platform_device *pdev)
 		return -EINVAL;
 	}
 
-	if (priv->hw_version == MVPP22) {
+	if (priv->hw_version != MVPP21) {
+		if (mvpp2_read(priv, MVPP2_VER_ID_REG) == MVPP2_VER_PP23)
+			priv->hw_version = MVPP23;
+	}
+
+	if (priv->hw_version == MVPP21)
+		priv->max_port_rxqs = 8;
+	else
+		priv->max_port_rxqs = 32;
+
+	priv->custom_dma_mask = false;
+	if (priv->hw_version != MVPP21) {
+		/* If dma_mask points to coherent_dma_mask, setting both will
+		 * override the value of the other. This is problematic as the
+		 * PPv2 driver uses a 32-bit-mask for coherent accesses (txq,
+		 * rxq, bm) and a 40-bit mask for all other accesses.
+		 */
+		if (pdev->dev.dma_mask == &pdev->dev.coherent_dma_mask) {
+			pdev->dev.dma_mask =
+				kzalloc(sizeof(*pdev->dev.dma_mask),
+					GFP_KERNEL);
+			if (!pdev->dev.dma_mask) {
+				err = -ENOMEM;
+				goto err_mg_clk;
+			}
+
+			priv->custom_dma_mask = true;
+		}
+
 		err = dma_set_mask(&pdev->dev, MVPP2_DESC_DMA_MASK);
 		if (err)
-			goto err_axi_clk;
+			goto err_dma_mask;
+
 		/* Sadly, the BM pools all share the same register to
 		 * store the high 32 bits of their address. So they
 		 * must all have the same high 32 bits, which forces
@@ -5929,8 +7535,25 @@ static int mvpp2_probe(struct platform_device *pdev)
 		 */
 		err = dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32));
 		if (err)
-			goto err_axi_clk;
+			goto err_dma_mask;
+	}
+
+	/* Assign the reserved memory region to the device for DMA allocations,
+	 * if a memory-region phandle is found.
+	 */
+	if (dev_of_node(&pdev->dev))
+		of_reserved_mem_device_init_by_idx(&pdev->dev,
+						   pdev->dev.of_node, 0);
+
+	/* Configure branch prediction switch */
+	if (priv->hw_version == MVPP21)
+		static_branch_enable(&mvpp21_variant);
+	if (recycle) {
+		dev_info(&pdev->dev,
+			 "kernel space packet recycling feature enabled\n");
+		static_branch_enable(&mvpp2_recycle_ena);
 	}
+	/* else - keep the DEFINE_STATIC_KEY_FALSE */
 
 	/* Map DTS-active ports. Should be done before FIFO mvpp2_init */
 	fwnode_for_each_available_child_node(fwnode, port_fwnode) {
@@ -5938,6 +7561,9 @@ static int mvpp2_probe(struct platform_device *pdev)
 			priv->port_map |= BIT(i);
 	}
 
+	/* Init mss lock */
+	spin_lock_init(&priv->mss_spinlock);
+
 	/* Initialize network controller */
 	err = mvpp2_init(pdev, priv);
 	if (err < 0) {
@@ -5973,6 +7599,12 @@ static int mvpp2_probe(struct platform_device *pdev)
 		goto err_port_probe;
 	}
 
+	if (priv->global_tx_fc && priv->hw_version != MVPP21) {
+		err = mvpp2_enable_global_fc(priv);
+		if (err)
+			dev_warn(&pdev->dev, "CM3 firmware not running, TX FC disabled\n");
+	}
+
 	mvpp2_dbgfs_init(priv, pdev->name);
 
 	platform_set_drvdata(pdev, priv);
@@ -5985,19 +7617,29 @@ static int mvpp2_probe(struct platform_device *pdev)
 			mvpp2_port_remove(priv->port_list[i]);
 		i++;
 	}
+err_dma_mask:
+	if (priv->custom_dma_mask) {
+		kfree(pdev->dev.dma_mask);
+		pdev->dev.dma_mask = &pdev->dev.coherent_dma_mask;
+	}
 err_axi_clk:
 	clk_disable_unprepare(priv->axi_clk);
 
 err_mg_core_clk:
-	if (priv->hw_version == MVPP22)
+	if (priv->hw_version != MVPP21)
 		clk_disable_unprepare(priv->mg_core_clk);
 err_mg_clk:
-	if (priv->hw_version == MVPP22)
+	if (priv->hw_version != MVPP21)
 		clk_disable_unprepare(priv->mg_clk);
 err_gop_clk:
 	clk_disable_unprepare(priv->gop_clk);
 err_pp_clk:
 	clk_disable_unprepare(priv->pp_clk);
+err_cm3:
+	if (!has_acpi_companion(&pdev->dev))
+		gen_pool_free(priv->sram_pool, (unsigned long)priv->cm3_base,
+			      MSS_SRAM_SIZE);
+
 	return err;
 }
 
@@ -6005,11 +7647,14 @@ static int mvpp2_remove(struct platform_device *pdev)
 {
 	struct mvpp2 *priv = platform_get_drvdata(pdev);
 	struct fwnode_handle *fwnode = pdev->dev.fwnode;
-	int i = 0, poolnum = MVPP2_BM_POOLS_NUM;
 	struct fwnode_handle *port_fwnode;
+	int i = 0;
 
 	mvpp2_dbgfs_cleanup(priv);
 
+	flush_workqueue(priv->stats_queue);
+	destroy_workqueue(priv->stats_queue);
+
 	fwnode_for_each_available_child_node(fwnode, port_fwnode) {
 		if (priv->port_list[i]) {
 			mutex_destroy(&priv->port_list[i]->gather_stats_lock);
@@ -6018,15 +7663,10 @@ static int mvpp2_remove(struct platform_device *pdev)
 		i++;
 	}
 
-	destroy_workqueue(priv->stats_queue);
-
-	if (priv->percpu_pools)
-		poolnum = mvpp2_get_nrxqs(priv) * 2;
-
-	for (i = 0; i < poolnum; i++) {
+	for (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {
 		struct mvpp2_bm_pool *bm_pool = &priv->bm_pools[i];
 
-		mvpp2_bm_pool_destroy(&pdev->dev, priv, bm_pool);
+		mvpp2_bm_pool_destroy(pdev, priv, bm_pool);
 	}
 
 	for (i = 0; i < MVPP2_MAX_THREADS; i++) {
@@ -6038,6 +7678,17 @@ static int mvpp2_remove(struct platform_device *pdev)
 				  aggr_txq->descs_dma);
 	}
 
+	if (priv->custom_dma_mask) {
+		kfree(pdev->dev.dma_mask);
+		pdev->dev.dma_mask = &pdev->dev.coherent_dma_mask;
+	}
+
+	if (!has_acpi_companion(&pdev->dev)) {
+		gen_pool_free(priv->sram_pool, (unsigned long)priv->cm3_base,
+			      MSS_SRAM_SIZE);
+		gen_pool_destroy(priv->sram_pool);
+	}
+
 	if (is_acpi_node(port_fwnode))
 		return 0;
 
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.c b/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.c
index 2ce386393ac0..b0b9387270b0 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.c
@@ -560,12 +560,8 @@ static void mvpp2_prs_dsa_tag_set(struct mvpp2 *priv, int port, bool add,
 					     MVPP2_PRS_TCAM_DSA_TAGGED_BIT);
 
 			/* Set ai bits for next iteration */
-			if (extend)
-				mvpp2_prs_sram_ai_update(&pe, 1,
-							MVPP2_PRS_SRAM_AI_MASK);
-			else
-				mvpp2_prs_sram_ai_update(&pe, 0,
-							MVPP2_PRS_SRAM_AI_MASK);
+			mvpp2_prs_sram_ai_update(&pe, extend,
+						 MVPP2_PRS_SRAM_AI_MASK);
 
 			/* Set result info bits to 'single vlan' */
 			mvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_SINGLE,
@@ -915,15 +911,15 @@ static int mvpp2_prs_ip4_proto(struct mvpp2 *priv, unsigned short proto,
 	mvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
 	pe.index = tid;
 
-	/* Set next lu to IPv4 */
-	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
-	mvpp2_prs_sram_shift_set(&pe, 12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Finished: go to flowid generation */
+	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+
 	/* Set L4 offset */
 	mvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,
 				  sizeof(struct iphdr) - 4,
 				  MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
-	mvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
-				 MVPP2_PRS_IPV4_DIP_AI_BIT);
+	mvpp2_prs_sram_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
 	mvpp2_prs_sram_ri_update(&pe, ri, ri_mask | MVPP2_PRS_RI_IP_FRAG_MASK);
 
 	mvpp2_prs_tcam_data_byte_set(&pe, 2, 0x00,
@@ -932,7 +928,8 @@ static int mvpp2_prs_ip4_proto(struct mvpp2 *priv, unsigned short proto,
 				     MVPP2_PRS_TCAM_PROTO_MASK);
 
 	mvpp2_prs_tcam_data_byte_set(&pe, 5, proto, MVPP2_PRS_TCAM_PROTO_MASK);
-	mvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
+	mvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				 MVPP2_PRS_IPV4_DIP_AI_BIT);
 	/* Unmask all ports */
 	mvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
 
@@ -1000,12 +997,17 @@ static int mvpp2_prs_ip4_cast(struct mvpp2 *priv, unsigned short l3_cast)
 		return -EINVAL;
 	}
 
-	/* Finished: go to flowid generation */
-	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
-	mvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	/* Go again to ipv4 */
+	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
 
-	mvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+	mvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
 				 MVPP2_PRS_IPV4_DIP_AI_BIT);
+
+	/* Shift back to IPv4 proto */
+	mvpp2_prs_sram_shift_set(&pe, -12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+	mvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
+
 	/* Unmask all ports */
 	mvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
 
@@ -1167,6 +1169,21 @@ static void mvpp2_prs_mh_init(struct mvpp2 *priv)
 	/* Update shadow table and hw entry */
 	mvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MH);
 	mvpp2_prs_hw_write(priv, &pe);
+
+	/* Set MH entry that skip parser */
+	pe.index = MVPP2_PE_MH_SKIP_PRS;
+	mvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MH);
+	mvpp2_prs_sram_shift_set(&pe, MVPP2_MH_SIZE,
+				 MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	mvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+
+	/* Mask all ports */
+	mvpp2_prs_tcam_port_map_set(&pe, 0);
+
+	/* Update shadow table and hw entry */
+	mvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MH);
+	mvpp2_prs_hw_write(priv, &pe);
 }
 
 /* Set default entires (place holder) for promiscuous, non-promiscuous and
@@ -1426,8 +1443,9 @@ static int mvpp2_prs_etype_init(struct mvpp2 *priv)
 	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
 	mvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4,
 				 MVPP2_PRS_RI_L3_PROTO_MASK);
-	/* Skip eth_type + 4 bytes of IP header */
-	mvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,
+	/* goto ipv4 dest-address (skip eth_type + IP-header-size - 4) */
+	mvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN +
+				 sizeof(struct iphdr) - 4,
 				 MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
 	/* Set L3 offset */
 	mvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
@@ -1631,8 +1649,9 @@ static int mvpp2_prs_pppoe_init(struct mvpp2 *priv)
 	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
 	mvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4_OPT,
 				 MVPP2_PRS_RI_L3_PROTO_MASK);
-	/* Skip eth_type + 4 bytes of IP header */
-	mvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,
+	/* goto ipv4 dest-address (skip eth_type + IP-header-size - 4) */
+	mvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN +
+				 sizeof(struct iphdr) - 4,
 				 MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
 	/* Set L3 offset */
 	mvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
@@ -1762,19 +1781,20 @@ static int mvpp2_prs_ip4_init(struct mvpp2 *priv)
 	mvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
 	pe.index = MVPP2_PE_IP4_PROTO_UN;
 
-	/* Set next lu to IPv4 */
-	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
-	mvpp2_prs_sram_shift_set(&pe, 12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Finished: go to flowid generation */
+	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+
 	/* Set L4 offset */
 	mvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,
 				  sizeof(struct iphdr) - 4,
 				  MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
-	mvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
-				 MVPP2_PRS_IPV4_DIP_AI_BIT);
+	mvpp2_prs_sram_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
 	mvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,
 				 MVPP2_PRS_RI_L4_PROTO_MASK);
 
-	mvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
+	mvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				 MVPP2_PRS_IPV4_DIP_AI_BIT);
 	/* Unmask all ports */
 	mvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
 
@@ -1787,14 +1807,19 @@ static int mvpp2_prs_ip4_init(struct mvpp2 *priv)
 	mvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
 	pe.index = MVPP2_PE_IP4_ADDR_UN;
 
-	/* Finished: go to flowid generation */
-	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
-	mvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	/* Go again to ipv4 */
+	mvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
+
+	mvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				 MVPP2_PRS_IPV4_DIP_AI_BIT);
+
+	/* Shift back to IPv4 proto */
+	mvpp2_prs_sram_shift_set(&pe, -12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
 	mvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UCAST,
 				 MVPP2_PRS_RI_L3_ADDR_MASK);
+	mvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
 
-	mvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
-				 MVPP2_PRS_IPV4_DIP_AI_BIT);
 	/* Unmask all ports */
 	mvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
 
@@ -1941,7 +1966,8 @@ static int mvpp2_prs_ip6_init(struct mvpp2 *priv)
 }
 
 /* Find tcam entry with matched pair <vid,port> */
-static int mvpp2_prs_vid_range_find(struct mvpp2_port *port, u16 vid, u16 mask)
+static int mvpp2_prs_vid_range_find(struct mvpp2 *priv, int port_id, u16 vid,
+				    u16 mask)
 {
 	unsigned char byte[2], enable[2];
 	struct mvpp2_prs_entry pe;
@@ -1949,13 +1975,13 @@ static int mvpp2_prs_vid_range_find(struct mvpp2_port *port, u16 vid, u16 mask)
 	int tid;
 
 	/* Go through the all entries with MVPP2_PRS_LU_VID */
-	for (tid = MVPP2_PRS_VID_PORT_FIRST(port->id);
-	     tid <= MVPP2_PRS_VID_PORT_LAST(port->id); tid++) {
-		if (!port->priv->prs_shadow[tid].valid ||
-		    port->priv->prs_shadow[tid].lu != MVPP2_PRS_LU_VID)
+	for (tid = MVPP2_PRS_VID_PORT_FIRST(port_id);
+	     tid <= MVPP2_PRS_VID_PORT_LAST(port_id); tid++) {
+		if (!priv->prs_shadow[tid].valid ||
+		    priv->prs_shadow[tid].lu != MVPP2_PRS_LU_VID)
 			continue;
 
-		mvpp2_prs_init_from_hw(port->priv, &pe, tid);
+		mvpp2_prs_init_from_hw(priv, &pe, tid);
 
 		mvpp2_prs_tcam_data_byte_get(&pe, 2, &byte[0], &enable[0]);
 		mvpp2_prs_tcam_data_byte_get(&pe, 3, &byte[1], &enable[1]);
@@ -1985,7 +2011,7 @@ int mvpp2_prs_vid_entry_add(struct mvpp2_port *port, u16 vid)
 	memset(&pe, 0, sizeof(pe));
 
 	/* Scan TCAM and see if entry with this <vid,port> already exist */
-	tid = mvpp2_prs_vid_range_find(port, vid, mask);
+	tid = mvpp2_prs_vid_range_find(priv, port->id, vid, mask);
 
 	reg_val = mvpp2_read(priv, MVPP2_MH_REG(port->id));
 	if (reg_val & MVPP2_DSA_EXTENDED)
@@ -2043,7 +2069,7 @@ void mvpp2_prs_vid_entry_remove(struct mvpp2_port *port, u16 vid)
 	int tid;
 
 	/* Scan TCAM and see if entry with this <vid,port> already exist */
-	tid = mvpp2_prs_vid_range_find(port, vid, 0xfff);
+	tid = mvpp2_prs_vid_range_find(priv, port->id, vid, 0xfff);
 
 	/* No such entry */
 	if (tid < 0)
@@ -2061,7 +2087,8 @@ void mvpp2_prs_vid_remove_all(struct mvpp2_port *port)
 
 	for (tid = MVPP2_PRS_VID_PORT_FIRST(port->id);
 	     tid <= MVPP2_PRS_VID_PORT_LAST(port->id); tid++) {
-		if (priv->prs_shadow[tid].valid) {
+		if (priv->prs_shadow[tid].valid &&
+		    priv->prs_shadow[tid].lu == MVPP2_PRS_LU_VID) {
 			mvpp2_prs_hw_inv(priv, tid);
 			priv->prs_shadow[tid].valid = false;
 		}
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.h b/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.h
index 4b68dd374733..460f14af9407 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.h
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_prs.h
@@ -51,6 +51,7 @@
 #define MVPP2_PRS_AI_MASK			0xff
 #define MVPP2_PRS_PORT_MASK			0xff
 #define MVPP2_PRS_LU_MASK			0xf
+#define MVPP2_PRS_WORD_MASK			0xffff
 
 /* TCAM entries in registers are accessed using 16 data bits + 16 enable bits */
 #define MVPP2_PRS_BYTE_TO_WORD(byte)	((byte) / 2)
@@ -103,10 +104,11 @@
 #define MVPP2_PE_MAC_RANGE_START	(MVPP2_PE_MAC_RANGE_END - \
 						MVPP2_PRS_MAC_RANGE_SIZE + 1)
 /* VLAN filtering range */
-#define MVPP2_PE_VID_FILT_RANGE_END     (MVPP2_PRS_TCAM_SRAM_SIZE - 31)
+#define MVPP2_PE_VID_FILT_RANGE_END     (MVPP2_PRS_TCAM_SRAM_SIZE - 32)
 #define MVPP2_PE_VID_FILT_RANGE_START   (MVPP2_PE_VID_FILT_RANGE_END - \
 					 MVPP2_PRS_VLAN_FILT_RANGE_SIZE + 1)
-#define MVPP2_PE_LAST_FREE_TID          (MVPP2_PE_MAC_RANGE_START - 1)
+#define MVPP2_PE_LAST_FREE_TID		(MVPP2_PE_MAC_RANGE_START - 1)
+#define MVPP2_PE_MH_SKIP_PRS		(MVPP2_PRS_TCAM_SRAM_SIZE - 31)
 #define MVPP2_PE_IP6_EXT_PROTO_UN	(MVPP2_PRS_TCAM_SRAM_SIZE - 30)
 #define MVPP2_PE_IP6_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 29)
 #define MVPP2_PE_IP4_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 28)
-- 
2.31.1

