From ff40518c5d6ed530fc31629934a8be7ef5f5c99c Mon Sep 17 00:00:00 2001
From: Subbaraya Sundeep <sbhatta@marvell.com>
Date: Tue, 2 Apr 2019 14:46:42 +0530
Subject: [PATCH 0118/1921] octeontx2-pf: Protect mailbox access against race
 conditions

Mailbox access needs to be serialized from the time messages are
allocated in mailbox and sent to AF or else it leads to race
condition (say changing netdev settings and accessing
PHC simultaneously)

Change-Id: Ifbf6afe8674eb3c395cac1dabe03c9e8d50b5c07
Signed-off-by: Subbaraya Sundeep <sbhatta@marvell.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 .../marvell/octeontx2/nic/otx2_common.c       | 100 ++++++++++++++----
 .../marvell/octeontx2/nic/otx2_common.h       |  17 +++
 .../ethernet/marvell/octeontx2/nic/otx2_pf.c  |  55 +++++++---
 .../marvell/octeontx2/nic/otx2_txrx.c         |  10 +-
 4 files changed, 144 insertions(+), 38 deletions(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
index 045f66b628d5..f20125429b8a 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
@@ -28,11 +28,16 @@ void otx2_update_lmac_stats(struct otx2_nic *pfvf)
 
 	if (!netif_running(pfvf->netdev))
 		return;
+
+	otx2_mbox_lock(&pfvf->mbox);
 	req = otx2_mbox_alloc_msg_cgx_stats(&pfvf->mbox);
-	if (!req)
+	if (!req) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return;
+	}
 
 	otx2_sync_mbox_msg(&pfvf->mbox);
+	otx2_mbox_unlock(&pfvf->mbox);
 }
 
 int otx2_update_rq_stats(struct otx2_nic *pfvf, int qidx)
@@ -107,14 +112,20 @@ void otx2_get_stats64(struct net_device *netdev,
 int otx2_hw_set_mac_addr(struct otx2_nic *pfvf, struct net_device *netdev)
 {
 	struct nix_set_mac_addr *req;
+	int err;
 
+	otx2_mbox_lock(&pfvf->mbox);
 	req = otx2_mbox_alloc_msg_nix_set_mac_addr(&pfvf->mbox);
-	if (!req)
+	if (!req) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 
 	ether_addr_copy(req->mac_addr, netdev->dev_addr);
 
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	otx2_mbox_unlock(&pfvf->mbox);
+	return err;
 }
 
 int otx2_set_mac_address(struct net_device *netdev, void *p)
@@ -135,14 +146,20 @@ int otx2_set_mac_address(struct net_device *netdev, void *p)
 int otx2_hw_set_mtu(struct otx2_nic *pfvf, int mtu)
 {
 	struct nix_frs_cfg *req;
+	int err;
 
+	otx2_mbox_lock(&pfvf->mbox);
 	req = otx2_mbox_alloc_msg_nix_set_hw_frs(&pfvf->mbox);
-	if (!req)
+	if (!req) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 
 	req->update_smq = true;
 	req->maxlen = mtu + OTX2_ETH_HLEN;
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	otx2_mbox_unlock(&pfvf->mbox);
+	return err;
 }
 
 int otx2_change_mtu(struct net_device *netdev, int new_mtu)
@@ -165,15 +182,21 @@ int otx2_set_flowkey_cfg(struct otx2_nic *pfvf)
 {
 	struct otx2_rss_info *rss = &pfvf->hw.rss_info;
 	struct nix_rss_flowkey_cfg *req;
+	int err;
 
+	otx2_mbox_lock(&pfvf->mbox);
 	req = otx2_mbox_alloc_msg_nix_rss_flowkey_cfg(&pfvf->mbox);
-	if (!req)
+	if (!req) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 	req->mcam_index = -1; /* Default or reserved index */
 	req->flowkey_cfg = rss->flowkey_cfg;
 	req->group = DEFAULT_RSS_CONTEXT_GROUP;
 
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	otx2_mbox_unlock(&pfvf->mbox);
+	return err;
 }
 
 int otx2_set_rss_table(struct otx2_nic *pfvf)
@@ -183,6 +206,7 @@ int otx2_set_rss_table(struct otx2_nic *pfvf)
 	struct nix_aq_enq_req *aq;
 	int idx, err;
 
+	otx2_mbox_lock(mbox);
 	/* Get memory to put this msg */
 	for (idx = 0; idx < rss->rss_size; idx++) {
 		aq = otx2_mbox_alloc_msg_nix_aq_enq(mbox);
@@ -191,11 +215,15 @@ int otx2_set_rss_table(struct otx2_nic *pfvf)
 			 * Flush it and retry
 			 */
 			err = otx2_sync_mbox_msg(mbox);
-			if (err)
+			if (err) {
+				otx2_mbox_unlock(mbox);
 				return err;
+			}
 			aq = otx2_mbox_alloc_msg_nix_aq_enq(mbox);
-			if (!aq)
+			if (!aq) {
+				otx2_mbox_unlock(mbox);
 				return -ENOMEM;
+			}
 		}
 
 		aq->rss.rq = rss->ind_tbl[idx];
@@ -205,7 +233,9 @@ int otx2_set_rss_table(struct otx2_nic *pfvf)
 		aq->ctype = NIX_AQ_CTYPE_RSS;
 		aq->op = NIX_AQ_INSTOP_INIT;
 	}
-	return otx2_sync_mbox_msg(mbox);
+	err = otx2_sync_mbox_msg(mbox);
+	otx2_mbox_unlock(mbox);
+	return err;
 }
 
 void otx2_set_rss_key(struct otx2_nic *pfvf)
@@ -397,7 +427,7 @@ int otx2_txschq_config(struct otx2_nic *pfvf, int lvl)
 int otx2_txsch_alloc(struct otx2_nic *pfvf)
 {
 	struct nix_txsch_alloc_req *req;
-	int lvl;
+	int lvl, err;
 
 	/* Get memory to put this msg */
 	req = otx2_mbox_alloc_msg_nix_txsch_alloc(&pfvf->mbox);
@@ -408,7 +438,10 @@ int otx2_txsch_alloc(struct otx2_nic *pfvf)
 	for (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++)
 		req->schq[lvl] = 1;
 
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	if (err)
+		return err;
+	return 0;
 }
 
 int otx2_txschq_stop(struct otx2_nic *pfvf)
@@ -416,13 +449,17 @@ int otx2_txschq_stop(struct otx2_nic *pfvf)
 	struct nix_txsch_free_req *free_req;
 	int lvl, schq;
 
+	otx2_mbox_lock(&pfvf->mbox);
 	/* Free the transmit schedulers */
 	free_req = otx2_mbox_alloc_msg_nix_txsch_free(&pfvf->mbox);
-	if (!free_req)
+	if (!free_req) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 
 	free_req->flags = TXSCHQ_FREE_ALL;
 	WARN_ON(otx2_sync_mbox_msg(&pfvf->mbox));
+	otx2_mbox_unlock(&pfvf->mbox);
 
 	/* Clear the txschq list */
 	for (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {
@@ -982,7 +1019,7 @@ int otx2_config_npa(struct otx2_nic *pfvf)
 	struct otx2_qset *qset = &pfvf->qset;
 	struct npa_lf_alloc_req  *npalf;
 	struct otx2_hw *hw = &pfvf->hw;
-	int aura_cnt;
+	int aura_cnt, err;
 
 	/* Pool - Stack of free buffer pointers
 	 * Aura - Alloc/frees pointers from/to pool for NIX DMA.
@@ -1006,22 +1043,29 @@ int otx2_config_npa(struct otx2_nic *pfvf)
 	aura_cnt = ilog2(roundup_pow_of_two(hw->pool_cnt));
 	npalf->aura_sz = (aura_cnt >= ilog2(128)) ? (aura_cnt - 6) : 1;
 
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	if (err)
+		return err;
+	return 0;
 }
 
 int otx2_detach_resources(struct mbox *mbox)
 {
 	struct rsrc_detach *detach;
 
+	otx2_mbox_lock(mbox);
 	detach = otx2_mbox_alloc_msg_detach_resources(mbox);
-	if (!detach)
+	if (!detach) {
+		otx2_mbox_unlock(mbox);
 		return -ENOMEM;
+	}
 
 	/* detach all */
 	detach->partial = false;
 
 	/* Send detach request to AF */
 	otx2_mbox_msg_send(&mbox->mbox, 0);
+	otx2_mbox_unlock(mbox);
 	return 0;
 }
 
@@ -1031,27 +1075,37 @@ int otx2_attach_npa_nix(struct otx2_nic *pfvf)
 	struct msg_req *msix;
 	int err;
 
+	otx2_mbox_lock(&pfvf->mbox);
 	/* Get memory to put this msg */
 	attach = otx2_mbox_alloc_msg_attach_resources(&pfvf->mbox);
-	if (!attach)
+	if (!attach) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 
 	attach->npalf = true;
 	attach->nixlf = true;
 
 	/* Send attach request to AF */
 	err = otx2_sync_mbox_msg(&pfvf->mbox);
-	if (err)
+	if (err) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return err;
+	}
 
 	/* Get NPA and NIX MSIX vector offsets */
 	msix = otx2_mbox_alloc_msg_msix_offset(&pfvf->mbox);
-	if (!msix)
+	if (!msix) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 
 	err = otx2_sync_mbox_msg(&pfvf->mbox);
-	if (err)
+	if (err) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return err;
+	}
+	otx2_mbox_unlock(&pfvf->mbox);
 
 	if (pfvf->hw.npa_msixoff == MSIX_VECTOR_INVALID ||
 	    pfvf->hw.nix_msixoff == MSIX_VECTOR_INVALID) {
@@ -1066,18 +1120,22 @@ void otx2_ctx_disable(struct mbox *mbox, int type, bool npa)
 {
 	struct hwctx_disable_req *req;
 
+	otx2_mbox_lock(mbox);
 	/* Request AQ to disable this context */
 	if (npa)
 		req = otx2_mbox_alloc_msg_npa_hwctx_disable(mbox);
 	else
 		req = otx2_mbox_alloc_msg_nix_hwctx_disable(mbox);
 
-	if (!req)
+	if (!req) {
+		otx2_mbox_unlock(mbox);
 		return;
+	}
 
 	req->ctype = type;
 
 	WARN_ON(otx2_sync_mbox_msg(mbox));
+	otx2_mbox_unlock(mbox);
 }
 
 int otx2_nix_config_bp(struct otx2_nic *pfvf, bool enable)
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
index be030aba88b0..aa7d7199a1ed 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
@@ -100,6 +100,7 @@ struct  mbox {
 	struct work_struct	mbox_up_wrk;
 	struct otx2_nic		*pfvf;
 	void *bbuf_base; /* Bounce buffer for mbox memory */
+	atomic_t		lock; /* serialize mailbox access */
 };
 
 struct otx2_hw {
@@ -221,6 +222,22 @@ static inline void otx2_sync_mbox_bbuf(struct otx2_mbox *mbox, int devid)
 	       hw_mbase + mbox->rx_start, msg_size + msgs_offset);
 }
 
+static inline void otx2_mbox_lock_init(struct mbox *mbox)
+{
+	atomic_set(&mbox->lock, 0);
+}
+
+static inline void otx2_mbox_lock(struct mbox *mbox)
+{
+	while (!(atomic_add_return(1, &mbox->lock) == 1))
+		cpu_relax();
+}
+
+static inline void otx2_mbox_unlock(struct mbox *mbox)
+{
+	atomic_set(&mbox->lock, 0);
+}
+
 /* With the absence of API for 128-bit IO memory access for arm64,
  * implement required operations at place.
  */
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
index 5fae856fef56..560e73873d33 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
@@ -350,6 +350,7 @@ static int otx2_pfaf_mbox_init(struct otx2_nic *pf)
 
 	INIT_WORK(&mbox->mbox_wrk, otx2_pfaf_mbox_handler);
 	INIT_WORK(&mbox->mbox_up_wrk, otx2_pfaf_mbox_up_handler);
+	otx2_mbox_lock_init(&pf->mbox);
 
 	return 0;
 exit:
@@ -360,31 +361,43 @@ static int otx2_pfaf_mbox_init(struct otx2_nic *pf)
 static int otx2_cgx_config_linkevents(struct otx2_nic *pf, bool enable)
 {
 	struct msg_req *msg;
+	int err;
 
+	otx2_mbox_lock(&pf->mbox);
 	if (enable)
 		msg = otx2_mbox_alloc_msg_cgx_start_linkevents(&pf->mbox);
 	else
 		msg = otx2_mbox_alloc_msg_cgx_stop_linkevents(&pf->mbox);
 
-	if (!msg)
+	if (!msg) {
+		otx2_mbox_unlock(&pf->mbox);
 		return -ENOMEM;
+	}
 
-	return otx2_sync_mbox_msg(&pf->mbox);
+	err = otx2_sync_mbox_msg(&pf->mbox);
+	otx2_mbox_unlock(&pf->mbox);
+	return err;
 }
 
 static int otx2_cgx_config_loopback(struct otx2_nic *pf, bool enable)
 {
 	struct msg_req *msg;
+	int err;
 
+	otx2_mbox_lock(&pf->mbox);
 	if (enable)
 		msg = otx2_mbox_alloc_msg_cgx_intlbk_enable(&pf->mbox);
 	else
 		msg = otx2_mbox_alloc_msg_cgx_intlbk_disable(&pf->mbox);
 
-	if (!msg)
+	if (!msg) {
+		otx2_mbox_unlock(&pf->mbox);
 		return -ENOMEM;
+	}
 
-	return otx2_sync_mbox_msg(&pf->mbox);
+	err = otx2_sync_mbox_msg(&pf->mbox);
+	otx2_mbox_unlock(&pf->mbox);
+	return err;
 }
 
 int otx2_set_real_num_queues(struct net_device *netdev,
@@ -506,7 +519,7 @@ static void otx2_disable_napi(struct otx2_nic *pf)
 static int otx2_init_hw_resources(struct otx2_nic *pf)
 {
 	struct otx2_hw *hw = &pf->hw;
-	int err, lvl;
+	int err = 0, lvl;
 
 	/* Set required NPA LF's pool counts
 	 * Auras and Pools are used in a 1:1 mapping,
@@ -516,15 +529,16 @@ static int otx2_init_hw_resources(struct otx2_nic *pf)
 	hw->sqpool_cnt = hw->tx_queues;
 	hw->pool_cnt = hw->rqpool_cnt + hw->sqpool_cnt;
 
+	otx2_mbox_lock(&pf->mbox);
 	/* NPA init */
 	err = otx2_config_npa(pf);
 	if (err)
-		return err;
+		goto exit;
 
 	/* NIX init */
 	err = otx2_config_nix(pf);
 	if (err)
-		return err;
+		goto exit;
 
 	/* Enable backpressure */
 	otx2_nix_config_bp(pf, true);
@@ -532,28 +546,29 @@ static int otx2_init_hw_resources(struct otx2_nic *pf)
 	/* Init Auras and pools used by NIX RQ, for free buffer ptrs */
 	err = otx2_rq_aura_pool_init(pf);
 	if (err)
-		return err;
+		goto exit;
 
 	/* Init Auras and pools used by NIX SQ, for queueing SQEs */
 	err = otx2_sq_aura_pool_init(pf);
 	if (err)
-		return err;
+		goto exit;
 
 	err = otx2_txsch_alloc(pf);
 	if (err)
-		return err;
+		goto exit;
 
 	err = otx2_config_nix_queues(pf);
 	if (err)
-		return err;
+		goto exit;
 
 	for (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {
 		err = otx2_txschq_config(pf, lvl);
 		if (err)
-			return err;
+			goto exit;
 	}
-
-	return 0;
+exit:
+	otx2_mbox_unlock(&pf->mbox);
+	return err;
 }
 
 static void otx2_free_hw_resources(struct otx2_nic *pf)
@@ -570,9 +585,11 @@ static void otx2_free_hw_resources(struct otx2_nic *pf)
 	if (err)
 		dev_err(pf->dev, "RVUPF: Failed to stop/free TX schedulers\n");
 
+	otx2_mbox_lock(mbox);
 	/* Disable backpressure */
 	if (!(pf->pcifunc & RVU_PFVF_FUNC_MASK))
 		otx2_nix_config_bp(pf, false);
+	otx2_mbox_unlock(mbox);
 
 	/* Disable SQs */
 	otx2_ctx_disable(mbox, NIX_AQ_CTYPE_SQ, false);
@@ -609,20 +626,24 @@ static void otx2_free_hw_resources(struct otx2_nic *pf)
 		qmem_free(pf->dev, cq->cqe);
 	}
 
+	otx2_mbox_lock(mbox);
 	/* Reset NIX LF */
 	req = otx2_mbox_alloc_msg_nix_lf_free(mbox);
 	if (req)
 		WARN_ON(otx2_sync_mbox_msg(mbox));
+	otx2_mbox_unlock(mbox);
 
 	/* Disable NPA Pool and Aura hw context */
 	otx2_ctx_disable(mbox, NPA_AQ_CTYPE_POOL, true);
 	otx2_ctx_disable(mbox, NPA_AQ_CTYPE_AURA, true);
 	otx2_aura_pool_free(pf);
 
+	otx2_mbox_lock(mbox);
 	/* Reset NPA LF */
 	req = otx2_mbox_alloc_msg_npa_lf_free(mbox);
 	if (req)
 		WARN_ON(otx2_sync_mbox_msg(mbox));
+	otx2_mbox_unlock(mbox);
 }
 
 static netdev_tx_t otx2_xmit(struct sk_buff *skb, struct net_device *netdev)
@@ -880,9 +901,12 @@ static void otx2_set_rx_mode(struct net_device *netdev)
 	if (!(netdev->flags & IFF_UP))
 		return;
 
+	otx2_mbox_lock(&pf->mbox);
 	req = otx2_mbox_alloc_msg_nix_set_rx_mode(&pf->mbox);
-	if (!req)
+	if (!req) {
+		otx2_mbox_unlock(&pf->mbox);
 		return;
+	}
 
 	req->mode = NIX_RX_MODE_UCAST;
 
@@ -893,6 +917,7 @@ static void otx2_set_rx_mode(struct net_device *netdev)
 		req->mode |= NIX_RX_MODE_ALLMULTI;
 
 	otx2_sync_mbox_msg_busy_poll(&pf->mbox);
+	otx2_mbox_unlock(&pf->mbox);
 }
 
 static void otx2_reset_task(struct work_struct *work)
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
index fcb0c7e3a28a..7c56c9116763 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
@@ -763,14 +763,20 @@ bool otx2_sq_append_skb(struct net_device *netdev, struct otx2_snd_queue *sq,
 int otx2_rxtx_enable(struct otx2_nic *pfvf, bool enable)
 {
 	struct msg_req *msg;
+	int err;
 
+	otx2_mbox_lock(&pfvf->mbox);
 	if (enable)
 		msg = otx2_mbox_alloc_msg_nix_lf_start_rx(&pfvf->mbox);
 	else
 		msg = otx2_mbox_alloc_msg_nix_lf_stop_rx(&pfvf->mbox);
 
-	if (!msg)
+	if (!msg) {
+		otx2_mbox_unlock(&pfvf->mbox);
 		return -ENOMEM;
+	}
 
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	otx2_mbox_unlock(&pfvf->mbox);
+	return err;
 }
-- 
2.31.1

