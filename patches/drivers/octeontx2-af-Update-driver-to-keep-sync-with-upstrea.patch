From de665f370d243e37aeda6130bf0bc5a1941542e0 Mon Sep 17 00:00:00 2001
From: Geetha sowjanya <gakula@marvell.com>
Date: Wed, 28 Jul 2021 14:44:31 +0530
Subject: [PATCH 1664/1921] octeontx2-af: Update driver to keep sync with
 upstream driver

This patch backport below upstream changes
1. Code placement.
2. Typo errors
3. Type casting issues.
4. Replace "det_dma_mask and dma_set_mask_and_coherent APIs with
 pci_set_consistent_dma_mask".
5. Initialize struct npc_lt_def_cfg as constant.

Change-Id: I7208624243f6f70f343d9f749bbc1becb8b18508
Signed-off-by: Geetha sowjanya <gakula@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/57405
Reviewed-by: Sunil Kovvuri Goutham <sgoutham@marvell.com>
Tested-by: Sunil Kovvuri Goutham <sgoutham@marvell.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 .../ethernet/marvell/octeontx2/af/common.h    |   4 +-
 .../net/ethernet/marvell/octeontx2/af/rvu.c   |  44 +-
 .../net/ethernet/marvell/octeontx2/af/rvu.h   |  81 ++-
 .../ethernet/marvell/octeontx2/af/rvu_cn10k.c | 521 +++++++++---------
 .../ethernet/marvell/octeontx2/af/rvu_nix.c   |   4 +-
 .../ethernet/marvell/octeontx2/af/rvu_npc.c   |  12 +-
 6 files changed, 328 insertions(+), 338 deletions(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/af/common.h b/drivers/net/ethernet/marvell/octeontx2/af/common.h
index 7bd418384679..1a3e6f375727 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/common.h
+++ b/drivers/net/ethernet/marvell/octeontx2/af/common.h
@@ -156,7 +156,7 @@ enum nix_scheduler {
 #define	NIC_HW_MAX_FRS			9212
 #define	SDP_HW_MAX_FRS			65535
 #define CN10K_LMAC_LINK_MAX_FRS		16380 /* 16k - FCS */
-#define CN10K_LBK_LINK_MAX_FRS		65535
+#define CN10K_LBK_LINK_MAX_FRS		65535 /* 64k */
 
 /* NIX RX action operation*/
 #define NIX_RX_ACTIONOP_DROP		(0x0ull)
@@ -165,7 +165,7 @@ enum nix_scheduler {
 #define NIX_RX_ACTIONOP_MCAST		(0x3ull)
 #define NIX_RX_ACTIONOP_RSS		(0x4ull)
 /* Use the RX action set in the default unicast entry */
-#define NIX_RX_ACTION_DEFAULT	(0xfull)
+#define NIX_RX_ACTION_DEFAULT		(0xfull)
 
 /* NIX TX action operation*/
 #define NIX_TX_ACTIONOP_DROP		(0x0ull)
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu.c
index d47fae195df6..628d451af193 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu.c
@@ -713,7 +713,7 @@ static int rvu_setup_msix_resources(struct rvu *rvu)
 	}
 
 	/* HW interprets RVU_AF_MSIXTR_BASE address as an IOVA, hence
-	 * create a IOMMU mapping for the physcial address configured by
+	 * create an IOMMU mapping for the physical address configured by
 	 * firmware and reconfig RVU_AF_MSIXTR_BASE with IOVA.
 	 */
 	cfg = rvu_read64(rvu, BLKADDR_RVUM, RVU_PRIV_CONST);
@@ -722,8 +722,6 @@ static int rvu_setup_msix_resources(struct rvu *rvu)
 		phy_addr = rvu->fwdata->msixtr_base;
 	else
 		phy_addr = rvu_read64(rvu, BLKADDR_RVUM, RVU_AF_MSIXTR_BASE);
-	/* Register save */
-	rvu->msixtr_base_phy = phy_addr;
 	iova = dma_map_resource(rvu->dev, phy_addr,
 				max_msix * PCI_MSIX_ENTRY_SIZE,
 				DMA_BIDIRECTIONAL, 0);
@@ -733,6 +731,7 @@ static int rvu_setup_msix_resources(struct rvu *rvu)
 
 	rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_MSIXTR_BASE, (u64)iova);
 	rvu->msix_base_iova = iova;
+	rvu->msixtr_base_phy = phy_addr;
 
 	return 0;
 }
@@ -764,7 +763,7 @@ static void rvu_free_hw_resources(struct rvu *rvu)
 		kfree(block->lf.bmap);
 	}
 
-	/* Free MSIX and TIM bitmaps */
+	/* Free MSIX bitmaps */
 	for (id = 0; id < hw->total_pfs; id++) {
 		pfvf = &rvu->pf[id];
 		kfree(pfvf->msix.bmap);
@@ -796,7 +795,7 @@ static void rvu_setup_pfvf_macaddress(struct rvu *rvu)
 	u64 *mac;
 
 	for (pf = 0; pf < hw->total_pfs; pf++) {
-		/* For PF0 (AF) get mac address only for AFVFs (LBKVFs) */
+		/* For PF0(AF), Assign MAC address to only VFs (LBKVFs) */
 		if (!pf)
 			goto lbkvf;
 
@@ -819,7 +818,7 @@ static void rvu_setup_pfvf_macaddress(struct rvu *rvu)
 		/* Assign MAC address to VFs*/
 		rvu_get_pf_numvfs(rvu, pf, &numvfs, &hwvf);
 		for (vf = 0; vf < numvfs; vf++, hwvf++) {
-			pfvf =  &rvu->hwvf[hwvf];
+			pfvf = &rvu->hwvf[hwvf];
 			if (rvu->fwdata && hwvf < VF_MACNUM_MAX) {
 				mac = &rvu->fwdata->vf_macs[hwvf];
 				if (*mac)
@@ -829,7 +828,7 @@ static void rvu_setup_pfvf_macaddress(struct rvu *rvu)
 			} else {
 				eth_random_addr(pfvf->mac_addr);
 			}
-		ether_addr_copy(pfvf->default_mac, pfvf->mac_addr);
+			ether_addr_copy(pfvf->default_mac, pfvf->mac_addr);
 		}
 	}
 }
@@ -2100,9 +2099,9 @@ int rvu_mbox_handler_set_vf_perm(struct rvu *rvu, struct set_vf_perm *req,
 	target = (pcifunc & ~RVU_PFVF_FUNC_MASK) | (req->vf + 1);
 	pfvf = rvu_get_pfvf(rvu, target);
 
-	if (req->flags & RESET_VF_PERM)
+	if (req->flags & RESET_VF_PERM) {
 		pfvf->flags &= RVU_CLEAR_VF_PERM;
-	else if (test_bit(PF_SET_VF_TRUSTED, &pfvf->flags) ^
+	} else if (test_bit(PF_SET_VF_TRUSTED, &pfvf->flags) ^
 		 (req->flags & VF_TRUSTED)) {
 		change_bit(PF_SET_VF_TRUSTED, &pfvf->flags);
 		/* disable multicast and promisc entries */
@@ -2366,7 +2365,7 @@ static int rvu_get_mbox_regions(struct rvu *rvu, void **mbox_addr,
 				bar4 = rvupf_read64(rvu, RVU_PF_VF_BAR4_ADDR);
 				bar4 += region * MBOX_SIZE;
 			}
-			mbox_addr[region] = ioremap_wc(bar4, MBOX_SIZE);
+			mbox_addr[region] = (void *)ioremap_wc(bar4, MBOX_SIZE);
 			if (!mbox_addr[region])
 				goto error;
 		}
@@ -2386,7 +2385,7 @@ static int rvu_get_mbox_regions(struct rvu *rvu, void **mbox_addr,
 					  RVU_AF_PF_BAR4_ADDR);
 			bar4 += region * MBOX_SIZE;
 		}
-		mbox_addr[region] = ioremap_wc(bar4, MBOX_SIZE);
+		mbox_addr[region] = (void *)ioremap_wc(bar4, MBOX_SIZE);
 		if (!mbox_addr[region])
 			goto error;
 	}
@@ -2394,7 +2393,7 @@ static int rvu_get_mbox_regions(struct rvu *rvu, void **mbox_addr,
 
 error:
 	while (region--)
-		iounmap(mbox_addr[region]);
+		iounmap((void __iomem *)mbox_addr[region]);
 	return -ENOMEM;
 }
 
@@ -2484,7 +2483,7 @@ static int rvu_mbox_init(struct rvu *rvu, struct mbox_wq_info *mw,
 	destroy_workqueue(mw->mbox_wq);
 unmap_regions:
 	while (num--)
-		iounmap(mbox_regions[num]);
+		iounmap((void __iomem *)mbox_regions[num]);
 free_regions:
 	kfree(mbox_regions);
 	return err;
@@ -2874,12 +2873,11 @@ static void rvu_afvf_queue_flr_work(struct rvu *rvu, int start_vf, int numvfs)
 	for (vf = 0; vf < numvfs; vf++) {
 		if (!(intr & BIT_ULL(vf)))
 			continue;
+		dev = vf + start_vf + rvu->hw->total_pfs;
+		queue_work(rvu->flr_wq, &rvu->flr_wrk[dev].work);
 		/* Clear and disable the interrupt */
 		rvupf_write64(rvu, RVU_PF_VFFLR_INTX(reg), BIT_ULL(vf));
 		rvupf_write64(rvu, RVU_PF_VFFLR_INT_ENA_W1CX(reg), BIT_ULL(vf));
-
-		dev = vf + start_vf + rvu->hw->total_pfs;
-		queue_work(rvu->flr_wq, &rvu->flr_wrk[dev].work);
 	}
 }
 
@@ -2895,14 +2893,14 @@ static irqreturn_t rvu_flr_intr_handler(int irq, void *rvu_irq)
 
 	for (pf = 0; pf < rvu->hw->total_pfs; pf++) {
 		if (intr & (1ULL << pf)) {
+			/* PF is already dead do only AF related operations */
+			queue_work(rvu->flr_wq, &rvu->flr_wrk[pf].work);
 			/* clear interrupt */
 			rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_PFFLR_INT,
 				    BIT_ULL(pf));
 			/* Disable the interrupt */
 			rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_PFFLR_INT_ENA_W1C,
 				    BIT_ULL(pf));
-			/* PF is already dead do only AF related operations */
-			queue_work(rvu->flr_wq, &rvu->flr_wrk[pf].work);
 		}
 	}
 
@@ -3430,15 +3428,9 @@ static int rvu_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 		goto err_disable_device;
 	}
 
-	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
-	if (err) {
-		dev_err(dev, "Unable to set DMA mask\n");
-		goto err_release_regions;
-	}
-
-	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	err = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(48));
 	if (err) {
-		dev_err(dev, "Unable to set consistent DMA mask\n");
+		dev_err(dev, "DMA mask config failed, abort\n");
 		goto err_release_regions;
 	}
 
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu.h b/drivers/net/ethernet/marvell/octeontx2/af/rvu.h
index 93852ce445f1..a5cde0127bef 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu.h
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu.h
@@ -188,21 +188,6 @@ struct npc_mcam {
 	struct list_head mcam_rules;
 };
 
-/* KPU profile adapter structure which is used to translate between built-in and
- * firmware KPU profile structures.
- */
-struct npc_kpu_profile_adapter {
-	const char		*name;
-	u64			version;
-	struct npc_lt_def_cfg	*lt_def;
-	struct npc_kpu_profile_action	*ikpu; /* array[pkinds] */
-	struct npc_kpu_profile	*kpu; /* array[kpus] */
-	struct npc_mcam_kex	*mkex;
-	bool			custom; /* true if loadable profile used */
-	size_t			pkinds;
-	size_t			kpus;
-};
-
 struct sso_rsrc {
 	u8      sso_hws;
 	u16     sso_hwgrps;
@@ -413,9 +398,9 @@ struct rvu_hwinfo {
 	u8	npc_intfs;         /* No of interfaces */
 	u8	npc_kpu_entries;   /* No of KPU entries */
 	u16	npc_counters;	   /* No of match stats counters */
+	u32	lbk_bufsize;	   /* FIFO size supported by LBK */
 	bool	npc_ext_set;	   /* Extended register set */
 	u64     npc_stat_ena;      /* Match stats enable bit */
-	u32	lbk_bufsize;	   /* FIFO size supported by LBK */
 
 	struct hw_cap    cap;
 	struct rvu_block block[BLK_COUNT]; /* Block info */
@@ -438,7 +423,7 @@ struct mbox_wq_info {
 };
 
 struct rvu_fwdata {
-#define RVU_FWDATA_HEADER_MAGIC	0xCFDA	/*Custom Firmware Data*/
+#define RVU_FWDATA_HEADER_MAGIC	0xCFDA	/* Custom Firmware Data*/
 #define RVU_FWDATA_VERSION	0x0001
 	u32 header_magic;
 	u32 version;		/* version id */
@@ -463,6 +448,21 @@ struct rvu_fwdata {
 
 struct ptp;
 
+/* KPU profile adapter structure gathering all KPU configuration data and abstracting out the
+ * source where it came from.
+ */
+struct npc_kpu_profile_adapter {
+	const char			*name;
+	u64				version;
+	const struct npc_lt_def_cfg	*lt_def;
+	const struct npc_kpu_profile_action	*ikpu; /* array[pkinds] */
+	const struct npc_kpu_profile	*kpu; /* array[kpus] */
+	struct npc_mcam_kex		*mkex;
+	bool				custom;
+	size_t				pkinds;
+	size_t				kpus;
+};
+
 #define RVU_SWITCH_LBK_CHAN	63
 
 struct rvu_switch {
@@ -500,7 +500,7 @@ struct rvu {
 	char			*irq_name;
 	bool			*irq_allocated;
 	dma_addr_t		msix_base_iova;
-	u64			msixtr_base_phy;/* Register reset value */
+	u64			msixtr_base_phy; /* Register reset value */
 
 	/* CGX */
 #define PF_CGXMAP_BASE		1 /* PF 0 is reserved for RVU PF */
@@ -508,7 +508,7 @@ struct rvu {
 	u8			cgx_mapped_pfs;
 	u8			cgx_cnt_max;	 /* CGX port count max */
 	u8			*pf2cgxlmac_map; /* pf to cgx_lmac map */
-	u64			*cgxlmac2pf_map; /* bitmap of mapped pfs for
+	u16			*cgxlmac2pf_map; /* bitmap of mapped pfs for
 						  * every cgx lmac port
 						  */
 	unsigned long		pf_notify_bmap; /* Flags for PF notification */
@@ -773,8 +773,6 @@ int rvu_cgx_nix_cuml_stats(struct rvu *rvu, void *cgxd, int lmac_id, int index,
 			   int rxtxflag, u64 *stat);
 bool is_cgx_config_permitted(struct rvu *rvu, u16 pcifunc);
 bool rvu_cgx_is_pkind_config_permitted(struct rvu *rvu, u16 pcifunc);
-bool is_mac_feature_supported(struct rvu *rvu, int pf, int feature);
-u32  rvu_cgx_get_fifolen(struct rvu *rvu);
 void *rvu_first_cgx_pdata(struct rvu *rvu);
 int cgxlmac_to_pf(struct rvu *rvu, int cgx_id, int lmac_id);
 
@@ -809,7 +807,6 @@ void rvu_nix_freemem(struct rvu *rvu);
 int rvu_get_nixlf_count(struct rvu *rvu);
 void rvu_nix_lf_teardown(struct rvu *rvu, u16 pcifunc, int blkaddr, int npalf);
 int nix_get_nixlf(struct rvu *rvu, u16 pcifunc, int *nixlf, int *nix_blkaddr);
-void rvu_nix_reset_mac(struct rvu_pfvf *pfvf, int pcifunc);
 bool rvu_nix_is_ptp_tx_enabled(struct rvu *rvu, u16 pcifunc);
 int nix_update_mce_list(struct rvu *rvu, u16 pcifunc,
 			struct nix_mce_list *mce_list,
@@ -818,16 +815,16 @@ void nix_get_mce_list(struct rvu *rvu, u16 pcifunc, int type,
 		      struct nix_mce_list **mce_list, int *mce_idx);
 struct nix_hw *get_nix_hw(struct rvu_hwinfo *hw, int blkaddr);
 int rvu_get_next_nix_blkaddr(struct rvu *rvu, int blkaddr);
-int rvu_get_nix_blkaddr(struct rvu *rvu, u16 pcifunc);
+void rvu_nix_reset_mac(struct rvu_pfvf *pfvf, int pcifunc);
+int nix_get_struct_ptrs(struct rvu *rvu, u16 pcifunc,
+			struct nix_hw **nix_hw, int *blkaddr);
 int rvu_nix_setup_ratelimit_aggr(struct rvu *rvu, u16 pcifunc,
 				 u16 rq_idx, u16 match_id);
 int nix_aq_context_read(struct rvu *rvu, struct nix_hw *nix_hw,
 			struct nix_cn10k_aq_enq_req *aq_req,
 			struct nix_cn10k_aq_enq_rsp *aq_rsp,
 			u16 pcifunc, u8 ctype, u32 qidx);
-int nix_get_struct_ptrs(struct rvu *rvu, u16 pcifunc,
-			struct nix_hw **nix_hw, int *blkaddr);
-
+int rvu_get_nix_blkaddr(struct rvu *rvu, u16 pcifunc);
 /* NPC APIs */
 int rvu_npc_init(struct rvu *rvu);
 void rvu_npc_freemem(struct rvu *rvu);
@@ -862,21 +859,23 @@ void rvu_npc_get_mcam_entry_alloc_info(struct rvu *rvu, u16 pcifunc,
 void rvu_npc_get_mcam_counter_alloc_info(struct rvu *rvu, u16 pcifunc,
 					 int blkaddr, int *alloc_cnt,
 					 int *enable_cnt);
+bool is_npc_intf_tx(u8 intf);
+bool is_npc_intf_rx(u8 intf);
+bool is_npc_interface_valid(struct rvu *rvu, u8 intf);
+int npc_mcam_verify_channel(struct rvu *rvu, u16 pcifunc, u8 intf, u16 channel);
 int npc_flow_steering_init(struct rvu *rvu, int blkaddr);
 const char *npc_get_field_name(u8 hdr);
-int npc_mcam_verify_channel(struct rvu *rvu, u16 pcifunc, u8 intf, u16 channel);
 int npc_get_bank(struct npc_mcam *mcam, int index);
 void npc_mcam_enable_flows(struct rvu *rvu, u16 target);
 void npc_mcam_disable_flows(struct rvu *rvu, u16 target);
 void npc_enable_mcam_entry(struct rvu *rvu, struct npc_mcam *mcam,
 			   int blkaddr, int index, bool enable);
 void npc_read_mcam_entry(struct rvu *rvu, struct npc_mcam *mcam,
-			 int blkaddr, u16 src,
-			 struct mcam_entry *entry, u8 *intf, u8 *ena);
-bool is_npc_intf_tx(u8 intf);
-bool is_npc_intf_rx(u8 intf);
-bool is_npc_interface_valid(struct rvu *rvu, u8 intf);
-
+			 int blkaddr, u16 src, struct mcam_entry *entry,
+			 u8 *intf, u8 *ena);
+bool is_mac_feature_supported(struct rvu *rvu, int pf, int feature);
+u32  rvu_cgx_get_fifolen(struct rvu *rvu);
+void *rvu_first_cgx_pdata(struct rvu *rvu);
 int npc_get_nixlf_mcam_index(struct npc_mcam *mcam, u16 pcifunc, int nixlf,
 			     int type);
 bool is_mcam_entry_enabled(struct rvu *rvu, struct npc_mcam *mcam, int blkaddr,
@@ -889,6 +888,14 @@ void rvu_cpt_unregister_interrupts(struct rvu *rvu);
 int rvu_cpt_lf_teardown(struct rvu *rvu, u16 pcifunc, int blkaddr, int lf,
 			int slot);
 
+/* CN10K RVU */
+int rvu_set_channels_base(struct rvu *rvu);
+void rvu_program_channels(struct rvu *rvu);
+
+/* CN10K RVU - LMT*/
+void rvu_reset_lmt_map_tbl(struct rvu *rvu, u16 pcifunc);
+void rvu_apr_block_cn10k_init(struct rvu *rvu);
+
 /* TIM APIs */
 int rvu_tim_init(struct rvu *rvu);
 int rvu_tim_lf_teardown(struct rvu *rvu, u16 pcifunc, int lf, int slot);
@@ -929,14 +936,6 @@ int rvu_npc_set_parse_mode(struct rvu *rvu, u16 pcifunc, u64 mode, u8 dir,
 			   u64 pkind);
 void rvu_tim_hw_fixes(struct rvu *rvu, int blkaddr);
 
-/* CN10K RVU */
-int rvu_set_channels_base(struct rvu *rvu);
-void rvu_program_channels(struct rvu *rvu);
-
-/* CN10K RVU - LMT*/
-void rvu_reset_lmt_map_tbl(struct rvu *rvu, u16 pcifunc);
-void rvu_apr_block_cn10k_init(struct rvu *rvu);
-
 /* CN10K NIX */
 void rvu_nix_block_cn10k_init(struct rvu *rvu, struct nix_hw *nix_hw);
 
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_cn10k.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu_cn10k.c
index b89cecab1430..7d29b7f73675 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_cn10k.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_cn10k.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
-/* Marvell OcteonTx2 RVU Admin Function driver
+/*  Marvell RPM CN10K driver
  *
  * Copyright (C) 2020 Marvell.
  */
@@ -16,6 +16,265 @@
 #define LMT_MAP_TABLE_SIZE		(128 * 1024)
 #define LMT_MAPTBL_ENTRY_SIZE		16
 
+static int lmtst_map_table_ops(struct rvu *rvu, u32 index, u64 *val,
+			       int lmt_tbl_op)
+{
+	void __iomem *lmt_map_base;
+	u64 tbl_base;
+
+	tbl_base = rvu_read64(rvu, BLKADDR_APR, APR_AF_LMT_MAP_BASE);
+
+	lmt_map_base = ioremap_wc(tbl_base, LMT_MAP_TABLE_SIZE);
+	if (!lmt_map_base) {
+		dev_err(rvu->dev, "Failed to setup lmt map table mapping!!\n");
+		return -ENOMEM;
+	}
+
+	if (lmt_tbl_op == LMT_TBL_OP_READ) {
+		*val = readq(lmt_map_base + index);
+	} else {
+		writeq((*val), (lmt_map_base + index));
+		/* Flushing the AP interceptor cache to make APR_LMT_MAP_ENTRY_S
+		 * changes effective. Write 1 for flush and read is being used as a
+		 * barrier and sets up a data dependency. Write to 0 after a write
+		 * to 1 to complete the flush.
+		 */
+		rvu_write64(rvu, BLKADDR_APR, APR_AF_LMT_CTL, BIT_ULL(0));
+		rvu_read64(rvu, BLKADDR_APR, APR_AF_LMT_CTL);
+		rvu_write64(rvu, BLKADDR_APR, APR_AF_LMT_CTL, 0x00);
+	}
+
+	iounmap(lmt_map_base);
+	return 0;
+}
+
+#define LMT_MAP_TBL_W1_OFF  8
+static u32 rvu_get_lmtst_tbl_index(struct rvu *rvu, u16 pcifunc)
+{
+	return ((rvu_get_pf(pcifunc) * rvu->hw->total_vfs) +
+		(pcifunc & RVU_PFVF_FUNC_MASK)) * LMT_MAPTBL_ENTRY_SIZE;
+}
+
+static int rvu_get_lmtaddr(struct rvu *rvu, u16 pcifunc,
+			   u64 iova, u64 *lmt_addr)
+{
+	u64 pa, val, pf;
+	int err;
+
+	if (!iova) {
+		dev_err(rvu->dev, "%s Requested Null address for transulation\n", __func__);
+		return -EINVAL;
+	}
+
+	rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_ADDR_REQ, iova);
+	pf = rvu_get_pf(pcifunc) & 0x1F;
+	val = BIT_ULL(63) | BIT_ULL(14) | BIT_ULL(13) | pf << 8 |
+	      ((pcifunc & RVU_PFVF_FUNC_MASK) & 0xFF);
+	rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_TXN_REQ, val);
+
+	err = rvu_poll_reg(rvu, BLKADDR_RVUM, RVU_AF_SMMU_ADDR_RSP_STS, BIT_ULL(0), false);
+	if (err) {
+		dev_err(rvu->dev, "%s LMTLINE iova transulation failed\n", __func__);
+		return err;
+	}
+	val = rvu_read64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_ADDR_RSP_STS);
+	if (val & ~0x1ULL) {
+		dev_err(rvu->dev, "%s LMTLINE iova transulation failed err:%llx\n", __func__, val);
+		return -EIO;
+	}
+	/* PA[51:12] = RVU_AF_SMMU_TLN_FLIT1[60:21]
+	 * PA[11:0] = IOVA[11:0]
+	 */
+	pa = rvu_read64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_TLN_FLIT1) >> 21;
+	pa &= GENMASK_ULL(39, 0);
+	*lmt_addr = (pa << 12) | (iova  & 0xFFF);
+
+	return 0;
+}
+
+static int rvu_update_lmtaddr(struct rvu *rvu, u16 pcifunc, u64 lmt_addr)
+{
+	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);
+	u32 tbl_idx;
+	int err = 0;
+	u64 val;
+
+	/* Read the current lmt addr of pcifunc */
+	tbl_idx = rvu_get_lmtst_tbl_index(rvu, pcifunc);
+	err = lmtst_map_table_ops(rvu, tbl_idx, &val, LMT_TBL_OP_READ);
+	if (err) {
+		dev_err(rvu->dev,
+			"Failed to read LMT map table: index 0x%x err %d\n",
+			tbl_idx, err);
+		return err;
+	}
+
+	/* Storing the seondary's lmt base address as this needs to be
+	 * reverted in FLR. Also making sure this default value doesn't
+	 * get overwritten on multiple calls to this mailbox.
+	 */
+	if (!pfvf->lmt_base_addr)
+		pfvf->lmt_base_addr = val;
+
+	/* Update the LMT table with new addr */
+	err = lmtst_map_table_ops(rvu, tbl_idx, &lmt_addr, LMT_TBL_OP_WRITE);
+	if (err) {
+		dev_err(rvu->dev,
+			"Failed to update LMT map table: index 0x%x err %d\n",
+			tbl_idx, err);
+		return err;
+	}
+	return 0;
+}
+
+int rvu_mbox_handler_lmtst_tbl_setup(struct rvu *rvu,
+				     struct lmtst_tbl_setup_req *req,
+				     struct msg_rsp *rsp)
+{
+	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, req->hdr.pcifunc);
+	u32 pri_tbl_idx, tbl_idx;
+	u64 lmt_addr;
+	int err = 0;
+	u64 val;
+
+	/* Check if PF_FUNC wants to use it's own local memory as LMTLINE
+	 * region, if so, convert that IOVA to physical address and
+	 * populate LMT table with that address
+	 */
+	if (req->use_local_lmt_region) {
+		err = rvu_get_lmtaddr(rvu, req->hdr.pcifunc,
+				      req->lmt_iova, &lmt_addr);
+		if (err < 0)
+			return err;
+
+		/* Update the lmt addr for this PFFUNC in the LMT table */
+		err = rvu_update_lmtaddr(rvu, req->hdr.pcifunc, lmt_addr);
+		if (err)
+			return err;
+	}
+
+	/* Reconfiguring lmtst map table in lmt region shared mode i.e. make
+	 * multiple PF_FUNCs to share an LMTLINE region, so primary/base
+	 * pcifunc (which is passed as an argument to mailbox) is the one
+	 * whose lmt base address will be shared among other secondary
+	 * pcifunc (will be the one who is calling this mailbox).
+	 */
+	if (req->base_pcifunc) {
+		/* Calculating the LMT table index equivalent to primary
+		 * pcifunc.
+		 */
+		pri_tbl_idx = rvu_get_lmtst_tbl_index(rvu, req->base_pcifunc);
+
+		/* Read the base lmt addr of the primary pcifunc */
+		err = lmtst_map_table_ops(rvu, pri_tbl_idx, &val,
+					  LMT_TBL_OP_READ);
+		if (err) {
+			dev_err(rvu->dev,
+				"Failed to read LMT map table: index 0x%x err %d\n",
+				pri_tbl_idx, err);
+			goto error;
+		}
+
+		/* Update the base lmt addr of secondary with primary's base
+		 * lmt addr.
+		 */
+		err = rvu_update_lmtaddr(rvu, req->hdr.pcifunc, val);
+		if (err)
+			return err;
+	}
+
+	/* This mailbox can also be used to update word1 of APR_LMT_MAP_ENTRY_S
+	 * like enabling scheduled LMTST, disable LMTLINE prefetch, disable
+	 * early completion for ordered LMTST.
+	 */
+	if (req->sch_ena || req->dis_sched_early_comp || req->dis_line_pref) {
+		tbl_idx = rvu_get_lmtst_tbl_index(rvu, req->hdr.pcifunc);
+		err = lmtst_map_table_ops(rvu, tbl_idx + LMT_MAP_TBL_W1_OFF,
+					  &val, LMT_TBL_OP_READ);
+		if (err) {
+			dev_err(rvu->dev,
+				"Failed to read LMT map table: index 0x%x err %d\n",
+				tbl_idx + LMT_MAP_TBL_W1_OFF, err);
+			goto error;
+		}
+
+		/* Storing lmt map table entry word1 default value as this needs
+		 * to be reverted in FLR. Also making sure this default value
+		 * doesn't get overwritten on multiple calls to this mailbox.
+		 */
+		if (!pfvf->lmt_map_ent_w1)
+			pfvf->lmt_map_ent_w1 = val;
+
+		/* Disable early completion for Ordered LMTSTs. */
+		if (req->dis_sched_early_comp)
+			val |= (req->dis_sched_early_comp <<
+				APR_LMT_MAP_ENT_DIS_SCH_CMP_SHIFT);
+		/* Enable scheduled LMTST */
+		if (req->sch_ena)
+			val |= (req->sch_ena << APR_LMT_MAP_ENT_SCH_ENA_SHIFT) |
+				req->ssow_pf_func;
+		/* Disables LMTLINE prefetch before receiving store data. */
+		if (req->dis_line_pref)
+			val |= (req->dis_line_pref <<
+				APR_LMT_MAP_ENT_DIS_LINE_PREF_SHIFT);
+
+		err = lmtst_map_table_ops(rvu, tbl_idx + LMT_MAP_TBL_W1_OFF,
+					  &val, LMT_TBL_OP_WRITE);
+		if (err) {
+			dev_err(rvu->dev,
+				"Failed to update LMT map table: index 0x%x err %d\n",
+				tbl_idx + LMT_MAP_TBL_W1_OFF, err);
+			goto error;
+		}
+	}
+
+error:
+	return err;
+}
+
+/* Resetting the lmtst map table to original default values */
+void rvu_reset_lmt_map_tbl(struct rvu *rvu, u16 pcifunc)
+{
+	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);
+	u32 tbl_idx;
+	int err;
+
+	if (is_rvu_otx2(rvu))
+		return;
+
+	if (pfvf->lmt_base_addr || pfvf->lmt_map_ent_w1) {
+		/* This corresponds to lmt map table index */
+		tbl_idx = rvu_get_lmtst_tbl_index(rvu, pcifunc);
+		/* Reverting back original lmt base addr for respective
+		 * pcifunc.
+		 */
+		if (pfvf->lmt_base_addr) {
+			err = lmtst_map_table_ops(rvu, tbl_idx,
+						  &pfvf->lmt_base_addr,
+						  LMT_TBL_OP_WRITE);
+			if (err)
+				dev_err(rvu->dev,
+					"Failed to update LMT map table: index 0x%x err %d\n",
+					tbl_idx, err);
+			pfvf->lmt_base_addr = 0;
+		}
+		/* Reverting back to orginal word1 val of lmtst map table entry
+		 * which underwent changes.
+		 */
+		if (pfvf->lmt_map_ent_w1) {
+			err = lmtst_map_table_ops(rvu,
+						  tbl_idx + LMT_MAP_TBL_W1_OFF,
+						  &pfvf->lmt_map_ent_w1,
+						  LMT_TBL_OP_WRITE);
+			if (err)
+				dev_err(rvu->dev,
+					"Failed to update LMT map table: index 0x%x err %d\n",
+					tbl_idx + LMT_MAP_TBL_W1_OFF, err);
+			pfvf->lmt_map_ent_w1 = 0;
+		}
+	}
+}
+
 int rvu_set_channels_base(struct rvu *rvu)
 {
 	u16 nr_lbk_chans, nr_sdp_chans, nr_cgx_chans, nr_cpt_chans;
@@ -279,266 +538,6 @@ void rvu_program_channels(struct rvu *rvu)
 	rvu_rpm_set_channels(rvu);
 }
 
-static int lmtst_map_table_ops(struct rvu *rvu, u32 index, u64 *val,
-			       int lmt_tbl_op)
-{
-	void __iomem *lmt_map_base;
-	u64 tbl_base;
-
-	tbl_base = rvu_read64(rvu, BLKADDR_APR, APR_AF_LMT_MAP_BASE),
-
-	lmt_map_base = ioremap_wc(tbl_base, LMT_MAP_TABLE_SIZE);
-	if (!lmt_map_base) {
-		dev_err(rvu->dev, "Failed to setup lmt map table mapping!!\n");
-		return -ENOMEM;
-	}
-
-	if (lmt_tbl_op == LMT_TBL_OP_READ) {
-		*val = readq(lmt_map_base + index);
-	} else {
-		writeq((*val), (lmt_map_base + index));
-		/* Flushing the AP interceptor cache to make APR_LMT_MAP_ENTRY_S
-		 * changes effective.
-		 * First write 1 for one-time flush.
-		 * Read a 0 after writing 1 to confirm flush has been executed.
-		 * Write to 0 after a write to 1 to complete the flush.
-		 */
-		rvu_write64(rvu, BLKADDR_APR, APR_AF_LMT_CTL, BIT_ULL(0));
-		rvu_read64(rvu, BLKADDR_APR, APR_AF_LMT_CTL);
-		rvu_write64(rvu, BLKADDR_APR, APR_AF_LMT_CTL, 0x00);
-	}
-
-	iounmap(lmt_map_base);
-	return 0;
-}
-
-#define LMT_MAP_TBL_W1_OFF  8
-static u32 rvu_get_lmtst_tbl_index(struct rvu *rvu, u16 pcifunc)
-{
-	return ((rvu_get_pf(pcifunc) * rvu->hw->total_vfs) +
-		(pcifunc & RVU_PFVF_FUNC_MASK)) * LMT_MAPTBL_ENTRY_SIZE;
-}
-
-static int rvu_get_lmtaddr(struct rvu *rvu, u16 pcifunc,
-			   u64 iova, u64 *lmt_addr)
-{
-	u64 pa, val, pf;
-	int err;
-
-	if (!iova) {
-		dev_err(rvu->dev, "%s Requested Null address for transulation\n", __func__);
-		return -EINVAL;
-	}
-
-	rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_ADDR_REQ, iova);
-	pf = rvu_get_pf(pcifunc) & 0x1F;
-	val = BIT_ULL(63) | BIT_ULL(14) | BIT_ULL(13) | pf << 8 |
-	      ((pcifunc & RVU_PFVF_FUNC_MASK) & 0xFF);
-	rvu_write64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_TXN_REQ, val);
-
-	err = rvu_poll_reg(rvu, BLKADDR_RVUM, RVU_AF_SMMU_ADDR_RSP_STS, BIT_ULL(0), false);
-	if (err) {
-		dev_err(rvu->dev, "%s LMTLINE iova transulation failed\n", __func__);
-		return err;
-	}
-	val = rvu_read64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_ADDR_RSP_STS);
-	if (val & ~0x1ULL) {
-		dev_err(rvu->dev, "%s LMTLINE iova transulation failed err:%llx\n", __func__, val);
-		return -EIO;
-	}
-	/* PA[51:12] = RVU_AF_SMMU_TLN_FLIT1[60:21]
-	 * PA[11:0] = IOVA[11:0]
-	 */
-	pa = rvu_read64(rvu, BLKADDR_RVUM, RVU_AF_SMMU_TLN_FLIT1) >> 21;
-	pa &= GENMASK_ULL(39, 0);
-	*lmt_addr = (pa << 12) | (iova  & 0xFFF);
-
-	return 0;
-}
-
-static int rvu_update_lmtaddr(struct rvu *rvu, u16 pcifunc, u64 lmt_addr)
-{
-	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);
-	u32 tbl_idx;
-	int err = 0;
-	u64 val;
-
-	/* Read the current lmt addr of pcifunc */
-	tbl_idx = rvu_get_lmtst_tbl_index(rvu, pcifunc);
-	err = lmtst_map_table_ops(rvu, tbl_idx, &val, LMT_TBL_OP_READ);
-	if (err) {
-		dev_err(rvu->dev,
-			"Failed to read LMT map table: index 0x%x err %d\n",
-			tbl_idx, err);
-		return err;
-	}
-
-	/* Storing the seondary's lmt base address as this needs to be
-	 * reverted in FLR. Also making sure this default value doesn't
-	 * get overwritten on multiple calls to this mailbox.
-	 */
-	if (!pfvf->lmt_base_addr)
-		pfvf->lmt_base_addr = val;
-
-	/* Update the LMT table with new addr */
-	err = lmtst_map_table_ops(rvu, tbl_idx, &lmt_addr, LMT_TBL_OP_WRITE);
-	if (err) {
-		dev_err(rvu->dev,
-			"Failed to update LMT map table: index 0x%x err %d\n",
-			tbl_idx, err);
-		return err;
-	}
-	return 0;
-}
-
-int rvu_mbox_handler_lmtst_tbl_setup(struct rvu *rvu,
-				     struct lmtst_tbl_setup_req *req,
-				     struct msg_rsp *rsp)
-{
-	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, req->hdr.pcifunc);
-	u32 pri_tbl_idx, tbl_idx;
-	u64 lmt_addr;
-	int err = 0;
-	u64 val;
-
-	/* Check if PF_FUNC wants to use it's own local memory as LMTLINE
-	 * region, if so, convert that IOVA to physical address and
-	 * populate LMT table with that address
-	 */
-	if (req->use_local_lmt_region) {
-		err = rvu_get_lmtaddr(rvu, req->hdr.pcifunc,
-				      req->lmt_iova, &lmt_addr);
-		if (err < 0)
-			return err;
-
-		/* Update the lmt addr for this PFFUNC in the LMT table */
-		err = rvu_update_lmtaddr(rvu, req->hdr.pcifunc, lmt_addr);
-		if (err)
-			return err;
-	}
-
-	/* Reconfiguring lmtst map table in lmt region shared mode i.e. make
-	 * multiple PF_FUNCs to share an LMTLINE region, so primary/base
-	 * pcifunc (which is passed as an argument to mailbox) is the one
-	 * whose lmt base address will be shared among other secondary
-	 * pcifunc (will be the one who is calling this mailbox).
-	 */
-	if (req->base_pcifunc) {
-		/* Calculating the LMT table index equivalent to primary
-		 * pcifunc.
-		 */
-		pri_tbl_idx = rvu_get_lmtst_tbl_index(rvu, req->base_pcifunc);
-
-		/* Read the base lmt addr of the primary pcifunc */
-		err = lmtst_map_table_ops(rvu, pri_tbl_idx, &val,
-					  LMT_TBL_OP_READ);
-		if (err) {
-			dev_err(rvu->dev,
-				"Failed to read LMT map table: index 0x%x err %d\n",
-				pri_tbl_idx, err);
-			goto error;
-		}
-
-		/* Update the base lmt addr of secondary with primary's base
-		 * lmt addr.
-		 */
-		err = rvu_update_lmtaddr(rvu, req->hdr.pcifunc, val);
-		if (err)
-			return err;
-	}
-
-	/* This mailbox can also be used to update word1 of APR_LMT_MAP_ENTRY_S
-	 * like enabling scheduled LMTST, disable LMTLINE prefetch, disable
-	 * early completion for ordered LMTST.
-	 */
-	if (req->sch_ena || req->dis_sched_early_comp || req->dis_line_pref) {
-		tbl_idx = rvu_get_lmtst_tbl_index(rvu, req->hdr.pcifunc);
-		err = lmtst_map_table_ops(rvu, tbl_idx + LMT_MAP_TBL_W1_OFF,
-					  &val, LMT_TBL_OP_READ);
-		if (err) {
-			dev_err(rvu->dev,
-				"Failed to read LMT map table: index 0x%x err %d\n",
-				tbl_idx + LMT_MAP_TBL_W1_OFF, err);
-			goto error;
-		}
-
-		/* Storing lmt map table entry word1 default value as this needs
-		 * to be reverted in FLR. Also making sure this default value
-		 * doesn't get overwritten on multiple calls to this mailbox.
-		 */
-		if (!pfvf->lmt_map_ent_w1)
-			pfvf->lmt_map_ent_w1 = val;
-
-		/* Disable early completion for Ordered LMTSTs. */
-		if (req->dis_sched_early_comp)
-			val |= (req->dis_sched_early_comp <<
-				APR_LMT_MAP_ENT_DIS_SCH_CMP_SHIFT);
-		/* Enable scheduled LMTST */
-		if (req->sch_ena)
-			val |= (req->sch_ena << APR_LMT_MAP_ENT_SCH_ENA_SHIFT) |
-				req->ssow_pf_func;
-		/* Disables LMTLINE prefetch before receiving store data. */
-		if (req->dis_line_pref)
-			val |= (req->dis_line_pref <<
-				APR_LMT_MAP_ENT_DIS_LINE_PREF_SHIFT);
-
-		err = lmtst_map_table_ops(rvu, tbl_idx + LMT_MAP_TBL_W1_OFF,
-					  &val, LMT_TBL_OP_WRITE);
-		if (err) {
-			dev_err(rvu->dev,
-				"Failed to update LMT map table: index 0x%x err %d\n",
-				tbl_idx + LMT_MAP_TBL_W1_OFF, err);
-			goto error;
-		}
-	}
-
-error:
-	return err;
-}
-
-/* Resetting the lmtst map table to original default values */
-void rvu_reset_lmt_map_tbl(struct rvu *rvu, u16 pcifunc)
-{
-	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);
-	u32 tbl_idx;
-	int err;
-
-	if (is_rvu_otx2(rvu))
-		return;
-
-	if (pfvf->lmt_base_addr || pfvf->lmt_map_ent_w1) {
-		/* This corresponds to lmt map table index */
-		tbl_idx = rvu_get_lmtst_tbl_index(rvu, pcifunc);
-		/* Reverting back original lmt base addr for respective
-		 * pcifunc.
-		 */
-		if (pfvf->lmt_base_addr) {
-			err = lmtst_map_table_ops(rvu, tbl_idx,
-						  &pfvf->lmt_base_addr,
-						  LMT_TBL_OP_WRITE);
-			if (err)
-				dev_err(rvu->dev,
-					"Failed to update LMT map table: index 0x%x err %d\n",
-					tbl_idx, err);
-			pfvf->lmt_base_addr = 0;
-		}
-		/* Reverting back to orginal word1 val of lmtst map table entry
-		 * which underwent changes.
-		 */
-		if (pfvf->lmt_map_ent_w1) {
-			err = lmtst_map_table_ops(rvu,
-						  tbl_idx + LMT_MAP_TBL_W1_OFF,
-						  &pfvf->lmt_map_ent_w1,
-						  LMT_TBL_OP_WRITE);
-			if (err)
-				dev_err(rvu->dev,
-					"Failed to update LMT map table: index 0x%x err %d\n",
-					tbl_idx + LMT_MAP_TBL_W1_OFF, err);
-			pfvf->lmt_map_ent_w1 = 0;
-		}
-	}
-}
-
 void rvu_sso_block_cn10k_init(struct rvu *rvu, int blkaddr)
 {
 	u64 reg;
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c
index 5b7c4b88ed03..968de68177cb 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c
@@ -4178,8 +4178,8 @@ static int nix_aq_init(struct rvu *rvu, struct rvu_block *block)
 
 static int rvu_nix_block_init(struct rvu *rvu, struct nix_hw *nix_hw)
 {
+	const struct npc_lt_def_cfg *ltdefs;
 	struct rvu_hwinfo *hw = rvu->hw;
-	struct npc_lt_def_cfg *ltdefs;
 	int blkaddr = nix_hw->blkaddr;
 	struct rvu_block *block;
 	int err;
@@ -4802,7 +4802,7 @@ bool rvu_nix_is_ptp_tx_enabled(struct rvu *rvu, u16 pcifunc)
 /* NIX ingress policers or bandwidth profiles APIs */
 static void nix_config_rx_pkt_policer_precolor(struct rvu *rvu, int blkaddr)
 {
-	struct npc_lt_def_cfg *ltdefs;
+	const struct npc_lt_def_cfg *ltdefs;
 
 	ltdefs = rvu->kpu.lt_def;
 
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_npc.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu_npc.c
index c2f457dd7964..3032cc2c89b1 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_npc.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_npc.c
@@ -1351,7 +1351,7 @@ static void npc_load_mkex_profile(struct rvu *rvu, int blkaddr,
 }
 
 static void npc_config_kpuaction(struct rvu *rvu, int blkaddr,
-				 struct npc_kpu_profile_action *kpuaction,
+				 const struct npc_kpu_profile_action *kpuaction,
 				 int kpu, int entry, bool pkind)
 {
 	struct npc_kpu_action0 action0 = {0};
@@ -1393,7 +1393,7 @@ static void npc_config_kpuaction(struct rvu *rvu, int blkaddr,
 }
 
 static void npc_config_kpucam(struct rvu *rvu, int blkaddr,
-			      struct npc_kpu_profile_cam *kpucam,
+			      const struct npc_kpu_profile_cam *kpucam,
 			      int kpu, int entry)
 {
 	struct npc_kpu_cam cam0 = {0};
@@ -1421,7 +1421,7 @@ static inline u64 enable_mask(int count)
 }
 
 static void npc_program_kpu_profile(struct rvu *rvu, int blkaddr, int kpu,
-				    struct npc_kpu_profile *profile)
+				    const struct npc_kpu_profile *profile)
 {
 	int entry, num_entries, max_entries;
 	u64 entry_mask;
@@ -1516,7 +1516,7 @@ static int npc_apply_custom_kpu(struct rvu *rvu,
 	if (NPC_KPU_VER_MIN(profile->version) <
 	    NPC_KPU_VER_MIN(NPC_KPU_PROFILE_VER)) {
 		dev_warn(rvu->dev,
-			 "Invalid KPU profile version: %d.%d.%d expected vesion <= %d.%d.%d\n",
+			 "Invalid KPU profile version: %d.%d.%d expected version <= %d.%d.%d\n",
 			 NPC_KPU_VER_MAJ(profile->version),
 			 NPC_KPU_VER_MIN(profile->version),
 			 NPC_KPU_VER_PATCH(profile->version),
@@ -1592,7 +1592,7 @@ static int npc_fwdb_detect_load_prfl_img(struct rvu *rvu, uint64_t prfl_sz,
 {
 	struct npc_coalesced_kpu_prfl *img_data = NULL;
 	int i = 0, rc = -EINVAL;
-	void *kpu_prfl_addr;
+	void __iomem *kpu_prfl_addr;
 	u16 offset;
 
 	img_data = (struct npc_coalesced_kpu_prfl __force *)rvu->kpu_prfl_addr;
@@ -1611,7 +1611,7 @@ static int npc_fwdb_detect_load_prfl_img(struct rvu *rvu, uint64_t prfl_sz,
 	while (i < img_data->num_prfl) {
 		/* Profile image offsets are rounded up to next 8 multiple.*/
 		offset = ALIGN_8B_CEIL(offset);
-		kpu_prfl_addr = (void *)((uintptr_t)rvu->kpu_prfl_addr +
+		kpu_prfl_addr = (void __iomem *)((uintptr_t)rvu->kpu_prfl_addr +
 					 offset);
 		rc = npc_load_kpu_prfl_img(rvu, kpu_prfl_addr,
 					   img_data->prfl_sz[i], kpu_profile);
-- 
2.31.1

