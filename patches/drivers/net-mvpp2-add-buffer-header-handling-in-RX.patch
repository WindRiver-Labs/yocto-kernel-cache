From cac4fe72fae6f5dc9be3067ae8df0ce12bd50a43 Mon Sep 17 00:00:00 2001
From: Stefan Chulski <stefanc@marvell.com>
Date: Wed, 26 May 2021 13:20:47 +0300
Subject: [PATCH 1606/1921] net: mvpp2: add buffer header handling in RX

If Link Partner sends frames larger than RX buffer size, MAC mark it
as oversize but still would pass it to the Packet Processor.
In this scenario, Packet Processor scatter frame between multiple buffers,
but only a single buffer would be returned to the Buffer Manager pool and
it would not refill the poll.

Patch add handling of oversize error with buffer header handling, so all
buffers would be returned to the Buffer Manager pool.

Change-Id: I83a991464beced95f24f99c6860391d3f8344b79
Signed-off-by: Stefan Chulski <stefanc@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/52701
Tested-by: sa_ip-sw-jenkins <sa_ip-sw-jenkins@marvell.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2/mvpp2.h    | 22 +++++++
 .../net/ethernet/marvell/mvpp2/mvpp2_main.c   | 61 ++++++++++++++-----
 2 files changed, 69 insertions(+), 14 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2.h b/drivers/net/ethernet/marvell/mvpp2/mvpp2.h
index 23e559218af1..849b5bb0d8b8 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2.h
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2.h
@@ -858,6 +858,14 @@ enum mvpp2_prs_l3_cast {
 #define MVPP2_PRS_TCAM_SRAM_SIZE        256
 #define MVPP2_N_FLOWS   52
 
+/* Buffer header info bits */
+#define MVPP2_B_HDR_INFO_MC_ID_MASK	0xfff
+#define MVPP2_B_HDR_INFO_MC_ID(info)	((info) & MVPP2_B_HDR_INFO_MC_ID_MASK)
+#define MVPP2_B_HDR_INFO_LAST_OFFS	12
+#define MVPP2_B_HDR_INFO_LAST_MASK	BIT(12)
+#define MVPP2_B_HDR_INFO_IS_LAST(info) \
+	   (((info) & MVPP2_B_HDR_INFO_LAST_MASK) >> MVPP2_B_HDR_INFO_LAST_OFFS)
+
 /* Definitions */
 
 /* Shared Packet Processor resources */
@@ -1123,6 +1131,20 @@ struct mvpp2_port {
 #define MVPP2_RXD_L3_IP6		BIT(30)
 #define MVPP2_RXD_BUF_HDR		BIT(31)
 
+struct mvpp2_buff_hdr {
+	__le32 next_dma_addr;
+	__le32 next_cookie_addr;
+	__le16 byte_count;
+	__le16 info;
+	__le16 reserved1;	/* bm_qset (for future use, BM) */
+	u8 next_dma_addr_high;
+	u8 next_cookie_addr_high;
+	__le16 reserved2;
+	__le16 reserved3;
+	__le16 reserved4;
+	__le16 reserved5;
+};
+
 /* HW TX descriptor for PPv2.1 */
 struct mvpp21_tx_desc {
 	__le32 command;		/* Options used by HW for packet transmitting.*/
diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
index d33dfbb9ae6e..59d935fd0ca3 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
@@ -4146,6 +4146,32 @@ struct sk_buff *mvpp2_build_skb(void *data, unsigned int frag_size,
 	return skb;
 }
 
+static void mvpp2_buff_hdr_pool_put(struct mvpp2_port *port, struct mvpp2_rx_desc *rx_desc,
+				    int pool, u32 rx_status)
+{
+	dma_addr_t dma_addr, dma_addr_next;
+	struct mvpp2_buff_hdr *buff_hdr;
+	phys_addr_t phys_addr;
+
+	dma_addr = mvpp2_rxdesc_dma_addr_get(port, rx_desc);
+	phys_addr = dma_to_phys(port->dev->dev.parent, dma_addr);
+
+	do {
+		buff_hdr = (struct mvpp2_buff_hdr *)phys_to_virt(phys_addr);
+
+		dma_addr_next = le32_to_cpu(buff_hdr->next_dma_addr);
+
+		if (port->priv->hw_version >= MVPP22)
+			dma_addr_next |= ((u64)buff_hdr->next_dma_addr_high << 32);
+
+		mvpp2_bm_pool_put(port, pool, dma_addr);
+
+		dma_addr = dma_addr_next;
+		phys_addr = dma_to_phys(port->dev->dev.parent, dma_addr);
+
+	} while (!MVPP2_B_HDR_INFO_IS_LAST(le16_to_cpu(buff_hdr->info)));
+}
+
 /* Main rx processing */
 static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 		    int rx_todo, struct mvpp2_rx_queue *rxq)
@@ -4194,20 +4220,6 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 			MVPP2_RXD_BM_POOL_ID_OFFS;
 		bm_pool = &port->priv->bm_pools[pool];
 
-		/* In case of an error, release the requested buffer pointer
-		 * to the Buffer Manager. This request process is controlled
-		 * by the hardware, and the information about the buffer is
-		 * comprised by the RX descriptor.
-		 */
-		if (rx_status & MVPP2_RXD_ERR_SUMMARY) {
-err_drop_frame:
-			dev->stats.rx_errors++;
-			mvpp2_rx_error(port, rx_desc);
-			/* Return the buffer to the pool */
-			mvpp2_bm_pool_put(port, pool, dma_addr);
-			continue;
-		}
-
 		if (bm_pool->frag_size > PAGE_SIZE)
 			frag_size = 0;
 		else
@@ -4224,6 +4236,27 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 		dma_unmap_single(dev->dev.parent, dma_addr,
 				 bm_pool->buf_size, DMA_FROM_DEVICE);
 
+		/* Buffer header not supported */
+		if (rx_status & MVPP2_RXD_BUF_HDR)
+			goto err_drop_frame;
+
+		/* In case of an error, release the requested buffer pointer
+		 * to the Buffer Manager. This request process is controlled
+		 * by the hardware, and the information about the buffer is
+		 * comprised by the RX descriptor.
+		 */
+		if (rx_status & MVPP2_RXD_ERR_SUMMARY) {
+err_drop_frame:
+			dev->stats.rx_errors++;
+			mvpp2_rx_error(port, rx_desc);
+			/* Return the buffer to the pool */
+			if (rx_status & MVPP2_RXD_BUF_HDR)
+				mvpp2_buff_hdr_pool_put(port, rx_desc, pool, rx_status);
+			else
+				mvpp2_bm_pool_put(port, pool, dma_addr);
+			continue;
+		}
+
 		prefetch(data + NET_SKB_PAD); /* packet header */
 
 		skb = mvpp2_build_skb(data, frag_size,
-- 
2.31.1

