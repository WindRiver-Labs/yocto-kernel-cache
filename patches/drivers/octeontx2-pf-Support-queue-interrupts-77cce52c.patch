From 32a3dd9a0d796bf106af9fe66d38b306efbc9fab Mon Sep 17 00:00:00 2001
From: Aleksey Makarov <amakarov@marvell.com>
Date: Fri, 12 Apr 2019 18:06:26 +0530
Subject: [PATCH 0117/1921] octeontx2-pf: Support queue interrupts

Queue interrupts communicate errors from completion, receive and send
queues to the driver.  This patch supports them.  In the case of error
it just reinitializes the driver.

There exist 64 queue interrupts.
All errors from completion, send and receive queues are reported to
the queue interrupt 0;

Change-Id: Iab0c14c49f21b74f4032509dd7bfcbac719873eb
Signed-off-by: Aleksey Makarov <amakarov@marvell.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 .../marvell/octeontx2/nic/otx2_common.c       | 22 ++++-
 .../ethernet/marvell/octeontx2/nic/otx2_pf.c  | 97 +++++++++++++++++++
 .../marvell/octeontx2/nic/otx2_struct.h       | 29 ++++++
 3 files changed, 147 insertions(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
index 077e30da871c..045f66b628d5 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
@@ -471,6 +471,8 @@ static int otx2_rq_init(struct otx2_nic *pfvf, u16 qidx, u16 lpb_aura)
 	aq->rq.lpb_sizem1 = (DMA_BUFFER_LEN / 8) - 1;
 	aq->rq.xqe_imm_size = 0; /* Copying of packet to CQE not needed */
 	aq->rq.flow_tagw = 32; /* Copy full 32bit flow_tag to CQE header */
+	aq->rq.rq_int_ena = NIX_RQINT_BITS;
+	aq->rq.qint_idx = 0;
 	aq->rq.lpb_drop_ena = 1; /* Enable RED dropping for AURA */
 	aq->rq.xqe_drop_ena = 1; /* Enable RED dropping for CQ/SSO */
 	/* Due to HW errata #34873 minimum 600 unused CQE need to maintaine to
@@ -551,6 +553,8 @@ static int otx2_sq_init(struct otx2_nic *pfvf, u16 qidx, u16 sqb_aura)
 	aq->sq.default_chan = pfvf->tx_chan_base;
 	aq->sq.sqe_stype = NIX_STYPE_STF; /* Cache SQB */
 	aq->sq.sqb_aura = sqb_aura;
+	aq->sq.sq_int_ena = NIX_SQINT_BITS;
+	aq->sq.qint_idx = 0;
 	/* Due pipelining impact minimum 2000 unused SQ CQE's
 	 * need to maintain to avoid CQ overflow.
 	 */
@@ -604,6 +608,8 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	aq->cq.cint_idx = (qidx < pfvf->hw.rx_queues) ? qidx
 				: (qidx - pfvf->hw.rx_queues);
 	cq->cint_idx = aq->cq.cint_idx;
+	aq->cq.cq_err_int_ena = NIX_CQERRINT_BITS;
+	aq->cq.qint_idx = 0;
 	aq->cq.avg_level = 255;
 
 	if (qidx < pfvf->hw.rx_queues) {
@@ -663,6 +669,8 @@ int otx2_config_nix_queues(struct otx2_nic *pfvf)
 int otx2_config_nix(struct otx2_nic *pfvf)
 {
 	struct nix_lf_alloc_req  *nixlf;
+	struct nix_lf_alloc_rsp *rsp;
+	int err;
 
 	pfvf->qset.xqe_size = NIX_XQESZ_W16 ? 128 : 512;
 
@@ -688,7 +696,19 @@ int otx2_config_nix(struct otx2_nic *pfvf)
 	 */
 	nixlf->rx_cfg = BIT_ULL(33) | BIT_ULL(35) | BIT_ULL(37);
 
-	return otx2_sync_mbox_msg(&pfvf->mbox);
+	err = otx2_sync_mbox_msg(&pfvf->mbox);
+	if (err)
+		return err;
+
+	rsp = (struct nix_lf_alloc_rsp *)otx2_mbox_get_rsp(&pfvf->mbox.mbox, 0,
+							   &nixlf->hdr);
+	if (IS_ERR(rsp))
+		return PTR_ERR(rsp);
+
+	if (rsp->qints < 1)
+		return -ENXIO;
+
+	return rsp->hdr.rc;
 }
 
 void otx2_free_aura_ptr(struct otx2_nic *pfvf, int type)
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
index 27647979ee11..5fae856fef56 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c
@@ -20,6 +20,7 @@
 #include "otx2_reg.h"
 #include "otx2_common.h"
 #include "otx2_txrx.h"
+#include "otx2_struct.h"
 
 #define DRV_NAME	"octeontx2-nicpf"
 #define DRV_STRING	"Marvell OcteonTX2 NIC Physical Function Driver"
@@ -405,6 +406,71 @@ int otx2_set_real_num_queues(struct net_device *netdev,
 	return err;
 }
 
+static irqreturn_t otx2_q_intr_handler(int irq, void *data)
+{
+	struct otx2_nic *pf = data;
+	atomic64_t *ptr;
+	u64 qidx = 0;
+	u64 val;
+
+	/* CQ */
+	for (qidx = 0; qidx < pf->qset.cq_cnt; qidx++) {
+		ptr = pf->reg_base + NIX_LF_CQ_OP_INT;
+		val = atomic64_fetch_add_relaxed((qidx << 44) |
+						 NIX_CQERRINT_BITS, ptr);
+
+		if (!(val & (NIX_CQERRINT_BITS | BIT_ULL(42))))
+			continue;
+
+		if (val & BIT_ULL(42)) {
+			dev_err(pf->dev, "CQ%lld: error reading NIX_LF_CQ_OP_INT\n",
+				qidx);
+		} else {
+			if (val & BIT_ULL(NIX_CQERRINT_DOOR_ERR))
+				dev_err(pf->dev, "CQ%lld: Doorbell error",
+					qidx);
+			if (val & BIT_ULL(NIX_CQERRINT_WR_FULL))
+				dev_err(pf->dev, "CQ%lld: Write full. A CQE to be added has been dropped because the CQ is full",
+					qidx);
+			if (val & BIT_ULL(NIX_CQERRINT_CQE_FAULT))
+				dev_err(pf->dev, "CQ%lld: Memory fault on CQE write to LLC/DRAM",
+					qidx);
+		}
+
+		schedule_work(&pf->reset_task);
+	}
+
+	/* SQ */
+	for (qidx = 0; qidx < pf->hw.tx_queues; qidx++) {
+		ptr = pf->reg_base + NIX_LF_SQ_OP_INT;
+		val = atomic64_fetch_add_relaxed((qidx << 44) | NIX_SQINT_BITS,
+						 ptr);
+		if (!(val & (NIX_SQINT_BITS | BIT_ULL(42))))
+			continue;
+
+		if (val & BIT_ULL(42)) {
+			dev_err(pf->dev, "SQ%lld: error reading NIX_LF_SQ_OP_INT\n",
+				qidx);
+		} else {
+			if (val & BIT_ULL(NIX_SQINT_LMT_ERR))
+				dev_err(pf->dev, "SQ%lld: LMT store error",
+					qidx);
+			if (val & BIT_ULL(NIX_SQINT_MNQ_ERR))
+				dev_err(pf->dev, "SQ%lld: Meta-descriptor enqueue error",
+					qidx);
+			if (val & BIT_ULL(NIX_SQINT_SEND_ERR))
+				dev_err(pf->dev, "SQ%lld: Send error", qidx);
+			if (val & BIT_ULL(NIX_SQINT_SQB_ALLOC_FAIL))
+				dev_err(pf->dev, "SQ%lld: SQB allocation failed",
+					qidx);
+		}
+
+		schedule_work(&pf->reset_task);
+	}
+
+	return IRQ_HANDLED;
+}
+
 static irqreturn_t otx2_cq_intr_handler(int irq, void *cq_irq)
 {
 	struct otx2_cq_poll *cq_poll = (struct otx2_cq_poll *)cq_irq;
@@ -618,6 +684,7 @@ int otx2_open(struct net_device *netdev)
 	/* CQ size of SQ */
 	qset->sqe_cnt = qset->sqe_cnt ? qset->sqe_cnt : Q_COUNT(Q_SIZE_4K);
 
+	err = -ENOMEM;
 	qset->cq = kcalloc(pf->qset.cq_cnt,
 			   sizeof(struct otx2_cq_queue), GFP_KERNEL);
 	if (!qset->cq)
@@ -673,6 +740,24 @@ int otx2_open(struct net_device *netdev)
 	if (err)
 		goto err_disable_napi;
 
+	/* Register Queue IRQ handlers */
+	vec = pf->hw.nix_msixoff + NIX_LF_QINT_VEC_START;
+	irq_name = &pf->hw.irq_name[vec * NAME_SIZE];
+
+	snprintf(irq_name, NAME_SIZE, "%s-qerr", pf->netdev->name);
+
+	err = request_irq(pci_irq_vector(pf->pdev, vec),
+			  otx2_q_intr_handler, 0, irq_name, pf);
+	if (err) {
+		dev_err(pf->dev,
+			"RVUPF%d: IRQ registration failed for QERR\n",
+			rvu_get_pf(pf->pcifunc));
+		goto err_disable_napi;
+	}
+
+	/* Enable QINT IRQ */
+	otx2_write64(pf, NIX_LF_QINTX_ENA_W1S(0), BIT_ULL(0));
+
 	/* Register CQ IRQ handlers */
 	vec = pf->hw.nix_msixoff + NIX_LF_CINT_VEC_START;
 	for (qidx = 0; qidx < pf->hw.cint_cnt; qidx++) {
@@ -716,6 +801,11 @@ int otx2_open(struct net_device *netdev)
 
 err_free_cints:
 	otx2_free_cints(pf, qidx);
+	vec = pci_irq_vector(pf->pdev,
+			     pf->hw.nix_msixoff + NIX_LF_QINT_VEC_START);
+	otx2_write64(pf, NIX_LF_QINTX_ENA_W1C(0), BIT_ULL(0));
+	synchronize_irq(vec);
+	free_irq(vec, pf);
 err_disable_napi:
 	otx2_disable_napi(pf);
 	otx2_free_hw_resources(pf);
@@ -746,6 +836,13 @@ int otx2_stop(struct net_device *netdev)
 	netif_carrier_off(netdev);
 	netif_tx_stop_all_queues(netdev);
 
+	/* Cleanup Queue IRQ */
+	vec = pci_irq_vector(pf->pdev,
+			     pf->hw.nix_msixoff + NIX_LF_QINT_VEC_START);
+	otx2_write64(pf, NIX_LF_QINTX_ENA_W1C(0), BIT_ULL(0));
+	synchronize_irq(vec);
+	free_irq(vec, pf);
+
 	/* Cleanup CQ NAPI and IRQ */
 	vec = pf->hw.nix_msixoff + NIX_LF_CINT_VEC_START;
 	for (qidx = 0; qidx < pf->hw.cint_cnt; qidx++) {
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_struct.h b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_struct.h
index 4870fb459ca5..7faf0338a20a 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_struct.h
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_struct.h
@@ -360,4 +360,33 @@ struct nix_sqe_sg_s {
 #endif
 };
 
+enum nix_cqerrint_e {
+	NIX_CQERRINT_DOOR_ERR = 0,
+	NIX_CQERRINT_WR_FULL = 1,
+	NIX_CQERRINT_CQE_FAULT = 2,
+};
+
+#define NIX_CQERRINT_BITS (BIT_ULL(NIX_CQERRINT_DOOR_ERR) | \
+			   BIT_ULL(NIX_CQERRINT_WR_FULL) | \
+			   BIT_ULL(NIX_CQERRINT_CQE_FAULT))
+
+enum nix_rqint_e {
+	NIX_RQINT_DROP = 0,
+	NIX_RQINT_RED = 1,
+};
+
+#define NIX_RQINT_BITS (BIT_ULL(NIX_RQINT_DROP) | BIT_ULL(NIX_RQINT_RED))
+
+enum nix_sqint_e {
+	NIX_SQINT_LMT_ERR = 0,
+	NIX_SQINT_MNQ_ERR = 1,
+	NIX_SQINT_SEND_ERR = 2,
+	NIX_SQINT_SQB_ALLOC_FAIL = 3,
+};
+
+#define NIX_SQINT_BITS (BIT_ULL(NIX_SQINT_LMT_ERR) | \
+			BIT_ULL(NIX_SQINT_MNQ_ERR) | \
+			BIT_ULL(NIX_SQINT_SEND_ERR) | \
+			BIT_ULL(NIX_SQINT_SQB_ALLOC_FAIL))
+
 #endif /* OTX2_STRUCT_H */
-- 
2.31.1

