From 5ea3bc6e702e32b454cdb2512d69a5bfcf00a38c Mon Sep 17 00:00:00 2001
From: Peter Swain <pswain@marvell.com>
Date: Thu, 16 May 2019 12:52:48 -0700
Subject: [PATCH 0415/1921] mmc: cavium: execute_tuning for octeontx2

At higher speeds, the sampling point for valid
capture on command or data lines becomes more
critical. The board, chip, device, temperature,
voltage changes can all cause variations in the
sampling delay. This needs to be evaluated
dynamically by invoking the tuning function.

For tuning the data in, host sends ext_csd command
to get 512 bytes of ext_csd data for tap delays
ranging from 0 to 63. For certain delay value,
the EXT_CSD command would be successful. Adding
more delay, in the iteration, would start causing
failure. The patch picks up the first success and
then the first failure after the first success. It
then takes an average of that. The average is to be
programmed in the MIO_EMM_TIMING.DATA_IN.

For cmd in tuning, host sends a SEND_STATUS command
and gets a response. Host checks for CRC error in
response and terms them success or failure for
varying values of tap from 0 to 63. Similar to the
above mechanism, an average is take to program
MIO_EMM_TIMING.CMD_IN.

Repaired execute_tuning, added card_reset
These changes make it functional on cn96xx:

- cope with NULL mmc->card when get_ext_csd called early
  as part of slot init, and still try to give a sensible
  result for the invoking ops->execute_tuning(mmc, opcode)

- similar small first-run problems

- add ops->hw_reset to allow card, not necessarily all the
  cards on a common slot and/or vmmc, to be reset.
  This should speed up multi-card bringups when HS200/HS400
  ops->execute_tuning gets called, as when tuning to a new
  mmc/sd while accessing another mmc/sd.
  Earlier code was observed to thrash.
  This should reliably clear any hung mmc/core ops where
  tuning ops have spun the knobs too far, invoking CRCs

- advertise mmc_can_retune(mmc) during slot_probe

Expanded  two tap-adjusters to four,
which excute in order, sanitizing CMD_IN/OUT comms
before moving on to DATA_IN/OUT.
(DATA_IN tuning best done with CMD19/14/21, and the opcode
passthrough should help the mmc/core smarts best tune that)

For each of these 4 dimensions, sweep all the possible
tap values, find the longest run of correct response,
and save the midpoint as current tuning.
Uses the mmc/core-supplied CMD19/CMD14/CMD21 machine
for tuning DATA[0-7], and for CMD path SEND_STATUS
and READ_EXT_CSD.

When not in DDR modes (like HS400) skip DATA_OUT_TAP

This will get called again on mmc_switch actions, like
escalation along HS->HS200->HS400 path, speed/power
budget changes, or CRC errors.

Default tuning: every cmd/data in/out tap count 32 (max/2)
runs at 100MHz HS400 (ddr, 8bit) with no CRC errs observed.
On cn96-A0 this is the max supported clock

Forcing CRC errors (by hand-modifying MIO_EMM_TIMING to
provoke failure) causes re-tuning to happen, command retry
to happen in the mmc/core logic, and transfers complete

Change footprint (apart from what's mentioned above)

- cvm_mmc_set_timing() is a common func for all the places
  MIO_EMM_TIMING is changed

- slot->tuned flag is set when tuning has been run for a
  particular bus config, else defaults are imposed until
  tuning run can refine them

- emmc_io_drive_setup() & cvm_mmc_configure_delay() should
  be run when a slot-switch or power/timing/width switch
  is done.  Previously this could be missed

- host->current_req (used by irq code to find request/slot)
  was not being NULLed on multi-sector DMA failure, so CRC
  errors during read/write ops caused exception when _next_
  command (perhaps a tuning command in response to the fail)
  was handled

- cvm_mmc_send_status() expanded to pass back status as
    cvm_mmc_r1_cmd(mmc, &status, opcode)
  to be examined by tuning code. Now takes an opcode arg
  (if zero, SEND_STATUS is done).

- internal functions for sending status & ext_csd for tuning
  do not yet reliably teardown on timeout, so data structures
  are static to avoid out-of-scope refs from irq context when
  the previously on-stack objects are explored. Still ugly,
  Will study mmc/core shootdown code more, and clean this up.
  Or could just leave them static.

Change-Id: I36c8368406ee41da0cd227957515971340b60a21
Signed-off-by: Peter Swain <pswain@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/8548
Tested-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
Reviewed-by: Chandrakala Chavva <cchavva@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/26910
Reviewed-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 drivers/mmc/host/cavium.c | 314 +++++++++++++++++++++++++++++++++++++-
 drivers/mmc/host/cavium.h |   3 +-
 2 files changed, 309 insertions(+), 8 deletions(-)

diff --git a/drivers/mmc/host/cavium.c b/drivers/mmc/host/cavium.c
index f0c49bab8909..91d263de6531 100644
--- a/drivers/mmc/host/cavium.c
+++ b/drivers/mmc/host/cavium.c
@@ -362,7 +362,7 @@ static void emmc_io_drive_setup(struct cvm_mmc_slot *slot)
 	struct cvm_mmc_host *host = slot->host;
 
 	/* Setup drive and slew only for 9x */
-	if (is_mmc_otx2(slot->host)) {
+	if (is_mmc_otx2(host)) {
 		if ((slot->drive < 0) || (slot->slew < 0))
 			return;
 		/* Setup the emmc interface current drive
@@ -644,10 +644,13 @@ irqreturn_t cvm_mmc_interrupt(int irq, void *dev_id)
 
 	/* follow CMD6 timing/width with IMMEDIATE switch */
 	if (slot && slot->cmd6_pending) {
-		if (host_done && !req->cmd->error)
+		if (host_done && !req->cmd->error) {
 			do_switch(host, slot->want_switch);
-		else if (slot)
+			emmc_io_drive_setup(slot);
+			cvm_mmc_configure_delay(slot);
+		} else if (slot) {
 			slot->cmd6_pending = false;
+		}
 	}
 
 	host->current_req = NULL;
@@ -811,6 +814,9 @@ static void cvm_mmc_dma_request(struct mmc_host *mmc,
 	struct mmc_data *data;
 	u64 emm_dma, addr, int_enable_mask = 0;
 
+	/* cleared by successful termination */
+	mrq->cmd->error = -EINVAL;
+
 	if (!mrq->data || !mrq->data->sg || !mrq->data->sg_len ||
 	    !mrq->stop || mrq->stop->opcode != MMC_STOP_TRANSMISSION) {
 		dev_err(&mmc->card->dev,
@@ -821,14 +827,12 @@ static void cvm_mmc_dma_request(struct mmc_host *mmc,
 	cvm_mmc_switch_to(slot);
 
 	data = mrq->data;
+
 	pr_debug("DMA request  blocks: %d  block_size: %d  total_size: %d\n",
 		 data->blocks, data->blksz, data->blocks * data->blksz);
 	if (data->timeout_ns)
 		set_wdog(slot, data->timeout_ns);
 
-	WARN_ON(host->current_req);
-	host->current_req = mrq;
-
 	emm_dma = prepare_ext_dma(mmc, mrq);
 	addr = prepare_dma(host, data);
 	if (!addr) {
@@ -836,7 +840,11 @@ static void cvm_mmc_dma_request(struct mmc_host *mmc,
 		goto error;
 	}
 
+	mrq->host = mmc;
 	host->dma_active = true;
+	WARN_ON(host->current_req);
+	host->current_req = mrq;
+
 	int_enable_mask = MIO_EMM_INT_CMD_ERR | MIO_EMM_INT_DMA_DONE |
 			MIO_EMM_INT_DMA_ERR;
 
@@ -985,8 +993,8 @@ static void cvm_mmc_request(struct mmc_host *mmc, struct mmc_request *mrq)
 	mods = cvm_mmc_get_cr_mods(cmd);
 
 	WARN_ON(host->current_req);
-	host->current_req = mrq;
 	mrq->host = mmc;
+	host->current_req = mrq;
 
 	if (cmd->data) {
 		if (cmd->data->flags & MMC_DATA_READ)
@@ -1032,7 +1040,203 @@ static void cvm_mmc_request(struct mmc_host *mmc, struct mmc_request *mrq)
 	writeq(emm_cmd, host->base + MIO_EMM_CMD(host));
 	if (cmd->opcode == MMC_SWITCH)
 		udelay(1300);
+}
+
+static void cvm_mmc_wait_done(struct mmc_request *cvm_mrq)
+{
+	complete(&cvm_mrq->completion);
+}
+
+static int cvm_mmc_r1_cmd(struct mmc_host *mmc, u32 *statp, u32 opcode)
+{
+	static struct mmc_command cmd = {};
+	static struct mmc_request cvm_mrq = {};
+
+	if (!opcode)
+		opcode = MMC_SEND_STATUS;
+	cmd.opcode = opcode;
+	if (mmc->card)
+		cmd.arg = mmc->card->rca << 16;
+	else
+		cmd.arg = 1 << 16;
+	cmd.flags = MMC_RSP_SPI_R2 | MMC_RSP_R1 | MMC_CMD_AC;
+	cmd.data = NULL;
+	cvm_mrq.cmd = &cmd;
+
+	init_completion(&cvm_mrq.completion);
+	cvm_mrq.done = cvm_mmc_wait_done;
+
+	cvm_mmc_request(mmc, &cvm_mrq);
+	if (!wait_for_completion_timeout(&cvm_mrq.completion,
+			msecs_to_jiffies(10))) {
+		mmc_abort_tuning(mmc, opcode);
+		return -ETIMEDOUT;
+	}
+
+	if (statp)
+		*statp = cmd.resp[0];
+
+	return cvm_mrq.cmd->error;
+}
+
+static int cvm_mmc_data_tuning(struct mmc_host *mmc, u32 *statp, u32 opcode)
+{
+	int err = 0;
+	u8 *ext_csd;
+	static struct mmc_command cmd = {};
+	static struct mmc_data data = {};
+	static struct mmc_request cvm_mrq = {};
+	static struct scatterlist sg;
+	struct cvm_mmc_slot *slot = mmc_priv(mmc);
+	struct mmc_card *card = mmc->card;
+
+	if (!(slot->cached_switch & MIO_EMM_SWITCH_HS400_TIMING)) {
+		struct cvm_mmc_host *host = slot->host;
+		int edetail = -EINVAL;
+		int core_opinion;
+
+		host->release_bus(host);
+		core_opinion =
+			mmc_send_tuning(mmc, opcode, &edetail);
+		host->acquire_bus(host);
+
+		/* only accept mmc/core opinion  when it's happy */
+		if (!core_opinion)
+			return core_opinion;
+	}
+
+	/* EXT_CSD supported only after ver 3 */
+	if (card && card->csd.mmca_vsn <= CSD_SPEC_VER_3)
+		return -EOPNOTSUPP;
+	/*
+	 * As the ext_csd is so large and mostly unused, we don't store the
+	 * raw block in mmc_card.
+	 */
+	ext_csd = kzalloc(BLKSZ_EXT_CSD, GFP_KERNEL);
+	if (!ext_csd)
+		return -ENOMEM;
+
+	cvm_mrq.cmd = &cmd;
+	cvm_mrq.data = &data;
+	cmd.data = &data;
+
+	cmd.opcode = MMC_SEND_EXT_CSD;
+	cmd.arg = 0;
+	cmd.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_ADTC;
+
+	data.blksz = BLKSZ_EXT_CSD;
+	data.blocks = 1;
+	data.flags = MMC_DATA_READ;
+	data.sg = &sg;
+	data.sg_len = 1;
 
+	sg_init_one(&sg, ext_csd, BLKSZ_EXT_CSD);
+
+	/* set timeout */
+	if (card) {
+		/* SD cards use a 100 multiplier rather than 10 */
+		u32 mult = mmc_card_sd(card) ? 100 : 10;
+
+		data.timeout_ns = card->csd.taac_ns * mult;
+		data.timeout_clks = card->csd.taac_clks * mult;
+	} else {
+		data.timeout_ns = 50 * NSEC_PER_MSEC;
+	}
+
+	init_completion(&cvm_mrq.completion);
+	cvm_mrq.done = cvm_mmc_wait_done;
+
+	cvm_mmc_request(mmc, &cvm_mrq);
+	if (!wait_for_completion_timeout(&cvm_mrq.completion,
+			msecs_to_jiffies(100))) {
+		mmc_abort_tuning(mmc, cmd.opcode);
+		err = -ETIMEDOUT;
+	}
+
+	data.sg_len = 0; /* FIXME: catch over-time completions? */
+	kfree(ext_csd);
+
+	if (err)
+		return err;
+
+	if (statp)
+		*statp = cvm_mrq.cmd->resp[0];
+
+	return cvm_mrq.cmd->error;
+}
+
+/* adjusters for the 4 otx2 delay line taps */
+struct adj {
+	const char *name;
+	u64 mask;
+	int (*test)(struct mmc_host *mmc, u32 *statp, u32 opcode);
+	u32 opcode;
+	bool ddr_only;
+};
+
+static int adjust_tuning(struct mmc_host *mmc, struct adj *adj, u32 opcode)
+{
+	int err, start_run = -1, best_run = 0, best_start = -1;
+	bool prev_ok = false;
+	u64 timing, tap;
+	struct cvm_mmc_slot *slot = mmc_priv(mmc);
+	struct cvm_mmc_host *host = slot->host;
+	char how[MAX_NO_OF_TAPS+1] = "";
+
+	/* loop over range+1 to simplify processing */
+	for (tap = 0; tap <= MAX_NO_OF_TAPS; tap++, prev_ok = !err) {
+		if (tap < MAX_NO_OF_TAPS) {
+			timing = readq(host->base + MIO_EMM_TIMING(host));
+			timing &= ~adj->mask;
+			timing |= (tap << __bf_shf(adj->mask));
+			writeq(timing, host->base + MIO_EMM_TIMING(host));
+
+			err = adj->test(mmc, NULL, opcode);
+
+			how[tap] = "-+"[!err];
+		} else {
+			/*
+			 * putting the end+1 case in loop simplifies
+			 * logic, allowing 'prev_ok' to process a
+			 * sweet spot in tuning which extends to wall.
+			 */
+			err = -EINVAL;
+		}
+
+		if (!err) {
+			/*
+			 * If no CRC/etc errors in response, but previous
+			 * failed, note the start of a new run
+			 */
+			if (!prev_ok)
+				start_run = tap;
+		} else if (prev_ok) {
+			int run = tap - 1 - start_run;
+
+			/* did we just exit a wider sweet spot? */
+			if (start_run >= 0 && run > best_run) {
+				best_start = start_run;
+				best_run = run;
+			}
+		}
+	}
+
+	if (best_start < 0) {
+		dev_warn(host->dev, "%s tuning %s failed\n",
+			mmc_hostname(mmc), adj->name);
+		return -EINVAL;
+	}
+
+	tap = best_start + best_run / 2;
+	how[tap] = '@';
+	dev_dbg(host->dev, "%s/%s %d/%lld/%d %s\n",
+		mmc_hostname(mmc), adj->name,
+		best_start, tap, best_start + best_run,
+		how);
+	slot->taps &= ~adj->mask;
+	slot->taps |= (tap << __bf_shf(adj->mask));
+	cvm_mmc_set_timing(slot);
+	return 0;
 }
 
 static u32 max_supported_frequency(struct cvm_mmc_host *host)
@@ -1053,6 +1257,7 @@ static u32 max_supported_frequency(struct cvm_mmc_host *host)
 
 static void cvm_mmc_set_ios(struct mmc_host *mmc, struct mmc_ios *ios)
 {
+
 	struct cvm_mmc_slot *slot = mmc_priv(mmc);
 	struct cvm_mmc_host *host = slot->host;
 	int clk_period = 0, power_class = 10, bus_width = 0;
@@ -1173,11 +1378,105 @@ static void cvm_mmc_set_ios(struct mmc_host *mmc, struct mmc_ios *ios)
 	host->release_bus(host);
 }
 
+static struct adj adj[] = {
+	{ "CMD_IN", MIO_EMM_TIMING_CMD_IN,
+		cvm_mmc_r1_cmd, MMC_SEND_STATUS, },
+	{ "DATA_IN", MIO_EMM_TIMING_DATA_IN,
+		cvm_mmc_data_tuning, },
+	{ NULL, },
+};
+
+static int cvm_scan_tuning(struct mmc_host *mmc, u32 opcode)
+{
+	struct cvm_mmc_slot *slot = mmc_priv(mmc);
+	struct adj *a;
+	int ret;
+
+	for (a = adj; a->name; a++) {
+		if (a->ddr_only && !cvm_is_mmc_timing_ddr(slot))
+			continue;
+
+		ret = adjust_tuning(mmc, a,
+			a->opcode ?: opcode);
+
+		if (ret)
+			return ret;
+	}
+
+	cvm_mmc_set_timing(slot);
+	return 0;
+}
+
+static int cvm_execute_tuning(struct mmc_host *mmc, u32 opcode)
+{
+	struct cvm_mmc_slot *slot = mmc_priv(mmc);
+	struct cvm_mmc_host *host = slot->host;
+	int clk_period, hz;
+
+	int ret;
+
+	do {
+		u64 emm_switch =
+			readq(host->base + MIO_EMM_MODE(host, slot->bus_id));
+
+		clk_period = FIELD_GET(MIO_EMM_SWITCH_CLK_LO, emm_switch);
+		dev_info(slot->host->dev, "%s re-tuning\n",
+			mmc_hostname(mmc));
+		ret = cvm_scan_tuning(mmc, opcode);
+		if (ret) {
+			int inc = clk_period >> 3;
+
+			if (!inc)
+				inc++;
+			clk_period += inc;
+			hz = host->sys_freq / (2 * clk_period);
+			pr_debug("clk_period %d += %d, now %d Hz\n",
+				clk_period - inc, inc, hz);
+
+			if (hz < 400000)
+				break;
+
+			slot->clock = hz;
+			mmc->ios.clock = hz;
+
+			emm_switch &= ~MIO_EMM_SWITCH_CLK_LO;
+			emm_switch |= FIELD_PREP(MIO_EMM_SWITCH_CLK_LO,
+						clk_period);
+			emm_switch &= ~MIO_EMM_SWITCH_CLK_HI;
+			emm_switch |= FIELD_PREP(MIO_EMM_SWITCH_CLK_HI,
+						clk_period);
+			do_switch(host, emm_switch);
+		}
+	} while (ret);
+
+	return ret;
+}
+
+static void cvm_mmc_reset(struct mmc_host *mmc)
+{
+	struct cvm_mmc_slot *slot = mmc_priv(mmc);
+	struct cvm_mmc_host *host = slot->host;
+	u64 r;
+
+	cvm_mmc_reset_bus(slot);
+
+	r = FIELD_PREP(MIO_EMM_CMD_VAL, 1) |
+		FIELD_PREP(MIO_EMM_CMD_BUS_ID, slot->bus_id);
+
+	writeq(r, host->base + MIO_EMM_CMD(host));
+
+	do {
+		r = readq(host->base + MIO_EMM_RSP_STS(host));
+	} while (!(r & MIO_EMM_RSP_STS_CMD_DONE));
+}
+
 static const struct mmc_host_ops cvm_mmc_ops = {
 	.request        = cvm_mmc_request,
 	.set_ios        = cvm_mmc_set_ios,
 	.get_ro		= mmc_gpio_get_ro,
 	.get_cd		= mmc_gpio_get_cd,
+	.hw_reset	= cvm_mmc_reset,
+	.execute_tuning = cvm_execute_tuning,
 };
 
 static void cvm_mmc_set_clock(struct cvm_mmc_slot *slot, unsigned int clock)
@@ -1360,6 +1659,7 @@ int cvm_mmc_of_slot_probe(struct device *dev, struct cvm_mmc_host *host)
 	mmc->max_blk_size = 512;
 	/* DMA block count field is 15 bits */
 	mmc->max_blk_count = 32767;
+	mmc_can_retune(mmc);
 
 	slot->clock = mmc->f_min;
 	slot->bus_id = id;
diff --git a/drivers/mmc/host/cavium.h b/drivers/mmc/host/cavium.h
index b20b9fca1595..60c42b1f6ee4 100644
--- a/drivers/mmc/host/cavium.h
+++ b/drivers/mmc/host/cavium.h
@@ -22,6 +22,7 @@
 #include <linux/pci.h>
 
 #define CAVIUM_MAX_MMC		4
+#define BLKSZ_EXT_CSD		512
 #define MRVL_OCTEONTX2_96XX_PARTNUM 0xB2
 
 /* Subsystem Device ID */
@@ -51,7 +52,7 @@
 #define PS_5000			(5000)
 #define PS_2500			(2500)
 #define PS_400			(400)
-#define MAX_NO_OF_TAPS		(63)
+#define MAX_NO_OF_TAPS		64
 
 
 /* DMA register addresses */
-- 
2.31.1

