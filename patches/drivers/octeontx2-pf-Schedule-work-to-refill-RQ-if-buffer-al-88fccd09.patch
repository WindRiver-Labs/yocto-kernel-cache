From d5bfbac0a0c3903003817834af5443a2c21da312 Mon Sep 17 00:00:00 2001
From: Geetha sowjanya <gakula@marvell.com>
Date: Fri, 21 Jun 2019 11:42:54 +0530
Subject: [PATCH 0211/1921] octeontx2-pf: Schedule work to refill RQ if buffer
 alloc fails in atomic context.

Receive buffer allocation in NAPI may fail to get free pages when system
is low on free memory. In such scenarios schedule a delayed work to alloc
Receive buffers in non-atomic context.

Change-Id: I5bfcd5f0a5e7e812fad9c7d4feceb41c9ddce1f8
Signed-off-by: Geetha sowjanya <gakula@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/11405
Reviewed-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
Tested-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
[WK: The original patch got from Marvell sdk11.21.09]
Signed-off-by: Wenlin Kang <wenlin.kang@windriver.com>
---
 .../marvell/octeontx2/nic/otx2_common.c       | 52 +++++++++++++++++++
 .../marvell/octeontx2/nic/otx2_common.h       |  6 +++
 .../marvell/octeontx2/nic/otx2_txrx.c         | 34 ++++++++----
 .../marvell/octeontx2/nic/otx2_txrx.h         |  2 +
 4 files changed, 85 insertions(+), 9 deletions(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
index a2816b98323a..ff6a7782b68f 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.c
@@ -747,6 +747,7 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	cq->rbpool = &qset->pool[pool_id];
 
 	cq->cq_idx = qidx;
+	cq->refill_task_sched = false;
 
 	/* Get memory to put this msg */
 	aq = otx2_mbox_alloc_msg_nix_aq_enq(&pfvf->mbox);
@@ -786,6 +787,45 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	return otx2_sync_mbox_msg(&pfvf->mbox);
 }
 
+static void otx2_pool_refill_task(struct work_struct *work)
+{
+	struct otx2_cq_queue *cq;
+	struct otx2_pool *rbpool;
+	struct refill_work *wrk;
+	int qidx, free_ptrs = 0;
+	struct otx2_nic *pfvf;
+	s64 bufptr;
+
+	wrk = container_of(work, struct refill_work, pool_refill_work.work);
+	pfvf = wrk->pf;
+	qidx = wrk - pfvf->refill_wrk;
+	cq = &pfvf->qset.cq[qidx];
+	rbpool = cq->rbpool;
+	free_ptrs = cq->pool_ptrs;
+
+	while (cq->pool_ptrs) {
+		bufptr = otx2_alloc_rbuf(pfvf, rbpool, GFP_KERNEL);
+		if (bufptr <= 0) {
+			/* Schedule a WQ if we fails to free atleast half of the
+			 * pointers else enable napi for this RQ.
+			 */
+			if (!((free_ptrs - cq->pool_ptrs) > free_ptrs / 2)) {
+				struct delayed_work *dwork;
+
+				dwork = &wrk->pool_refill_work;
+				schedule_delayed_work(dwork,
+						      msecs_to_jiffies(100));
+			} else {
+				cq->refill_task_sched = false;
+			}
+			return;
+		}
+		otx2_aura_freeptr(pfvf, qidx, bufptr + OTX2_HEAD_ROOM);
+		cq->pool_ptrs--;
+	}
+	cq->refill_task_sched = false;
+}
+
 int otx2_config_nix_queues(struct otx2_nic *pfvf)
 {
 	int qidx, err;
@@ -815,6 +855,18 @@ int otx2_config_nix_queues(struct otx2_nic *pfvf)
 			return err;
 	}
 
+	/* Initialize work queue for receive buffer refill */
+
+	pfvf->refill_wrk = devm_kcalloc(pfvf->dev, pfvf->qset.cq_cnt,
+					sizeof(struct refill_work), GFP_KERNEL);
+	if (!pfvf->refill_wrk)
+		return -ENOMEM;
+
+	for (qidx = 0; qidx < pfvf->qset.cq_cnt; qidx++) {
+		pfvf->refill_wrk[qidx].pf = pfvf;
+		INIT_DELAYED_WORK(&pfvf->refill_wrk[qidx].pool_refill_work,
+				  otx2_pool_refill_task);
+	}
 	return 0;
 }
 
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
index a4f0decd4907..171510b55595 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h
@@ -158,6 +158,11 @@ struct flr_work {
 	struct otx2_nic *pf;
 };
 
+struct refill_work {
+	struct delayed_work pool_refill_work;
+	struct otx2_nic *pf;
+};
+
 struct otx2_nic {
 	void __iomem		*reg_base;
 	struct pci_dev		*pdev;
@@ -195,6 +200,7 @@ struct otx2_nic {
 	struct list_head	flows;
 	struct workqueue_struct	*flr_wq;
 	struct flr_work		*flr_wrk;
+	struct refill_work	*refill_wrk;
 
 	u8			hw_rx_tstamp;
 	u8			hw_tx_tstamp;
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
index a210dd392a72..62f1e3167bfa 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
@@ -222,8 +222,7 @@ static inline void otx2_set_rxtstamp(struct otx2_nic *pfvf, struct sk_buff *skb)
 }
 
 static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
-				 struct otx2_cq_queue *cq, void *cqe,
-				 int *pool_ptrs)
+				 struct otx2_cq_queue *cq, void *cqe)
 {
 	struct nix_cqe_hdr_s *cqe_hdr = (struct nix_cqe_hdr_s *)cqe;
 	struct otx2_qset *qset = &pfvf->qset;
@@ -295,7 +294,7 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 				otx2_skb_add_frag(pfvf, skb, *iova, len);
 			}
 			iova++;
-			(*pool_ptrs)++;
+			cq->pool_ptrs++;
 		}
 
 		/* When SEGS = 1, only one IOVA is followed by NIX_RX_SG_S.
@@ -344,9 +343,9 @@ static void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,
 int otx2_napi_handler(struct otx2_cq_queue *cq,
 		      struct otx2_nic *pfvf, int budget)
 {
-	int tx_pkts = 0, tx_bytes = 0, pool_ptrs = 0;
 	struct otx2_pool *rbpool = cq->rbpool;
 	int processed_cqe = 0, workdone = 0;
+	int tx_pkts = 0, tx_bytes = 0;
 	struct nix_cqe_hdr_s *cqe_hdr;
 	struct netdev_queue *txq;
 	u64 cq_status;
@@ -398,7 +397,7 @@ int otx2_napi_handler(struct otx2_cq_queue *cq,
 		switch (cqe_hdr->cqe_type) {
 		case NIX_XQE_TYPE_RX:
 			/* Receive packet handler*/
-			otx2_rcv_pkt_handler(pfvf, cq, cqe_hdr, &pool_ptrs);
+			otx2_rcv_pkt_handler(pfvf, cq, cqe_hdr);
 			workdone++;
 			break;
 		case NIX_XQE_TYPE_SEND:
@@ -416,16 +415,28 @@ int otx2_napi_handler(struct otx2_cq_queue *cq,
 		netdev_tx_completed_queue(txq, tx_pkts, tx_bytes);
 	}
 
-	if (!pool_ptrs)
+	if (!cq->pool_ptrs)
 		return 0;
 
 	/* Refill pool with new buffers */
-	while (pool_ptrs) {
+	while (cq->pool_ptrs) {
 		bufptr = otx2_alloc_rbuf(pfvf, rbpool, GFP_ATOMIC);
-		if (bufptr <= 0)
+		if (bufptr <= 0) {
+			struct refill_work *work;
+			struct delayed_work *dwork;
+
+			work = &pfvf->refill_wrk[cq->cq_idx];
+			dwork = &work->pool_refill_work;
+			/* Schedule a task if no other task is running */
+			if (!cq->refill_task_sched) {
+				cq->refill_task_sched = true;
+				schedule_delayed_work(dwork,
+						      msecs_to_jiffies(100));
+			}
 			break;
+		}
 		otx2_aura_freeptr(pfvf, cq->cq_idx, bufptr + OTX2_HEAD_ROOM);
-		pool_ptrs--;
+		cq->pool_ptrs--;
 	}
 	otx2_get_page(rbpool);
 
@@ -452,6 +463,11 @@ int otx2_poll(struct napi_struct *napi, int budget)
 		cq = &qset->cq[cq_idx];
 		qcount = otx2_read64(pfvf, NIX_LF_CINTX_CNT(cq_poll->cint_idx));
 		qcount = (qcount >> 32) & 0xFFFF;
+		/* If the RQ refill WQ task is running, skip napi
+		 * scheduler for this queue.
+		 */
+		if (cq->refill_task_sched)
+			continue;
 		workdone += otx2_napi_handler(cq, pfvf, budget);
 		if (workdone && qcount == 1)
 			break;
diff --git a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h
index 411250a9456c..d6ca35951278 100644
--- a/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h
+++ b/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.h
@@ -97,7 +97,9 @@ struct otx2_pool {
 struct otx2_cq_queue {
 	u8			cq_idx;
 	u8			cint_idx; /* CQ interrupt id */
+	u8			refill_task_sched;
 	u16			cqe_size;
+	u16			pool_ptrs;
 	u32			cqe_cnt;
 	u32			cq_head;
 	u32			cq_tail;
-- 
2.31.1

