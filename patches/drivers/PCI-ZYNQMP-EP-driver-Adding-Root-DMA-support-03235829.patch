From 7395609735b951f2c089a044bb94b70a3bf63673 Mon Sep 17 00:00:00 2001
From: Ravi Shankar Jonnalagadda <venkata.ravi.jonnalagadda@xilinx.com>
Date: Fri, 3 Apr 2020 13:39:43 +0530
Subject: [PATCH 1312/1852] PCI: ZYNQMP EP driver: Adding Root DMA support

commit 01de0658653e6291dc83df4a166d7cd0c8dc6e7b from
https://github.com/Xilinx/linux-xlnx.git

PS PCIe DMA can be configured to be both End Point or Root DMA
This patch adds support for Root DMA.

Signed-off-by: Ravi Shankar Jonnalagadda <venkata.ravi.jonnalagadda@xilinx.com>
Signed-off-by: Michal Simek <michal.simek@xilinx.com>
Signed-off-by: Bharat Kumar Gogada <bharat.kumar.gogada@xilinx.com>
State: pending
Signed-off-by: Yaliang Wang <Yaliang.Wang@windriver.com>
---
 drivers/dma/xilinx/xilinx_ps_pcie.h          |   1 +
 drivers/dma/xilinx/xilinx_ps_pcie_main.c     |  37 +-
 drivers/dma/xilinx/xilinx_ps_pcie_platform.c | 792 +++++++++++++------
 3 files changed, 591 insertions(+), 239 deletions(-)

diff --git a/drivers/dma/xilinx/xilinx_ps_pcie.h b/drivers/dma/xilinx/xilinx_ps_pcie.h
index 8fbfd09407aa..81d634d15447 100644
--- a/drivers/dma/xilinx/xilinx_ps_pcie.h
+++ b/drivers/dma/xilinx/xilinx_ps_pcie.h
@@ -21,6 +21,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/mempool.h>
+#include <linux/of.h>
 #include <linux/pci.h>
 #include <linux/property.h>
 #include <linux/platform_device.h>
diff --git a/drivers/dma/xilinx/xilinx_ps_pcie_main.c b/drivers/dma/xilinx/xilinx_ps_pcie_main.c
index e8cc681bb0ef..cb3151219083 100644
--- a/drivers/dma/xilinx/xilinx_ps_pcie_main.c
+++ b/drivers/dma/xilinx/xilinx_ps_pcie_main.c
@@ -35,17 +35,20 @@ static u32 channel_properties_axi_pcie[] = {
 	(u32)(CHANNEL_POLL_TIMER_FREQUENCY) };
 
 static struct property_entry generic_pcie_ep_property[] = {
-	PROPERTY_ENTRY_U32("xlnx,numchannels", (u32)MAX_NUMBER_OF_CHANNELS),
-	PROPERTY_ENTRY_U32_ARRAY("channel0", channel_properties_pcie_axi),
-	PROPERTY_ENTRY_U32_ARRAY("channel1", channel_properties_axi_pcie),
-	PROPERTY_ENTRY_U32_ARRAY("channel2", channel_properties_pcie_axi),
-	PROPERTY_ENTRY_U32_ARRAY("channel3", channel_properties_axi_pcie),
+	PROPERTY_ENTRY_U32("numchannels", (u32)MAX_NUMBER_OF_CHANNELS),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel0",
+				 channel_properties_pcie_axi),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel1",
+				 channel_properties_axi_pcie),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel2",
+				 channel_properties_pcie_axi),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel3",
+				 channel_properties_axi_pcie),
 	{ },
 };
 
 static const struct platform_device_info xlnx_std_platform_dev_info = {
 	.name           = XLNX_PLATFORM_DRIVER_NAME,
-	.id             = -1,
 	.properties     = generic_pcie_ep_property,
 };
 
@@ -63,8 +66,6 @@ static int ps_pcie_dma_probe(struct pci_dev *pdev,
 	struct platform_device *platform_dev;
 	struct platform_device_info platform_dev_info;
 
-	static int pcie_board_num;
-
 	dev_info(&pdev->dev, "PS PCIe DMA Driver probe\n");
 
 	err = pcim_enable_device(pdev);
@@ -95,10 +96,20 @@ static int ps_pcie_dma_probe(struct pci_dev *pdev,
 
 	pci_set_master(pdev);
 
+	/* For Root DMA platform device will be created through device tree */
+	if (pdev->vendor == PCI_VENDOR_ID_XILINX &&
+	    pdev->device == ZYNQMP_RC_DMA_DEVID)
+		return 0;
+
 	memcpy(&platform_dev_info, &xlnx_std_platform_dev_info,
 	       sizeof(xlnx_std_platform_dev_info));
 
-	platform_dev_info.id = pcie_board_num;
+	/* Do device specific channel configuration changes to
+	 * platform_dev_info.properties if required
+	 * More information on channel properties can be found
+	 * at Documentation/devicetree/bindings/dma/xilinx/ps-pcie-dma.txt
+	 */
+
 	platform_dev_info.parent = &pdev->dev;
 	platform_dev_info.data = &pdev;
 	platform_dev_info.size_data = sizeof(struct pci_dev **);
@@ -112,8 +123,6 @@ static int ps_pcie_dma_probe(struct pci_dev *pdev,
 
 	pci_set_drvdata(pdev, platform_dev);
 
-	pcie_board_num++;
-
 	dev_info(&pdev->dev, "PS PCIe DMA driver successfully probed\n");
 
 	return 0;
@@ -121,6 +130,7 @@ static int ps_pcie_dma_probe(struct pci_dev *pdev,
 
 static struct pci_device_id ps_pcie_dma_tbl[] = {
 	{ PCI_DEVICE(PCI_VENDOR_ID_XILINX, ZYNQMP_DMA_DEVID) },
+	{ PCI_DEVICE(PCI_VENDOR_ID_XILINX, ZYNQMP_RC_DMA_DEVID) },
 	{ }
 };
 
@@ -165,9 +175,8 @@ static void ps_pcie_dma_remove(struct pci_dev *pdev)
 
 	platform_dev = (struct platform_device *)pci_get_drvdata(pdev);
 
-	WARN_ON(!platform_dev);
-
-	platform_device_unregister(platform_dev);
+	if (platform_dev)
+		platform_device_unregister(platform_dev);
 }
 
 /**
diff --git a/drivers/dma/xilinx/xilinx_ps_pcie_platform.c b/drivers/dma/xilinx/xilinx_ps_pcie_platform.c
index 985a5e1dd05e..17ad2cbfdeec 100644
--- a/drivers/dma/xilinx/xilinx_ps_pcie_platform.c
+++ b/drivers/dma/xilinx/xilinx_ps_pcie_platform.c
@@ -22,7 +22,7 @@
 
 #define MIN_SW_INTR_TRANSACTIONS       2
 
-#define CHANNEL_PROPERTY_LENGTH 20
+#define CHANNEL_PROPERTY_LENGTH 50
 #define WORKQ_NAME_SIZE		100
 #define INTR_HANDLR_NAME_SIZE   100
 
@@ -104,6 +104,8 @@
 #define AXI_ATTRIBUTE       0x3
 #define PCI_ATTRIBUTE       0x2
 
+#define ROOTDMA_Q_READ_ATTRIBUTE 0x8
+
 /*
  * User Id programmed into Source Q will be copied into Status Q of Destination
  */
@@ -193,16 +195,19 @@ enum PACKET_CONTEXT_AVAILABILITY {
 };
 
 struct ps_pcie_transfer_elements {
-	struct scatterlist *src_sgl;
-	unsigned int srcq_num_elemets;
-	struct scatterlist *dst_sgl;
-	unsigned int dstq_num_elemets;
+	struct list_head node;
+	dma_addr_t src_pa;
+	dma_addr_t dst_pa;
+	u32 transfer_bytes;
 };
 
 struct  ps_pcie_tx_segment {
 	struct list_head node;
 	struct dma_async_tx_descriptor async_tx;
-	struct ps_pcie_transfer_elements tx_elements;
+	struct list_head transfer_nodes;
+	u32 src_elements;
+	u32 dst_elements;
+	u32 total_transfer_bytes;
 };
 
 struct ps_pcie_intr_segment {
@@ -222,9 +227,7 @@ struct PACKET_TRANSFER_PARAMS {
 	enum PACKET_CONTEXT_AVAILABILITY availability_status;
 	u16 idx_sop;
 	u16 idx_eop;
-	struct scatterlist *sgl;
 	struct ps_pcie_tx_segment *seg;
-	u32 requested_bytes;
 };
 
 enum CHANNEL_STATE {
@@ -285,6 +288,10 @@ enum dev_channel_properties {
  * @pdst_sta_bd: Virtual address of Dst Status Q buffer Descriptors
  * @dst_staprobe_idx: Holds index of Status Q to be examined for updates
  * @dst_sta_hw_probe_idx: Holds index of max limit of Dst Status Q for hardware
+ * @@read_attribute: Describes the attributes of buffer in srcq
+ * @@write_attribute: Describes the attributes of buffer in dstq
+ * @@intr_status_offset: Register offset to be cheked on receiving interrupt
+ * @@intr_status_offset: Register offset to be used to control interrupts
  * @ppkt_ctx_srcq: Virtual address of packet context to Src Q updates
  * @idx_ctx_srcq_head: Holds index of packet context to be filled for Source Q
  * @idx_ctx_srcq_tail: Holds index of packet context to be examined for Source Q
@@ -374,6 +381,12 @@ struct ps_pcie_dma_chan {
 	u32 dst_staprobe_idx;
 	u32 dst_sta_hw_probe_idx;
 
+	u32 read_attribute;
+	u32 write_attribute;
+
+	u32 intr_status_offset;
+	u32 intr_control_offset;
+
 	struct PACKET_TRANSFER_PARAMS *ppkt_ctx_srcq;
 	u16 idx_ctx_srcq_head;
 	u16 idx_ctx_srcq_tail;
@@ -393,6 +406,7 @@ struct ps_pcie_dma_chan {
 	struct list_head active_interrupts_list;
 
 	mempool_t *transactions_pool;
+	mempool_t *tx_elements_pool;
 	mempool_t *intr_transactions_pool;
 
 	struct workqueue_struct *sw_intrs_wrkq;
@@ -423,6 +437,7 @@ struct ps_pcie_dma_chan {
 
 /*
  * struct xlnx_pcie_dma_device - Driver specific platform device structure
+ * @is_rootdma: Indicates whether the dma instance is root port dma
  * @dma_buf_ext_addr: Indicates whether target system is 32 bit or 64 bit
  * @bar_mask: Indicates available pcie bars
  * @board_number: Count value of platform device
@@ -434,8 +449,12 @@ struct ps_pcie_dma_chan {
  * @irq_vecs: Number of irq vectors allocated to pci device
  * @pci_dev: Parent pci device which created this platform device
  * @bar_info: PCIe bar related information
+ * @platform_irq_vec: Platform irq vector number for root dma
+ * @rootdma_vendor: PCI Vendor id for root dma
+ * @rootdma_device: PCI Device id for root dma
  */
 struct xlnx_pcie_dma_device {
+	bool is_rootdma;
 	bool dma_buf_ext_addr;
 	u32 bar_mask;
 	u16 board_number;
@@ -447,6 +466,9 @@ struct xlnx_pcie_dma_device {
 	void __iomem *reg_base;
 	struct pci_dev *pci_dev;
 	struct BAR_PARAMS bar_info[MAX_BARS];
+	int platform_irq_vec;
+	u16 rootdma_vendor;
+	u16 rootdma_device;
 };
 
 #define to_xilinx_chan(chan) \
@@ -465,6 +487,7 @@ static void ps_pcie_dma_clr_mask(struct ps_pcie_dma_chan *chan, u32 reg,
 static void ps_pcie_dma_set_mask(struct ps_pcie_dma_chan *chan, u32 reg,
 				 u32 mask);
 static int irq_setup(struct xlnx_pcie_dma_device *xdev);
+static int platform_irq_setup(struct xlnx_pcie_dma_device *xdev);
 static int chan_intr_setup(struct xlnx_pcie_dma_device *xdev);
 static int device_intr_setup(struct xlnx_pcie_dma_device *xdev);
 static int irq_probe(struct xlnx_pcie_dma_device *xdev);
@@ -473,8 +496,10 @@ static irqreturn_t ps_pcie_dma_dev_intr_handler(int irq, void *data);
 static irqreturn_t ps_pcie_dma_chan_intr_handler(int irq, void *data);
 static int init_hw_components(struct ps_pcie_dma_chan *chan);
 static int init_sw_components(struct ps_pcie_dma_chan *chan);
+static void update_channel_read_attribute(struct ps_pcie_dma_chan *chan);
+static void update_channel_write_attribute(struct ps_pcie_dma_chan *chan);
 static void ps_pcie_chan_reset(struct ps_pcie_dma_chan *chan);
-static void poll_completed_transactions(unsigned long arg);
+static void poll_completed_transactions(struct timer_list *t);
 static bool check_descriptors_for_two_queues(struct ps_pcie_dma_chan *chan,
 					     struct ps_pcie_tx_segment *seg);
 static bool check_descriptors_for_all_queues(struct ps_pcie_dma_chan *chan,
@@ -515,10 +540,10 @@ static void xlnx_ps_pcie_dma_free_chan_resources(struct dma_chan *dchan);
 static int xlnx_ps_pcie_dma_alloc_chan_resources(struct dma_chan *dchan);
 static dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx);
 static dma_cookie_t xilinx_intr_tx_submit(struct dma_async_tx_descriptor *tx);
-static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_dma_sg(
-		struct dma_chan *channel, struct scatterlist *dst_sg,
-		unsigned int dst_nents, struct scatterlist *src_sg,
-		unsigned int src_nents, unsigned long flags);
+static struct dma_async_tx_descriptor *
+xlnx_ps_pcie_dma_prep_memcpy(struct dma_chan *channel, dma_addr_t dma_dst,
+			     dma_addr_t dma_src, size_t len,
+			     unsigned long flags);
 static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
 		struct dma_chan *channel, struct scatterlist *sgl,
 		unsigned int sg_len, enum dma_transfer_direction direction,
@@ -527,6 +552,10 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_interrupt(
 		struct dma_chan *channel, unsigned long flags);
 static void xlnx_ps_pcie_dma_issue_pending(struct dma_chan *channel);
 static int xlnx_ps_pcie_dma_terminate_all(struct dma_chan *channel);
+static int read_rootdma_config(struct platform_device *platform_dev,
+			       struct xlnx_pcie_dma_device *xdev);
+static int read_epdma_config(struct platform_device *platform_dev,
+			     struct xlnx_pcie_dma_device *xdev);
 static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev);
 static int xlnx_pcie_dma_driver_remove(struct platform_device *platform_dev);
 
@@ -684,6 +713,22 @@ static int irq_setup(struct xlnx_pcie_dma_device *xdev)
 	return err;
 }
 
+static int platform_irq_setup(struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+
+	err = devm_request_irq(xdev->dev,
+			       xdev->platform_irq_vec,
+			       ps_pcie_dma_dev_intr_handler,
+			       IRQF_SHARED,
+			       "PS PCIe Root DMA Handler", xdev);
+	if (err)
+		dev_err(xdev->dev, "Couldn't request irq %d\n",
+			xdev->platform_irq_vec);
+
+	return err;
+}
+
 /**
  * irq_probe - Checks which interrupt types can be serviced by hardware
  *
@@ -718,7 +763,7 @@ static int ps_pcie_check_intr_status(struct ps_pcie_dma_chan *chan)
 	if (chan->state != CHANNEL_AVAILABLE)
 		return err;
 
-	status = chan->chan_base->pcie_intr_status;
+	status = ps_pcie_dma_read(chan, chan->intr_status_offset);
 
 	if (status & DMA_INTSTATUS_SGLINTR_BIT) {
 		if (chan->primary_desc_cleanup) {
@@ -726,9 +771,8 @@ static int ps_pcie_check_intr_status(struct ps_pcie_dma_chan *chan)
 				   &chan->handle_primary_desc_cleanup);
 		}
 		/* Clearing Persistent bit */
-		chan->chan_base->pcie_intr_status =
-			chan->chan_base->pcie_intr_status |
-			DMA_INTSTATUS_SGLINTR_BIT;
+		ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+				     DMA_INTSTATUS_SGLINTR_BIT);
 		err = 0;
 	}
 
@@ -736,9 +780,8 @@ static int ps_pcie_check_intr_status(struct ps_pcie_dma_chan *chan)
 		if (chan->sw_intrs_wrkq)
 			queue_work(chan->sw_intrs_wrkq, &chan->handle_sw_intrs);
 		/* Clearing Persistent bit */
-		chan->chan_base->pcie_intr_status =
-			chan->chan_base->pcie_intr_status |
-			DMA_INTSTATUS_SWINTR_BIT;
+		ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+				     DMA_INTSTATUS_SWINTR_BIT);
 		err = 0;
 	}
 
@@ -771,9 +814,8 @@ static int ps_pcie_check_intr_status(struct ps_pcie_dma_chan *chan)
 			chan->chan_base->stad_q_size,
 			chan->chan_base->stad_q_next);
 		/* Clearing Persistent bit */
-		chan->chan_base->pcie_intr_status =
-			chan->chan_base->pcie_intr_status |
-			DMA_INTSTATUS_DMAERR_BIT;
+		ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+				     DMA_INTSTATUS_DMAERR_BIT);
 
 		handle_error(chan);
 
@@ -792,7 +834,13 @@ static int init_hw_components(struct ps_pcie_dma_chan *chan)
 			upper_32_bits(chan->src_sgl_bd_pa);
 		chan->chan_base->src_q_size = chan->total_descriptors;
 		chan->chan_base->src_q_limit = 0;
-		chan->chan_base->src_q_low =
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->src_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						     | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->src_q_low = 0;
+		}
+		chan->chan_base->src_q_low |=
 			(lower_32_bits((chan->src_sgl_bd_pa))
 			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
 			| DMA_QPTRLO_Q_ENABLE_BIT;
@@ -802,7 +850,13 @@ static int init_hw_components(struct ps_pcie_dma_chan *chan)
 			upper_32_bits(chan->src_sta_bd_pa);
 		chan->chan_base->stas_q_size = chan->total_descriptors;
 		chan->chan_base->stas_q_limit = chan->total_descriptors - 1;
-		chan->chan_base->stas_q_low =
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->stas_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						      | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->stas_q_low = 0;
+		}
+		chan->chan_base->stas_q_low |=
 			(lower_32_bits(chan->src_sta_bd_pa)
 			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
 			| DMA_QPTRLO_Q_ENABLE_BIT;
@@ -815,7 +869,13 @@ static int init_hw_components(struct ps_pcie_dma_chan *chan)
 			upper_32_bits(chan->dst_sgl_bd_pa);
 		chan->chan_base->dst_q_size = chan->total_descriptors;
 		chan->chan_base->dst_q_limit = 0;
-		chan->chan_base->dst_q_low =
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->dst_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						     | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->dst_q_low = 0;
+		}
+		chan->chan_base->dst_q_low |=
 			(lower_32_bits(chan->dst_sgl_bd_pa)
 			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
 			| DMA_QPTRLO_Q_ENABLE_BIT;
@@ -825,7 +885,13 @@ static int init_hw_components(struct ps_pcie_dma_chan *chan)
 			upper_32_bits(chan->dst_sta_bd_pa);
 		chan->chan_base->stad_q_size = chan->total_descriptors;
 		chan->chan_base->stad_q_limit = chan->total_descriptors - 1;
-		chan->chan_base->stad_q_low =
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->stad_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						      | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->stad_q_low = 0;
+		}
+		chan->chan_base->stad_q_low |=
 			(lower_32_bits(chan->dst_sta_bd_pa)
 			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
 			| DMA_QPTRLO_Q_ENABLE_BIT;
@@ -834,10 +900,63 @@ static int init_hw_components(struct ps_pcie_dma_chan *chan)
 	return 0;
 }
 
+static void update_channel_read_attribute(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->xdev->is_rootdma) {
+		/* For Root DMA, Host Memory and Buffer Descriptors
+		 * will be on AXI side
+		 */
+		if (chan->srcq_buffer_location == BUFFER_LOC_PCI) {
+			chan->read_attribute = (AXI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		} else if (chan->srcq_buffer_location == BUFFER_LOC_AXI) {
+			chan->read_attribute = AXI_ATTRIBUTE <<
+					       SRC_CTL_ATTRIB_BIT_SHIFT;
+		}
+	} else {
+		if (chan->srcq_buffer_location == BUFFER_LOC_PCI) {
+			chan->read_attribute = PCI_ATTRIBUTE <<
+					       SRC_CTL_ATTRIB_BIT_SHIFT;
+		} else if (chan->srcq_buffer_location == BUFFER_LOC_AXI) {
+			chan->read_attribute = (AXI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		}
+	}
+}
+
+static void update_channel_write_attribute(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->xdev->is_rootdma) {
+		/* For Root DMA, Host Memory and Buffer Descriptors
+		 * will be on AXI side
+		 */
+		if (chan->dstq_buffer_location == BUFFER_LOC_PCI) {
+			chan->write_attribute = (AXI_ATTRIBUTE <<
+						 SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		} else if (chan->srcq_buffer_location == BUFFER_LOC_AXI) {
+			chan->write_attribute = AXI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT;
+		}
+	} else {
+		if (chan->dstq_buffer_location == BUFFER_LOC_PCI) {
+			chan->write_attribute = PCI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT;
+		} else if (chan->dstq_buffer_location == BUFFER_LOC_AXI) {
+			chan->write_attribute = (AXI_ATTRIBUTE <<
+						 SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		}
+	}
+	chan->write_attribute |= SOURCE_CONTROL_BACK_TO_BACK_PACK_BIT;
+}
+
 static int init_sw_components(struct ps_pcie_dma_chan *chan)
 {
-	if ((chan->ppkt_ctx_srcq) && (chan->psrc_sgl_bd) &&
-	    (chan->psrc_sta_bd)) {
+	if (chan->ppkt_ctx_srcq && chan->psrc_sgl_bd &&
+	    chan->psrc_sta_bd) {
 		memset(chan->ppkt_ctx_srcq, 0,
 		       sizeof(struct PACKET_TRANSFER_PARAMS)
 		       * chan->total_descriptors);
@@ -859,8 +978,8 @@ static int init_sw_components(struct ps_pcie_dma_chan *chan)
 		chan->idx_ctx_srcq_tail = 0;
 	}
 
-	if ((chan->ppkt_ctx_dstq) && (chan->pdst_sgl_bd) &&
-	    (chan->pdst_sta_bd)) {
+	if (chan->ppkt_ctx_dstq && chan->pdst_sgl_bd &&
+	    chan->pdst_sta_bd) {
 		memset(chan->ppkt_ctx_dstq, 0,
 		       sizeof(struct PACKET_TRANSFER_PARAMS)
 		       * chan->total_descriptors);
@@ -905,12 +1024,12 @@ static void ps_pcie_chan_reset(struct ps_pcie_dma_chan *chan)
 /**
  * poll_completed_transactions - Function invoked by poll timer
  *
- * @arg: Pointer to PS PCIe DMA channel information
+ * @t: Pointer to timer triggering this callback
  * Return: void
  */
-static void poll_completed_transactions(unsigned long arg)
+static void poll_completed_transactions(struct timer_list *t)
 {
-	struct ps_pcie_dma_chan *chan = (struct ps_pcie_dma_chan *)arg;
+	struct ps_pcie_dma_chan *chan = from_timer(chan, t, poll_timer);
 
 	if (chan->state == CHANNEL_AVAILABLE) {
 		queue_work(chan->primary_desc_cleanup,
@@ -923,14 +1042,14 @@ static void poll_completed_transactions(unsigned long arg)
 static bool check_descriptors_for_two_queues(struct ps_pcie_dma_chan *chan,
 					     struct ps_pcie_tx_segment *seg)
 {
-	if (seg->tx_elements.src_sgl) {
+	if (seg->src_elements) {
 		if (chan->src_avail_descriptors >=
-		    seg->tx_elements.srcq_num_elemets) {
+		    seg->src_elements) {
 			return true;
 		}
-	} else if (seg->tx_elements.dst_sgl) {
+	} else if (seg->dst_elements) {
 		if (chan->dst_avail_descriptors >=
-		    seg->tx_elements.dstq_num_elemets) {
+		    seg->dst_elements) {
 			return true;
 		}
 	}
@@ -941,10 +1060,10 @@ static bool check_descriptors_for_two_queues(struct ps_pcie_dma_chan *chan,
 static bool check_descriptors_for_all_queues(struct ps_pcie_dma_chan *chan,
 					     struct ps_pcie_tx_segment *seg)
 {
-	if ((chan->src_avail_descriptors >=
-		seg->tx_elements.srcq_num_elemets) &&
-	    (chan->dst_avail_descriptors >=
-		seg->tx_elements.dstq_num_elemets)) {
+	if (chan->src_avail_descriptors >=
+		seg->src_elements &&
+	    chan->dst_avail_descriptors >=
+		seg->dst_elements) {
 		return true;
 	}
 
@@ -978,9 +1097,8 @@ static void xlnx_ps_pcie_update_srcq(struct ps_pcie_dma_chan *chan,
 {
 	struct SOURCE_DMA_DESCRIPTOR *pdesc;
 	struct PACKET_TRANSFER_PARAMS *pkt_ctx = NULL;
-	struct scatterlist *sgl_ptr;
-	unsigned int i;
-	u32 read_attribute = 0;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	u32 i = 0;
 
 	pkt_ctx = chan->ppkt_ctx_srcq + chan->idx_ctx_srcq_head;
 	if (pkt_ctx->availability_status == IN_USE) {
@@ -992,44 +1110,39 @@ static void xlnx_ps_pcie_update_srcq(struct ps_pcie_dma_chan *chan,
 	}
 
 	pkt_ctx->availability_status = IN_USE;
-	pkt_ctx->sgl = seg->tx_elements.src_sgl;
 
-	if (chan->srcq_buffer_location == BUFFER_LOC_PCI) {
+	if (chan->srcq_buffer_location == BUFFER_LOC_PCI)
 		pkt_ctx->seg = seg;
-		read_attribute = PCI_ATTRIBUTE << SRC_CTL_ATTRIB_BIT_SHIFT;
-	} else
-		read_attribute = (AXI_ATTRIBUTE << SRC_CTL_ATTRIB_BIT_SHIFT) |
-				 SOURCE_CONTROL_BD_LOC_AXI;
 
 	/*  Get the address of the next available DMA Descriptor */
 	pdesc = chan->psrc_sgl_bd + chan->src_sgl_freeidx;
 	pkt_ctx->idx_sop = chan->src_sgl_freeidx;
 
 	/* Build transactions using information in the scatter gather list */
-	for_each_sg(seg->tx_elements.src_sgl, sgl_ptr,
-		    seg->tx_elements.srcq_num_elemets, i) {
+	list_for_each_entry(ele, &seg->transfer_nodes, node) {
 		if (chan->xdev->dma_buf_ext_addr) {
 			pdesc->system_address =
-				(u64)sg_dma_address(sgl_ptr);
+				(u64)ele->src_pa;
 		} else {
 			pdesc->system_address =
-				(u32)sg_dma_address(sgl_ptr);
+				(u32)ele->src_pa;
 		}
 
-		pdesc->control_byte_count = (sg_dma_len(sgl_ptr) &
+		pdesc->control_byte_count = (ele->transfer_bytes &
 					    SOURCE_CONTROL_BD_BYTE_COUNT_MASK) |
-					    read_attribute;
-		if (pkt_ctx->seg)
-			pkt_ctx->requested_bytes += sg_dma_len(sgl_ptr);
+					    chan->read_attribute;
 
 		pdesc->user_handle = chan->idx_ctx_srcq_head;
 		pdesc->user_id = DEFAULT_UID;
 		/* Check if this is last descriptor */
-		if (i == (seg->tx_elements.srcq_num_elemets - 1)) {
+		if (i == (seg->src_elements - 1)) {
 			pkt_ctx->idx_eop = chan->src_sgl_freeidx;
-			pdesc->control_byte_count = pdesc->control_byte_count |
-						SOURCE_CONTROL_BD_EOP_BIT |
-						SOURCE_CONTROL_BD_INTR_BIT;
+			pdesc->control_byte_count |= SOURCE_CONTROL_BD_EOP_BIT;
+			if ((seg->async_tx.flags & DMA_PREP_INTERRUPT) ==
+						   DMA_PREP_INTERRUPT) {
+				pdesc->control_byte_count |=
+					SOURCE_CONTROL_BD_INTR_BIT;
+			}
 		}
 		chan->src_sgl_freeidx++;
 		if (chan->src_sgl_freeidx == chan->total_descriptors)
@@ -1038,6 +1151,7 @@ static void xlnx_ps_pcie_update_srcq(struct ps_pcie_dma_chan *chan,
 		spin_lock(&chan->src_desc_lock);
 		chan->src_avail_descriptors--;
 		spin_unlock(&chan->src_desc_lock);
+		i++;
 	}
 
 	chan->chan_base->src_q_limit = chan->src_sgl_freeidx;
@@ -1051,9 +1165,8 @@ static void xlnx_ps_pcie_update_dstq(struct ps_pcie_dma_chan *chan,
 {
 	struct DEST_DMA_DESCRIPTOR *pdesc;
 	struct PACKET_TRANSFER_PARAMS *pkt_ctx = NULL;
-	struct scatterlist *sgl_ptr;
-	unsigned int i;
-	u32 write_attribute = 0;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	u32 i = 0;
 
 	pkt_ctx = chan->ppkt_ctx_dstq + chan->idx_ctx_dstq_head;
 	if (pkt_ctx->availability_status == IN_USE) {
@@ -1066,41 +1179,29 @@ static void xlnx_ps_pcie_update_dstq(struct ps_pcie_dma_chan *chan,
 	}
 
 	pkt_ctx->availability_status = IN_USE;
-	pkt_ctx->sgl = seg->tx_elements.dst_sgl;
 
-	if (chan->dstq_buffer_location == BUFFER_LOC_PCI) {
+	if (chan->dstq_buffer_location == BUFFER_LOC_PCI)
 		pkt_ctx->seg = seg;
-		write_attribute = (PCI_ATTRIBUTE << SRC_CTL_ATTRIB_BIT_SHIFT) |
-					SOURCE_CONTROL_BACK_TO_BACK_PACK_BIT;
-	} else
-		write_attribute = (AXI_ATTRIBUTE << SRC_CTL_ATTRIB_BIT_SHIFT) |
-					SOURCE_CONTROL_BD_LOC_AXI |
-					SOURCE_CONTROL_BACK_TO_BACK_PACK_BIT;
 
 	pdesc = chan->pdst_sgl_bd + chan->dst_sgl_freeidx;
 	pkt_ctx->idx_sop = chan->dst_sgl_freeidx;
 
 	/* Build transactions using information in the scatter gather list */
-	for_each_sg(seg->tx_elements.dst_sgl, sgl_ptr,
-		    seg->tx_elements.dstq_num_elemets, i) {
+	list_for_each_entry(ele, &seg->transfer_nodes, node) {
 		if (chan->xdev->dma_buf_ext_addr) {
 			pdesc->system_address =
-				(u64)sg_dma_address(sgl_ptr);
+				(u64)ele->dst_pa;
 		} else {
 			pdesc->system_address =
-				(u32)sg_dma_address(sgl_ptr);
+				(u32)ele->dst_pa;
 		}
-
-		pdesc->control_byte_count = (sg_dma_len(sgl_ptr) &
+		pdesc->control_byte_count = (ele->transfer_bytes &
 					SOURCE_CONTROL_BD_BYTE_COUNT_MASK) |
-						write_attribute;
-
-		if (pkt_ctx->seg)
-			pkt_ctx->requested_bytes += sg_dma_len(sgl_ptr);
+						chan->write_attribute;
 
 		pdesc->user_handle = chan->idx_ctx_dstq_head;
 		/* Check if this is last descriptor */
-		if (i == (seg->tx_elements.dstq_num_elemets - 1))
+		if (i == (seg->dst_elements - 1))
 			pkt_ctx->idx_eop = chan->dst_sgl_freeidx;
 		chan->dst_sgl_freeidx++;
 		if (chan->dst_sgl_freeidx == chan->total_descriptors)
@@ -1109,6 +1210,7 @@ static void xlnx_ps_pcie_update_dstq(struct ps_pcie_dma_chan *chan,
 		spin_lock(&chan->dst_desc_lock);
 		chan->dst_avail_descriptors--;
 		spin_unlock(&chan->dst_desc_lock);
+		i++;
 	}
 
 	chan->chan_base->dst_q_limit = chan->dst_sgl_freeidx;
@@ -1141,10 +1243,10 @@ static void ps_pcie_chan_program_work(struct work_struct *work)
 		list_del(&seg->node);
 		spin_unlock(&chan->active_list_lock);
 
-		if (seg->tx_elements.src_sgl)
+		if (seg->src_elements)
 			xlnx_ps_pcie_update_srcq(chan, seg);
 
-		if (seg->tx_elements.dst_sgl)
+		if (seg->dst_elements)
 			xlnx_ps_pcie_update_dstq(chan, seg);
 	}
 }
@@ -1169,6 +1271,7 @@ static void dst_cleanup_work(struct work_struct *work)
 	struct dmaengine_result rslt;
 	u32 completed_bytes;
 	u32 dstq_desc_idx;
+	struct ps_pcie_transfer_elements *ele, *ele_nxt;
 
 	psta_bd = chan->pdst_sta_bd + chan->dst_staprobe_idx;
 
@@ -1259,10 +1362,16 @@ static void dst_cleanup_work(struct work_struct *work)
 			dma_cookie_complete(&ppkt_ctx->seg->async_tx);
 			spin_unlock(&chan->cookie_lock);
 			rslt.result = DMA_TRANS_NOERROR;
-			rslt.residue = ppkt_ctx->requested_bytes -
+			rslt.residue = ppkt_ctx->seg->total_transfer_bytes -
 					completed_bytes;
 			dmaengine_desc_get_callback_invoke(&ppkt_ctx->seg->async_tx,
 							   &rslt);
+			list_for_each_entry_safe(ele, ele_nxt,
+						 &ppkt_ctx->seg->transfer_nodes,
+						 node) {
+				list_del(&ele->node);
+				mempool_free(ele, chan->tx_elements_pool);
+			}
 			mempool_free(ppkt_ctx->seg, chan->transactions_pool);
 		}
 		memset(ppkt_ctx, 0, sizeof(struct PACKET_TRANSFER_PARAMS));
@@ -1291,6 +1400,7 @@ static void src_cleanup_work(struct work_struct *work)
 	struct dmaengine_result rslt;
 	u32 completed_bytes;
 	u32 srcq_desc_idx;
+	struct ps_pcie_transfer_elements *ele, *ele_nxt;
 
 	psta_bd = chan->psrc_sta_bd + chan->src_staprobe_idx;
 
@@ -1378,10 +1488,16 @@ static void src_cleanup_work(struct work_struct *work)
 			dma_cookie_complete(&ppkt_ctx->seg->async_tx);
 			spin_unlock(&chan->cookie_lock);
 			rslt.result = DMA_TRANS_NOERROR;
-			rslt.residue = ppkt_ctx->requested_bytes -
+			rslt.residue = ppkt_ctx->seg->total_transfer_bytes -
 					completed_bytes;
 			dmaengine_desc_get_callback_invoke(&ppkt_ctx->seg->async_tx,
 							   &rslt);
+			list_for_each_entry_safe(ele, ele_nxt,
+						 &ppkt_ctx->seg->transfer_nodes,
+						 node) {
+				list_del(&ele->node);
+				mempool_free(ele, chan->tx_elements_pool);
+			}
 			mempool_free(ppkt_ctx->seg, chan->transactions_pool);
 		}
 		memset(ppkt_ctx, 0, sizeof(struct PACKET_TRANSFER_PARAMS));
@@ -1408,7 +1524,7 @@ static void ps_pcie_chan_primary_work(struct work_struct *work)
 				handle_primary_desc_cleanup);
 
 	/* Disable interrupts for Channel */
-	ps_pcie_dma_clr_mask(chan, DMA_PCIE_INTR_CNTRL_REG_OFFSET,
+	ps_pcie_dma_clr_mask(chan, chan->intr_control_offset,
 			     DMA_INTCNTRL_ENABLINTR_BIT);
 
 	if (chan->psrc_sgl_bd) {
@@ -1430,7 +1546,7 @@ static void ps_pcie_chan_primary_work(struct work_struct *work)
 		wait_for_completion_interruptible(&chan->dstq_work_complete);
 
 	/* Enable interrupts for channel */
-	ps_pcie_dma_set_mask(chan, DMA_PCIE_INTR_CNTRL_REG_OFFSET,
+	ps_pcie_dma_set_mask(chan, chan->intr_control_offset,
 			     DMA_INTCNTRL_ENABLINTR_BIT);
 
 	if (chan->chan_programming) {
@@ -1442,6 +1558,137 @@ static void ps_pcie_chan_primary_work(struct work_struct *work)
 		mod_timer(&chan->poll_timer, jiffies + chan->poll_timer_freq);
 }
 
+static int read_rootdma_config(struct platform_device *platform_dev,
+			       struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+	struct resource *r;
+
+	err = dma_set_mask(&platform_dev->dev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_info(&platform_dev->dev, "Cannot set 64 bit DMA mask\n");
+		err = dma_set_mask(&platform_dev->dev, DMA_BIT_MASK(32));
+		if (err) {
+			dev_err(&platform_dev->dev, "DMA mask set error\n");
+			return err;
+		}
+	}
+
+	err = dma_set_coherent_mask(&platform_dev->dev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_info(&platform_dev->dev, "Cannot set 64 bit consistent DMA mask\n");
+		err = dma_set_coherent_mask(&platform_dev->dev,
+					    DMA_BIT_MASK(32));
+		if (err) {
+			dev_err(&platform_dev->dev, "Cannot set consistent DMA mask\n");
+			return err;
+		}
+	}
+
+	r = platform_get_resource_byname(platform_dev, IORESOURCE_MEM,
+					 "ps_pcie_regbase");
+	if (!r) {
+		dev_err(&platform_dev->dev,
+			"Unable to find memory resource for root dma\n");
+		return PTR_ERR(r);
+	}
+
+	xdev->reg_base = devm_ioremap_resource(&platform_dev->dev, r);
+	if (IS_ERR(xdev->reg_base)) {
+		dev_err(&platform_dev->dev, "ioresource error for root dma\n");
+		return PTR_ERR(xdev->reg_base);
+	}
+
+	xdev->platform_irq_vec =
+		platform_get_irq_byname(platform_dev,
+					"ps_pcie_rootdma_intr");
+	if (xdev->platform_irq_vec < 0) {
+		dev_err(&platform_dev->dev,
+			"Unable to get interrupt number for root dma\n");
+		return xdev->platform_irq_vec;
+	}
+
+	err = device_property_read_u16(&platform_dev->dev, "dma_vendorid",
+				       &xdev->rootdma_vendor);
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to find RootDMA PCI Vendor Id\n");
+		return err;
+	}
+
+	err = device_property_read_u16(&platform_dev->dev, "dma_deviceid",
+				       &xdev->rootdma_device);
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to find RootDMA PCI Device Id\n");
+		return err;
+	}
+
+	xdev->common.dev = xdev->dev;
+
+	return 0;
+}
+
+static int read_epdma_config(struct platform_device *platform_dev,
+			     struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+	struct pci_dev *pdev;
+	u16 i;
+	void __iomem * const *pci_iomap;
+	unsigned long pci_bar_length;
+
+	pdev = *((struct pci_dev **)(platform_dev->dev.platform_data));
+	xdev->pci_dev = pdev;
+
+	for (i = 0; i < MAX_BARS; i++) {
+		if (pci_resource_len(pdev, i) == 0)
+			continue;
+		xdev->bar_mask = xdev->bar_mask | (1 << (i));
+	}
+
+	err = pcim_iomap_regions(pdev, xdev->bar_mask, PLATFORM_DRIVER_NAME);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot request PCI regions, aborting\n");
+		return err;
+	}
+
+	pci_iomap = pcim_iomap_table(pdev);
+	if (!pci_iomap) {
+		err = -ENOMEM;
+		return err;
+	}
+
+	for (i = 0; i < MAX_BARS; i++) {
+		pci_bar_length = pci_resource_len(pdev, i);
+		if (pci_bar_length == 0) {
+			xdev->bar_info[i].BAR_LENGTH = 0;
+			xdev->bar_info[i].BAR_PHYS_ADDR = 0;
+			xdev->bar_info[i].BAR_VIRT_ADDR = NULL;
+		} else {
+			xdev->bar_info[i].BAR_LENGTH =
+				pci_bar_length;
+			xdev->bar_info[i].BAR_PHYS_ADDR =
+				pci_resource_start(pdev, i);
+			xdev->bar_info[i].BAR_VIRT_ADDR =
+				(void *)pci_iomap[i];
+		}
+	}
+
+	xdev->reg_base = pci_iomap[DMA_BAR_NUMBER];
+
+	err = irq_probe(xdev);
+	if (err < 0) {
+		dev_err(&pdev->dev, "Cannot probe irq lines for device %d\n",
+			platform_dev->id);
+		return err;
+	}
+
+	xdev->common.dev = &pdev->dev;
+
+	return 0;
+}
+
 static int probe_channel_properties(struct platform_device *platform_dev,
 				    struct xlnx_pcie_dma_device *xdev,
 				    u16 channel_number)
@@ -1454,7 +1701,7 @@ static int probe_channel_properties(struct platform_device *platform_dev,
 	struct ps_pcie_dma_channel_match *xlnx_match;
 
 	snprintf(propertyname, CHANNEL_PROPERTY_LENGTH,
-		 "channel%d", channel_number);
+		 "ps_pcie_channel%d", channel_number);
 
 	channel = &xdev->channels[channel_number];
 
@@ -1582,18 +1829,31 @@ static int probe_channel_properties(struct platform_device *platform_dev,
 
 	channel->xdev = xdev;
 	channel->channel_number = channel_number;
-	channel->dev = &xdev->pci_dev->dev;
+
+	if (xdev->is_rootdma) {
+		channel->dev = xdev->dev;
+		channel->intr_status_offset = DMA_AXI_INTR_STATUS_REG_OFFSET;
+		channel->intr_control_offset = DMA_AXI_INTR_CNTRL_REG_OFFSET;
+	} else {
+		channel->dev = &xdev->pci_dev->dev;
+		channel->intr_status_offset = DMA_PCIE_INTR_STATUS_REG_OFFSET;
+		channel->intr_control_offset = DMA_PCIE_INTR_CNTRL_REG_OFFSET;
+	}
+
 	channel->chan_base =
 	(struct DMA_ENGINE_REGISTERS *)((__force char *)(xdev->reg_base) +
 				 (channel_number * DMA_CHANNEL_REGS_SIZE));
 
-	if (((channel->chan_base->dma_channel_status) &
+	if ((channel->chan_base->dma_channel_status &
 				DMA_STATUS_DMA_PRES_BIT) == 0) {
 		dev_err(&platform_dev->dev,
 			"Hardware reports channel not present\n");
 		return -ENOTSUPP;
 	}
 
+	update_channel_read_attribute(channel);
+	update_channel_write_attribute(channel);
+
 	xlnx_match = devm_kzalloc(&platform_dev->dev,
 				  sizeof(struct ps_pcie_dma_channel_match),
 				  GFP_KERNEL);
@@ -1601,12 +1861,18 @@ static int probe_channel_properties(struct platform_device *platform_dev,
 	if (!xlnx_match)
 		return -ENOMEM;
 
-	xlnx_match->pci_vendorid = xdev->pci_dev->vendor;
-	xlnx_match->pci_deviceid = xdev->pci_dev->device;
+	if (xdev->is_rootdma) {
+		xlnx_match->pci_vendorid = xdev->rootdma_vendor;
+		xlnx_match->pci_deviceid = xdev->rootdma_device;
+	} else {
+		xlnx_match->pci_vendorid = xdev->pci_dev->vendor;
+		xlnx_match->pci_deviceid = xdev->pci_dev->device;
+		xlnx_match->bar_params = xdev->bar_info;
+	}
+
 	xlnx_match->board_number = xdev->board_number;
 	xlnx_match->channel_number = channel_number;
 	xlnx_match->direction = xdev->channels[channel_number].direction;
-	xlnx_match->bar_params = xdev->bar_info;
 
 	channel->common.private = (void *)xlnx_match;
 
@@ -1620,6 +1886,8 @@ static void xlnx_ps_pcie_destroy_mempool(struct ps_pcie_dma_chan *chan)
 {
 	mempool_destroy(chan->transactions_pool);
 
+	mempool_destroy(chan->tx_elements_pool);
+
 	mempool_destroy(chan->intr_transactions_pool);
 }
 
@@ -1691,7 +1959,7 @@ static int xlnx_ps_pcie_channel_activate(struct ps_pcie_dma_chan *chan)
 	reg = reg << DMA_INTCNTRL_SGCOLSCCNT_BIT_SHIFT;
 
 	/* Enable Interrupts for channel */
-	ps_pcie_dma_set_mask(chan, DMA_PCIE_INTR_CNTRL_REG_OFFSET,
+	ps_pcie_dma_set_mask(chan, chan->intr_control_offset,
 			     reg | DMA_INTCNTRL_ENABLINTR_BIT |
 			     DMA_INTCNTRL_DMAERRINTR_BIT |
 			     DMA_INTCNTRL_DMASGINTR_BIT);
@@ -1706,7 +1974,7 @@ static int xlnx_ps_pcie_channel_activate(struct ps_pcie_dma_chan *chan)
 	spin_unlock(&chan->channel_lock);
 
 	/* Activate timer if required */
-	if ((chan->coalesce_count > 0) && !chan->poll_timer.function)
+	if (chan->coalesce_count > 0 && !chan->poll_timer.function)
 		xlnx_ps_pcie_alloc_poll_timer(chan);
 
 	return 0;
@@ -1715,11 +1983,11 @@ static int xlnx_ps_pcie_channel_activate(struct ps_pcie_dma_chan *chan)
 static void xlnx_ps_pcie_channel_quiesce(struct ps_pcie_dma_chan *chan)
 {
 	/* Disable interrupts for Channel */
-	ps_pcie_dma_clr_mask(chan, DMA_PCIE_INTR_CNTRL_REG_OFFSET,
+	ps_pcie_dma_clr_mask(chan, chan->intr_control_offset,
 			     DMA_INTCNTRL_ENABLINTR_BIT);
 
 	/* Delete timer if it is created */
-	if ((chan->coalesce_count > 0) && (!chan->poll_timer.function))
+	if (chan->coalesce_count > 0 && !chan->poll_timer.function)
 		xlnx_ps_pcie_free_poll_timer(chan);
 
 	/* Flush descriptor cleaning work queues */
@@ -1731,7 +1999,7 @@ static void xlnx_ps_pcie_channel_quiesce(struct ps_pcie_dma_chan *chan)
 		flush_workqueue(chan->chan_programming);
 
 	/*  Clear the persistent bits */
-	ps_pcie_dma_set_mask(chan, DMA_PCIE_INTR_STATUS_REG_OFFSET,
+	ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
 			     DMA_INTSTATUS_DMAERR_BIT |
 			     DMA_INTSTATUS_SGLINTR_BIT |
 			     DMA_INTSTATUS_SWINTR_BIT);
@@ -1744,19 +2012,6 @@ static void xlnx_ps_pcie_channel_quiesce(struct ps_pcie_dma_chan *chan)
 	spin_unlock(&chan->channel_lock);
 }
 
-static u32 total_bytes_in_sgl(struct scatterlist *sgl,
-			      unsigned int num_entries)
-{
-	u32 total_bytes = 0;
-	struct scatterlist *sgl_ptr;
-	unsigned int i;
-
-	for_each_sg(sgl, sgl_ptr, num_entries, i)
-		total_bytes += sg_dma_len(sgl_ptr);
-
-	return total_bytes;
-}
-
 static void ivk_cbk_intr_seg(struct ps_pcie_intr_segment *intr_seg,
 			     struct ps_pcie_dma_chan *chan,
 			     enum dmaengine_tx_result result)
@@ -1784,17 +2039,13 @@ static void ivk_cbk_seg(struct ps_pcie_tx_segment *seg,
 	spin_unlock(&chan->cookie_lock);
 
 	rslt.result = result;
-	if (seg->tx_elements.src_sgl &&
+	if (seg->src_elements &&
 	    chan->srcq_buffer_location == BUFFER_LOC_PCI) {
-		rslt.residue =
-			total_bytes_in_sgl(seg->tx_elements.src_sgl,
-					   seg->tx_elements.srcq_num_elemets);
+		rslt.residue = seg->total_transfer_bytes;
 		prslt = &rslt;
-	} else if (seg->tx_elements.dst_sgl &&
+	} else if (seg->dst_elements &&
 		   chan->dstq_buffer_location == BUFFER_LOC_PCI) {
-		rslt.residue =
-			total_bytes_in_sgl(seg->tx_elements.dst_sgl,
-					   seg->tx_elements.dstq_num_elemets);
+		rslt.residue = seg->total_transfer_bytes;
 		prslt = &rslt;
 	} else {
 		prslt = NULL;
@@ -1822,6 +2073,7 @@ static void ivk_cbk_for_pending(struct ps_pcie_dma_chan *chan)
 	struct PACKET_TRANSFER_PARAMS *ppkt_ctxt;
 	struct ps_pcie_tx_segment *seg, *seg_nxt;
 	struct ps_pcie_intr_segment *intr_seg, *intr_seg_next;
+	struct ps_pcie_transfer_elements *ele, *ele_nxt;
 
 	if (chan->ppkt_ctx_srcq) {
 		if (chan->idx_ctx_srcq_tail != chan->idx_ctx_srcq_head) {
@@ -1860,6 +2112,11 @@ static void ivk_cbk_for_pending(struct ps_pcie_dma_chan *chan)
 		spin_lock(&chan->active_list_lock);
 		list_del(&seg->node);
 		spin_unlock(&chan->active_list_lock);
+		list_for_each_entry_safe(ele, ele_nxt,
+					 &seg->transfer_nodes, node) {
+			list_del(&ele->node);
+			mempool_free(ele, chan->tx_elements_pool);
+		}
 		mempool_free(seg, chan->transactions_pool);
 	}
 
@@ -1868,6 +2125,11 @@ static void ivk_cbk_for_pending(struct ps_pcie_dma_chan *chan)
 		spin_lock(&chan->pending_list_lock);
 		list_del(&seg->node);
 		spin_unlock(&chan->pending_list_lock);
+		list_for_each_entry_safe(ele, ele_nxt,
+					 &seg->transfer_nodes, node) {
+			list_del(&ele->node);
+			mempool_free(ele, chan->tx_elements_pool);
+		}
 		mempool_free(seg, chan->transactions_pool);
 	}
 
@@ -1914,10 +2176,8 @@ static void xlnx_ps_pcie_free_poll_timer(struct ps_pcie_dma_chan *chan)
 
 static int xlnx_ps_pcie_alloc_poll_timer(struct ps_pcie_dma_chan *chan)
 {
-	init_timer(&chan->poll_timer);
-	chan->poll_timer.function = poll_completed_transactions;
+	timer_setup(&chan->poll_timer, poll_completed_transactions, 0);
 	chan->poll_timer.expires = jiffies + chan->poll_timer_freq;
-	chan->poll_timer.data = (unsigned long)chan;
 
 	add_timer(&chan->poll_timer);
 
@@ -2110,6 +2370,13 @@ static int xlnx_ps_pcie_alloc_mempool(struct ps_pcie_dma_chan *chan)
 	if (!chan->transactions_pool)
 		goto no_transactions_pool;
 
+	chan->tx_elements_pool =
+		mempool_create_kmalloc_pool(chan->total_descriptors,
+					    sizeof(struct ps_pcie_transfer_elements));
+
+	if (!chan->tx_elements_pool)
+		goto no_tx_elements_pool;
+
 	chan->intr_transactions_pool =
 	mempool_create_kmalloc_pool(MIN_SW_INTR_TRANSACTIONS,
 				    sizeof(struct ps_pcie_intr_segment));
@@ -2120,8 +2387,9 @@ static int xlnx_ps_pcie_alloc_mempool(struct ps_pcie_dma_chan *chan)
 	return 0;
 
 no_intr_transactions_pool:
+	mempool_destroy(chan->tx_elements_pool);
+no_tx_elements_pool:
 	mempool_destroy(chan->transactions_pool);
-
 no_transactions_pool:
 	return -ENOMEM;
 }
@@ -2454,46 +2722,84 @@ static dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx)
 	return cookie;
 }
 
-static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_dma_sg(
-		struct dma_chan *channel, struct scatterlist *dst_sg,
-		unsigned int dst_nents, struct scatterlist *src_sg,
-		unsigned int src_nents, unsigned long flags)
+/**
+ * xlnx_ps_pcie_dma_prep_memcpy - prepare descriptors for a memcpy transaction
+ * @channel: DMA channel
+ * @dma_dst: destination address
+ * @dma_src: source address
+ * @len: transfer length
+ * @flags: transfer ack flags
+ *
+ * Return: Async transaction descriptor on success and NULL on failure
+ */
+static struct dma_async_tx_descriptor *
+xlnx_ps_pcie_dma_prep_memcpy(struct dma_chan *channel, dma_addr_t dma_dst,
+			     dma_addr_t dma_src, size_t len,
+			     unsigned long flags)
 {
 	struct ps_pcie_dma_chan *chan = to_xilinx_chan(channel);
 	struct ps_pcie_tx_segment *seg = NULL;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	struct ps_pcie_transfer_elements *ele_nxt = NULL;
+	u32 i;
 
 	if (chan->state != CHANNEL_AVAILABLE)
 		return NULL;
 
-	if (dst_nents == 0 || src_nents == 0)
-		return NULL;
-
-	if (!dst_sg || !src_sg)
-		return NULL;
-
 	if (chan->num_queues != DEFAULT_DMA_QUEUES) {
-		dev_err(
-		chan->dev,
-		"Only prep_slave_sg is supported for channel %d\n",
-		chan->channel_number);
+		dev_err(chan->dev, "Only prep_slave_sg for channel %d\n",
+			chan->channel_number);
 		return NULL;
 	}
 
 	seg = mempool_alloc(chan->transactions_pool, GFP_ATOMIC);
 	if (!seg) {
-		dev_err(
-		chan->dev,
-		"Unable to allocate tx segment for channel %d\n",
-		chan->channel_number);
+		dev_err(chan->dev, "Tx segment alloc for channel %d\n",
+			chan->channel_number);
 		return NULL;
 	}
 
 	memset(seg, 0, sizeof(*seg));
+	INIT_LIST_HEAD(&seg->transfer_nodes);
+
+	for (i = 0; i < len / MAX_TRANSFER_LENGTH; i++) {
+		ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+		if (!ele) {
+			dev_err(chan->dev, "Tx element %d for channel %d\n",
+				i, chan->channel_number);
+			goto err_elements_prep_memcpy;
+		}
+		ele->src_pa = dma_src + (i * MAX_TRANSFER_LENGTH);
+		ele->dst_pa = dma_dst + (i * MAX_TRANSFER_LENGTH);
+		ele->transfer_bytes = MAX_TRANSFER_LENGTH;
+		list_add_tail(&ele->node, &seg->transfer_nodes);
+		seg->src_elements++;
+		seg->dst_elements++;
+		seg->total_transfer_bytes += ele->transfer_bytes;
+		ele = NULL;
+	}
+
+	if (len % MAX_TRANSFER_LENGTH) {
+		ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+		if (!ele) {
+			dev_err(chan->dev, "Tx element %d for channel %d\n",
+				i, chan->channel_number);
+			goto err_elements_prep_memcpy;
+		}
+		ele->src_pa = dma_src + (i * MAX_TRANSFER_LENGTH);
+		ele->dst_pa = dma_dst + (i * MAX_TRANSFER_LENGTH);
+		ele->transfer_bytes = len % MAX_TRANSFER_LENGTH;
+		list_add_tail(&ele->node, &seg->transfer_nodes);
+		seg->src_elements++;
+		seg->dst_elements++;
+		seg->total_transfer_bytes += ele->transfer_bytes;
+	}
 
-	seg->tx_elements.dst_sgl = dst_sg;
-	seg->tx_elements.dstq_num_elemets = dst_nents;
-	seg->tx_elements.src_sgl = src_sg;
-	seg->tx_elements.srcq_num_elemets = src_nents;
+	if (seg->src_elements > chan->total_descriptors) {
+		dev_err(chan->dev, "Insufficient descriptors in channel %d for dma transaction\n",
+			chan->channel_number);
+		goto err_elements_prep_memcpy;
+	}
 
 	dma_async_tx_descriptor_init(&seg->async_tx, &chan->common);
 	seg->async_tx.flags = flags;
@@ -2501,6 +2807,14 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_dma_sg(
 	seg->async_tx.tx_submit = xilinx_dma_tx_submit;
 
 	return &seg->async_tx;
+
+err_elements_prep_memcpy:
+	list_for_each_entry_safe(ele, ele_nxt, &seg->transfer_nodes, node) {
+		list_del(&ele->node);
+		mempool_free(ele, chan->tx_elements_pool);
+	}
+	mempool_free(seg, chan->transactions_pool);
+	return NULL;
 }
 
 static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
@@ -2510,6 +2824,10 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
 {
 	struct ps_pcie_dma_chan *chan = to_xilinx_chan(channel);
 	struct ps_pcie_tx_segment *seg = NULL;
+	struct scatterlist *sgl_ptr;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	struct ps_pcie_transfer_elements *ele_nxt = NULL;
+	u32 i, j;
 
 	if (chan->state != CHANNEL_AVAILABLE)
 		return NULL;
@@ -2521,7 +2839,7 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
 		return NULL;
 
 	if (chan->num_queues != TWO_DMA_QUEUES) {
-		dev_err(chan->dev, "Only prep_dma_sg is supported channel %d\n",
+		dev_err(chan->dev, "Only prep_dma_memcpy is supported channel %d\n",
 			chan->channel_number);
 		return NULL;
 	}
@@ -2535,16 +2853,56 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
 
 	memset(seg, 0, sizeof(*seg));
 
-	if (chan->direction == DMA_TO_DEVICE) {
-		seg->tx_elements.src_sgl = sgl;
-		seg->tx_elements.srcq_num_elemets = sg_len;
-		seg->tx_elements.dst_sgl = NULL;
-		seg->tx_elements.dstq_num_elemets = 0;
-	} else {
-		seg->tx_elements.src_sgl = NULL;
-		seg->tx_elements.srcq_num_elemets = 0;
-		seg->tx_elements.dst_sgl = sgl;
-		seg->tx_elements.dstq_num_elemets = sg_len;
+	for_each_sg(sgl, sgl_ptr, sg_len, j) {
+		for (i = 0; i < sg_dma_len(sgl_ptr) / MAX_TRANSFER_LENGTH; i++) {
+			ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+			if (!ele) {
+				dev_err(chan->dev, "Tx element %d for channel %d\n",
+					i, chan->channel_number);
+				goto err_elements_prep_slave_sg;
+			}
+			if (chan->direction == DMA_TO_DEVICE) {
+				ele->src_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->src_elements++;
+			} else {
+				ele->dst_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->dst_elements++;
+			}
+			ele->transfer_bytes = MAX_TRANSFER_LENGTH;
+			list_add_tail(&ele->node, &seg->transfer_nodes);
+			seg->total_transfer_bytes += ele->transfer_bytes;
+			ele = NULL;
+		}
+		if (sg_dma_len(sgl_ptr) % MAX_TRANSFER_LENGTH) {
+			ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+			if (!ele) {
+				dev_err(chan->dev, "Tx element %d for channel %d\n",
+					i, chan->channel_number);
+				goto err_elements_prep_slave_sg;
+			}
+			if (chan->direction == DMA_TO_DEVICE) {
+				ele->src_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->src_elements++;
+			} else {
+				ele->dst_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->dst_elements++;
+			}
+			ele->transfer_bytes = sg_dma_len(sgl_ptr) %
+						MAX_TRANSFER_LENGTH;
+			list_add_tail(&ele->node, &seg->transfer_nodes);
+			seg->total_transfer_bytes += ele->transfer_bytes;
+		}
+	}
+
+	if (max(seg->src_elements, seg->dst_elements) >
+		chan->total_descriptors) {
+		dev_err(chan->dev, "Insufficient descriptors in channel %d for dma transaction\n",
+			chan->channel_number);
+		goto err_elements_prep_slave_sg;
 	}
 
 	dma_async_tx_descriptor_init(&seg->async_tx, &chan->common);
@@ -2553,6 +2911,14 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
 	seg->async_tx.tx_submit = xilinx_dma_tx_submit;
 
 	return &seg->async_tx;
+
+err_elements_prep_slave_sg:
+	list_for_each_entry_safe(ele, ele_nxt, &seg->transfer_nodes, node) {
+		list_del(&ele->node);
+		mempool_free(ele, chan->tx_elements_pool);
+	}
+	mempool_free(seg, chan->transactions_pool);
+	return NULL;
 }
 
 static void xlnx_ps_pcie_dma_issue_pending(struct dma_chan *channel)
@@ -2640,22 +3006,15 @@ static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_interrupt(
 
 static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev)
 {
-	struct pci_dev *pdev;
-	int err = 0;
+	int err, i;
 	struct xlnx_pcie_dma_device *xdev;
-	u16 i = 0;
-	void __iomem * const *pci_iomap;
-	unsigned long pci_bar_length;
-
-	pdev = *((struct pci_dev **)(platform_dev->dev.platform_data));
+	static u16 board_number;
 
 	xdev = devm_kzalloc(&platform_dev->dev,
 			    sizeof(struct xlnx_pcie_dma_device), GFP_KERNEL);
 
-	if (!xdev) {
-		err = -ENOMEM;
-		goto platform_driver_probe_return;
-	}
+	if (!xdev)
+		return -ENOMEM;
 
 #ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
 	xdev->dma_buf_ext_addr = true;
@@ -2663,14 +3022,17 @@ static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev)
 	xdev->dma_buf_ext_addr = false;
 #endif
 
-	xdev->pci_dev = pdev;
+	xdev->is_rootdma = device_property_read_bool(&platform_dev->dev,
+						     "rootdma");
+
 	xdev->dev = &platform_dev->dev;
+	xdev->board_number = board_number;
 
-	if (device_property_read_u32(&platform_dev->dev, "xlnx,numchannels",
-				     &xdev->num_channels) != 0) {
+	err = device_property_read_u32(&platform_dev->dev, "numchannels",
+				       &xdev->num_channels);
+	if (err) {
 		dev_err(&platform_dev->dev,
 			"Unable to find numchannels property\n");
-		err = -EINVAL;
 		goto platform_driver_probe_return;
 	}
 
@@ -2686,57 +3048,29 @@ static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev)
 						sizeof(struct ps_pcie_dma_chan)
 							* xdev->num_channels,
 						GFP_KERNEL);
-
 	if (!xdev->channels) {
 		err = -ENOMEM;
 		goto platform_driver_probe_return;
 	}
 
-	for (i = 0; i < MAX_BARS; i++) {
-		if (pci_resource_len(pdev, i) == 0)
-			continue;
-		xdev->bar_mask = xdev->bar_mask | (1 << (i));
-	}
-	err = pcim_iomap_regions(pdev, xdev->bar_mask, PLATFORM_DRIVER_NAME);
-	if (err) {
-		dev_err(&pdev->dev, "Cannot request PCI regions, aborting\n");
-		return err;
-	}
+	if (xdev->is_rootdma)
+		err = read_rootdma_config(platform_dev, xdev);
+	else
+		err = read_epdma_config(platform_dev, xdev);
 
-	pci_iomap = pcim_iomap_table(pdev);
-	if (!pci_iomap) {
-		err = -ENOMEM;
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to initialize dma configuration\n");
 		goto platform_driver_probe_return;
 	}
 
-	for (i = 0; i < MAX_BARS; i++) {
-		pci_bar_length = pci_resource_len(pdev, i);
-		if (pci_bar_length == 0) {
-			xdev->bar_info[i].BAR_LENGTH = 0;
-			xdev->bar_info[i].BAR_PHYS_ADDR = 0;
-			xdev->bar_info[i].BAR_VIRT_ADDR = NULL;
-		} else {
-			xdev->bar_info[i].BAR_LENGTH =
-				pci_bar_length;
-			xdev->bar_info[i].BAR_PHYS_ADDR =
-				pci_resource_start(pdev, i);
-			xdev->bar_info[i].BAR_VIRT_ADDR =
-				pci_iomap[i];
-		}
-	}
-	xdev->reg_base = pci_iomap[DMA_BAR_NUMBER];
-
-	xdev->board_number = platform_dev->id;
-
 	/* Initialize the DMA engine */
-	xdev->common.dev = &pdev->dev;
-
 	INIT_LIST_HEAD(&xdev->common.channels);
 
 	dma_cap_set(DMA_SLAVE, xdev->common.cap_mask);
 	dma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);
-	dma_cap_set(DMA_SG, xdev->common.cap_mask);
 	dma_cap_set(DMA_INTERRUPT, xdev->common.cap_mask);
+	dma_cap_set(DMA_MEMCPY, xdev->common.cap_mask);
 
 	xdev->common.src_addr_widths = DMA_SLAVE_BUSWIDTH_UNDEFINED;
 	xdev->common.dst_addr_widths = DMA_SLAVE_BUSWIDTH_UNDEFINED;
@@ -2750,7 +3084,7 @@ static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev)
 	xdev->common.device_issue_pending = xlnx_ps_pcie_dma_issue_pending;
 	xdev->common.device_prep_dma_interrupt =
 		xlnx_ps_pcie_dma_prep_interrupt;
-	xdev->common.device_prep_dma_sg = xlnx_ps_pcie_dma_prep_dma_sg;
+	xdev->common.device_prep_dma_memcpy = xlnx_ps_pcie_dma_prep_memcpy;
 	xdev->common.device_prep_slave_sg = xlnx_ps_pcie_dma_prep_slave_sg;
 	xdev->common.residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;
 
@@ -2758,36 +3092,35 @@ static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev)
 		err = probe_channel_properties(platform_dev, xdev, i);
 
 		if (err != 0) {
-			dev_err(&pdev->dev,
+			dev_err(xdev->dev,
 				"Unable to read channel properties\n");
 			goto platform_driver_probe_return;
 		}
 	}
 
-	err = irq_probe(xdev);
-	if (err < 0) {
-		dev_err(&pdev->dev, "Cannot probe irq lines for device %d\n",
-			platform_dev->id);
-		goto platform_driver_probe_return;
-	}
-
-	err = irq_setup(xdev);
+	if (xdev->is_rootdma)
+		err = platform_irq_setup(xdev);
+	else
+		err = irq_setup(xdev);
 	if (err) {
-		dev_err(&pdev->dev, "Cannot request irq lines for device %d\n",
-			platform_dev->id);
+		dev_err(xdev->dev, "Cannot request irq lines for device %d\n",
+			xdev->board_number);
 		goto platform_driver_probe_return;
 	}
 
 	err = dma_async_device_register(&xdev->common);
 	if (err) {
-		dev_err(&pdev->dev,
+		dev_err(xdev->dev,
 			"Unable to register board %d with dma framework\n",
-			platform_dev->id);
+			xdev->board_number);
 		goto platform_driver_probe_return;
 	}
 
 	platform_set_drvdata(platform_dev, xdev);
 
+	board_number++;
+
+	dev_info(&platform_dev->dev, "PS PCIe Platform driver probed\n");
 	return 0;
 
 platform_driver_probe_return:
@@ -2808,9 +3141,18 @@ static int xlnx_pcie_dma_driver_remove(struct platform_device *platform_dev)
 	return 0;
 }
 
+#ifdef CONFIG_OF
+static const struct of_device_id xlnx_pcie_root_dma_of_ids[] = {
+	{ .compatible = "xlnx,ps_pcie_dma-1.00.a", },
+	{}
+};
+MODULE_DEVICE_TABLE(of, xlnx_pcie_root_dma_of_ids);
+#endif
+
 static struct platform_driver xlnx_pcie_dma_driver = {
 	.driver = {
 		.name = XLNX_PLATFORM_DRIVER_NAME,
+		.of_match_table = of_match_ptr(xlnx_pcie_root_dma_of_ids),
 		.owner = THIS_MODULE,
 	},
 	.probe =  xlnx_pcie_dma_driver_probe,
-- 
2.31.1

