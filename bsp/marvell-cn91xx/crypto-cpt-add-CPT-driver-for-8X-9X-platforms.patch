From b7144634985fae3616048e85680b6de73ba54f43 Mon Sep 17 00:00:00 2001
From: "Bartosik, Lukasz" <Lukasz.Bartosik@cavium.com>
Date: Tue, 27 Nov 2018 14:32:35 +0300
Subject: [PATCH 0771/1051] crypto: cpt - add CPT driver for 8X/9X platforms

Implement common CPT driver for both 8X and 9X platforms.

Signed-off-by: Lukasz Bartosik <lukasz.bartosik@cavium.com>
[Kevin: The original patch got from Marvell sdk10.0_19.06.
The cptpf_main.c was deleted before this patch and also
replace bitmap_to_u32array() with bitmap_to_arr32().]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 crypto/testmgr.c                              |    8 +
 drivers/crypto/Makefile                       |    3 +-
 drivers/crypto/cavium/cpt/8x/Makefile         |   13 +
 drivers/crypto/cavium/cpt/8x/cpt8x_common.h   |   53 +
 drivers/crypto/cavium/cpt/8x/cpt8x_debug.c    |   75 +
 drivers/crypto/cavium/cpt/8x/cpt8x_pf.h       |   35 +
 drivers/crypto/cavium/cpt/8x/cpt8x_pf_main.c  |  353 ++++
 drivers/crypto/cavium/cpt/8x/cpt8x_pf_mbox.c  |  195 ++
 drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.c   |  178 ++
 drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.h   |   18 +
 drivers/crypto/cavium/cpt/8x/cpt8x_ucode.c    |  189 ++
 .../cavium/cpt/{cptvf.h => 8x/cpt8x_vf.h}     |   98 +-
 .../cpt/{cptvf_main.c => 8x/cpt8x_vf_main.c}  |  434 +++--
 .../cpt/{cptvf_mbox.c => 8x/cpt8x_vf_mbox.c}  |   58 +-
 drivers/crypto/cavium/cpt/9x/Makefile         |   14 +
 drivers/crypto/cavium/cpt/9x/cpt9x_common.h   |   38 +
 drivers/crypto/cavium/cpt/9x/cpt9x_debug.c    |   59 +
 drivers/crypto/cavium/cpt/9x/cpt9x_lf.h       |  111 ++
 drivers/crypto/cavium/cpt/9x/cpt9x_lf_main.c  | 1141 +++++++++++
 .../crypto/cavium/cpt/9x/cpt9x_mbox_common.c  |  314 +++
 .../crypto/cavium/cpt/9x/cpt9x_mbox_common.h  |   56 +
 .../crypto/cavium/cpt/9x/cpt9x_passthrough.c  |  121 ++
 .../crypto/cavium/cpt/9x/cpt9x_passthrough.h  |   16 +
 drivers/crypto/cavium/cpt/9x/cpt9x_pf.h       |   67 +
 drivers/crypto/cavium/cpt/9x/cpt9x_pf_main.c  |  713 +++++++
 drivers/crypto/cavium/cpt/9x/cpt9x_pf_mbox.c  |  417 ++++
 drivers/crypto/cavium/cpt/9x/cpt9x_quota.c    |  190 ++
 drivers/crypto/cavium/cpt/9x/cpt9x_quota.h    |   86 +
 drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.c   |  161 ++
 drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.h   |   16 +
 drivers/crypto/cavium/cpt/9x/cpt9x_ucode.c    |  258 +++
 drivers/crypto/cavium/cpt/9x/cpt9x_vf.h       |   34 +
 drivers/crypto/cavium/cpt/9x/cpt9x_vf_main.c  |  295 +++
 drivers/crypto/cavium/cpt/9x/cpt9x_vf_mbox.c  |  134 ++
 drivers/crypto/cavium/cpt/Kconfig             |   33 +-
 drivers/crypto/cavium/cpt/Makefile            |    3 -
 drivers/crypto/cavium/cpt/common/cpt_algs.c   | 1608 ++++++++++++++++
 drivers/crypto/cavium/cpt/common/cpt_algs.h   |  204 ++
 drivers/crypto/cavium/cpt/common/cpt_common.h |  228 +++
 drivers/crypto/cavium/cpt/common/cpt_debug.c  |   27 +
 drivers/crypto/cavium/cpt/common/cpt_debug.h  |   30 +
 .../cavium/cpt/{ => common}/cpt_hw_types.h    |  595 +++++-
 drivers/crypto/cavium/cpt/common/cpt_reqmgr.c |   41 +
 drivers/crypto/cavium/cpt/common/cpt_reqmgr.h |  534 ++++++
 drivers/crypto/cavium/cpt/common/cpt_ucode.c  | 1678 +++++++++++++++++
 drivers/crypto/cavium/cpt/common/cpt_ucode.h  |  200 ++
 drivers/crypto/cavium/cpt/cpt_common.h        |  156 --
 drivers/crypto/cavium/cpt/cptpf.h             |   64 -
 drivers/crypto/cavium/cpt/cptpf_mbox.c        |  163 --
 drivers/crypto/cavium/cpt/cptvf_algs.c        |  524 -----
 drivers/crypto/cavium/cpt/cptvf_algs.h        |  120 --
 drivers/crypto/cavium/cpt/cptvf_reqmanager.c  |  594 ------
 drivers/crypto/cavium/cpt/request_manager.h   |  147 --
 53 files changed, 10860 insertions(+), 2040 deletions(-)
 create mode 100644 drivers/crypto/cavium/cpt/8x/Makefile
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_common.h
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_debug.c
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_pf.h
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_pf_main.c
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_pf_mbox.c
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.c
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.h
 create mode 100644 drivers/crypto/cavium/cpt/8x/cpt8x_ucode.c
 rename drivers/crypto/cavium/cpt/{cptvf.h => 8x/cpt8x_vf.h} (53%)
 rename drivers/crypto/cavium/cpt/{cptvf_main.c => 8x/cpt8x_vf_main.c} (63%)
 rename drivers/crypto/cavium/cpt/{cptvf_mbox.c => 8x/cpt8x_vf_mbox.c} (76%)
 create mode 100644 drivers/crypto/cavium/cpt/9x/Makefile
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_common.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_debug.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_lf.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_lf_main.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_pf.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_pf_main.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_pf_mbox.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_quota.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_quota.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_ucode.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_vf.h
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_vf_main.c
 create mode 100644 drivers/crypto/cavium/cpt/9x/cpt9x_vf_mbox.c
 delete mode 100644 drivers/crypto/cavium/cpt/Makefile
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_algs.c
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_algs.h
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_common.h
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_debug.c
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_debug.h
 rename drivers/crypto/cavium/cpt/{ => common}/cpt_hw_types.h (53%)
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_reqmgr.c
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_reqmgr.h
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_ucode.c
 create mode 100644 drivers/crypto/cavium/cpt/common/cpt_ucode.h
 delete mode 100644 drivers/crypto/cavium/cpt/cpt_common.h
 delete mode 100644 drivers/crypto/cavium/cpt/cptpf.h
 delete mode 100644 drivers/crypto/cavium/cpt/cptpf_mbox.c
 delete mode 100644 drivers/crypto/cavium/cpt/cptvf_algs.c
 delete mode 100644 drivers/crypto/cavium/cpt/cptvf_algs.h
 delete mode 100644 drivers/crypto/cavium/cpt/cptvf_reqmanager.c
 delete mode 100644 drivers/crypto/cavium/cpt/request_manager.h

diff --git a/crypto/testmgr.c b/crypto/testmgr.c
index d0142ef090b4..52fb90d11bfe 100644
--- a/crypto/testmgr.c
+++ b/crypto/testmgr.c
@@ -639,6 +639,8 @@ static int __test_aead(struct crypto_aead *tfm, int enc,
 
 		j++;
 
+		pr_err("alg: aead%s: Running test %d for %s\n", d, j, algo);
+
 		/* some templates have no input data but they will
 		 * touch input
 		 */
@@ -752,6 +754,8 @@ static int __test_aead(struct crypto_aead *tfm, int enc,
 		if (!template[i].np)
 			continue;
 
+		pr_err("alg: aead%s: Running test %d for %s\n", d, j, algo);
+
 		j++;
 
 		if (template[i].iv)
@@ -1119,6 +1123,8 @@ static int __test_skcipher(struct crypto_skcipher *tfm, int enc,
 		if (fips_enabled && template[i].fips_skip)
 			continue;
 
+		pr_err("alg: skcipher%s: Running test %d for %s\n", d, j, algo);
+
 		if (template[i].iv && !(template[i].generates_iv && enc))
 			memcpy(iv, template[i].iv, ivsize);
 		else
@@ -1198,6 +1204,8 @@ static int __test_skcipher(struct crypto_skcipher *tfm, int enc,
 		if (fips_enabled && template[i].fips_skip)
 			continue;
 
+		pr_err("alg: skcipher%s: Running test %d for %s\n", d, j, algo);
+
 		if (template[i].iv && !(template[i].generates_iv && enc))
 			memcpy(iv, template[i].iv, ivsize);
 		else
diff --git a/drivers/crypto/Makefile b/drivers/crypto/Makefile
index 7ae87b4f6c8d..0d8cd189c4a2 100644
--- a/drivers/crypto/Makefile
+++ b/drivers/crypto/Makefile
@@ -7,7 +7,6 @@ obj-$(CONFIG_CRYPTO_DEV_CAVIUM_ZIP) += cavium/
 obj-$(CONFIG_CRYPTO_DEV_CCP) += ccp/
 obj-$(CONFIG_CRYPTO_DEV_CCREE) += ccree/
 obj-$(CONFIG_CRYPTO_DEV_CHELSIO) += chelsio/
-obj-$(CONFIG_CRYPTO_DEV_CPT) += cavium/cpt/
 obj-$(CONFIG_CRYPTO_DEV_NITROX) += cavium/nitrox/
 obj-$(CONFIG_CRYPTO_DEV_EXYNOS_RNG) += exynos-rng.o
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM) += caam/
@@ -45,3 +44,5 @@ obj-$(CONFIG_CRYPTO_DEV_VMX) += vmx/
 obj-$(CONFIG_CRYPTO_DEV_BCM_SPU) += bcm/
 obj-$(CONFIG_CRYPTO_DEV_SAFEXCEL) += inside-secure/
 obj-$(CONFIG_CRYPTO_DEV_ARTPEC6) += axis/
+obj-$(CONFIG_CRYPTO_DEV_OCTEONTX_CPT) += cavium/cpt/8x/
+obj-$(CONFIG_CRYPTO_DEV_OCTEONTX2_CPT) += cavium/cpt/9x/
diff --git a/drivers/crypto/cavium/cpt/8x/Makefile b/drivers/crypto/cavium/cpt/8x/Makefile
new file mode 100644
index 000000000000..abee10f1625d
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/Makefile
@@ -0,0 +1,13 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-$(CONFIG_CRYPTO_DEV_OCTEONTX_CPT) += cptpf8x.o cptvf8x.o
+
+common-objs := cpt8x_debug.o ../common/cpt_debug.o
+cptpf8x-objs := cpt8x_pf_main.o cpt8x_pf_mbox.o cpt8x_ucode.o ../common/cpt_ucode.o ${common-objs}
+cptvf8x-objs := cpt8x_vf_main.o cpt8x_vf_mbox.o cpt8x_reqmgr.o ../common/cpt_algs.o ../common/cpt_reqmgr.o
+
+ifeq ($(CONFIG_CRYPTO_DEV_OCTEONTX_CPT), m)
+	cptvf8x-objs += ${common-objs}
+endif
+
+ccflags-y := -I$(src)/../common/
+ccflags-y += -I$(src)/../../../../net/ethernet/cavium/octeontx-83xx/
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_common.h b/drivers/crypto/cavium/cpt/8x/cpt8x_common.h
new file mode 100644
index 000000000000..cc0da4b1648f
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_common.h
@@ -0,0 +1,53 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT8X_COMMON_H
+#define __CPT8X_COMMON_H
+
+#include <linux/types.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+
+/* Maximum number of AE and SE VFs */
+#define CPT_8X_MAX_VFS_NUM	64
+
+/* Flags to indicate the features supported */
+#define CPT_FLAG_SRIOV_ENABLED BIT(1)
+#define CPT_FLAG_VF_DRIVER BIT(2)
+#define CPT_FLAG_DEVICE_READY BIT(3)
+
+#define cpt_sriov_enabled(cpt) ((cpt)->flags & CPT_FLAG_SRIOV_ENABLED)
+#define cpt_vf_driver(cpt) ((cpt)->flags & CPT_FLAG_VF_DRIVER)
+#define cpt_device_ready(cpt) ((cpt)->flags & CPT_FLAG_DEVICE_READY)
+
+#define CPT_MBOX_MSG_TIMEOUT 2000
+
+/* VF-PF message opcodes */
+enum cpt_mbox_opcode {
+	CPT_MSG_VF_UP = 1,
+	CPT_MSG_VF_DOWN,
+	CPT_MSG_READY,
+	CPT_MSG_QLEN,
+	CPT_MSG_QBIND_GRP,
+	CPT_MSG_VQ_PRIORITY,
+	CPT_MSG_PF_TYPE,
+	CPT_MSG_ACK,
+	CPT_MSG_NACK
+};
+
+/* CPT mailbox structure */
+struct cpt_mbox {
+	u64 msg; /* Message type MBOX[0] */
+	u64 data;/* Data         MBOX[1] */
+};
+
+void dump_mbox_msg(struct device *dev, struct cpt_mbox *mbox_msg, int vf_id);
+
+#endif /* __CPT8X_COMMON_H */
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_debug.c b/drivers/crypto/cavium/cpt/8x/cpt8x_debug.c
new file mode 100644
index 000000000000..9aa66befdf51
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_debug.c
@@ -0,0 +1,75 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/device.h>
+#include "cpt8x_common.h"
+
+#define MAX_RAW_DATA_STR_SIZE	64
+
+static char *get_opcode_str(int msg_opcode)
+{
+	char *str = "Unknown";
+
+	switch (msg_opcode) {
+	case CPT_MSG_VF_UP:
+		str = "UP";
+	break;
+
+	case CPT_MSG_VF_DOWN:
+		str = "DOWN";
+	break;
+
+	case CPT_MSG_READY:
+		str = "READY";
+	break;
+
+	case CPT_MSG_QLEN:
+		str = "QLEN";
+	break;
+
+	case CPT_MSG_QBIND_GRP:
+		str = "QBIND_GRP";
+	break;
+
+	case CPT_MSG_VQ_PRIORITY:
+		str = "VQ_PRIORITY";
+	break;
+
+	case CPT_MSG_PF_TYPE:
+		str = "PF_TYPE";
+	break;
+
+	case CPT_MSG_ACK:
+		str = "ACK";
+	break;
+
+	case CPT_MSG_NACK:
+		str = "NACK";
+	break;
+	}
+
+	return str;
+}
+
+void dump_mbox_msg(struct device *dev, struct cpt_mbox *mbox_msg, int vf_id)
+{
+	char raw_data_str[MAX_RAW_DATA_STR_SIZE];
+	char *opcode_str;
+
+	opcode_str = get_opcode_str(mbox_msg->msg);
+	hex_dump_to_buffer(mbox_msg, sizeof(struct cpt_mbox), 16, 8,
+			   raw_data_str, MAX_RAW_DATA_STR_SIZE, false);
+	if (vf_id >= 0)
+		dev_info(dev, "Receive from VF%d %s opcode raw_data %s",
+			 vf_id, opcode_str, raw_data_str);
+	else
+		dev_info(dev, "Receive from PF %s opcode raw_data %s",
+			 opcode_str, raw_data_str);
+}
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_pf.h b/drivers/crypto/cavium/cpt/8x/cpt8x_pf.h
new file mode 100644
index 000000000000..10fd049393dd
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_pf.h
@@ -0,0 +1,35 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT8X_PF_H
+#define __CPT8X_PF_H
+
+#include "cpt8x_common.h"
+#include "cpt_ucode.h"
+
+/**
+ * cpt device structure
+ */
+struct cpt_device {
+	void __iomem *reg_base; /* Register start address */
+	struct pci_dev *pdev; /* Pci device handle */
+	struct engine_groups eng_grps;	/* Engine groups information */
+	struct list_head list;
+	u32 flags;	/* Flags to hold device status bits */
+	u8 pf_type;	/* PF type 83xx_SE or 83xx_AE */
+	u8 max_vfs;	/* Maximum number of VFs supported by the CPT */
+	u8 vfs_enabled;	/* Number of enabled VFs */
+	u8 vfs_in_use;	/* Number of VFs in use */
+};
+
+void cpt_mbox_intr_handler(struct cpt_device *cpt, int mbx);
+void cpt_disable_all_cores(struct cpt_device *cpt);
+
+#endif /* __CPT8X_PF_H */
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_pf_main.c b/drivers/crypto/cavium/cpt/8x/cpt8x_pf_main.c
new file mode 100644
index 000000000000..2a3ca7bc9da2
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_pf_main.c
@@ -0,0 +1,353 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt8x_pf.h"
+
+#define DRV_NAME	"octeontx-cpt"
+#define DRV_VERSION	"1.0"
+
+DEFINE_CPT_DEBUG_PARM(debug);
+
+DEFINE_MUTEX(octeontx_cpt_devices_lock);
+LIST_HEAD(octeontx_cpt_devices);
+
+static void cpt_disable_mbox_interrupts(struct cpt_device *cpt)
+{
+	/* Clear mbox(0) interupts for all vfs */
+	writeq(~0ull, cpt->reg_base + CPT_PF_MBOX_ENA_W1CX(0));
+}
+
+static void cpt_enable_mbox_interrupts(struct cpt_device *cpt)
+{
+	/* Set mbox(0) interupts for all vfs */
+	writeq(~0ull, cpt->reg_base + CPT_PF_MBOX_ENA_W1SX(0));
+}
+
+static irqreturn_t cpt_mbx0_intr_handler(int irq, void *cpt_irq)
+{
+	struct cpt_device *cpt = (struct cpt_device *)cpt_irq;
+
+	cpt_mbox_intr_handler(cpt, 0);
+
+	return IRQ_HANDLED;
+}
+
+static void cpt_reset(struct cpt_device *cpt)
+{
+	writeq(1, cpt->reg_base + CPT_PF_RESET);
+}
+
+static void cpt_find_max_enabled_cores(struct cpt_device *cpt)
+{
+	union cptx_pf_constants pf_cnsts = {0};
+
+	pf_cnsts.u = readq(cpt->reg_base + CPT_PF_CONSTANTS);
+	cpt->eng_grps.avail.max_se_cnt = pf_cnsts.s.se;
+	cpt->eng_grps.avail.max_ie_cnt = 0;
+	cpt->eng_grps.avail.max_ae_cnt = pf_cnsts.s.ae;
+}
+
+static u32 cpt_check_bist_status(struct cpt_device *cpt)
+{
+	union cptx_pf_bist_status bist_sts = {0};
+
+	bist_sts.u = readq(cpt->reg_base + CPT_PF_BIST_STATUS);
+	return bist_sts.u;
+}
+
+static u64 cpt_check_exe_bist_status(struct cpt_device *cpt)
+{
+	union cptx_pf_exe_bist_status bist_sts = {0};
+
+	bist_sts.u = readq(cpt->reg_base + CPT_PF_EXE_BIST_STATUS);
+	return bist_sts.u;
+}
+
+static int cpt_device_init(struct cpt_device *cpt)
+{
+	u64 bist;
+	u16 sdevid;
+	struct device *dev = &cpt->pdev->dev;
+
+	/* Reset the PF when probed first */
+	cpt_reset(cpt);
+	mdelay(100);
+
+	pci_read_config_word(cpt->pdev, PCI_SUBSYSTEM_ID, &sdevid);
+
+	/*Check BIST status*/
+	bist = (u64)cpt_check_bist_status(cpt);
+	if (bist) {
+		dev_err(dev, "RAM BIST failed with code 0x%llx", bist);
+		return -ENODEV;
+	}
+
+	bist = cpt_check_exe_bist_status(cpt);
+	if (bist) {
+		dev_err(dev, "Engine BIST failed with code 0x%llx", bist);
+		return -ENODEV;
+	}
+
+	/*Get max enabled cores */
+	cpt_find_max_enabled_cores(cpt);
+
+	if (sdevid == CPT_81XX_PCI_PF_SUBSYS_ID) {
+		cpt->pf_type = CPT_81XX;
+	} else if ((sdevid == CPT_83XX_PCI_PF_SUBSYS_ID) &&
+		   (cpt->eng_grps.avail.max_se_cnt == 0)) {
+		cpt->pf_type = CPT_AE_83XX;
+	} else if ((sdevid == CPT_83XX_PCI_PF_SUBSYS_ID) &&
+		   (cpt->eng_grps.avail.max_ae_cnt == 0)) {
+		cpt->pf_type = CPT_SE_83XX;
+	}
+
+	/* Get max VQs/VFs supported by the device */
+	cpt->max_vfs = pci_sriov_get_totalvfs(cpt->pdev);
+
+	/*TODO: Get CLK frequency*/
+	/*Disable all cores*/
+	cpt_disable_all_cores(cpt);
+	/* PF is ready */
+	cpt->flags |= CPT_FLAG_DEVICE_READY;
+
+	return 0;
+}
+
+static int cpt_register_interrupts(struct cpt_device *cpt)
+{
+	int ret;
+	struct device *dev = &cpt->pdev->dev;
+	u32 num_vec = 0;
+	u32 mbox_int_idx = ((cpt->pf_type == CPT_81XX) ?
+			    CPT_81XX_PF_MBOX_INT :
+			    CPT_83XX_PF_MBOX_INT);
+
+	/* Enable MSI-X */
+	num_vec = ((cpt->pf_type == CPT_81XX) ? CPT_81XX_PF_MSIX_VECTORS :
+			CPT_83XX_PF_MSIX_VECTORS);
+	ret = pci_alloc_irq_vectors(cpt->pdev, num_vec, num_vec, PCI_IRQ_MSIX);
+	if (ret < 0) {
+		dev_err(&cpt->pdev->dev, "Request for #%d msix vectors failed\n",
+					num_vec);
+		return ret;
+	}
+
+	/* Register mailbox interrupt handlers */
+	ret = request_irq(pci_irq_vector(cpt->pdev,
+				CPT_PF_INT_VEC_E_MBOXX(mbox_int_idx, 0)),
+				cpt_mbx0_intr_handler, 0, "CPT Mbox0", cpt);
+	if (ret)
+		goto fail;
+
+	/* Enable mailbox interrupt */
+	cpt_enable_mbox_interrupts(cpt);
+	return 0;
+
+fail:
+	dev_err(dev, "Request irq failed\n");
+	pci_disable_msix(cpt->pdev);
+	return ret;
+}
+
+static void cpt_unregister_interrupts(struct cpt_device *cpt)
+{
+	u32 mbox_int_idx = ((cpt->pf_type == CPT_81XX) ?
+			    CPT_81XX_PF_MBOX_INT :
+			    CPT_83XX_PF_MBOX_INT);
+
+	cpt_disable_mbox_interrupts(cpt);
+	free_irq(pci_irq_vector(cpt->pdev,
+				CPT_PF_INT_VEC_E_MBOXX(mbox_int_idx, 0)), cpt);
+	pci_disable_msix(cpt->pdev);
+}
+
+
+static int cpt_sriov_configure(struct pci_dev *pdev, int numvfs)
+{
+	struct cpt_device *cpt = pci_get_drvdata(pdev);
+	int ret = -EBUSY, disable = 0;
+
+	mutex_lock(&octeontx_cpt_devices_lock);
+	if (cpt->vfs_in_use)
+		goto exit;
+
+	/*
+	 * Currently we do not register any asymmetric algorithms
+	 * therefore we don't allow to enable VFs for 83xx AE
+	 */
+	if (cpt->pf_type == CPT_AE_83XX) {
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	ret = 0;
+	if (cpt->flags & CPT_FLAG_SRIOV_ENABLED)
+		disable = 1;
+
+	if (disable) {
+		pci_disable_sriov(pdev);
+		cpt_set_eng_grps_is_rdonly(&cpt->eng_grps, false);
+		cpt->flags &= ~CPT_FLAG_SRIOV_ENABLED;
+		cpt->vfs_enabled = 0;
+	}
+
+	if (numvfs > 0) {
+		ret = cpt_try_create_default_eng_grps(cpt->pdev,
+						      &cpt->eng_grps,
+						      cpt->pf_type);
+		if (ret)
+			goto exit;
+
+		cpt->vfs_enabled = numvfs;
+		ret = pci_enable_sriov(pdev, numvfs);
+		if (ret) {
+			cpt->vfs_enabled = 0;
+			goto exit;
+		}
+
+		cpt_set_eng_grps_is_rdonly(&cpt->eng_grps, true);
+		cpt->flags |= CPT_FLAG_SRIOV_ENABLED;
+		ret = numvfs;
+	}
+
+	dev_notice(&cpt->pdev->dev, "VFs enabled: %d\n", ret);
+exit:
+	mutex_unlock(&octeontx_cpt_devices_lock);
+	return ret;
+}
+
+static int cpt_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct cpt_device *cpt;
+	int err;
+
+	cpt = devm_kzalloc(dev, sizeof(*cpt), GFP_KERNEL);
+	if (!cpt)
+		return -ENOMEM;
+
+	pci_set_drvdata(pdev, cpt);
+	cpt->pdev = pdev;
+	cpt_set_dbg_level(debug);
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(dev, "Failed to enable PCI device\n");
+		pci_set_drvdata(pdev, NULL);
+		return err;
+	}
+
+	err = pci_request_regions(pdev, DRV_NAME);
+	if (err) {
+		dev_err(dev, "PCI request regions failed 0x%x\n", err);
+		goto cpt_err_disable_device;
+	}
+
+	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "Unable to get usable DMA configuration\n");
+		goto cpt_err_release_regions;
+	}
+
+	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");
+		goto cpt_err_release_regions;
+	}
+
+	/* MAP PF's configuration registers */
+	cpt->reg_base = pcim_iomap(pdev, PCI_CPT_PF_8X_CFG_BAR, 0);
+	if (!cpt->reg_base) {
+		dev_err(dev, "Cannot map config register space, aborting\n");
+		err = -ENOMEM;
+		goto cpt_err_release_regions;
+	}
+
+	/* CPT device HW initialization */
+	cpt_device_init(cpt);
+
+	/* Register interrupts */
+	err = cpt_register_interrupts(cpt);
+	if (err)
+		goto cpt_err_release_regions;
+
+	/* Initialize engine groups */
+	err = cpt_init_eng_grps(pdev, &cpt->eng_grps, cpt->pf_type);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+	INIT_LIST_HEAD(&cpt->list);
+	mutex_lock(&octeontx_cpt_devices_lock);
+	list_add(&cpt->list, &octeontx_cpt_devices);
+	mutex_unlock(&octeontx_cpt_devices_lock);
+
+	return 0;
+
+cpt_err_unregister_interrupts:
+	cpt_unregister_interrupts(cpt);
+cpt_err_release_regions:
+	pci_release_regions(pdev);
+cpt_err_disable_device:
+	pci_disable_device(pdev);
+	pci_set_drvdata(pdev, NULL);
+	return err;
+}
+
+static void cpt_remove(struct pci_dev *pdev)
+{
+	struct cpt_device *cpt = pci_get_drvdata(pdev);
+	struct cpt_device *curr;
+
+	if (!cpt)
+		return;
+
+	mutex_lock(&octeontx_cpt_devices_lock);
+	list_for_each_entry(curr, &octeontx_cpt_devices, list) {
+		if (curr == cpt) {
+			list_del(&cpt->list);
+			break;
+		}
+	}
+	mutex_unlock(&octeontx_cpt_devices_lock);
+
+	/* Disable VFs */
+	pci_disable_sriov(pdev);
+	/* Cleanup engine groups */
+	cpt_cleanup_eng_grps(pdev, &cpt->eng_grps);
+	/* Disable CPT PF interrupts */
+	cpt_unregister_interrupts(cpt);
+	/* Disengage SE and AE cores from all groups*/
+	cpt_disable_all_cores(cpt);
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	pci_set_drvdata(pdev, NULL);
+}
+
+/* Supported devices */
+static const struct pci_device_id cpt_id_table[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, CPT_PCI_PF_8X_DEVICE_ID) },
+	{ 0, }  /* end of table */
+};
+
+static struct pci_driver cpt_pci_driver = {
+	.name = DRV_NAME,
+	.id_table = cpt_id_table,
+	.probe = cpt_probe,
+	.remove = cpt_remove,
+	.sriov_configure = cpt_sriov_configure
+};
+
+module_pci_driver(cpt_pci_driver);
+
+MODULE_AUTHOR("Marvell International Ltd.");
+MODULE_DESCRIPTION("Marvell OcteonTX CPT Physical Function Driver");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, cpt_id_table);
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_pf_mbox.c b/drivers/crypto/cavium/cpt/8x/cpt8x_pf_mbox.c
new file mode 100644
index 000000000000..421ab6c9e7a2
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_pf_mbox.c
@@ -0,0 +1,195 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt_debug.h"
+#include "cpt8x_pf.h"
+
+static void cpt_send_msg_to_vf(struct cpt_device *cpt, int vf,
+			       struct cpt_mbox *mbx)
+{
+	/* Writing mbox(0) causes interrupt */
+	writeq(mbx->data, cpt->reg_base + CPT_PF_VFX_MBOXX(vf, 1));
+	writeq(mbx->msg, cpt->reg_base + CPT_PF_VFX_MBOXX(vf, 0));
+}
+
+/* ACKs VF's mailbox message
+ * @vf: VF to which ACK to be sent
+ */
+static void cpt_mbox_send_ack(struct cpt_device *cpt, int vf,
+			      struct cpt_mbox *mbx)
+{
+	mbx->data = 0ull;
+	mbx->msg = CPT_MSG_ACK;
+	cpt_send_msg_to_vf(cpt, vf, mbx);
+}
+
+/* NACKs VF's mailbox message that PF is not able to complete the action */
+void cptpf_mbox_send_nack(struct cpt_device *cpt,  int vf,
+			  struct cpt_mbox *mbx)
+{
+	mbx->data = 0ull;
+	mbx->msg = CPT_MSG_NACK;
+	cpt_send_msg_to_vf(cpt, vf, mbx);
+}
+
+static void cpt_clear_mbox_intr(struct cpt_device *cpt, u32 vf)
+{
+	/* W1C for the VF */
+	writeq(1ull << vf, cpt->reg_base + CPT_PF_MBOX_INTX(0));
+}
+
+/*
+ *  Configure QLEN/Chunk sizes for VF
+ */
+static void cpt_cfg_qlen_for_vf(struct cpt_device *cpt, int vf, u32 size)
+{
+	union cptx_pf_qx_ctl pf_qx_ctl;
+
+	pf_qx_ctl.u = readq(cpt->reg_base + CPT_PF_QX_CTL(vf));
+	pf_qx_ctl.s.size = size;
+	pf_qx_ctl.s.cont_err = true;
+	writeq(pf_qx_ctl.u, cpt->reg_base + CPT_PF_QX_CTL(vf));
+}
+
+/*
+ * Configure VQ priority
+ */
+static void cpt_cfg_vq_priority(struct cpt_device *cpt, int vf, u32 pri)
+{
+	union cptx_pf_qx_ctl pf_qx_ctl;
+
+	pf_qx_ctl.u = readq(cpt->reg_base + CPT_PF_QX_CTL(vf));
+	pf_qx_ctl.s.pri = pri;
+	writeq(pf_qx_ctl.u, cpt->reg_base + CPT_PF_QX_CTL(vf));
+}
+
+static int cpt_bind_vq_to_grp(struct cpt_device *cpt, u8 q, u8 grp)
+{
+	struct device *dev = &cpt->pdev->dev;
+	struct engine_group_info *eng_grp;
+	union cptx_pf_qx_ctl pf_qx_ctl;
+	struct microcode *ucode;
+
+	if (q >= cpt->max_vfs) {
+		dev_err(dev, "Requested queue %d is > than maximum avail %d",
+			q, cpt->max_vfs);
+		return -EINVAL;
+	}
+
+	if (grp >= CPT_MAX_ENGINE_GROUPS) {
+		dev_err(dev, "Requested group %d is > than maximum avail %d",
+			grp, CPT_MAX_ENGINE_GROUPS);
+		return -EINVAL;
+	}
+
+	eng_grp = &cpt->eng_grps.grp[grp];
+	if (!eng_grp->is_enabled) {
+		dev_err(dev, "Requested engine group %d is disabled", grp);
+		return -EINVAL;
+	}
+
+	pf_qx_ctl.u = readq(cpt->reg_base + CPT_PF_QX_CTL(q));
+	pf_qx_ctl.s.grp = grp;
+	writeq(pf_qx_ctl.u, cpt->reg_base + CPT_PF_QX_CTL(q));
+
+	if (eng_grp->mirror.is_ena)
+		ucode = &eng_grp->g->grp[eng_grp->mirror.idx].ucode[0];
+	else
+		ucode = &eng_grp->ucode[0];
+
+	if (cpt_uc_supports_eng_type(ucode, SE_TYPES))
+		return SE_TYPES;
+	else if (cpt_uc_supports_eng_type(ucode, AE_TYPES))
+		return AE_TYPES;
+	else
+		return BAD_CPT_VF_TYPE;
+}
+
+/* Interrupt handler to handle mailbox messages from VFs */
+static void cpt_handle_mbox_intr(struct cpt_device *cpt, int vf)
+{
+	int vftype = 0;
+	struct cpt_mbox mbx = {};
+	struct device *dev = &cpt->pdev->dev;
+	/*
+	 * MBOX[0] contains msg
+	 * MBOX[1] contains data
+	 */
+	mbx.msg  = readq(cpt->reg_base + CPT_PF_VFX_MBOXX(vf, 0));
+	mbx.data = readq(cpt->reg_base + CPT_PF_VFX_MBOXX(vf, 1));
+
+	if (cpt_is_dbg_level_en(CPT_DBG_MBOX_MSGS))
+		dump_mbox_msg(&cpt->pdev->dev, &mbx, vf);
+
+	switch (mbx.msg) {
+	case CPT_MSG_VF_UP:
+		try_module_get(THIS_MODULE);
+		mbx.msg  = CPT_MSG_VF_UP;
+		mbx.data = cpt->vfs_enabled;
+		cpt_send_msg_to_vf(cpt, vf, &mbx);
+		break;
+	case CPT_MSG_READY:
+		mbx.msg  = CPT_MSG_READY;
+		mbx.data = vf;
+		cpt_send_msg_to_vf(cpt, vf, &mbx);
+		break;
+	case CPT_MSG_VF_DOWN:
+		/* First msg in VF teardown sequence */
+		module_put(THIS_MODULE);
+		cpt_mbox_send_ack(cpt, vf, &mbx);
+		break;
+	case CPT_MSG_QLEN:
+		cpt_cfg_qlen_for_vf(cpt, vf, mbx.data);
+		cpt_mbox_send_ack(cpt, vf, &mbx);
+		break;
+	case CPT_MSG_QBIND_GRP:
+		vftype = cpt_bind_vq_to_grp(cpt, vf, (u8)mbx.data);
+		if ((vftype != AE_TYPES) && (vftype != SE_TYPES)) {
+			dev_err(dev, "VF%d binding to eng group %llu failed",
+				vf, mbx.data);
+			cptpf_mbox_send_nack(cpt, vf, &mbx);
+		} else {
+			mbx.msg = CPT_MSG_QBIND_GRP;
+			mbx.data = vftype;
+			cpt_send_msg_to_vf(cpt, vf, &mbx);
+		}
+		break;
+	case CPT_MSG_PF_TYPE:
+		mbx.msg = CPT_MSG_PF_TYPE;
+		mbx.data = cpt->pf_type;
+		cpt_send_msg_to_vf(cpt, vf, &mbx);
+		break;
+	case CPT_MSG_VQ_PRIORITY:
+		cpt_cfg_vq_priority(cpt, vf, mbx.data);
+		cpt_mbox_send_ack(cpt, vf, &mbx);
+		break;
+	default:
+		dev_err(&cpt->pdev->dev, "Invalid msg from VF%d, msg 0x%llx\n",
+			vf, mbx.msg);
+		break;
+	}
+}
+
+void cpt_mbox_intr_handler (struct cpt_device *cpt, int mbx)
+{
+	u64 intr;
+	u8  vf;
+
+	intr = readq(cpt->reg_base + CPT_PF_MBOX_INTX(0));
+	if (cpt_is_dbg_level_en(CPT_DBG_MBOX_MSGS))
+		dev_info(&cpt->pdev->dev,
+			 "PF interrupt mbox%d mask 0x%llx\n", mbx, intr);
+	for (vf = 0; vf < cpt->max_vfs; vf++) {
+		if (intr & (1ULL << vf)) {
+			cpt_handle_mbox_intr(cpt, vf);
+			cpt_clear_mbox_intr(cpt, vf);
+		}
+	}
+}
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.c b/drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.c
new file mode 100644
index 000000000000..bdee26fedc7c
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.c
@@ -0,0 +1,178 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt_reqmgr.h"
+#include "cpt8x_vf.h"
+#include "cpt8x_reqmgr.h"
+
+inline void fill_cpt_inst(union cpt_inst_s *cptinst,
+			  struct cpt_info_buffer *info,
+			  struct cpt_iq_command *iq_cmd)
+{
+	cptinst->u[0] = 0x0;
+	cptinst->s8x.doneint = true;
+	cptinst->s8x.res_addr = (u64)info->comp_baddr;
+	cptinst->u[2] = 0x0;
+	cptinst->s8x.wq_ptr = 0;
+	cptinst->s8x.ei0 = iq_cmd->cmd.u64;
+	cptinst->s8x.ei1 = iq_cmd->dptr;
+	cptinst->s8x.ei2 = iq_cmd->rptr;
+	cptinst->s8x.ei3 = iq_cmd->cptr.u64;
+}
+
+inline int process_ccode(struct pci_dev *pdev, union cpt_res_s *cpt_status,
+			 struct cpt_info_buffer *cpt_info,
+			 struct cpt_request_info *req, u32 *res_code)
+{
+	u8 ccode = cpt_status->s8x.compcode;
+	union error_code ecode;
+
+	ecode.u = be64_to_cpu(*((u64 *) cpt_info->out_buffer));
+	switch (ccode) {
+	case CPT_8X_COMP_E_FAULT:
+		dev_err(&pdev->dev,
+			"Request failed with DMA fault\n");
+		dump_sg_list(pdev, req);
+	break;
+
+	case CPT_8X_COMP_E_SWERR:
+		dev_err(&pdev->dev,
+			"Request failed with software error code %d\n",
+			ecode.s.ccode);
+		dump_sg_list(pdev, req);
+	break;
+
+	case CPT_8X_COMP_E_HWERR:
+		dev_err(&pdev->dev,
+			"Request failed with hardware error\n");
+		dump_sg_list(pdev, req);
+	break;
+
+	case COMPLETION_CODE_INIT:
+		/* check for timeout */
+		if (time_after_eq(jiffies,
+				  (cpt_info->time_in +
+				  (CPT_COMMAND_TIMEOUT * HZ)))) {
+			dev_err(&pdev->dev, "Request timed out\n");
+		} else if ((ccode == (COMPLETION_CODE_INIT)) &&
+			   (cpt_info->extra_time <
+			    TIME_IN_RESET_COUNT)) {
+			cpt_info->time_in = jiffies;
+			cpt_info->extra_time++;
+			return 1;
+		}
+	break;
+
+	case CPT_8X_COMP_E_GOOD:
+		/* Check microcode completion code */
+		if (ecode.s.ccode) {
+			dev_err(&pdev->dev,
+				"Request failed with software error code 0x%x\n",
+				ecode.s.ccode);
+			dump_sg_list(pdev, req);
+			break;
+		}
+
+		/* Request has been processed with success */
+		*res_code = 0;
+	break;
+
+	default:
+		dev_err(&pdev->dev, "Request returned invalid status\n");
+	break;
+	}
+
+	return 0;
+}
+
+/*
+ * On 8X platform the parameter db_count is used as a count for ringing
+ * door bell. The valid values for db_count are:
+ * 0 - 1 CPT instruction will be enqueued however CPT will not be informed
+ * 1 - 1 CPT instruction will be enqueued and CPT will be informed
+ */
+inline void send_cpt_cmd(union cpt_inst_s *cptinst, u32 db_count, void *obj)
+{
+	struct cpt_vf *cptvf = (struct cpt_vf *) obj;
+	struct command_qinfo *qinfo = &cptvf->cqinfo;
+	struct command_queue *queue = &qinfo->queue[0];
+	u8 *ent;
+
+	/* lock commad queue */
+	spin_lock(&queue->lock);
+	ent = &queue->qhead->head[queue->idx * qinfo->cmd_size];
+	memcpy(ent, (void *) cptinst, qinfo->cmd_size);
+
+	if (++queue->idx >= queue->qhead->size / 64) {
+		struct command_chunk *curr = queue->qhead;
+
+		if (list_is_last(&curr->nextchunk, &queue->chead))
+			queue->qhead = queue->base;
+		else
+			queue->qhead = list_next_entry(queue->qhead, nextchunk);
+		queue->idx = 0;
+	}
+	/* make sure all memory stores are done before ringing doorbell */
+	smp_wmb();
+	cptvf_write_vq_doorbell(cptvf, db_count);
+	/* unlock command queue */
+	spin_unlock(&queue->lock);
+}
+
+inline void send_cpt_cmds_in_batch(union cpt_inst_s *cptinst, u32 num,
+				   void *obj)
+{
+	struct cpt_vf *cptvf = (struct cpt_vf *) obj;
+	int i;
+
+	for (i = 0; i < num; i++)
+		send_cpt_cmd(&cptinst[i], 0, obj);
+
+	cptvf_write_vq_doorbell(cptvf, num);
+}
+
+inline void send_cpt_cmds_for_speed_test(union cpt_inst_s *cptinst, u32 num,
+					 void *obj)
+{
+	send_cpt_cmds_in_batch(cptinst, num, obj);
+}
+
+inline int cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev)
+{
+	return 0;
+}
+
+inline void cptvf_post_process(struct cptvf_wqe *wqe)
+{
+	process_pending_queue(wqe->cptvf->pdev, &wqe->cptvf->pqinfo.queue[0]);
+}
+
+inline int cpt_do_request(struct pci_dev *pdev, struct cpt_request_info *req,
+		   int cpu_num)
+{
+	struct cpt_vf *cptvf = pci_get_drvdata(pdev);
+
+	if (!cpt_device_ready(cptvf)) {
+		dev_err(&pdev->dev, "CPT Device is not ready");
+		return -ENODEV;
+	}
+
+	if ((cptvf->vftype == SE_TYPES) && (!req->ctrl.s.se_req)) {
+		dev_err(&pdev->dev, "CPTVF-%d of SE TYPE got AE request",
+			cptvf->vfid);
+		return -EINVAL;
+	} else if ((cptvf->vftype == AE_TYPES) && (req->ctrl.s.se_req)) {
+		dev_err(&pdev->dev, "CPTVF-%d of AE TYPE got SE request",
+			cptvf->vfid);
+		return -EINVAL;
+	}
+
+	return process_request(pdev, req, &cptvf->pqinfo.queue[0], cptvf);
+}
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.h b/drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.h
new file mode 100644
index 000000000000..eae063318575
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_reqmgr.h
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT8X_REQUEST_MANAGER_H
+#define __CPT8X_REQUEST_MANAGER_H
+
+#include "cpt_common.h"
+
+void vq_post_process(struct cpt_vf *cptvf, u32 qno);
+
+#endif /* __CPT8X_REQUEST_MANAGER_H */
diff --git a/drivers/crypto/cavium/cpt/8x/cpt8x_ucode.c b/drivers/crypto/cavium/cpt/8x/cpt8x_ucode.c
new file mode 100644
index 000000000000..4d2ce34cee1a
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_ucode.c
@@ -0,0 +1,189 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt_ucode.h"
+#include "cpt8x_pf.h"
+
+static struct bitmap get_cores_bmap(struct device *dev,
+				    struct engine_group_info *eng_grp)
+{
+	struct bitmap bmap = { 0 };
+	bool found = false;
+	int i;
+
+	if (eng_grp->g->engs_num > CPT_8X_MAX_ENGINES) {
+		dev_err(dev, "8X plat unsupported number of engines %d",
+			eng_grp->g->engs_num);
+		return bmap;
+	}
+
+	for (i = 0; i  < MAX_ENGS_PER_GRP; i++)
+		if (eng_grp->engs[i].type) {
+			bitmap_or(bmap.bits, bmap.bits,
+				  eng_grp->engs[i].bmap,
+				  eng_grp->g->engs_num);
+			bmap.size = eng_grp->g->engs_num;
+			found = true;
+		}
+
+	if (!found)
+		dev_err(dev, "No engines reserved for engine group %d",
+			eng_grp->idx);
+	return bmap;
+}
+
+int cpt_detach_and_disable_cores(struct engine_group_info *eng_grp, void *obj)
+{
+	struct cpt_device *cpt = (struct cpt_device *) obj;
+	struct bitmap bmap = { 0 };
+	int timeout = 10;
+	int i, busy;
+	u64 reg;
+
+	bmap = get_cores_bmap(&cpt->pdev->dev, eng_grp);
+	if (!bmap.size)
+		return -EINVAL;
+
+	/* Detach the cores from group */
+	reg = readq(cpt->reg_base + CPT_PF_GX_EN(eng_grp->idx));
+	for_each_set_bit(i, bmap.bits, bmap.size) {
+		if (reg & (1ull << i)) {
+			eng_grp->g->eng_ref_cnt[i]--;
+			reg &= ~(1ull << i);
+		}
+	}
+	writeq(reg, cpt->reg_base + CPT_PF_GX_EN(eng_grp->idx));
+
+	/* Wait for cores to become idle */
+	do {
+		busy = 0;
+		usleep_range(10000, 20000);
+		if (timeout-- < 0)
+			return -EBUSY;
+
+		reg = readq(cpt->reg_base + CPT_PF_EXEC_BUSY);
+		for_each_set_bit(i, bmap.bits, bmap.size)
+			if (reg & (1ull << i)) {
+				busy = 1;
+				break;
+			}
+	} while (busy);
+
+	/* Disable the cores only if they are not used anymore */
+	reg = readq(cpt->reg_base + CPT_PF_EXE_CTL);
+	for_each_set_bit(i, bmap.bits, bmap.size)
+		if (!eng_grp->g->eng_ref_cnt[i])
+			reg &= ~(1ull << i);
+	writeq(reg, cpt->reg_base + CPT_PF_EXE_CTL);
+
+	return 0;
+}
+
+int cpt_set_ucode_base(struct engine_group_info *eng_grp, void *obj)
+{
+	struct cpt_device *cpt = (struct cpt_device *) obj;
+	dma_addr_t dma_addr;
+	struct bitmap bmap;
+	int i;
+
+	bmap = get_cores_bmap(&cpt->pdev->dev, eng_grp);
+	if (!bmap.size)
+		return -EINVAL;
+
+	if (eng_grp->mirror.is_ena)
+		dma_addr =
+		       eng_grp->g->grp[eng_grp->mirror.idx].ucode[0].align_dma;
+	else
+		dma_addr = eng_grp->ucode[0].align_dma;
+
+	/* Set UCODE_BASE only for the cores which are not used,
+	 * other cores should have already valid UCODE_BASE set
+	 */
+	for_each_set_bit(i, bmap.bits, bmap.size)
+		if (!eng_grp->g->eng_ref_cnt[i])
+			writeq((u64) dma_addr, cpt->reg_base +
+				CPT_PF_ENGX_UCODE_BASE(i));
+	return 0;
+}
+
+int cpt_attach_and_enable_cores(struct engine_group_info *eng_grp, void *obj)
+{
+	struct cpt_device *cpt = (struct cpt_device *) obj;
+	struct bitmap bmap;
+	u64 reg;
+	int i;
+
+	bmap = get_cores_bmap(&cpt->pdev->dev, eng_grp);
+	if (!bmap.size)
+		return -EINVAL;
+
+	/* Attach the cores to the group */
+	reg = readq(cpt->reg_base + CPT_PF_GX_EN(eng_grp->idx));
+	for_each_set_bit(i, bmap.bits, bmap.size) {
+		if (!(reg & (1ull << i))) {
+			eng_grp->g->eng_ref_cnt[i]++;
+			reg |= 1ull << i;
+		}
+	}
+	writeq(reg, cpt->reg_base + CPT_PF_GX_EN(eng_grp->idx));
+
+	/* Enable the cores */
+	reg = readq(cpt->reg_base + CPT_PF_EXE_CTL);
+	for_each_set_bit(i, bmap.bits, bmap.size)
+		reg |= 1ull << i;
+	writeq(reg, cpt->reg_base + CPT_PF_EXE_CTL);
+
+	return 0;
+}
+
+void cpt_print_engines_mask(struct engine_group_info *eng_grp, void *obj,
+			    char *buf, int size)
+{
+	struct cpt_device *cpt = (struct cpt_device *) obj;
+	struct bitmap bmap;
+	u32 mask[2];
+
+	bmap = get_cores_bmap(&cpt->pdev->dev, eng_grp);
+	if (!bmap.size) {
+		scnprintf(buf, size, "unknown");
+		return;
+	}
+
+	if (WARN_ON_ONCE(bmap.size > 2 * 32))
+		return;
+
+	bitmap_to_arr32(mask, bmap.bits, bmap.size);
+	scnprintf(buf, size, "%8.8x %8.8x", mask[1], mask[0]);
+}
+
+void cpt_disable_all_cores(struct cpt_device *cpt)
+{
+	u64 reg;
+	int grp, timeout = 100;
+
+	/* Disengage the cores from groups */
+	for (grp = 0; grp < CPT_MAX_ENGINE_GROUPS; grp++) {
+		writeq(0, cpt->reg_base + CPT_PF_GX_EN(grp));
+		udelay(CSR_DELAY);
+	}
+
+	reg = readq(cpt->reg_base + CPT_PF_EXEC_BUSY);
+	while (reg) {
+		udelay(CSR_DELAY);
+		reg = readq(cpt->reg_base + CPT_PF_EXEC_BUSY);
+		if (timeout--) {
+			dev_warn(&cpt->pdev->dev, "Cores still busy");
+			break;
+		}
+	}
+
+	/* Disable the cores */
+	writeq(0, cpt->reg_base + CPT_PF_EXE_CTL);
+}
diff --git a/drivers/crypto/cavium/cpt/cptvf.h b/drivers/crypto/cavium/cpt/8x/cpt8x_vf.h
similarity index 53%
rename from drivers/crypto/cavium/cpt/cptvf.h
rename to drivers/crypto/cavium/cpt/8x/cpt8x_vf.h
index 0a835a07d4f2..794b8252ee8f 100644
--- a/drivers/crypto/cavium/cpt/cptvf.h
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_vf.h
@@ -1,54 +1,34 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
  *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
  */
 
-#ifndef __CPTVF_H
-#define __CPTVF_H
+#ifndef __CPT8X_VF_H
+#define __CPT8X_VF_H
 
 #include <linux/list.h>
+#include <linux/interrupt.h>
 #include "cpt_common.h"
+#include "cpt_hw_types.h"
+#include "cpt8x_common.h"
 
 /* Default command queue length */
-#define CPT_CMD_QLEN 2046
+#define CPT_CMD_QLEN (4*2046)
 #define CPT_CMD_QCHUNK_SIZE 1023
-
-/* Default command timeout in seconds */
-#define CPT_COMMAND_TIMEOUT 4
-#define CPT_TIMER_THOLD	0xFFFF
 #define CPT_NUM_QS_PER_VF 1
-#define CPT_INST_SIZE 64
-#define CPT_NEXT_CHUNK_PTR_SIZE 8
-
-#define	CPT_VF_MSIX_VECTORS 2
-#define CPT_VF_INTR_MBOX_MASK BIT(0)
-#define CPT_VF_INTR_DOVF_MASK BIT(1)
-#define CPT_VF_INTR_IRDE_MASK BIT(2)
-#define CPT_VF_INTR_NWRP_MASK BIT(3)
-#define CPT_VF_INTR_SERR_MASK BIT(4)
-#define DMA_DIRECT_DIRECT 0 /* Input DIRECT, Output DIRECT */
-#define DMA_GATHER_SCATTER 1
-#define FROM_DPTR 1
-
-/**
- * Enumeration cpt_vf_int_vec_e
- *
- * CPT VF MSI-X Vector Enumeration
- * Enumerates the MSI-X interrupt vectors.
- */
-enum cpt_vf_int_vec_e {
-	CPT_VF_INT_VEC_E_MISC = 0x00,
-	CPT_VF_INT_VEC_E_DONE = 0x01
-};
 
 struct command_chunk {
 	u8 *head;
+	u8 *real_vaddr;
 	dma_addr_t dma_addr;
+	dma_addr_t real_dma_addr;
 	u32 size; /* Chunk size, max CPT_INST_CHUNK_MAX_SIZE */
-	struct hlist_node nextchunk;
+	struct list_head nextchunk;
 };
 
 struct command_queue {
@@ -58,7 +38,8 @@ struct command_queue {
 	struct command_chunk *qhead;	/* Command queue head, instructions
 					 * are inserted here
 					 */
-	struct hlist_head chead;
+	struct command_chunk *base;
+	struct list_head chead;
 };
 
 struct command_qinfo {
@@ -67,37 +48,29 @@ struct command_qinfo {
 	struct command_queue queue[CPT_NUM_QS_PER_VF];
 };
 
-struct pending_entry {
-	u8 busy; /* Entry status (free/busy) */
-
-	volatile u64 *completion_addr; /* Completion address */
-	void *post_arg;
-	void (*callback)(int, void *); /* Kernel ASYNC request callabck */
-	void *callback_arg; /* Kernel ASYNC request callabck arg */
-};
-
-struct pending_queue {
-	struct pending_entry *head;	/* head of the queue */
-	u32 front; /* Process work from here */
-	u32 rear; /* Append new work here */
-	atomic64_t pending_count;
-	spinlock_t lock; /* Queue lock */
-};
-
 struct pending_qinfo {
 	u32 nr_queues;	/* Number of queues supported */
-	u32 qlen; /* Queue length */
 	struct pending_queue queue[CPT_NUM_QS_PER_VF];
 };
 
 #define for_each_pending_queue(qinfo, q, i)	\
 	for (i = 0, q = &qinfo->queue[i]; i < qinfo->nr_queues; i++, \
-	     q = &qinfo->queue[i])
+	q = &qinfo->queue[i])
+
+struct cptvf_wqe {
+	struct tasklet_struct twork;
+	struct cpt_vf *cptvf;
+};
+
+struct cptvf_wqe_info {
+	struct cptvf_wqe vq_wqe[CPT_NUM_QS_PER_VF];
+};
 
 struct cpt_vf {
 	u16 flags; /* Flags to hold device status bits */
 	u8 vfid; /* Device Index 0...CPT_MAX_VF_NUM */
-	u8 vftype; /* VF type of SE_TYPE(1) or AE_TYPE(1) */
+	u8 num_vfs; /* Number of enabled VFs */
+	u8 vftype; /* VF type of SE_TYPE(2) or AE_TYPE(1) */
 	u8 vfgrp; /* VF group (0 - 8) */
 	u8 node; /* Operating node: Bits (46:44) in BAR0 address */
 	u8 priority; /* VF priority ring: 1-High proirity round
@@ -107,7 +80,7 @@ struct cpt_vf {
 	void __iomem *reg_base; /* Register start address */
 	void *wqe_info;	/* BH worker info */
 	/* MSI-X */
-	cpumask_var_t affinity_mask[CPT_VF_MSIX_VECTORS];
+	cpumask_var_t affinity_mask[CPT_8X_VF_MSIX_VECTORS];
 	/* Command and Pending queues */
 	u32 qsize;
 	u32 nr_queues;
@@ -120,13 +93,12 @@ struct cpt_vf {
 
 int cptvf_send_vf_up(struct cpt_vf *cptvf);
 int cptvf_send_vf_down(struct cpt_vf *cptvf);
-int cptvf_send_vf_to_grp_msg(struct cpt_vf *cptvf);
+int cptvf_send_vf_to_grp_msg(struct cpt_vf *cptvf, int group);
 int cptvf_send_vf_priority_msg(struct cpt_vf *cptvf);
 int cptvf_send_vq_size_msg(struct cpt_vf *cptvf);
 int cptvf_check_pf_ready(struct cpt_vf *cptvf);
 void cptvf_handle_mbox_intr(struct cpt_vf *cptvf);
-void cvm_crypto_exit(void);
-int cvm_crypto_init(struct cpt_vf *cptvf);
-void vq_post_process(struct cpt_vf *cptvf, u32 qno);
+void cptvf_post_process(struct cptvf_wqe *wqe);
 void cptvf_write_vq_doorbell(struct cpt_vf *cptvf, u32 val);
-#endif /* __CPTVF_H */
+
+#endif /* __CPT8X_VF_H */
diff --git a/drivers/crypto/cavium/cpt/cptvf_main.c b/drivers/crypto/cavium/cpt/8x/cpt8x_vf_main.c
similarity index 63%
rename from drivers/crypto/cavium/cpt/cptvf_main.c
rename to drivers/crypto/cavium/cpt/8x/cpt8x_vf_main.c
index 5c796ed55eba..9e16c18d6fae 100644
--- a/drivers/crypto/cavium/cpt/cptvf_main.c
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_vf_main.c
@@ -1,35 +1,27 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
  *
  * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
  */
 
 #include <linux/interrupt.h>
 #include <linux/module.h>
+#include "cpt8x_vf.h"
 
-#include "cptvf.h"
-
-#define DRV_NAME	"thunder-cptvf"
+#define DRV_NAME	"octeontx-cptvf"
 #define DRV_VERSION	"1.0"
 
-struct cptvf_wqe {
-	struct tasklet_struct twork;
-	void *cptvf;
-	u32 qno;
-};
-
-struct cptvf_wqe_info {
-	struct cptvf_wqe vq_wqe[CPT_NUM_QS_PER_VF];
-};
+DEFINE_CPT_DEBUG_PARM(debug);
 
 static void vq_work_handler(unsigned long data)
 {
-	struct cptvf_wqe_info *cwqe_info = (struct cptvf_wqe_info *)data;
-	struct cptvf_wqe *cwqe = &cwqe_info->vq_wqe[0];
+	struct cptvf_wqe_info *cwqe_info = (struct cptvf_wqe_info *) data;
 
-	vq_post_process(cwqe->cptvf, cwqe->qno);
+	cptvf_post_process(&cwqe_info->vq_wqe[0]);
 }
 
 static int init_worker_threads(struct cpt_vf *cptvf)
@@ -43,14 +35,13 @@ static int init_worker_threads(struct cpt_vf *cptvf)
 		return -ENOMEM;
 
 	if (cptvf->nr_queues) {
-		dev_info(&pdev->dev, "Creating VQ worker threads (%d)\n",
-			 cptvf->nr_queues);
+		dev_dbg(&pdev->dev, "Creating VQ worker threads (%d)\n",
+			cptvf->nr_queues);
 	}
 
 	for (i = 0; i < cptvf->nr_queues; i++) {
 		tasklet_init(&cwqe_info->vq_wqe[i].twork, vq_work_handler,
 			     (u64)cwqe_info);
-		cwqe_info->vq_wqe[i].qno = i;
 		cwqe_info->vq_wqe[i].cptvf = cptvf;
 	}
 
@@ -70,8 +61,8 @@ static void cleanup_worker_threads(struct cpt_vf *cptvf)
 		return;
 
 	if (cptvf->nr_queues) {
-		dev_info(&pdev->dev, "Cleaning VQ worker threads (%u)\n",
-			 cptvf->nr_queues);
+		dev_dbg(&pdev->dev, "Cleaning VQ worker threads (%u)\n",
+			cptvf->nr_queues);
 	}
 
 	for (i = 0; i < cptvf->nr_queues; i++)
@@ -92,14 +83,12 @@ static void free_pending_queues(struct pending_qinfo *pqinfo)
 
 		/* free single queue */
 		kzfree((queue->head));
-
 		queue->front = 0;
 		queue->rear = 0;
-
+		queue->qlen = 0;
 		return;
 	}
 
-	pqinfo->qlen = 0;
 	pqinfo->nr_queues = 0;
 }
 
@@ -112,8 +101,6 @@ static int alloc_pending_queues(struct pending_qinfo *pqinfo, u32 qlen,
 	struct pending_queue *queue = NULL;
 
 	pqinfo->nr_queues = nr_queues;
-	pqinfo->qlen = qlen;
-
 	size = (qlen * sizeof(struct pending_entry));
 
 	for_each_pending_queue(pqinfo, queue, i) {
@@ -123,9 +110,10 @@ static int alloc_pending_queues(struct pending_qinfo *pqinfo, u32 qlen,
 			goto pending_qfail;
 		}
 
+		queue->pending_count = 0;
 		queue->front = 0;
 		queue->rear = 0;
-		atomic64_set((&queue->pending_count), (0));
+		queue->qlen = qlen;
 
 		/* init queue spin lock */
 		spin_lock_init(&queue->lock);
@@ -164,8 +152,8 @@ static void cleanup_pending_queues(struct cpt_vf *cptvf)
 	if (!cptvf->nr_queues)
 		return;
 
-	dev_info(&pdev->dev, "Cleaning VQ pending queue (%u)\n",
-		 cptvf->nr_queues);
+	dev_dbg(&pdev->dev, "Cleaning VQ pending queue (%u)\n",
+		cptvf->nr_queues);
 	free_pending_queues(&cptvf->pqinfo);
 }
 
@@ -176,29 +164,29 @@ static void free_command_queues(struct cpt_vf *cptvf,
 	struct command_queue *queue = NULL;
 	struct command_chunk *chunk = NULL;
 	struct pci_dev *pdev = cptvf->pdev;
-	struct hlist_node *node;
 
 	/* clean up for each queue */
 	for (i = 0; i < cptvf->nr_queues; i++) {
 		queue = &cqinfo->queue[i];
-		if (hlist_empty(&cqinfo->queue[i].chead))
-			continue;
 
-		hlist_for_each_entry_safe(chunk, node, &cqinfo->queue[i].chead,
-					  nextchunk) {
+		while (!list_empty(&cqinfo->queue[i].chead)) {
+			chunk = list_first_entry(&cqinfo->queue[i].chead,
+					struct command_chunk, nextchunk);
+
 			dma_free_coherent(&pdev->dev, chunk->size,
-					  chunk->head,
-					  chunk->dma_addr);
+					  chunk->real_vaddr,
+					  chunk->real_dma_addr);
+			chunk->real_vaddr = NULL;
+			chunk->real_dma_addr = 0;
 			chunk->head = NULL;
 			chunk->dma_addr = 0;
-			hlist_del(&chunk->nextchunk);
+			list_del(&chunk->nextchunk);
 			kzfree(chunk);
 		}
-
 		queue->nchunks = 0;
 		queue->idx = 0;
-	}
 
+	}
 	/* common cleanup */
 	cqinfo->cmd_size = 0;
 }
@@ -211,6 +199,7 @@ static int alloc_command_queues(struct cpt_vf *cptvf,
 	size_t q_size;
 	struct command_queue *queue = NULL;
 	struct pci_dev *pdev = cptvf->pdev;
+	int align =  CPT_INST_Q_ALIGNMENT;
 
 	/* common init */
 	cqinfo->cmd_size = cmd_size;
@@ -228,7 +217,7 @@ static int alloc_command_queues(struct cpt_vf *cptvf,
 		u32 qcsize_bytes = cqinfo->qchunksize * cqinfo->cmd_size;
 
 		queue = &cqinfo->queue[i];
-		INIT_HLIST_HEAD(&cqinfo->queue[i].chead);
+		INIT_LIST_HEAD(&cqinfo->queue[i].chead);
 		do {
 			curr = kzalloc(sizeof(*curr), GFP_KERNEL);
 			if (!curr)
@@ -236,30 +225,33 @@ static int alloc_command_queues(struct cpt_vf *cptvf,
 
 			c_size = (rem_q_size > qcsize_bytes) ? qcsize_bytes :
 					rem_q_size;
-			curr->head = (u8 *)dma_zalloc_coherent(&pdev->dev,
-					  c_size + CPT_NEXT_CHUNK_PTR_SIZE,
-					  &curr->dma_addr, GFP_KERNEL);
-			if (!curr->head) {
+			curr->real_vaddr = (u8 *)dma_zalloc_coherent(&pdev->dev,
+				c_size + align + CPT_NEXT_CHUNK_PTR_SIZE,
+				&curr->real_dma_addr, GFP_KERNEL);
+			if (!curr->real_vaddr) {
 				dev_err(&pdev->dev, "Command Q (%d) chunk (%d) allocation failed\n",
 					i, queue->nchunks);
 				kfree(curr);
 				goto cmd_qfail;
 			}
-
+			curr->head = (uint8_t *) PTR_ALIGN(curr->real_vaddr,
+							   align);
+			curr->dma_addr =
+			    (dma_addr_t) PTR_ALIGN(curr->real_dma_addr, align);
 			curr->size = c_size;
+
 			if (queue->nchunks == 0) {
-				hlist_add_head(&curr->nextchunk,
-					       &cqinfo->queue[i].chead);
 				first = curr;
-			} else {
-				hlist_add_behind(&curr->nextchunk,
-						 &last->nextchunk);
+				queue->base  = first;
 			}
+			list_add_tail(&curr->nextchunk,
+				      &cqinfo->queue[i].chead);
 
 			queue->nchunks++;
 			rem_q_size -= c_size;
 			if (last)
-				*((u64 *)(&last->head[last->size])) = (u64)curr->dma_addr;
+				*((u64 *)(&last->head[last->size])) =
+					(u64)curr->dma_addr;
 
 			last = curr;
 		} while (rem_q_size);
@@ -283,11 +275,11 @@ static int init_command_queues(struct cpt_vf *cptvf, u32 qlen)
 	struct pci_dev *pdev = cptvf->pdev;
 	int ret;
 
-	/* setup AE command queues */
+	/* setup command queues */
 	ret = alloc_command_queues(cptvf, &cptvf->cqinfo, CPT_INST_SIZE,
 				   qlen);
 	if (ret) {
-		dev_err(&pdev->dev, "failed to allocate AE command queues (%u)\n",
+		dev_err(&pdev->dev, "Failed to allocate command queues (%u)\n",
 			cptvf->nr_queues);
 		return ret;
 	}
@@ -302,8 +294,8 @@ static void cleanup_command_queues(struct cpt_vf *cptvf)
 	if (!cptvf->nr_queues)
 		return;
 
-	dev_info(&pdev->dev, "Cleaning VQ command queue (%u)\n",
-		 cptvf->nr_queues);
+	dev_dbg(&pdev->dev, "Cleaning VQ command queue (%u)\n",
+		cptvf->nr_queues);
 	free_command_queues(cptvf, &cptvf->cqinfo);
 }
 
@@ -368,152 +360,147 @@ static void cptvf_write_vq_ctl(struct cpt_vf *cptvf, bool val)
 {
 	union cptx_vqx_ctl vqx_ctl;
 
-	vqx_ctl.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_CTL(0, 0));
+	vqx_ctl.u = readq(cptvf->reg_base + CPT_VQX_CTL(0));
 	vqx_ctl.s.ena = val;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_CTL(0, 0), vqx_ctl.u);
+	writeq(vqx_ctl.u, cptvf->reg_base + CPT_VQX_CTL(0));
 }
 
 void cptvf_write_vq_doorbell(struct cpt_vf *cptvf, u32 val)
 {
 	union cptx_vqx_doorbell vqx_dbell;
 
-	vqx_dbell.u = cpt_read_csr64(cptvf->reg_base,
-				     CPTX_VQX_DOORBELL(0, 0));
+	vqx_dbell.u = readq(cptvf->reg_base + CPT_VQX_DOORBELL(0));
 	vqx_dbell.s.dbell_cnt = val * 8; /* Num of Instructions * 8 words */
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_DOORBELL(0, 0),
-			vqx_dbell.u);
+	writeq(vqx_dbell.u, cptvf->reg_base + CPT_VQX_DOORBELL(0));
 }
 
 static void cptvf_write_vq_inprog(struct cpt_vf *cptvf, u8 val)
 {
 	union cptx_vqx_inprog vqx_inprg;
 
-	vqx_inprg.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_INPROG(0, 0));
+	vqx_inprg.u = readq(cptvf->reg_base + CPT_VQX_INPROG(0));
 	vqx_inprg.s.inflight = val;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_INPROG(0, 0), vqx_inprg.u);
+	writeq(vqx_inprg.u, cptvf->reg_base + CPT_VQX_INPROG(0));
 }
 
 static void cptvf_write_vq_done_numwait(struct cpt_vf *cptvf, u32 val)
 {
 	union cptx_vqx_done_wait vqx_dwait;
 
-	vqx_dwait.u = cpt_read_csr64(cptvf->reg_base,
-				     CPTX_VQX_DONE_WAIT(0, 0));
+	vqx_dwait.u = readq(cptvf->reg_base + CPT_VQX_DONE_WAIT(0));
 	vqx_dwait.s.num_wait = val;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_WAIT(0, 0),
-			vqx_dwait.u);
+	writeq(vqx_dwait.u, cptvf->reg_base + CPT_VQX_DONE_WAIT(0));
+}
+
+static u32 cptvf_read_vq_done_numwait(struct cpt_vf *cptvf)
+{
+	union cptx_vqx_done_wait vqx_dwait;
+
+	vqx_dwait.u = readq(cptvf->reg_base + CPT_VQX_DONE_WAIT(0));
+	return vqx_dwait.s.num_wait;
 }
 
 static void cptvf_write_vq_done_timewait(struct cpt_vf *cptvf, u16 time)
 {
 	union cptx_vqx_done_wait vqx_dwait;
 
-	vqx_dwait.u = cpt_read_csr64(cptvf->reg_base,
-				     CPTX_VQX_DONE_WAIT(0, 0));
+	vqx_dwait.u = readq(cptvf->reg_base + CPT_VQX_DONE_WAIT(0));
 	vqx_dwait.s.time_wait = time;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_WAIT(0, 0),
-			vqx_dwait.u);
+	writeq(vqx_dwait.u, cptvf->reg_base + CPT_VQX_DONE_WAIT(0));
+}
+
+
+static u16 cptvf_read_vq_done_timewait(struct cpt_vf *cptvf)
+{
+	union cptx_vqx_done_wait vqx_dwait;
+
+	vqx_dwait.u = readq(cptvf->reg_base + CPT_VQX_DONE_WAIT(0));
+	return vqx_dwait.s.time_wait;
 }
 
 static void cptvf_enable_swerr_interrupts(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_ena_w1s vqx_misc_ena;
 
-	vqx_misc_ena.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_ENA_W1S(0, 0));
+	vqx_misc_ena.u = readq(cptvf->reg_base + CPT_VQX_MISC_ENA_W1S(0));
 	/* Set mbox(0) interupts for the requested vf */
 	vqx_misc_ena.s.swerr = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_ENA_W1S(0, 0),
-			vqx_misc_ena.u);
+	writeq(vqx_misc_ena.u, cptvf->reg_base + CPT_VQX_MISC_ENA_W1S(0));
 }
 
 static void cptvf_enable_mbox_interrupts(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_ena_w1s vqx_misc_ena;
 
-	vqx_misc_ena.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_ENA_W1S(0, 0));
+	vqx_misc_ena.u = readq(cptvf->reg_base + CPT_VQX_MISC_ENA_W1S(0));
 	/* Set mbox(0) interupts for the requested vf */
 	vqx_misc_ena.s.mbox = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_ENA_W1S(0, 0),
-			vqx_misc_ena.u);
+	writeq(vqx_misc_ena.u, cptvf->reg_base + CPT_VQX_MISC_ENA_W1S(0));
 }
 
 static void cptvf_enable_done_interrupts(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_done_ena_w1s vqx_done_ena;
 
-	vqx_done_ena.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_DONE_ENA_W1S(0, 0));
+	vqx_done_ena.u = readq(cptvf->reg_base + CPT_VQX_DONE_ENA_W1S(0));
 	/* Set DONE interrupt for the requested vf */
 	vqx_done_ena.s.done = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_ENA_W1S(0, 0),
-			vqx_done_ena.u);
+	writeq(vqx_done_ena.u, cptvf->reg_base + CPT_VQX_DONE_ENA_W1S(0));
 }
 
 static void cptvf_clear_dovf_intr(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_int vqx_misc_int;
 
-	vqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_INT(0, 0));
+	vqx_misc_int.u = readq(cptvf->reg_base + CPT_VQX_MISC_INT(0));
 	/* W1C for the VF */
 	vqx_misc_int.s.dovf = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),
-			vqx_misc_int.u);
+	writeq(vqx_misc_int.u, cptvf->reg_base + CPT_VQX_MISC_INT(0));
 }
 
 static void cptvf_clear_irde_intr(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_int vqx_misc_int;
 
-	vqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_INT(0, 0));
+	vqx_misc_int.u = readq(cptvf->reg_base + CPT_VQX_MISC_INT(0));
 	/* W1C for the VF */
 	vqx_misc_int.s.irde = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),
-			vqx_misc_int.u);
+	writeq(vqx_misc_int.u, cptvf->reg_base + CPT_VQX_MISC_INT(0));
 }
 
 static void cptvf_clear_nwrp_intr(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_int vqx_misc_int;
 
-	vqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_INT(0, 0));
+	vqx_misc_int.u = readq(cptvf->reg_base + CPT_VQX_MISC_INT(0));
 	/* W1C for the VF */
 	vqx_misc_int.s.nwrp = 1;
-	cpt_write_csr64(cptvf->reg_base,
-			CPTX_VQX_MISC_INT(0, 0), vqx_misc_int.u);
+	writeq(vqx_misc_int.u, cptvf->reg_base + CPT_VQX_MISC_INT(0));
 }
 
 static void cptvf_clear_mbox_intr(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_int vqx_misc_int;
 
-	vqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_INT(0, 0));
+	vqx_misc_int.u = readq(cptvf->reg_base + CPT_VQX_MISC_INT(0));
 	/* W1C for the VF */
 	vqx_misc_int.s.mbox = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),
-			vqx_misc_int.u);
+	writeq(vqx_misc_int.u, cptvf->reg_base + CPT_VQX_MISC_INT(0));
 }
 
 static void cptvf_clear_swerr_intr(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_misc_int vqx_misc_int;
 
-	vqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_MISC_INT(0, 0));
+	vqx_misc_int.u = readq(cptvf->reg_base + CPT_VQX_MISC_INT(0));
 	/* W1C for the VF */
 	vqx_misc_int.s.swerr = 1;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),
-			vqx_misc_int.u);
+	writeq(vqx_misc_int.u, cptvf->reg_base + CPT_VQX_MISC_INT(0));
 }
 
 static u64 cptvf_read_vf_misc_intr_status(struct cpt_vf *cptvf)
 {
-	return cpt_read_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0));
+	return readq(cptvf->reg_base + CPT_VQX_MISC_INT(0));
 }
 
 static irqreturn_t cptvf_misc_intr_handler(int irq, void *cptvf_irq)
@@ -524,26 +511,26 @@ static irqreturn_t cptvf_misc_intr_handler(int irq, void *cptvf_irq)
 
 	intr = cptvf_read_vf_misc_intr_status(cptvf);
 	/*Check for MISC interrupt types*/
-	if (likely(intr & CPT_VF_INTR_MBOX_MASK)) {
+	if (likely(intr & CPT_8X_VF_INTR_MBOX_MASK)) {
 		dev_dbg(&pdev->dev, "Mailbox interrupt 0x%llx on CPT VF %d\n",
 			intr, cptvf->vfid);
 		cptvf_handle_mbox_intr(cptvf);
 		cptvf_clear_mbox_intr(cptvf);
-	} else if (unlikely(intr & CPT_VF_INTR_DOVF_MASK)) {
+	} else if (unlikely(intr & CPT_8X_VF_INTR_DOVF_MASK)) {
 		cptvf_clear_dovf_intr(cptvf);
 		/*Clear doorbell count*/
 		cptvf_write_vq_doorbell(cptvf, 0);
 		dev_err(&pdev->dev, "Doorbell overflow error interrupt 0x%llx on CPT VF %d\n",
 			intr, cptvf->vfid);
-	} else if (unlikely(intr & CPT_VF_INTR_IRDE_MASK)) {
+	} else if (unlikely(intr & CPT_8X_VF_INTR_IRDE_MASK)) {
 		cptvf_clear_irde_intr(cptvf);
 		dev_err(&pdev->dev, "Instruction NCB read error interrupt 0x%llx on CPT VF %d\n",
 			intr, cptvf->vfid);
-	} else if (unlikely(intr & CPT_VF_INTR_NWRP_MASK)) {
+	} else if (unlikely(intr & CPT_8X_VF_INTR_NWRP_MASK)) {
 		cptvf_clear_nwrp_intr(cptvf);
 		dev_err(&pdev->dev, "NCB response write error interrupt 0x%llx on CPT VF %d\n",
 			intr, cptvf->vfid);
-	} else if (unlikely(intr & CPT_VF_INTR_SERR_MASK)) {
+	} else if (unlikely(intr & CPT_8X_VF_INTR_SERR_MASK)) {
 		cptvf_clear_swerr_intr(cptvf);
 		dev_err(&pdev->dev, "Software error interrupt 0x%llx on CPT VF %d\n",
 			intr, cptvf->vfid);
@@ -571,7 +558,7 @@ static inline u32 cptvf_read_vq_done_count(struct cpt_vf *cptvf)
 {
 	union cptx_vqx_done vqx_done;
 
-	vqx_done.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_DONE(0, 0));
+	vqx_done.u = readq(cptvf->reg_base + CPT_VQX_DONE(0));
 	return vqx_done.s.done;
 }
 
@@ -580,11 +567,9 @@ static inline void cptvf_write_vq_done_ack(struct cpt_vf *cptvf,
 {
 	union cptx_vqx_done_ack vqx_dack_cnt;
 
-	vqx_dack_cnt.u = cpt_read_csr64(cptvf->reg_base,
-					CPTX_VQX_DONE_ACK(0, 0));
+	vqx_dack_cnt.u = readq(cptvf->reg_base + CPT_VQX_DONE_ACK(0));
 	vqx_dack_cnt.s.done_ack = ackcnt;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_ACK(0, 0),
-			vqx_dack_cnt.u);
+	writeq(vqx_dack_cnt.u, cptvf->reg_base + CPT_VQX_DONE_ACK(0));
 }
 
 static irqreturn_t cptvf_done_intr_handler(int irq, void *cptvf_irq)
@@ -637,7 +622,7 @@ static void cptvf_write_vq_saddr(struct cpt_vf *cptvf, u64 val)
 	union cptx_vqx_saddr vqx_saddr;
 
 	vqx_saddr.u = val;
-	cpt_write_csr64(cptvf->reg_base, CPTX_VQX_SADDR(0, 0), vqx_saddr.u);
+	writeq(vqx_saddr.u, cptvf->reg_base + CPT_VQX_SADDR(0));
 }
 
 void cptvf_device_init(struct cpt_vf *cptvf)
@@ -655,19 +640,158 @@ void cptvf_device_init(struct cpt_vf *cptvf)
 	base_addr = (u64)(cptvf->cqinfo.queue[0].qhead->dma_addr);
 	cptvf_write_vq_saddr(cptvf, base_addr);
 	/* Configure timerhold / coalescence */
-	cptvf_write_vq_done_timewait(cptvf, CPT_TIMER_THOLD);
-	cptvf_write_vq_done_numwait(cptvf, 1);
+	cptvf_write_vq_done_timewait(cptvf, CPT_TIMER_HOLD);
+	cptvf_write_vq_done_numwait(cptvf, CPT_COUNT_HOLD);
 	/* Enable the VQ */
 	cptvf_write_vq_ctl(cptvf, 1);
 	/* Flag the VF ready */
 	cptvf->flags |= CPT_FLAG_DEVICE_READY;
 }
 
+static ssize_t cptvf_type_show(struct device *dev,
+			       struct device_attribute *attr,
+			       char *buf)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+	char *msg;
+
+	switch (cptvf->vftype) {
+	case AE_TYPES:
+		msg = "AE";
+	break;
+
+	case SE_TYPES:
+		msg = "SE";
+	break;
+
+	default:
+		msg = "Invalid";
+	}
+
+	return scnprintf(buf, PAGE_SIZE, "%s\n", msg);
+}
+
+static ssize_t cptvf_engine_group_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", cptvf->vfgrp);
+}
+
+static ssize_t cptvf_engine_group_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t count)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+	int val, ret;
+
+	ret = kstrtoint(buf, 10, &val);
+	if (ret)
+		return ret;
+
+	if (val < 0)
+		return -EINVAL;
+
+	if (val >= CPT_MAX_ENGINE_GROUPS) {
+		dev_err(dev, "Engine group >= than max available groups %d",
+			CPT_MAX_ENGINE_GROUPS);
+		return -EINVAL;
+	}
+
+	ret = cptvf_send_vf_to_grp_msg(cptvf, val);
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t cptvf_coalesc_time_wait_show(struct device *dev,
+					    struct device_attribute *attr,
+					    char *buf)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+			 cptvf_read_vq_done_timewait(cptvf));
+}
+
+static ssize_t cptvf_coalesc_num_wait_show(struct device *dev,
+					   struct device_attribute *attr,
+					   char *buf)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+			 cptvf_read_vq_done_numwait(cptvf));
+}
+
+static ssize_t cptvf_coalesc_time_wait_store(struct device *dev,
+					     struct device_attribute *attr,
+					     const char *buf, size_t count)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+	long int val;
+	int ret;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret != 0)
+		return ret;
+
+	if (val < CPT_COALESC_MIN_TIME_WAIT ||
+	    val > CPT_COALESC_MAX_TIME_WAIT)
+		return -EINVAL;
+
+	cptvf_write_vq_done_timewait(cptvf, val);
+	return count;
+}
+
+static ssize_t cptvf_coalesc_num_wait_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf, size_t count)
+{
+	struct cpt_vf *cptvf = dev_get_drvdata(dev);
+	long int val;
+	int ret;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret != 0)
+		return ret;
+
+	if (val < CPT_COALESC_MIN_NUM_WAIT ||
+	    val > CPT_COALESC_MAX_NUM_WAIT)
+		return -EINVAL;
+
+	cptvf_write_vq_done_numwait(cptvf, val);
+	return count;
+}
+
+static DEVICE_ATTR(vf_type, 0444, cptvf_type_show, NULL);
+static DEVICE_ATTR(vf_engine_group, 0664, cptvf_engine_group_show,
+				   cptvf_engine_group_store);
+static DEVICE_ATTR(vf_coalesc_time_wait, 0664,
+		   cptvf_coalesc_time_wait_show, cptvf_coalesc_time_wait_store);
+static DEVICE_ATTR(vf_coalesc_num_wait, 0664,
+		   cptvf_coalesc_num_wait_show, cptvf_coalesc_num_wait_store);
+
+static struct attribute *vf_attrs[] = {
+	&dev_attr_vf_type.attr,
+	&dev_attr_vf_engine_group.attr,
+	&dev_attr_vf_coalesc_time_wait.attr,
+	&dev_attr_vf_coalesc_num_wait.attr,
+	NULL
+};
+
+static const struct attribute_group vf_sysfs_group = {
+	.attrs = vf_attrs,
+};
+
 static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
 	struct device *dev = &pdev->dev;
 	struct cpt_vf *cptvf;
-	int    err;
+	int err;
 
 	cptvf = devm_kzalloc(dev, sizeof(*cptvf), GFP_KERNEL);
 	if (!cptvf)
@@ -675,6 +799,8 @@ static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	pci_set_drvdata(pdev, cptvf);
 	cptvf->pdev = pdev;
+	cpt_set_dbg_level(debug);
+
 	err = pci_enable_device(pdev);
 	if (err) {
 		dev_err(dev, "Failed to enable PCI device\n");
@@ -687,6 +813,7 @@ static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		dev_err(dev, "PCI request regions failed 0x%x\n", err);
 		goto cptvf_err_disable_device;
 	}
+
 	/* Mark as VF driver */
 	cptvf->flags |= CPT_FLAG_VF_DRIVER;
 	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
@@ -702,7 +829,7 @@ static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	}
 
 	/* MAP PF's configuration registers */
-	cptvf->reg_base = pcim_iomap(pdev, 0, 0);
+	cptvf->reg_base = pcim_iomap(pdev, PCI_CPT_VF_8X_CFG_BAR, 0);
 	if (!cptvf->reg_base) {
 		dev_err(dev, "Cannot map config register space, aborting\n");
 		err = -ENOMEM;
@@ -710,15 +837,15 @@ static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	}
 
 	cptvf->node = dev_to_node(&pdev->dev);
-	err = pci_alloc_irq_vectors(pdev, CPT_VF_MSIX_VECTORS,
-			CPT_VF_MSIX_VECTORS, PCI_IRQ_MSIX);
+	err = pci_alloc_irq_vectors(pdev, CPT_8X_VF_MSIX_VECTORS,
+			CPT_8X_VF_MSIX_VECTORS, PCI_IRQ_MSIX);
 	if (err < 0) {
 		dev_err(dev, "Request for #%d msix vectors failed\n",
-			CPT_VF_MSIX_VECTORS);
+			CPT_8X_VF_MSIX_VECTORS);
 		goto cptvf_err_release_regions;
 	}
 
-	err = request_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC),
+	err = request_irq(pci_irq_vector(pdev, CPT_8X_VF_INT_VEC_E_MISC),
 			  cptvf_misc_intr_handler, 0, "CPT VF misc intr",
 			  cptvf);
 	if (err) {
@@ -755,8 +882,7 @@ static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	/* CPT VF device initialization */
 	cptvf_device_init(cptvf);
 	/* Send msg to PF to assign currnet Q to required group */
-	cptvf->vfgrp = 1;
-	err = cptvf_send_vf_to_grp_msg(cptvf);
+	err = cptvf_send_vf_to_grp_msg(cptvf, cptvf->vfgrp);
 	if (err) {
 		dev_err(dev, "PF not responding to VF_GRP msg");
 		goto cptvf_free_misc_irq;
@@ -769,38 +895,49 @@ static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		goto cptvf_free_misc_irq;
 	}
 
-	err = request_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_DONE),
+	err = request_irq(pci_irq_vector(pdev, CPT_8X_VF_INT_VEC_E_DONE),
 			  cptvf_done_intr_handler, 0, "CPT VF done intr",
 			  cptvf);
 	if (err) {
 		dev_err(dev, "Request done irq failed\n");
-		goto cptvf_free_misc_irq;
+		goto cptvf_free_done_irq;
 	}
 
-	/* Enable mailbox interrupt */
+	/* Enable done interrupt */
 	cptvf_enable_done_interrupts(cptvf);
 
 	/* Set irq affinity masks */
-	cptvf_set_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);
-	cptvf_set_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);
+	cptvf_set_irq_affinity(cptvf, CPT_8X_VF_INT_VEC_E_MISC);
+	cptvf_set_irq_affinity(cptvf, CPT_8X_VF_INT_VEC_E_DONE);
 
 	err = cptvf_send_vf_up(cptvf);
 	if (err) {
 		dev_err(dev, "PF not responding to UP msg");
 		goto cptvf_free_irq_affinity;
 	}
-	err = cvm_crypto_init(cptvf);
+
+	err = cvm_crypto_init(pdev, cptvf->vftype == SE_TYPES ? CPT_SE_83XX :
+			      CPT_AE_83XX, cptvf->vftype, 1, cptvf->num_vfs);
 	if (err) {
 		dev_err(dev, "Algorithm register failed\n");
 		goto cptvf_free_irq_affinity;
 	}
+
+	err = sysfs_create_group(&dev->kobj, &vf_sysfs_group);
+	if (err) {
+		dev_err(dev, "Creating sysfs entries failed\n");
+		goto cptvf_free_irq_affinity;
+	}
+
 	return 0;
 
 cptvf_free_irq_affinity:
-	cptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);
-	cptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);
+	cptvf_free_irq_affinity(cptvf, CPT_8X_VF_INT_VEC_E_DONE);
+	cptvf_free_irq_affinity(cptvf, CPT_8X_VF_INT_VEC_E_MISC);
+cptvf_free_done_irq:
+	free_irq(pci_irq_vector(pdev, CPT_8X_VF_INT_VEC_E_DONE), cptvf);
 cptvf_free_misc_irq:
-	free_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC), cptvf);
+	free_irq(pci_irq_vector(pdev, CPT_8X_VF_INT_VEC_E_MISC), cptvf);
 cptvf_free_vectors:
 	pci_free_irq_vectors(cptvf->pdev);
 cptvf_err_release_regions:
@@ -825,27 +962,23 @@ static void cptvf_remove(struct pci_dev *pdev)
 	if (cptvf_send_vf_down(cptvf)) {
 		dev_err(&pdev->dev, "PF not responding to DOWN msg");
 	} else {
-		cptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);
-		cptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);
-		free_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_DONE), cptvf);
-		free_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC), cptvf);
+		cvm_crypto_exit(pdev);
+		cptvf_free_irq_affinity(cptvf, CPT_8X_VF_INT_VEC_E_DONE);
+		cptvf_free_irq_affinity(cptvf, CPT_8X_VF_INT_VEC_E_MISC);
+		free_irq(pci_irq_vector(pdev, CPT_8X_VF_INT_VEC_E_DONE), cptvf);
+		free_irq(pci_irq_vector(pdev, CPT_8X_VF_INT_VEC_E_MISC), cptvf);
 		pci_free_irq_vectors(cptvf->pdev);
 		cptvf_sw_cleanup(cptvf);
+		sysfs_remove_group(&pdev->dev.kobj, &vf_sysfs_group);
 		pci_set_drvdata(pdev, NULL);
 		pci_release_regions(pdev);
 		pci_disable_device(pdev);
-		cvm_crypto_exit();
 	}
 }
 
-static void cptvf_shutdown(struct pci_dev *pdev)
-{
-	cptvf_remove(pdev);
-}
-
 /* Supported devices */
 static const struct pci_device_id cptvf_id_table[] = {
-	{PCI_VDEVICE(CAVIUM, CPT_81XX_PCI_VF_DEVICE_ID), 0},
+	{PCI_VDEVICE(CAVIUM, CPT_PCI_VF_8X_DEVICE_ID), 0},
 	{ 0, }  /* end of table */
 };
 
@@ -854,13 +987,12 @@ static struct pci_driver cptvf_pci_driver = {
 	.id_table = cptvf_id_table,
 	.probe = cptvf_probe,
 	.remove = cptvf_remove,
-	.shutdown = cptvf_shutdown,
 };
 
 module_pci_driver(cptvf_pci_driver);
 
-MODULE_AUTHOR("George Cherian <george.cherian@cavium.com>");
-MODULE_DESCRIPTION("Cavium Thunder CPT Virtual Function Driver");
+MODULE_AUTHOR("Marvell International Ltd.");
+MODULE_DESCRIPTION("Marvell OcteonTX CPT Virtual Function Driver");
 MODULE_LICENSE("GPL v2");
 MODULE_VERSION(DRV_VERSION);
 MODULE_DEVICE_TABLE(pci, cptvf_id_table);
diff --git a/drivers/crypto/cavium/cpt/cptvf_mbox.c b/drivers/crypto/cavium/cpt/8x/cpt8x_vf_mbox.c
similarity index 76%
rename from drivers/crypto/cavium/cpt/cptvf_mbox.c
rename to drivers/crypto/cavium/cpt/8x/cpt8x_vf_mbox.c
index d5ec3b8a9e61..e77b5e2410eb 100644
--- a/drivers/crypto/cavium/cpt/cptvf_mbox.c
+++ b/drivers/crypto/cavium/cpt/8x/cpt8x_vf_mbox.c
@@ -1,27 +1,28 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
  *
  * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
  */
 
-#include "cptvf.h"
+#include <linux/delay.h>
+#include "cpt8x_vf.h"
 
 static void cptvf_send_msg_to_pf(struct cpt_vf *cptvf, struct cpt_mbox *mbx)
 {
 	/* Writing mbox(1) causes interrupt */
-	cpt_write_csr64(cptvf->reg_base, CPTX_VFX_PF_MBOXX(0, 0, 0),
-			mbx->msg);
-	cpt_write_csr64(cptvf->reg_base, CPTX_VFX_PF_MBOXX(0, 0, 1),
-			mbx->data);
+	writeq(mbx->msg, cptvf->reg_base + CPT_VFX_PF_MBOXX(0, 0));
+	writeq(mbx->data, cptvf->reg_base + CPT_VFX_PF_MBOXX(0, 1));
 }
 
 /* ACKs PF's mailbox message
  */
 void cptvf_mbox_send_ack(struct cpt_vf *cptvf, struct cpt_mbox *mbx)
 {
-	mbx->msg = CPT_MBOX_MSG_TYPE_ACK;
+	mbx->msg = CPT_MSG_ACK;
 	cptvf_send_msg_to_pf(cptvf, mbx);
 }
 
@@ -30,7 +31,7 @@ void cptvf_mbox_send_ack(struct cpt_vf *cptvf, struct cpt_mbox *mbx)
  */
 void cptvf_mbox_send_nack(struct cpt_vf *cptvf, struct cpt_mbox *mbx)
 {
-	mbx->msg = CPT_MBOX_MSG_TYPE_NACK;
+	mbx->msg = CPT_MSG_NACK;
 	cptvf_send_msg_to_pf(cptvf, mbx);
 }
 
@@ -43,18 +44,22 @@ void cptvf_handle_mbox_intr(struct cpt_vf *cptvf)
 	 * MBOX[0] contains msg
 	 * MBOX[1] contains data
 	 */
-	mbx.msg  = cpt_read_csr64(cptvf->reg_base, CPTX_VFX_PF_MBOXX(0, 0, 0));
-	mbx.data = cpt_read_csr64(cptvf->reg_base, CPTX_VFX_PF_MBOXX(0, 0, 1));
-	dev_dbg(&cptvf->pdev->dev, "%s: Mailbox msg 0x%llx from PF\n",
-		__func__, mbx.msg);
+	mbx.msg  = readq(cptvf->reg_base + CPT_VFX_PF_MBOXX(0, 0));
+	mbx.data = readq(cptvf->reg_base + CPT_VFX_PF_MBOXX(0, 1));
+
+	if (cpt_is_dbg_level_en(CPT_DBG_MBOX_MSGS))
+		dump_mbox_msg(&cptvf->pdev->dev, &mbx, -1);
+
 	switch (mbx.msg) {
+	case CPT_MSG_VF_UP:
+		cptvf->pf_acked = true;
+		cptvf->num_vfs = mbx.data;
+		break;
 	case CPT_MSG_READY:
-	{
 		cptvf->pf_acked = true;
 		cptvf->vfid = mbx.data;
 		dev_dbg(&cptvf->pdev->dev, "Received VFID %d\n", cptvf->vfid);
 		break;
-	}
 	case CPT_MSG_QBIND_GRP:
 		cptvf->pf_acked = true;
 		cptvf->vftype = mbx.data;
@@ -62,10 +67,10 @@ void cptvf_handle_mbox_intr(struct cpt_vf *cptvf)
 			cptvf->vfid, ((mbx.data == SE_TYPES) ? "SE" : "AE"),
 			cptvf->vfgrp);
 		break;
-	case CPT_MBOX_MSG_TYPE_ACK:
+	case CPT_MSG_ACK:
 		cptvf->pf_acked = true;
 		break;
-	case CPT_MBOX_MSG_TYPE_NACK:
+	case CPT_MSG_NACK:
 		cptvf->pf_nacked = true;
 		break;
 	default:
@@ -113,7 +118,7 @@ int cptvf_check_pf_ready(struct cpt_vf *cptvf)
 
 	mbx.msg = CPT_MSG_READY;
 	if (cptvf_send_msg_to_pf_timeout(cptvf, &mbx)) {
-		dev_err(&pdev->dev, "PF didn't respond to READY msg\n");
+		dev_err(&pdev->dev, "PF didn't respond to ready msg\n");
 		return -EBUSY;
 	}
 
@@ -132,7 +137,7 @@ int cptvf_send_vq_size_msg(struct cpt_vf *cptvf)
 	mbx.msg = CPT_MSG_QLEN;
 	mbx.data = cptvf->qsize;
 	if (cptvf_send_msg_to_pf_timeout(cptvf, &mbx)) {
-		dev_err(&pdev->dev, "PF didn't respond to vq_size msg\n");
+		dev_err(&pdev->dev, "PF didn't respond to vq size msg\n");
 		return -EBUSY;
 	}
 
@@ -142,19 +147,20 @@ int cptvf_send_vq_size_msg(struct cpt_vf *cptvf)
 /*
  * Communicate VF group required to PF and get the VQ binded to that group
  */
-int cptvf_send_vf_to_grp_msg(struct cpt_vf *cptvf)
+int cptvf_send_vf_to_grp_msg(struct cpt_vf *cptvf, int group)
 {
 	struct pci_dev *pdev = cptvf->pdev;
 	struct cpt_mbox mbx = {};
 
 	mbx.msg = CPT_MSG_QBIND_GRP;
 	/* Convey group of the VF */
-	mbx.data = cptvf->vfgrp;
+	mbx.data = group;
 	if (cptvf_send_msg_to_pf_timeout(cptvf, &mbx)) {
-		dev_err(&pdev->dev, "PF didn't respond to vf_type msg\n");
+		dev_err(&pdev->dev, "PF didn't respond to group msg\n");
 		return -EBUSY;
 	}
 
+	cptvf->vfgrp = group;
 	return 0;
 }
 
@@ -170,7 +176,7 @@ int cptvf_send_vf_priority_msg(struct cpt_vf *cptvf)
 	/* Convey group of the VF */
 	mbx.data = cptvf->priority;
 	if (cptvf_send_msg_to_pf_timeout(cptvf, &mbx)) {
-		dev_err(&pdev->dev, "PF didn't respond to vf_type msg\n");
+		dev_err(&pdev->dev, "PF didn't respond to priority msg\n");
 		return -EBUSY;
 	}
 	return 0;
@@ -186,7 +192,7 @@ int cptvf_send_vf_up(struct cpt_vf *cptvf)
 
 	mbx.msg = CPT_MSG_VF_UP;
 	if (cptvf_send_msg_to_pf_timeout(cptvf, &mbx)) {
-		dev_err(&pdev->dev, "PF didn't respond to UP msg\n");
+		dev_err(&pdev->dev, "PF didn't respond to up msg\n");
 		return -EBUSY;
 	}
 
diff --git a/drivers/crypto/cavium/cpt/9x/Makefile b/drivers/crypto/cavium/cpt/9x/Makefile
new file mode 100644
index 000000000000..6e8f9f70d0fb
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/Makefile
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-$(CONFIG_CRYPTO_DEV_OCTEONTX2_CPT) += cptpf9x.o cptvf9x.o
+
+common-objs := cpt9x_mbox_common.o cpt9x_debug.o ../common/cpt_debug.o
+cptpf9x-objs := cpt9x_pf_main.o  cpt9x_pf_mbox.o cpt9x_ucode.o cpt9x_quota.o ../common/cpt_ucode.o ${common-objs}
+cptvf9x-objs := cpt9x_vf_main.o cpt9x_lf_main.o cpt9x_reqmgr.o cpt9x_vf_mbox.o cpt9x_passthrough.o ../common/cpt_algs.o ../common/cpt_reqmgr.o
+
+ifeq ($(CONFIG_CRYPTO_DEV_OCTEONTX2_CPT), m)
+	cptvf9x-objs += ${common-objs}
+endif
+
+ccflags-y := -I$(src)/../common/
+ccflags-y += -I$(src)/../../../../soc/marvell/octeontx2/
+ccflags-y += -I$(src)/../../../../net/ethernet/marvell/octeontx2/
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_common.h b/drivers/crypto/cavium/cpt/9x/cpt9x_common.h
new file mode 100644
index 000000000000..57094feeb59c
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_common.h
@@ -0,0 +1,38 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_COMMON_H
+#define __CPT9X_COMMON_H
+
+#include "rvu.h"
+
+#define CPT_9X_MAX_VFS_NUM		128
+#define CPT_9X_MAX_LFS_NUM		64
+
+#define RVU_PFFUNC(pf, func)	\
+	((((pf) & RVU_PFVF_PF_MASK) << RVU_PFVF_PF_SHIFT) | \
+	(((func) & RVU_PFVF_FUNC_MASK) << RVU_PFVF_FUNC_SHIFT))
+
+
+#define RVU_FUNC_ADDR_S(blk, slot, offs) ((blk << 20) | (slot << 12) | offs)
+
+static inline void cpt_write64(void __iomem *reg_base, u64 blk, u64 slot,
+			       u64 offs, u64 val)
+{
+	writeq_relaxed(val, reg_base + RVU_FUNC_ADDR_S(blk, slot, offs));
+}
+
+static inline u64 cpt_read64(void __iomem *reg_base, u64 blk, u64 slot,
+			     u64 offs)
+{
+	return readq_relaxed(reg_base + RVU_FUNC_ADDR_S(blk, slot, offs));
+}
+
+#endif /* __CPT9X_COMMON_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_debug.c b/drivers/crypto/cavium/cpt/9x/cpt9x_debug.c
new file mode 100644
index 000000000000..cb1cb7bcfb89
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_debug.c
@@ -0,0 +1,59 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt9x_mbox_common.h"
+
+static char *get_opcode_str(int msg_opcode)
+{
+	char *str = "Unknown";
+
+	switch (msg_opcode) {
+	case MBOX_MSG_READY:
+		str = "READY";
+	break;
+
+	case MBOX_MSG_FREE_RSRC_CNT:
+		str = "FREE_RSRC_CNT";
+	break;
+
+	case MBOX_MSG_ATTACH_RESOURCES:
+		str = "ATTACH_RESOURCES";
+	break;
+
+	case MBOX_MSG_DETACH_RESOURCES:
+		str = "DETACH_RESOURCES";
+	break;
+
+	case MBOX_MSG_MSIX_OFFSET:
+		str = "MSIX_OFFSET";
+	break;
+
+	case MBOX_MSG_CPT_RD_WR_REGISTER:
+		str = "RD_WR_REGISTER";
+	break;
+	}
+
+	return str;
+}
+
+void dump_mbox_msg(struct device *dev, struct mbox_msghdr *msg, int size)
+{
+	char *opcode_str;
+	u16 pf_id, vf_id;
+
+	opcode_str = get_opcode_str(msg->id);
+	pf_id = (msg->pcifunc >> RVU_PFVF_PF_SHIFT) & RVU_PFVF_PF_MASK;
+	vf_id = (msg->pcifunc >> RVU_PFVF_FUNC_SHIFT) & RVU_PFVF_FUNC_MASK;
+
+	dev_info(dev, "Receive %s opcode (PF%d/VF%d), size %d, rc %d",
+		 opcode_str, pf_id, vf_id, size, msg->rc);
+	print_hex_dump(KERN_INFO, "", DUMP_PREFIX_OFFSET, 16, 2, msg, size,
+		       false);
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_lf.h b/drivers/crypto/cavium/cpt/9x/cpt9x_lf.h
new file mode 100644
index 000000000000..da45a6eb3fad
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_lf.h
@@ -0,0 +1,111 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_LF_H
+#define __CPT9X_LF_H
+
+#include "cpt_hw_types.h"
+#include "cpt9x_common.h"
+
+/*
+ * CPT instruction and pending queues user requested length in CPT_INST_S msgs
+ */
+#define CPT_USER_REQUESTED_QLEN_MSGS	8200
+
+/*
+ * CPT instruction queue size passed to HW is in units of 40*CPT_INST_S
+ * messages.
+ *
+ * The 96XX HRM (chapter 19.14 CPT LF BAR Registers - CPT_LF_Q_SIZE) states:
+ * "The effective queue size to software is ([SIZE_DIV40]-1)*40 CPT_INST_S's"
+ */
+#define DIV40	40
+#define CPT_SIZE_DIV40	((CPT_USER_REQUESTED_QLEN_MSGS + DIV40 - 1)/DIV40 + 1)
+
+/*
+ * CPT instruction and pending queues length in CPT_INST_S messages
+ */
+#define CPT_INST_QLEN_MSGS	(CPT_SIZE_DIV40 * DIV40)
+
+/*
+ * CPT instruction queue length in bytes
+ */
+#define CPT_INST_QLEN_BYTES	(CPT_SIZE_DIV40 * DIV40 * CPT_INST_SIZE)
+
+/*
+ * Mask which selects all engine groups
+ */
+#define ALL_ENG_GRPS_MASK	0xFF
+
+/*
+ * Queue priority
+ */
+#define QUEUE_HI_PRIO	0x1
+#define QUEUE_LOW_PRIO	0x0
+
+struct lf_sysfs_cfg {
+	char name[NAME_LENGTH];
+	struct device_attribute eng_grps_mask_attr;
+	struct device_attribute coalesc_tw_attr;
+	struct device_attribute coalesc_nw_attr;
+	struct device_attribute prio_attr;
+#define ATTRS_NUM 5
+	struct attribute *attrs[ATTRS_NUM];
+	struct attribute_group attr_grp;
+	bool is_sysfs_grp_created;
+};
+
+struct instruction_queue {
+	u8 *vaddr;
+	u8 *real_vaddr;
+	dma_addr_t dma_addr;
+	dma_addr_t real_dma_addr;
+	u32 size;
+};
+
+struct cptlfs_info;
+struct cptlf_wqe {
+	struct tasklet_struct work;
+	struct cptlfs_info *lfs;
+	u8 lf_num;
+};
+
+struct cptlf_info {
+	struct cptlfs_info *lfs;		/* Ptr to cptlfs_info struct */
+	struct lf_sysfs_cfg sysfs_cfg;		/* LF sysfs config entries */
+	void *lmtline;				/* Address of LMTLINE */
+	void *ioreg;                            /* LMTLINE send register */
+	int msix_offset;			/* MSI-X interrupts offset */
+	cpumask_var_t affinity_mask;		/* IRQs affinity mask */
+	u8 irq_name[CPT_9X_LF_MSIX_VECTORS][32];/* Interrupts name */
+	u8 is_irq_reg[CPT_9X_LF_MSIX_VECTORS];  /* Is interrupt registered */
+	u8 slot;				/* Slot number of this LF */
+
+	/* Command and pending queues */
+	struct instruction_queue iqueue;/* Instruction queue */
+	struct pending_queue pqueue;	/* Pending queue */
+	struct cptlf_wqe *wqe;		/* Tasklet work info */
+};
+
+struct cptlfs_info {
+	/* Registers start address of VF/PF LFs are attached to */
+	void __iomem *reg_base;
+	struct pci_dev *pdev;   /* Device LFs are attached to */
+	struct cptlf_info lf[CPT_9X_MAX_LFS_NUM];
+	u8 kcrypto_eng_grp_num;	/* Kernel crypto engine group number */
+	u8 are_lfs_attached;	/* Whether CPT LFs are attached */
+	u8 lfs_num;		/* Number of CPT LFs */
+};
+
+int cptlf_init(struct pci_dev *pdev, void __iomem *reg_base,
+	       struct cptlfs_info *lfs, int lfs_num);
+int cptlf_shutdown(struct pci_dev *pdev, struct cptlfs_info *lfs);
+
+#endif /* __CPT9X_LF_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_lf_main.c b/drivers/crypto/cavium/cpt/9x/cpt9x_lf_main.c
new file mode 100644
index 000000000000..28628a2b34e3
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_lf_main.c
@@ -0,0 +1,1141 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt_reqmgr.h"
+#include "cpt9x_mbox_common.h"
+#include "cpt9x_common.h"
+#include "cpt9x_reqmgr.h"
+#include "otx2_reg.h"
+#include "rvu_reg.h"
+
+static int cptlf_get_done_time_wait(struct cptlf_info *lf)
+{
+	union cptx_vqx_done_wait done_wait;
+
+	done_wait.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				 CPT_LF_DONE_WAIT);
+	return done_wait.s.time_wait;
+}
+
+static void cptlf_do_set_done_time_wait(struct cptlf_info *lf, int time_wait)
+{
+	union cptx_vqx_done_wait done_wait;
+
+	done_wait.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				 CPT_LF_DONE_WAIT);
+	done_wait.s.time_wait = time_wait;
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_DONE_WAIT,
+		    done_wait.u);
+}
+
+static int cptlf_get_done_num_wait(struct cptlf_info *lf)
+{
+	union cptx_vqx_done_wait done_wait;
+
+	done_wait.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				 CPT_LF_DONE_WAIT);
+	return done_wait.s.num_wait;
+}
+
+static void cptlf_do_set_done_num_wait(struct cptlf_info *lf, int num_wait)
+{
+	union cptx_vqx_done_wait done_wait;
+
+	done_wait.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				 CPT_LF_DONE_WAIT);
+	done_wait.s.num_wait = num_wait;
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_DONE_WAIT,
+		    done_wait.u);
+}
+
+void cptlf_set_done_time_wait(struct cptlfs_info *lfs, int time_wait)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cptlf_do_set_done_time_wait(&lfs->lf[slot], time_wait);
+}
+
+void cptlf_set_done_num_wait(struct cptlfs_info *lfs, int num_wait)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cptlf_do_set_done_num_wait(&lfs->lf[slot], num_wait);
+}
+
+static void cptlf_set_iqueues_base_addr(struct cptlfs_info *lfs)
+{
+	union cptx_lf_q_base lf_q_base;
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		lf_q_base.u = lfs->lf[slot].iqueue.dma_addr;
+		cpt_write64(lfs->reg_base, BLKADDR_CPT0, slot, CPT_LF_Q_BASE,
+			    lf_q_base.u);
+	}
+}
+
+static void cptlf_do_set_iqueue_size(struct cptlf_info *lf)
+{
+	union cptx_lf_q_size lf_q_size = { .u = 0x0 };
+
+	lf_q_size.s.size_div40 = CPT_SIZE_DIV40;
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_Q_SIZE,
+		    lf_q_size.u);
+}
+
+static void cptlf_set_iqueues_size(struct cptlfs_info *lfs)
+{
+
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cptlf_do_set_iqueue_size(&lfs->lf[slot]);
+}
+
+static int cptlf_get_inflight(struct cptlf_info *lf)
+{
+	union cptx_lf_inprog lf_inprog;
+
+	lf_inprog.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				 CPT_LF_INPROG);
+
+	return lf_inprog.s.inflight;
+}
+
+static void cptlf_do_disable_iqueue(struct cptlf_info *lf)
+{
+	union cptx_lf_ctl lf_ctl = { .u = 0x0 };
+	union cptx_lf_inprog lf_inprog;
+	int timeout = 20;
+
+	/* Disable instructions enqueuing */
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_CTL,
+		    lf_ctl.u);
+
+	/* Wait for instruction queue to become empty */
+	do {
+		lf_inprog.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0,
+					 lf->slot, CPT_LF_INPROG);
+		if (!lf_inprog.s.inflight)
+			break;
+
+		usleep_range(10000, 20000);
+		if (timeout-- < 0) {
+			dev_err(&lf->lfs->pdev->dev,
+				"Error LF %d is still busy.\n", lf->slot);
+			break;
+		}
+
+	} while (1);
+
+	/* Disable executions in the LF's queue,
+	 * the queue should be empty at this point
+	 */
+	lf_inprog.s.eena = 0x0;
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_INPROG,
+		    lf_inprog.u);
+}
+
+static void cptlf_disable_iqueues(struct cptlfs_info *lfs)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cptlf_do_disable_iqueue(&lfs->lf[slot]);
+}
+
+
+static void cptlf_set_iqueue_enq(struct cptlf_info *lf, bool enable)
+{
+	union cptx_lf_ctl lf_ctl;
+
+	lf_ctl.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+			      CPT_LF_CTL);
+
+	/* Set iqueue's enqueuing */
+	lf_ctl.s.ena = enable ? 0x1 : 0x0;
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_CTL,
+		    lf_ctl.u);
+}
+
+static void cptlf_enable_iqueue_enq(struct cptlf_info *lf)
+{
+	cptlf_set_iqueue_enq(lf, true);
+}
+
+static void cptlf_set_iqueue_exec(struct cptlf_info *lf, bool enable)
+{
+	union cptx_lf_inprog lf_inprog;
+
+	lf_inprog.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				 CPT_LF_INPROG);
+
+	/* Set iqueue's execution */
+	lf_inprog.s.eena = enable ? 0x1 : 0x0;
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_INPROG,
+		    lf_inprog.u);
+}
+
+static void cptlf_enable_iqueue_exec(struct cptlf_info *lf)
+{
+	cptlf_set_iqueue_exec(lf, true);
+}
+
+static void cptlf_disable_iqueue_exec(struct cptlf_info *lf)
+{
+	cptlf_set_iqueue_exec(lf, false);
+}
+
+static void cptlf_enable_iqueues(struct cptlfs_info *lfs)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		cptlf_enable_iqueue_exec(&lfs->lf[slot]);
+		cptlf_enable_iqueue_enq(&lfs->lf[slot]);
+	}
+}
+
+static int cptlf_get_pri(struct pci_dev *pdev, struct cptlf_info *lf, int *pri)
+{
+	union cptx_af_lf_ctrl lf_ctrl;
+	int ret = 0;
+
+	ret = cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
+	if (ret)
+		goto err;
+
+	*pri = lf_ctrl.s.pri;
+err:
+	return ret;
+}
+
+static int cptlf_set_pri(struct pci_dev *pdev, struct cptlf_info *lf, int pri)
+{
+	union cptx_af_lf_ctrl lf_ctrl;
+	int ret = 0;
+
+	ret = cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
+	if (ret)
+		goto err;
+
+	lf_ctrl.s.pri = pri ? 1 : 0;
+
+	ret = cpt_write_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), lf_ctrl.u);
+	if (ret)
+		goto err;
+err:
+	return ret;
+}
+
+static int cptlf_get_eng_grps_mask(struct pci_dev *pdev, struct cptlf_info *lf,
+				   int *eng_grps_mask)
+{
+	union cptx_af_lf_ctrl lf_ctrl;
+	int ret = 0;
+
+	ret = cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
+	if (ret)
+		goto err;
+
+	*eng_grps_mask = lf_ctrl.s.grp;
+err:
+	return ret;
+}
+
+static int cptlf_set_eng_grps_mask(struct pci_dev *pdev, struct cptlf_info *lf,
+				   int eng_grps_mask)
+{
+	union cptx_af_lf_ctrl lf_ctrl;
+	int ret = 0;
+
+	ret = cpt_read_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), &lf_ctrl.u);
+	if (ret)
+		goto err;
+
+	lf_ctrl.s.grp = eng_grps_mask;
+
+	ret = cpt_write_af_reg(pdev, CPT_AF_LFX_CTL(lf->slot), lf_ctrl.u);
+	if (ret)
+		goto err;
+err:
+	return ret;
+}
+
+static int cptlf_set_grp_and_pri(struct pci_dev *pdev, struct cptlfs_info *lfs,
+				 int eng_grp_mask, int pri)
+{
+	int slot, ret = 0;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		ret = cptlf_set_pri(pdev, &lfs->lf[slot], pri);
+		if (ret)
+			goto err;
+
+		ret = cptlf_set_eng_grps_mask(pdev, &lfs->lf[slot],
+					      eng_grp_mask);
+		if (ret)
+			goto err;
+	}
+err:
+	return ret;
+}
+
+static void cptlf_hw_init(struct cptlfs_info *lfs)
+{
+	/* Disable instruction queues */
+	cptlf_disable_iqueues(lfs);
+
+	/* Set instruction queues base addresses */
+	cptlf_set_iqueues_base_addr(lfs);
+
+	/* Set instruction queues sizes */
+	cptlf_set_iqueues_size(lfs);
+
+	/* Set done interrupts time wait */
+	cptlf_set_done_time_wait(lfs, CPT_TIMER_HOLD);
+
+	/* Set done interrupts num wait */
+	cptlf_set_done_num_wait(lfs, CPT_COUNT_HOLD);
+
+	/* Enable instruction queues */
+	cptlf_enable_iqueues(lfs);
+}
+
+static void cptlf_hw_cleanup(struct cptlfs_info *lfs)
+{
+	/* Disable instruction queues */
+	cptlf_disable_iqueues(lfs);
+}
+
+static void cptlf_set_misc_intrs(struct cptlfs_info *lfs, u8 enable)
+{
+	union cptx_lf_misc_int_ena_w1s irq_misc = { .u = 0x0 };
+	u64 reg = enable ? CPT_LF_MISC_INT_ENA_W1S : CPT_LF_MISC_INT_ENA_W1C;
+	int slot;
+
+	irq_misc.s.fault = 0x1;
+	irq_misc.s.hwerr = 0x1;
+	irq_misc.s.irde = 0x1;
+	irq_misc.s.nqerr = 0x1;
+	irq_misc.s.nwrp = 0x1;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cpt_write64(lfs->reg_base, BLKADDR_CPT0, slot, reg, irq_misc.u);
+}
+
+static void cptlf_enable_misc_intrs(struct cptlfs_info *lfs)
+{
+	cptlf_set_misc_intrs(lfs, true);
+}
+
+static void cptlf_disable_misc_intrs(struct cptlfs_info *lfs)
+{
+	cptlf_set_misc_intrs(lfs, false);
+}
+
+static void cptlf_enable_done_intr(struct cptlfs_info *lfs)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cpt_write64(lfs->reg_base, BLKADDR_CPT0, slot,
+			    CPT_LF_DONE_INT_ENA_W1S, 0x1);
+}
+
+static void cptlf_disable_done_intr(struct cptlfs_info *lfs)
+{
+	int slot;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++)
+		cpt_write64(lfs->reg_base, BLKADDR_CPT0, slot,
+			    CPT_LF_DONE_INT_ENA_W1C, 0x1);
+}
+
+static inline int cptlf_read_done_cnt(struct cptlf_info *lf)
+{
+	union cptx_vqx_done irq_cnt;
+
+	irq_cnt.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+			       CPT_LF_DONE);
+	return irq_cnt.s.done;
+}
+
+static irqreturn_t cptlf_misc_intr_handler(int irq, void *cptlf)
+{
+	struct cptlf_info *lf = (struct cptlf_info *) cptlf;
+	union cptx_lf_misc_int irq_misc, irq_misc_ack;
+	struct device *dev = &lf->lfs->pdev->dev;
+
+	irq_misc.u = cpt_read64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+				CPT_LF_MISC_INT);
+	irq_misc_ack.u = 0x0;
+
+	if (irq_misc.s.fault) {
+		dev_err(dev, "Memory error detected while executing CPT_INST_S, LF %d.\n",
+			lf->slot);
+		irq_misc_ack.s.fault = 0x1;
+
+	} else if (irq_misc.s.hwerr) {
+		dev_err(dev, "HW error from an engine executing CPT_INST_S, LF %d.",
+			lf->slot);
+		irq_misc_ack.s.hwerr = 0x1;
+
+	} else if (irq_misc.s.nwrp) {
+		dev_err(dev, "SMMU fault while writing CPT_RES_S to CPT_INST_S[RES_ADDR], LF %d.\n",
+			lf->slot);
+		irq_misc_ack.s.nwrp = 0x1;
+
+	} else if (irq_misc.s.irde) {
+		dev_err(dev, "Memory error when accessing instruction memory queue CPT_LF_Q_BASE[ADDR].\n");
+		irq_misc_ack.s.irde = 0x1;
+
+	} else if (irq_misc.s.nqerr) {
+		dev_err(dev, "Error enqueuing an instruction received at CPT_LF_NQ.\n");
+		irq_misc_ack.s.nqerr = 0x1;
+
+	} else {
+		dev_err(dev, "Unhandled interrupt in CPT LF %d\n", lf->slot);
+		return IRQ_NONE;
+	}
+
+	/* Acknowledge interrupts */
+	cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot, CPT_LF_MISC_INT,
+		    irq_misc_ack.u);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t cptlf_done_intr_handler(int irq, void *cptlf)
+{
+	struct cptlf_info *lf = (struct cptlf_info *) cptlf;
+	int irq_cnt;
+
+	/* Read the number of completed requests */
+	irq_cnt = cptlf_read_done_cnt(lf);
+	if (irq_cnt) {
+
+		/* Acknowledge the number of completed requests */
+		cpt_write64(lf->lfs->reg_base, BLKADDR_CPT0, lf->slot,
+			    CPT_LF_DONE_ACK, irq_cnt);
+		if (unlikely(!lf->wqe)) {
+			dev_err(&lf->lfs->pdev->dev, "No work for LF %d\n",
+				lf->slot);
+			return IRQ_NONE;
+		}
+
+		/* Schedule processing of completed requests */
+		tasklet_hi_schedule(&lf->wqe->work);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int cptlf_do_register_interrrupts(struct cptlfs_info *lfs, int lf_num,
+					 int irq_offset, irq_handler_t handler)
+{
+	int ret = 0;
+
+	ret = request_irq(pci_irq_vector(lfs->pdev, lfs->lf[lf_num].msix_offset
+			  + irq_offset), handler, 0,
+			  lfs->lf[lf_num].irq_name[irq_offset],
+			  &lfs->lf[lf_num]);
+	if (ret)
+		goto error;
+
+	lfs->lf[lf_num].is_irq_reg[irq_offset] = true;
+error:
+	return ret;
+}
+
+static int cptlf_register_interrupts(struct cptlfs_info *lfs)
+{
+	int irq_offs, i;
+	int ret = 0;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		irq_offs = CPT_9X_LF_INT_VEC_E_MISC;
+		snprintf(lfs->lf[i].irq_name[irq_offs], 32, "CPTLF Misc%d", i);
+		ret = cptlf_do_register_interrrupts(lfs, i, irq_offs,
+						    cptlf_misc_intr_handler);
+		if (ret)
+			goto error;
+
+		irq_offs = CPT_9X_LF_INT_VEC_E_DONE;
+		snprintf(lfs->lf[i].irq_name[irq_offs], 32, "CPTLF Done%d", i);
+		ret = cptlf_do_register_interrrupts(lfs, i, irq_offs,
+						    cptlf_done_intr_handler);
+		if (ret)
+			goto error;
+	}
+error:
+	return ret;
+}
+
+static void cptlf_unregister_interrupts(struct cptlfs_info *lfs)
+{
+	int i, offs;
+
+	for (i = 0; i < lfs->lfs_num; i++)
+		for (offs = 0; offs < CPT_9X_LF_MSIX_VECTORS; offs++)
+			if (lfs->lf[i].is_irq_reg[offs]) {
+				free_irq(pci_irq_vector(lfs->pdev,
+							lfs->lf[i].msix_offset
+							+ offs),
+							&lfs->lf[i]);
+				lfs->lf[i].is_irq_reg[offs] = false;
+			}
+}
+
+static int cptlf_set_irqs_affinity(struct cptlfs_info *lfs)
+{
+	int slot, offs;
+	int ret = 0;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		if (!zalloc_cpumask_var(&lfs->lf[slot].affinity_mask,
+					GFP_KERNEL)) {
+			dev_err(&lfs->pdev->dev,
+				"cpumask allocation failed for LF %d", slot);
+			return -ENOMEM;
+		}
+
+		cpumask_set_cpu(cpumask_local_spread(slot,
+				dev_to_node(&lfs->pdev->dev)),
+				lfs->lf[slot].affinity_mask);
+
+		for (offs = 0; offs < CPT_9X_LF_MSIX_VECTORS; offs++) {
+			ret = irq_set_affinity_hint(pci_irq_vector(lfs->pdev,
+					lfs->lf[slot].msix_offset + offs),
+					lfs->lf[slot].affinity_mask);
+			if (ret)
+				return ret;
+		}
+	}
+
+	return ret;
+}
+
+static void cptlf_free_irqs_affinity(struct cptlfs_info *lfs)
+{
+	int slot, offs;
+
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		for (offs = 0; offs < CPT_9X_LF_MSIX_VECTORS; offs++)
+			irq_set_affinity_hint(pci_irq_vector(lfs->pdev,
+					      lfs->lf[slot].msix_offset +
+					      offs), NULL);
+		free_cpumask_var(lfs->lf[slot].affinity_mask);
+	}
+}
+
+static void cptlf_work_handler(unsigned long data)
+{
+	cpt_post_process((struct cptlf_wqe *) data);
+}
+
+static int init_tasklet_work(struct cptlfs_info *lfs)
+{
+	struct cptlf_wqe *wqe;
+	int i;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		wqe = kzalloc(sizeof(struct cptlf_wqe), GFP_KERNEL);
+		if (!wqe)
+			return -ENOMEM;
+
+		tasklet_init(&wqe->work, cptlf_work_handler, (u64) wqe);
+		wqe->lfs = lfs;
+		wqe->lf_num = i;
+		lfs->lf[i].wqe = wqe;
+	}
+
+	return 0;
+}
+
+static void cleanup_tasklet_work(struct cptlfs_info *lfs)
+{
+	int i;
+
+	for (i = 0; i <  lfs->lfs_num; i++) {
+		if (!lfs->lf[i].wqe)
+			continue;
+
+		tasklet_kill(&lfs->lf[i].wqe->work);
+		kfree(lfs->lf[i].wqe);
+		lfs->lf[i].wqe = NULL;
+	}
+}
+
+static void free_pending_queues(struct cptlfs_info *lfs)
+{
+	int i;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		kfree(lfs->lf[i].pqueue.head);
+		lfs->lf[i].pqueue.head = NULL;
+	}
+}
+
+static int alloc_pending_queues(struct cptlfs_info *lfs)
+{
+	int ret = 0;
+	int size, i;
+
+	if (!lfs->lfs_num)
+		return -EINVAL;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		lfs->lf[i].pqueue.qlen = CPT_INST_QLEN_MSGS;
+		size = lfs->lf[i].pqueue.qlen *
+		       sizeof(struct pending_entry);
+
+		lfs->lf[i].pqueue.head = kzalloc(size, GFP_KERNEL);
+		if (!lfs->lf[i].pqueue.head) {
+			ret = -ENOMEM;
+			goto error;
+		}
+
+		/* Initialize spin lock */
+		spin_lock_init(&lfs->lf[i].pqueue.lock);
+	}
+
+	return 0;
+error:
+	free_pending_queues(lfs);
+	return ret;
+}
+
+static void free_instruction_queues(struct cptlfs_info *lfs)
+{
+	int i;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		if (lfs->lf[i].iqueue.real_vaddr)
+			dma_free_coherent(&lfs->pdev->dev,
+					  lfs->lf[i].iqueue.size,
+					  lfs->lf[i].iqueue.real_vaddr,
+					  lfs->lf[i].iqueue.real_dma_addr);
+		lfs->lf[i].iqueue.real_vaddr = NULL;
+		lfs->lf[i].iqueue.vaddr = NULL;
+	}
+}
+
+static int alloc_instruction_queues(struct cptlfs_info *lfs)
+{
+	int ret = 0, i;
+
+	if (!lfs->lfs_num)
+		return -EINVAL;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+
+		lfs->lf[i].iqueue.size = CPT_INST_QLEN_BYTES +
+					 CPT_INST_Q_ALIGNMENT;
+		lfs->lf[i].iqueue.real_vaddr =
+			(u8 *) dma_zalloc_coherent(&lfs->pdev->dev,
+					lfs->lf[i].iqueue.size,
+					&lfs->lf[i].iqueue.real_dma_addr,
+					GFP_KERNEL);
+		if (!lfs->lf[i].iqueue.real_vaddr) {
+			dev_err(&lfs->pdev->dev,
+				"Inst queue allocation failed for LF %d\n", i);
+			ret = -ENOMEM;
+			goto error;
+		}
+
+		/* Align pointers */
+		lfs->lf[i].iqueue.vaddr =
+			(uint8_t *) PTR_ALIGN(lfs->lf[i].iqueue.real_vaddr,
+					      CPT_INST_Q_ALIGNMENT);
+		lfs->lf[i].iqueue.dma_addr =
+			(dma_addr_t) PTR_ALIGN(lfs->lf[i].iqueue.real_dma_addr,
+					       CPT_INST_Q_ALIGNMENT);
+	}
+
+	return 0;
+error:
+	free_instruction_queues(lfs);
+	return ret;
+}
+
+static int cptlf_sw_init(struct cptlfs_info *lfs)
+{
+	int ret = 0;
+
+	ret = alloc_instruction_queues(lfs);
+	if (ret) {
+		dev_err(&lfs->pdev->dev,
+			"Allocating instruction queues failed\n");
+		goto error;
+	}
+
+	ret = alloc_pending_queues(lfs);
+	if (ret) {
+		dev_err(&lfs->pdev->dev,
+			"Allocating pending queues failed\n");
+		goto error;
+	}
+
+	ret = init_tasklet_work(lfs);
+	if (ret) {
+		dev_err(&lfs->pdev->dev,
+			"Tasklet work init failed\n");
+		goto error;
+	}
+error:
+	return ret;
+}
+
+static void cptlf_sw_cleanup(struct cptlfs_info *lfs)
+{
+	cleanup_tasklet_work(lfs);
+	free_pending_queues(lfs);
+	free_instruction_queues(lfs);
+}
+
+inline void send_cpt_cmds_in_batch(union cpt_inst_s *cptinst, u32 num,
+				   void *obj)
+{
+	int i;
+
+	for (i = 0; i < (num & 0xFFFFFFFE); i += 2)
+		send_cpt_cmd(&cptinst[i], 2, obj);
+	if (num & 0x1)
+		send_cpt_cmd(&cptinst[num-1], 1, obj);
+}
+
+inline void send_cpt_cmds_for_speed_test(union cpt_inst_s *cptinst, u32 num,
+					 void *obj)
+{
+	struct cptlf_info *lf = (struct cptlf_info *) obj;
+
+	cptlf_do_disable_iqueue(lf);
+	/* We set queue size to reset CPT_LF_Q_INST_PTR
+	 * and CPT_LF_Q_GRP_PTR pointers
+	 */
+	cptlf_do_set_iqueue_size(lf);
+	cptlf_enable_iqueue_enq(lf);
+
+	send_cpt_cmds_in_batch(cptinst, num, obj);
+
+	cptlf_enable_iqueue_exec(lf);
+}
+
+static ssize_t cptlf_coalesc_time_wait_show(struct device *dev,
+					    struct device_attribute *attr,
+					    char *buf)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, coalesc_tw_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", cptlf_get_done_time_wait(lf));
+}
+
+static ssize_t cptlf_coalesc_time_wait_store(struct device *dev,
+					     struct device_attribute *attr,
+					     const char *buf, size_t count)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+	long int val;
+	int ret;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret != 0)
+		return ret;
+
+	if (val < CPT_COALESC_MIN_TIME_WAIT ||
+	    val > CPT_COALESC_MAX_TIME_WAIT)
+		return -EINVAL;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, coalesc_tw_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+
+	cptlf_do_set_done_time_wait(lf, val);
+	return count;
+}
+
+static ssize_t cptlf_coalesc_num_wait_show(struct device *dev,
+					   struct device_attribute *attr,
+					   char *buf)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, coalesc_nw_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", cptlf_get_done_num_wait(lf));
+}
+
+static ssize_t cptlf_coalesc_num_wait_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf, size_t count)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+	long int val;
+	int ret;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret != 0)
+		return ret;
+
+	if (val < CPT_COALESC_MIN_NUM_WAIT ||
+	    val > CPT_COALESC_MAX_NUM_WAIT)
+		return -EINVAL;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, coalesc_nw_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+
+	cptlf_do_set_done_num_wait(lf, val);
+	return count;
+}
+
+static ssize_t cptlf_priority_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+	struct pci_dev *pdev;
+	int pri, ret;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, prio_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+	pdev = container_of(dev, struct pci_dev, dev);
+
+	ret = cptlf_get_pri(pdev, lf, &pri);
+	if (ret)
+		return ret;
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", pri);
+}
+
+static ssize_t cptlf_priority_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+	struct pci_dev *pdev;
+	long int val;
+	int ret;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret)
+		return ret;
+
+	if (val < QUEUE_LOW_PRIO ||
+	    val > QUEUE_HI_PRIO)
+		return -EINVAL;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, prio_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+	pdev = container_of(dev, struct pci_dev, dev);
+
+	/* Queue's priority can be modified only if queue is quiescent */
+	if (cptlf_get_inflight(lf)) {
+		ret = -EPERM;
+		goto err_print;
+	}
+
+	cptlf_disable_iqueue_exec(lf);
+
+	if (cptlf_get_inflight(lf)) {
+		ret = -EPERM;
+		cptlf_enable_iqueue_exec(lf);
+		goto err_print;
+	}
+
+	ret = cptlf_set_pri(pdev, lf, val);
+	if (ret) {
+		cptlf_enable_iqueue_exec(lf);
+		goto err;
+	}
+
+	cptlf_enable_iqueue_exec(lf);
+	return count;
+
+err_print:
+	dev_err(&pdev->dev,
+		"Disable traffic before modifying queue's priority");
+err:
+	return ret;
+}
+
+static ssize_t cptlf_eng_grps_mask_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+	struct pci_dev *pdev;
+	int eng_grps_mask;
+	int ret;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, eng_grps_mask_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+	pdev = container_of(dev, struct pci_dev, dev);
+
+	ret = cptlf_get_eng_grps_mask(pdev, lf, &eng_grps_mask);
+	if (ret)
+		return ret;
+
+	return scnprintf(buf, PAGE_SIZE, "0x%2.2X\n", eng_grps_mask);
+}
+
+static ssize_t cptlf_eng_grps_mask_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
+{
+	struct lf_sysfs_cfg *cfg;
+	struct cptlf_info *lf;
+	struct pci_dev *pdev;
+	long int val;
+	int ret;
+
+	ret = kstrtol(buf, 16, &val);
+	if (ret)
+		return ret;
+
+	if (val < 1 ||
+	    val > ALL_ENG_GRPS_MASK)
+		return -EINVAL;
+
+	cfg = container_of(attr, struct lf_sysfs_cfg, eng_grps_mask_attr);
+	lf = container_of(cfg, struct cptlf_info, sysfs_cfg);
+	pdev = container_of(dev, struct pci_dev, dev);
+
+	/* Queue's engine groups mask can be
+	 * modified only if queue is quiescent
+	 */
+	if (cptlf_get_inflight(lf)) {
+		ret = -EPERM;
+		goto err_print;
+	}
+
+	cptlf_disable_iqueue_exec(lf);
+
+	if (cptlf_get_inflight(lf)) {
+		ret = -EPERM;
+		cptlf_enable_iqueue_exec(lf);
+		goto err_print;
+	}
+
+	ret = cptlf_set_eng_grps_mask(pdev, lf, val);
+	if (ret) {
+		cptlf_enable_iqueue_exec(lf);
+		goto err;
+	}
+
+	cptlf_enable_iqueue_exec(lf);
+	return count;
+
+err_print:
+	dev_err(&pdev->dev,
+		"Disable traffic before modifying queue's engine groups mask");
+err:
+	return ret;
+}
+
+static void cptlf_delete_sysfs_cfg(struct cptlfs_info *lfs)
+{
+	struct lf_sysfs_cfg *cfg;
+	int i;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		cfg = &lfs->lf[i].sysfs_cfg;
+		if (cfg->is_sysfs_grp_created) {
+			sysfs_remove_group(&lfs->pdev->dev.kobj,
+					   &cfg->attr_grp);
+			cfg->is_sysfs_grp_created = false;
+		}
+	}
+}
+
+static int cptlf_create_sysfs_cfg(struct cptlfs_info *lfs)
+{
+	struct lf_sysfs_cfg *cfg;
+	int i, ret = 0;
+
+	for (i = 0; i < lfs->lfs_num; i++) {
+		cfg = &lfs->lf[i].sysfs_cfg;
+		snprintf(cfg->name, NAME_LENGTH, "cpt_queue%d", i);
+
+		cfg->eng_grps_mask_attr.show = cptlf_eng_grps_mask_show;
+		cfg->eng_grps_mask_attr.store = cptlf_eng_grps_mask_store;
+		cfg->eng_grps_mask_attr.attr.name = "eng_grps_mask";
+		cfg->eng_grps_mask_attr.attr.mode = 0664;
+		sysfs_attr_init(&cfg->eng_grps_mask_attr.attr);
+
+		cfg->coalesc_tw_attr.show = cptlf_coalesc_time_wait_show;
+		cfg->coalesc_tw_attr.store = cptlf_coalesc_time_wait_store;
+		cfg->coalesc_tw_attr.attr.name = "coalescence_time_wait";
+		cfg->coalesc_tw_attr.attr.mode = 0664;
+		sysfs_attr_init(&cfg->coalesc_tw_attr.attr);
+
+		cfg->coalesc_nw_attr.show = cptlf_coalesc_num_wait_show;
+		cfg->coalesc_nw_attr.store = cptlf_coalesc_num_wait_store;
+		cfg->coalesc_nw_attr.attr.name = "coalescence_num_wait";
+		cfg->coalesc_nw_attr.attr.mode = 0664;
+		sysfs_attr_init(&cfg->coalesc_nw_attr.attr);
+
+		cfg->prio_attr.show = cptlf_priority_show;
+		cfg->prio_attr.store = cptlf_priority_store;
+		cfg->prio_attr.attr.name = "priority";
+		cfg->prio_attr.attr.mode = 0664;
+		sysfs_attr_init(&cfg->prio_attr.attr);
+
+		cfg->attrs[0] = &cfg->eng_grps_mask_attr.attr;
+		cfg->attrs[1] = &cfg->coalesc_tw_attr.attr;
+		cfg->attrs[2] = &cfg->coalesc_nw_attr.attr;
+		cfg->attrs[3] = &cfg->prio_attr.attr;
+		cfg->attrs[ATTRS_NUM - 1] = NULL;
+
+		cfg->attr_grp.name = cfg->name;
+		cfg->attr_grp.attrs = cfg->attrs;
+		ret = sysfs_create_group(&lfs->pdev->dev.kobj,
+					 &cfg->attr_grp);
+		if (ret)
+			goto err;
+		cfg->is_sysfs_grp_created = true;
+	}
+
+	return 0;
+err:
+	cptlf_delete_sysfs_cfg(lfs);
+	return ret;
+}
+
+int cptlf_init(struct pci_dev *pdev, void *reg_base,
+	       struct cptlfs_info *lfs, int lfs_num)
+{
+	int slot, ret = 0;
+
+	lfs->reg_base = reg_base;
+	lfs->lfs_num = lfs_num;
+	lfs->pdev = pdev;
+	for (slot = 0; slot < lfs->lfs_num; slot++) {
+		lfs->lf[slot].lfs = lfs;
+		lfs->lf[slot].slot = slot;
+		lfs->lf[slot].lmtline = lfs->reg_base +
+			RVU_FUNC_ADDR_S(BLKADDR_LMT, slot, LMT_LF_LMTLINEX(0));
+		lfs->lf[slot].ioreg = lfs->reg_base +
+			RVU_FUNC_ADDR_S(BLKADDR_CPT0, slot, CPT_LF_NQX(0));
+	}
+
+	/* Send request to attach LFs */
+	ret = cpt_attach_rscrs_msg(pdev);
+	if (ret)
+		goto cpt_err;
+
+	/* Get msix offsets for attached LFs */
+	ret = cpt_msix_offset_msg(pdev);
+	if (ret)
+		goto cpt_err_detach;
+
+	/* Initialize LFs software side */
+	ret = cptlf_sw_init(lfs);
+	if (ret)
+		goto cpt_err_detach;
+
+	/* Register LFs interrupts */
+	ret = cptlf_register_interrupts(lfs);
+	if (ret)
+		goto cpt_err_sw_cleanup;
+
+	/* Initialize LFs hardware side */
+	cptlf_hw_init(lfs);
+
+	/* Allow each LF to execute requests destined to any of 8 engine
+	 * groups and set queue priority of each LF to high
+	 */
+	ret = cptlf_set_grp_and_pri(pdev, lfs, ALL_ENG_GRPS_MASK,
+				    QUEUE_HI_PRIO);
+	if (ret)
+		goto cpt_err_unregister_interrupts;
+
+	/* Set interrupts affinity */
+	ret = cptlf_set_irqs_affinity(lfs);
+	if (ret)
+		goto cpt_err_unregister_interrupts;
+
+	/* Create sysfs configuration entries */
+	ret = cptlf_create_sysfs_cfg(lfs);
+	if (ret)
+		goto cpt_err_unregister_interrupts;
+
+	/* Enable interrupts */
+	cptlf_enable_misc_intrs(lfs);
+	cptlf_enable_done_intr(lfs);
+
+	/* Register crypto algorithms */
+	ret = cvm_crypto_init(pdev, CPT_96XX, SE_TYPES, lfs_num, 1);
+	if (ret) {
+		dev_err(&pdev->dev, "algorithms registration failed\n");
+		goto cpt_err_disable_irqs;
+	}
+
+	return 0;
+
+cpt_err_disable_irqs:
+	cptlf_disable_done_intr(lfs);
+	cptlf_disable_misc_intrs(lfs);
+	cptlf_free_irqs_affinity(lfs);
+	cptlf_hw_cleanup(lfs);
+cpt_err_unregister_interrupts:
+	cptlf_unregister_interrupts(lfs);
+cpt_err_sw_cleanup:
+	cptlf_sw_cleanup(lfs);
+cpt_err_detach:
+	cpt_detach_rscrs_msg(pdev);
+cpt_err:
+	return ret;
+}
+
+int cptlf_shutdown(struct pci_dev *pdev, struct cptlfs_info *lfs)
+{
+	int ret = 0;
+
+	/* Unregister crypto algorithms */
+	cvm_crypto_exit(pdev);
+
+	/* Disable interrupts */
+	cptlf_disable_done_intr(lfs);
+	cptlf_disable_misc_intrs(lfs);
+
+	/* Remove interrupts affinity */
+	cptlf_free_irqs_affinity(lfs);
+
+	/* Remove sysfs configuration entries */
+	cptlf_delete_sysfs_cfg(lfs);
+
+	/* Cleanup LFs hardware side */
+	cptlf_hw_cleanup(lfs);
+
+	/* Unregister LFs interrupts */
+	cptlf_unregister_interrupts(lfs);
+
+	/* Cleanup LFs software side */
+	cptlf_sw_cleanup(lfs);
+
+	/* Send request to detach LFs */
+	ret = cpt_detach_rscrs_msg(pdev);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.c b/drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.c
new file mode 100644
index 000000000000..12d0271508a8
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.c
@@ -0,0 +1,314 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt9x_mbox_common.h"
+
+static inline struct otx2_mbox *get_mbox(struct pci_dev *pdev)
+{
+	struct cptpf_dev *cptpf;
+	struct cptvf_dev *cptvf;
+
+	if (pdev->is_physfn) {
+		cptpf = (struct cptpf_dev *) pci_get_drvdata(pdev);
+		return &cptpf->afpf_mbox;
+	}
+
+	cptvf = (struct cptvf_dev *) pci_get_drvdata(pdev);
+	return &cptvf->pfvf_mbox;
+}
+
+static inline int get_pf_id(struct pci_dev *pdev)
+{
+	struct cptpf_dev *cptpf;
+
+	if (pdev->is_physfn) {
+		cptpf = (struct cptpf_dev *) pci_get_drvdata(pdev);
+		return cptpf->pf_id;
+	}
+
+	return 0;
+}
+
+static inline int get_vf_id(struct pci_dev *pdev)
+{
+	struct cptvf_dev *cptvf;
+
+	if (pdev->is_virtfn) {
+		cptvf = (struct cptvf_dev *) pci_get_drvdata(pdev);
+		return cptvf->vf_id;
+	}
+
+	return 0;
+}
+
+static inline struct free_rsrcs_rsp *get_limits(struct pci_dev *pdev)
+{
+	struct cptpf_dev *cptpf;
+	struct cptvf_dev *cptvf;
+
+	if (pdev->is_physfn) {
+		cptpf = (struct cptpf_dev *) pci_get_drvdata(pdev);
+		return &cptpf->limits;
+	}
+
+	cptvf = (struct cptvf_dev *) pci_get_drvdata(pdev);
+	return &cptvf->limits;
+}
+
+int cpt_send_mbox_msg(struct pci_dev *pdev)
+{
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	int ret = 0;
+
+	otx2_mbox_msg_send(mbox, 0);
+	ret = otx2_mbox_wait_for_rsp(mbox, 0);
+	if (ret == -EIO) {
+		dev_err(&pdev->dev, "RVU MBOX timeout.\n");
+		goto error;
+	} else if (ret) {
+		dev_err(&pdev->dev, "RVU MBOX error: %d.\n", ret);
+		ret = -EFAULT;
+		goto error;
+	}
+error:
+	return ret;
+}
+
+int cpt_send_ready_msg(struct pci_dev *pdev)
+{
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct mbox_msghdr *req;
+	int ret = 0;
+
+	req = otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+				      sizeof(struct ready_msg_rsp));
+
+	if (req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	req->id = MBOX_MSG_READY;
+	req->sig = OTX2_MBOX_REQ_SIG;
+	req->pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
+
+int cpt_get_rsrc_cnt(struct pci_dev *pdev)
+{
+	struct free_rsrcs_rsp *limits = get_limits(pdev);
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct mbox_msghdr *rsrc_req;
+	int ret = 0;
+
+	rsrc_req = otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*rsrc_req),
+					   sizeof(struct free_rsrcs_rsp));
+	if (rsrc_req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	rsrc_req->id = MBOX_MSG_FREE_RSRC_CNT;
+	rsrc_req->sig = OTX2_MBOX_REQ_SIG;
+	rsrc_req->pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+
+	if (!limits->cpt)
+		ret = -ENOENT;
+error:
+	return ret;
+}
+
+int cpt_attach_rscrs_msg(struct pci_dev *pdev)
+{
+	struct cptlfs_info *lfs = get_lfs_info(pdev);
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct rsrc_attach *req;
+	int ret = 0;
+
+	req = (struct rsrc_attach *)
+			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+						sizeof(struct msg_rsp));
+	if (req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	memset(req, 0, sizeof(*req));
+	req->hdr.id = MBOX_MSG_ATTACH_RESOURCES;
+	req->hdr.sig = OTX2_MBOX_REQ_SIG;
+	req->hdr.pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	req->cptlfs = lfs->lfs_num;
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+
+	if (!lfs->are_lfs_attached)
+		ret = -EINVAL;
+error:
+	return ret;
+}
+
+int cpt_detach_rscrs_msg(struct pci_dev *pdev)
+{
+	struct cptlfs_info *lfs = get_lfs_info(pdev);
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct rsrc_detach *req;
+	int ret = 0;
+
+	req = (struct rsrc_detach *)
+				otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+							sizeof(struct msg_rsp));
+	if (req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	memset(req, 0, sizeof(*req));
+	req->hdr.id = MBOX_MSG_DETACH_RESOURCES;
+	req->hdr.sig = OTX2_MBOX_REQ_SIG;
+	req->hdr.pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+
+	if (lfs->are_lfs_attached)
+		ret = -EINVAL;
+error:
+	return ret;
+}
+
+int cpt_msix_offset_msg(struct pci_dev *pdev)
+{
+	struct cptlfs_info *lfs = get_lfs_info(pdev);
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct mbox_msghdr *req;
+	int ret = 0, i;
+
+	req = otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*req),
+				      sizeof(struct msix_offset_rsp));
+	if (req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	memset(req, 0, sizeof(*req));
+	req->id = MBOX_MSG_MSIX_OFFSET;
+	req->sig = OTX2_MBOX_REQ_SIG;
+	req->pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+
+	for (i = 0; i < lfs->lfs_num; i++)
+		if (lfs->lf[i].msix_offset == MSIX_VECTOR_INVALID) {
+			dev_err(&pdev->dev,
+				"Invalid msix offset %d for LF %d\n",
+				lfs->lf[i].msix_offset, i);
+			ret = -EINVAL;
+			goto error;
+		}
+error:
+	return ret;
+}
+
+int cpt_send_af_reg_requests(struct pci_dev *pdev)
+{
+	return cpt_send_mbox_msg(pdev);
+}
+
+int cpt_add_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val)
+{
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct cpt_rd_wr_reg_msg *reg_msg;
+	int ret = 0;
+
+	reg_msg = (struct cpt_rd_wr_reg_msg *)
+			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*reg_msg),
+						sizeof(*reg_msg));
+	if (reg_msg == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	reg_msg->hdr.id = MBOX_MSG_CPT_RD_WR_REGISTER;
+	reg_msg->hdr.sig = OTX2_MBOX_REQ_SIG;
+	reg_msg->hdr.pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	reg_msg->is_write = 0;
+	reg_msg->reg_offset = reg;
+	reg_msg->ret_val = val;
+error:
+	return ret;
+}
+
+int cpt_add_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val)
+{
+	struct otx2_mbox *mbox = get_mbox(pdev);
+	struct cpt_rd_wr_reg_msg *reg_msg;
+	int ret = 0;
+
+	reg_msg = (struct cpt_rd_wr_reg_msg *)
+			otx2_mbox_alloc_msg_rsp(mbox, 0, sizeof(*reg_msg),
+						sizeof(*reg_msg));
+	if (reg_msg == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	reg_msg->hdr.id = MBOX_MSG_CPT_RD_WR_REGISTER;
+	reg_msg->hdr.sig = OTX2_MBOX_REQ_SIG;
+	reg_msg->hdr.pcifunc = RVU_PFFUNC(get_pf_id(pdev), get_vf_id(pdev));
+	reg_msg->is_write = 1;
+	reg_msg->reg_offset = reg;
+	reg_msg->val = val;
+error:
+	return ret;
+}
+
+int cpt_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val)
+{
+	int ret = 0;
+
+	ret = cpt_add_read_af_reg(pdev, reg, val);
+	if (ret)
+		goto error;
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
+
+int cpt_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val)
+{
+	int ret = 0;
+
+	ret = cpt_add_write_af_reg(pdev, reg, val);
+	if (ret)
+		goto error;
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.h b/drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.h
new file mode 100644
index 000000000000..06f46b7c2481
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_mbox_common.h
@@ -0,0 +1,56 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_MBOX_COMMON_H
+#define __CPT9X_MBOX_COMNON_H
+
+#include "cpt9x_pf.h"
+#include "cpt9x_vf.h"
+
+#define INVALID_KCRYPTO_ENG_GRP	0xFF
+
+/* Extended ready message response with engine group
+ * number for kernel crypto functionality
+ */
+struct ready_msg_rsp_ex {
+	struct ready_msg_rsp msg;
+	int eng_grp_num;
+};
+
+static inline struct cptlfs_info *get_lfs_info(struct pci_dev *pdev)
+{
+	struct cptpf_dev *cptpf;
+	struct cptvf_dev *cptvf;
+
+	if (pdev->is_physfn) {
+		cptpf = (struct cptpf_dev *) pci_get_drvdata(pdev);
+		return &cptpf->lfs;
+	}
+
+	cptvf = (struct cptvf_dev *) pci_get_drvdata(pdev);
+	return &cptvf->lfs;
+}
+
+int cpt_send_ready_msg(struct pci_dev *pdev);
+int cpt_get_rsrc_cnt(struct pci_dev *pdev);
+int cpt_attach_rscrs_msg(struct pci_dev *pdev);
+int cpt_detach_rscrs_msg(struct pci_dev *pdev);
+int cpt_msix_offset_msg(struct pci_dev *pdev);
+
+int cpt_send_af_reg_requests(struct pci_dev *pdev);
+int cpt_add_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val);
+int cpt_add_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val);
+int cpt_read_af_reg(struct pci_dev *pdev, u64 reg, u64 *val);
+int cpt_write_af_reg(struct pci_dev *pdev, u64 reg, u64 val);
+
+int cpt_send_mbox_msg(struct pci_dev *pdev);
+void dump_mbox_msg(struct device *dev, struct mbox_msghdr *msg, int size);
+
+#endif /* __CPT9X_MBOX_COMMON_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.c b/drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.c
new file mode 100644
index 000000000000..1b258ee35969
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.c
@@ -0,0 +1,121 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/crypto.h>
+#include <crypto/algapi.h>
+#include <crypto/hash.h>
+#include "cpt_common.h"
+#include "cpt_algs.h"
+#include "cpt9x_passthrough.h"
+
+static void passthrough_callback(struct crypto_async_request *req, int err)
+{
+	struct ablkcipher_request *areq;
+	void *ptr;
+
+	pr_info("Passthrough test callback, req = %p, err = %d\n", req, err);
+
+	areq = container_of(req, struct ablkcipher_request, base);
+	ptr = sg_virt(areq->src);
+	kfree(ptr);
+	kfree(areq->src);
+
+	ptr = sg_virt(areq->dst);
+	if (!err)
+		pr_info("Passthrough result data %s\n", (char *) ptr);
+	kfree(ptr);
+	kfree(areq->dst);
+
+	kfree(areq);
+
+	if (!err)
+		pr_info("Passthrough test succeed\n");
+	else
+		pr_info("Passthrough test failed\n");
+}
+
+static int cvm_passthrough(struct pci_dev *pdev, struct ablkcipher_request *req)
+{
+	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	int cpu;
+
+	memset(rctx, 0, sizeof(struct cvm_req_ctx));
+
+	req_info->ctrl.s.dma_mode = DMA_GATHER_SCATTER;
+	req_info->ctrl.s.se_req = SE_CORE_REQ;
+
+	req_info->req.opcode.s.major = MAJOR_OP_MISC |
+					DMA_MODE_FLAG(DMA_GATHER_SCATTER);
+	req_info->req.opcode.s.minor = 3;
+	req_info->req.param1 = req->nbytes; /* Data length */
+	req_info->req.param2 = 0x0;
+
+	req_info->callback = (void *) cvm_callback;
+	req_info->callback_arg = &req->base;
+	req_info->req_type = PASSTHROUGH_REQ;
+
+	req_info->in[0].vptr = sg_virt(req->src);
+	req_info->in[0].size = req->src->length;
+	req_info->incnt = 1;
+
+	req_info->out[0].vptr = sg_virt(req->dst);
+	req_info->out[0].size = req->dst->length;
+	req_info->outcnt = 1;
+
+	cpu = get_cpu();
+	put_cpu();
+
+	return cpt_do_request(pdev, req_info, cpu);
+}
+
+int run_passthrough_test(struct pci_dev *pdev, const char *buf, int size)
+{
+	struct ablkcipher_request *areq;
+	struct scatterlist *src, *dst;
+	void *ptr;
+	int ret = 0;
+
+	/* Allocate buffer for a request */
+	areq = kzalloc(sizeof(*areq) + sizeof(struct cvm_req_ctx), GFP_KERNEL);
+	if (!areq)
+		return -ENOMEM;
+
+	/* Allocate src buffer */
+	ptr = kzalloc(size, GFP_KERNEL);
+	if (!ptr)
+		return -ENOMEM;
+	src = kzalloc(sizeof(struct scatterlist), GFP_KERNEL);
+	if (!src)
+		return -ENOMEM;
+	memcpy(ptr, buf, size);
+	sg_init_one(src, ptr, size);
+
+	/* Allocate dst buffer */
+	ptr = kzalloc(size, GFP_KERNEL);
+	if (!ptr)
+		return -ENOMEM;
+	dst = kzalloc(sizeof(struct scatterlist), GFP_KERNEL);
+	if (!dst)
+		return -ENOMEM;
+	sg_init_one(dst, ptr, size);
+
+	ablkcipher_request_set_callback(areq, 0, passthrough_callback, NULL);
+	ablkcipher_request_set_crypt(areq, src, dst, size, NULL);
+
+	dev_info(&pdev->dev,
+		 "Run passthrough test size %d, data - %s\n", size, buf);
+
+	ret = cvm_passthrough(pdev, areq);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.h b/drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.h
new file mode 100644
index 000000000000..f47992f6b265
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_passthrough.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_PASSTHROUGH_H
+#define __CPT9X_PASSTHROUGH_H
+
+int run_passthrough_test(struct pci_dev *pdev, const char *buf, int size);
+
+#endif /* __CPT9X_PASSTHROUGH_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_pf.h b/drivers/crypto/cavium/cpt/9x/cpt9x_pf.h
new file mode 100644
index 000000000000..f59e17f57846
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_pf.h
@@ -0,0 +1,67 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_PF_H
+#define __CPT9X_PF_H
+
+#include "cpt_ucode.h"
+#include "cpt9x_lf.h"
+#include "cpt9x_quota.h"
+
+struct cptpf_dev;
+struct cptvf_info {
+	struct cptpf_dev	*cptpf;	/* PF pointer this VF belongs to */
+	struct work_struct	vfpf_mbox_work;
+	struct kobject		*limits_kobj;
+	struct pci_dev		*vf_dev;
+	int			vf_id;
+	int			intr_idx; /* vf_id % 64 */
+};
+
+struct cpt_limits {
+	struct mutex lock;
+	struct quotas *cpt;
+};
+
+struct cptpf_dev {
+	void __iomem *reg_base;		/* CPT PF registers start address */
+	void __iomem *afpf_mbox_base;	/* PF-AF mbox start address */
+	void __iomem *vfpf_mbox_base;   /* VF-PF mbox start address */
+	struct pci_dev *pdev;		/* PCI device handle */
+	struct cptvf_info vf[CPT_9X_MAX_VFS_NUM];
+	struct cptlfs_info lfs;		/* CPT LFs attached to this PF */
+	struct free_rsrcs_rsp limits;   /* Maximum limits for all VFs and PF */
+	struct cpt_limits vf_limits;	/* Limits for each VF */
+	struct engine_groups eng_grps;	/* Engine groups information */
+
+	/* AF <=> PF mbox */
+	struct otx2_mbox	afpf_mbox;
+	struct work_struct	afpf_mbox_work;
+	struct workqueue_struct *afpf_mbox_wq;
+
+	/* VF <=> PF mbox */
+	struct otx2_mbox	vfpf_mbox;
+	struct workqueue_struct *vfpf_mbox_wq;
+
+	bool irq_registered[CPT_96XX_PF_MSIX_VECTORS];	/* Is IRQ registered */
+	u8 pf_id;		/* RVU PF number */
+	u8 max_vfs;		/* Maximum number of VFs supported by CPT */
+	u8 enabled_vfs;		/* Number of enabled VFs */
+	u8 crypto_eng_grp;	/* Symmetric crypto engine group number */
+};
+
+irqreturn_t cptpf_afpf_mbox_intr(int irq, void *arg);
+irqreturn_t cptpf_vfpf_mbox_intr(int irq, void *arg);
+void cptpf_afpf_mbox_handler(struct work_struct *work);
+void cptpf_vfpf_mbox_handler(struct work_struct *work);
+int cpt_disable_all_cores(struct cptpf_dev *cptpf);
+int cptpf_send_crypto_eng_grp_msg(struct cptpf_dev *cptpf,
+				  int crypto_eng_grp);
+#endif /* __CPT9X_PF_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_pf_main.c b/drivers/crypto/cavium/cpt/9x/cpt9x_pf_main.c
new file mode 100644
index 000000000000..c52143a05110
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_pf_main.c
@@ -0,0 +1,713 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/firmware.h>
+#include "rvu_reg.h"
+#include "cpt9x_mbox_common.h"
+
+#define DRV_NAME	"octeontx2-cpt"
+#define DRV_VERSION	"1.0"
+
+DEFINE_CPT_DEBUG_PARM(debug);
+
+void cptpf_enable_vf_flr_intrs(struct cptpf_dev *cptpf)
+{
+	/* Clear interrupt if any */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(0),
+		    ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(1),
+		    ~0x0ULL);
+
+	/* Enable VF FLR interrupts */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFFLR_INT_ENA_W1SX(0), ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFFLR_INT_ENA_W1SX(1), ~0x0ULL);
+}
+
+void cptpf_disable_vf_flr_intrs(struct cptpf_dev *cptpf)
+{
+	/* Disable VF FLR interrupts */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFFLR_INT_ENA_W1CX(0), ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFFLR_INT_ENA_W1CX(1), ~0x0ULL);
+
+	/* Clear interrupt if any */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(0),
+		    ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(1),
+		    ~0x0ULL);
+}
+
+static void cptpf_enable_afpf_mbox_intrs(struct cptpf_dev *cptpf)
+{
+	/* Clear interrupt if any */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT, 0x1ULL);
+
+	/* Enable AF-PF interrupt */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT_ENA_W1S,
+		    0x1ULL);
+}
+
+static void cptpf_disable_afpf_mbox_intrs(struct cptpf_dev *cptpf)
+{
+	/* Disable AF-PF interrupt */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT_ENA_W1C,
+		    0x1ULL);
+
+	/* Clear interrupt if any */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT, 0x1ULL);
+}
+
+static void cptpf_enable_vfpf_mbox_intrs(struct cptpf_dev *cptpf, int numvfs)
+{
+	int ena_bits;
+
+	/* Clear any pending interrupts */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFPF_MBOX_INTX(0),
+		      ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFPF_MBOX_INTX(1),
+		      ~0x0ULL);
+
+	/* Enable VF interrupts for VFs from 0 to 63 */
+	ena_bits = ((numvfs - 1) % 64);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFPF_MBOX_INT_ENA_W1SX(0),
+		    GENMASK_ULL(ena_bits, 0));
+
+	if (numvfs > 64) {
+		/* Enable VF interrupts for VFs from 64 to 127 */
+		ena_bits = numvfs - 64 - 1;
+		cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+			    RVU_PF_VFPF_MBOX_INT_ENA_W1SX(1),
+			    GENMASK_ULL(ena_bits, 0));
+	}
+}
+
+static void cptpf_disable_vfpf_mbox_intrs(struct cptpf_dev *cptpf)
+{
+	/* Disable VF-PF interrupts */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFPF_MBOX_INT_ENA_W1CX(0),
+		    ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+		    RVU_PF_VFPF_MBOX_INT_ENA_W1CX(1),
+		    ~0x0ULL);
+
+	/* Clear any pending interrupts */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFPF_MBOX_INTX(0),
+		      ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFPF_MBOX_INTX(1),
+		      ~0x0ULL);
+}
+
+static irqreturn_t cptpf_vf_flr_intr(int irq, void *arg)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) arg;
+
+	/* Clear transaction pending register */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFTRPENDX(0),
+		    ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFTRPENDX(1),
+		    ~0x0ULL);
+
+	/* Clear interrupt if any */
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(0),
+		    ~0x0ULL);
+	cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_VFFLR_INTX(1),
+		    ~0x0ULL);
+
+	return IRQ_HANDLED;
+}
+
+static void cptpf_unregister_interrupts(struct cptpf_dev *cptpf)
+{
+	int i;
+
+	for (i = 0; i < CPT_96XX_PF_MSIX_VECTORS; i++) {
+		if (cptpf->irq_registered[i])
+			free_irq(pci_irq_vector(cptpf->pdev, i), cptpf);
+		cptpf->irq_registered[i] = false;
+	}
+
+	pci_free_irq_vectors(cptpf->pdev);
+}
+
+static int cptpf_register_interrupts(struct cptpf_dev *cptpf)
+{
+	u32 num_vec;
+	int ret;
+
+	num_vec = CPT_96XX_PF_MSIX_VECTORS;
+
+	/* Enable MSI-X */
+	ret = pci_alloc_irq_vectors(cptpf->pdev, num_vec, num_vec,
+				    PCI_IRQ_MSIX);
+	if (ret < 0) {
+		dev_err(&cptpf->pdev->dev,
+			"Request for %d msix vectors failed\n", num_vec);
+		return ret;
+	}
+
+	/* Register VF FLR interrupt handler */
+	ret = request_irq(pci_irq_vector(cptpf->pdev,
+			  RVU_PF_INT_VEC_VFFLR0), cptpf_vf_flr_intr, 0,
+			  "CPTPF FLR0", cptpf);
+	if (ret)
+		goto err;
+	cptpf->irq_registered[RVU_PF_INT_VEC_VFFLR0] = true;
+
+	ret = request_irq(pci_irq_vector(cptpf->pdev,
+			  RVU_PF_INT_VEC_VFFLR1), cptpf_vf_flr_intr, 0,
+			  "CPTPF FLR1", cptpf);
+	if (ret)
+		goto err;
+	cptpf->irq_registered[RVU_PF_INT_VEC_VFFLR1] = true;
+
+	/* Register AF-PF mailbox interrupt handler */
+	ret = request_irq(pci_irq_vector(cptpf->pdev,
+			  RVU_PF_INT_VEC_AFPF_MBOX), cptpf_afpf_mbox_intr, 0,
+			  "CPTAFPF Mbox", cptpf);
+	if (ret)
+		goto err;
+	cptpf->irq_registered[RVU_PF_INT_VEC_AFPF_MBOX] = true;
+
+	/* Register VF-PF mailbox interrupt handler */
+	ret = request_irq(pci_irq_vector(cptpf->pdev,
+			  RVU_PF_INT_VEC_VFPF_MBOX0), cptpf_vfpf_mbox_intr, 0,
+			  "CPTVFPF Mbox0", cptpf);
+	if (ret)
+		goto err;
+	cptpf->irq_registered[RVU_PF_INT_VEC_VFPF_MBOX0] = true;
+
+	ret = request_irq(pci_irq_vector(cptpf->pdev,
+			  RVU_PF_INT_VEC_VFPF_MBOX1), cptpf_vfpf_mbox_intr, 0,
+			  "CPTVFPF Mbox1", cptpf);
+	if (ret)
+		goto err;
+	cptpf->irq_registered[RVU_PF_INT_VEC_VFPF_MBOX1] = true;
+
+	return 0;
+err:
+	dev_err(&cptpf->pdev->dev, "Failed to register interrupts\n");
+	cptpf_unregister_interrupts(cptpf);
+	return ret;
+}
+
+static int cptpf_afpf_mbox_init(struct cptpf_dev *cptpf)
+{
+	int err;
+
+	cptpf->afpf_mbox_wq = alloc_workqueue("cpt_afpf_mailbox",
+					      WQ_UNBOUND | WQ_HIGHPRI |
+					      WQ_MEM_RECLAIM, 1);
+	if (!cptpf->afpf_mbox_wq)
+		return -ENOMEM;
+
+	err = otx2_mbox_init(&cptpf->afpf_mbox, cptpf->afpf_mbox_base,
+			     cptpf->pdev, cptpf->reg_base, MBOX_DIR_PFAF, 1);
+	if (err)
+		goto error;
+
+	INIT_WORK(&cptpf->afpf_mbox_work, cptpf_afpf_mbox_handler);
+	return 0;
+error:
+	destroy_workqueue(cptpf->afpf_mbox_wq);
+	return err;
+}
+
+static int cptpf_vfpf_mbox_init(struct cptpf_dev *cptpf, int numvfs)
+{
+	int err, i;
+
+	cptpf->vfpf_mbox_wq = alloc_workqueue("cpt_vfpf_mailbox",
+					      WQ_UNBOUND | WQ_HIGHPRI |
+					      WQ_MEM_RECLAIM, 1);
+	if (!cptpf->vfpf_mbox_wq)
+		return -ENOMEM;
+
+	err = otx2_mbox_init(&cptpf->vfpf_mbox, cptpf->vfpf_mbox_base,
+			     cptpf->pdev, cptpf->reg_base, MBOX_DIR_PFVF,
+			     numvfs);
+	if (err)
+		goto error;
+
+	for (i = 0; i < numvfs; i++) {
+		cptpf->vf[i].vf_id = i;
+		cptpf->vf[i].cptpf = cptpf;
+		cptpf->vf[i].intr_idx = i % 64;
+		INIT_WORK(&cptpf->vf[i].vfpf_mbox_work,
+			  cptpf_vfpf_mbox_handler);
+	}
+	return 0;
+error:
+	flush_workqueue(cptpf->vfpf_mbox_wq);
+	destroy_workqueue(cptpf->vfpf_mbox_wq);
+	return err;
+}
+
+static void cptpf_afpf_mbox_destroy(struct cptpf_dev *cptpf)
+{
+	flush_workqueue(cptpf->afpf_mbox_wq);
+	destroy_workqueue(cptpf->afpf_mbox_wq);
+	otx2_mbox_destroy(&cptpf->afpf_mbox);
+}
+
+static void cptpf_vfpf_mbox_destroy(struct cptpf_dev *cptpf)
+{
+	flush_workqueue(cptpf->vfpf_mbox_wq);
+	destroy_workqueue(cptpf->vfpf_mbox_wq);
+	otx2_mbox_destroy(&cptpf->vfpf_mbox);
+}
+
+static int cptpf_device_reset(struct cptpf_dev *cptpf)
+{
+	int timeout = 10;
+	int ret = 0;
+	u64 reg;
+
+	ret = cpt_write_af_reg(cptpf->pdev, CPT_AF_BLK_RST, 0x1);
+	if (ret)
+		goto error;
+
+	do {
+		ret = cpt_read_af_reg(cptpf->pdev, CPT_AF_BLK_RST, &reg);
+		if (ret)
+			goto error;
+
+		if (!((reg >> 63) & 0x1))
+			break;
+
+		usleep_range(10000, 20000);
+		if (timeout-- < 0)
+			return -EBUSY;
+	} while (1);
+error:
+	return ret;
+}
+
+static int cptpf_device_init(struct cptpf_dev *cptpf)
+{
+	union cptx_af_constants1 af_cnsts1 = {0};
+	struct device *dev = &cptpf->pdev->dev;
+	int ret = 0;
+	u16 sdevid;
+
+	/* Reset the CPT PF device */
+	ret = cptpf_device_reset(cptpf);
+	if (ret)
+		goto error;
+
+	/* Read PCI subsystem id */
+	ret = pci_read_config_word(cptpf->pdev, PCI_SUBSYSTEM_ID, &sdevid);
+	if (ret)
+		goto error;
+	if (sdevid != CPT_96XX_PCI_PF_SUBSYS_ID) {
+		dev_err(dev, "Invalid subsystem id\n");
+		ret = -ENODEV;
+		goto error;
+	}
+
+	/* Get number of SE, IE and AE engines */
+	ret = cpt_read_af_reg(cptpf->pdev, CPT_AF_CONSTANTS1, &af_cnsts1.u);
+	if (ret)
+		goto error;
+
+	cptpf->eng_grps.avail.max_se_cnt = af_cnsts1.s.se;
+	cptpf->eng_grps.avail.max_ie_cnt = af_cnsts1.s.ie;
+	cptpf->eng_grps.avail.max_ae_cnt = af_cnsts1.s.ae;
+
+	/* Disable all cores */
+	ret = cpt_disable_all_cores(cptpf);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
+
+static void cpt_destroy_sysfs_vf_limits(struct cptpf_dev *cptpf)
+{
+	struct cptvf_info *vf_info;
+	int i;
+
+	quotas_free(cptpf->vf_limits.cpt);
+	cptpf->vf_limits.cpt = NULL;
+
+	for (i = 0; i < cptpf->enabled_vfs; i++) {
+		vf_info = &cptpf->vf[i];
+		if (!vf_info->limits_kobj)
+			continue;
+
+		kobject_del(vf_info->limits_kobj);
+		vf_info->limits_kobj = NULL;
+		pci_dev_put(vf_info->vf_dev);
+		vf_info->vf_dev = NULL;
+	}
+}
+
+static int cpt_alloc_vf_limits(struct cptpf_dev *cptpf)
+{
+	int avail_lfs, lfs_per_vf;
+	int i, ret, online_cpus;
+
+	mutex_init(&cptpf->vf_limits.lock);
+
+	/* Create limit structures for CPT resource types */
+	cptpf->vf_limits.cpt = quotas_alloc(cptpf->enabled_vfs,
+					    cptpf->limits.cpt,
+					    cptpf->limits.cpt, 0,
+					    &cptpf->vf_limits.lock, NULL);
+	if (cptpf->vf_limits.cpt == NULL) {
+		dev_err(&cptpf->pdev->dev,
+			"Failed to allocate cpt limits structures");
+			return -ENOMEM;
+
+	}
+
+	avail_lfs = cptpf->vf_limits.cpt->max_sum;
+	online_cpus = num_online_cpus();
+	if (avail_lfs < online_cpus) {
+		dev_err(&cptpf->pdev->dev,
+			"CPT LFs num %d < than required for kernel crypto %d",
+			avail_lfs, online_cpus);
+		ret = -ENOENT;
+		goto error;
+	}
+	avail_lfs -= online_cpus;
+
+	lfs_per_vf = cptpf->enabled_vfs == 1 ?
+		     1 : avail_lfs / (cptpf->enabled_vfs - 1);
+	if (lfs_per_vf <= 0) {
+		dev_err(&cptpf->pdev->dev,
+			"Not enough CPT LFs %d for %d VFs",
+			avail_lfs, cptpf->enabled_vfs);
+		ret = -ENOENT;
+		goto error;
+	}
+
+	cptpf->vf_limits.cpt->a[0].val = online_cpus;
+	for (i = 1; i < cptpf->enabled_vfs; i++)
+		cptpf->vf_limits.cpt->a[i].val = lfs_per_vf;
+	return 0;
+error:
+	quotas_free(cptpf->vf_limits.cpt);
+	return ret;
+}
+
+static int cpt_create_sysfs_vf_limits(struct cptpf_dev *cptpf)
+{
+	struct pci_dev *pdev = NULL;
+	struct cptvf_info *vf_info;
+	int ret, i = 0;
+
+	/* loop through all the VFs and create sysfs entries for them */
+	while ((pdev = pci_get_device(cptpf->pdev->vendor,
+				      CPT_PCI_VF_9X_DEVICE_ID, pdev))) {
+		if (!pdev->is_virtfn || (pdev->physfn != cptpf->pdev))
+			continue;
+
+		vf_info = &cptpf->vf[i];
+		vf_info->vf_dev = pci_dev_get(pdev);
+		vf_info->limits_kobj = kobject_create_and_add("limits",
+							      &pdev->dev.kobj);
+		if (vf_info->limits_kobj == NULL) {
+			ret = -ENOMEM;
+			goto error;
+		}
+
+		if (quota_sysfs_create("cpt", vf_info->limits_kobj, &pdev->dev,
+				       &cptpf->vf_limits.cpt->a[i],
+				       NULL) != 0) {
+			dev_err(&cptpf->pdev->dev,
+				"Failed to create cpt limits sysfs for %s.",
+				pci_name(pdev));
+			ret = -EFAULT;
+			goto error;
+		}
+		i++;
+	}
+
+	return 0;
+error:
+	cpt_destroy_sysfs_vf_limits(cptpf);
+	return ret;
+}
+
+static int cptpf_sriov_configure(struct pci_dev *pdev, int numvfs)
+{
+	struct cptpf_dev *cptpf = pci_get_drvdata(pdev);
+	int ret = 0;
+
+	if (numvfs > cptpf->max_vfs)
+		numvfs = cptpf->max_vfs;
+
+	if (numvfs > 0) {
+		ret = cpt_try_create_default_eng_grps(cptpf->pdev,
+						      &cptpf->eng_grps,
+						      CPT_96XX);
+		if (ret)
+			goto error;
+
+		cptpf->enabled_vfs = numvfs;
+		ret = cpt_alloc_vf_limits(cptpf);
+		if (ret)
+			goto error;
+
+		ret = pci_enable_sriov(pdev, numvfs);
+		if (ret)
+			goto error;
+
+		ret = cpt_create_sysfs_vf_limits(cptpf);
+		if (ret)
+			goto error;
+
+		cpt_set_eng_grps_is_rdonly(&cptpf->eng_grps, true);
+		try_module_get(THIS_MODULE);
+		ret = numvfs;
+	} else {
+		pci_disable_sriov(pdev);
+		cpt_destroy_sysfs_vf_limits(cptpf);
+		cpt_set_eng_grps_is_rdonly(&cptpf->eng_grps, false);
+		module_put(THIS_MODULE);
+		cptpf->enabled_vfs = 0;
+	}
+
+	dev_notice(&cptpf->pdev->dev, "VFs enabled: %d\n", ret);
+	return ret;
+error:
+	cptpf->enabled_vfs = 0;
+	return ret;
+}
+
+static void cptpf_eng_grp_hndlr(void *obj)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) obj;
+	struct engine_group_info *grp;
+	int crypto_eng_grp = INVALID_KCRYPTO_ENG_GRP;
+	int i;
+
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+		grp = &cptpf->eng_grps.grp[i];
+		if (!grp->is_enabled)
+			continue;
+
+		if (cpt_eng_grp_has_eng_type(grp, SE_TYPES) &&
+		    !cpt_eng_grp_has_eng_type(grp, IE_TYPES) &&
+		    !cpt_eng_grp_has_eng_type(grp, AE_TYPES)) {
+			crypto_eng_grp = i;
+			break;
+		}
+	}
+
+	if (cptpf->crypto_eng_grp == crypto_eng_grp)
+		return;
+	cptpf_send_crypto_eng_grp_msg(cptpf, crypto_eng_grp);
+}
+
+static int cptpf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct cptpf_dev *cptpf;
+	u64 vfpf_mbox_base;
+	int err;
+
+	cptpf = devm_kzalloc(dev, sizeof(*cptpf), GFP_KERNEL);
+	if (!cptpf)
+		return -ENOMEM;
+
+	pci_set_drvdata(pdev, cptpf);
+	cptpf->pdev = pdev;
+	cptpf->crypto_eng_grp = INVALID_KCRYPTO_ENG_GRP;
+	cptpf->max_vfs = pci_sriov_get_totalvfs(pdev);
+	cpt_set_dbg_level(debug);
+
+	err = pcim_enable_device(pdev);
+	if (err) {
+		dev_err(dev, "Failed to enable PCI device\n");
+		goto cpt_err_set_drvdata;
+	}
+
+	err = pci_request_regions(pdev, DRV_NAME);
+	if (err) {
+		dev_err(dev, "PCI request regions failed 0x%x\n", err);
+		goto cpt_err_set_drvdata;
+	}
+
+	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "Unable to get usable DMA configuration\n");
+		goto cpt_err_release_regions;
+	}
+
+	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");
+		goto cpt_err_release_regions;
+	}
+
+	/* Map PF's configuration registers */
+	cptpf->reg_base = pcim_iomap(pdev, PCI_PF_REG_BAR_NUM, 0);
+	if (!cptpf->reg_base) {
+		dev_err(&pdev->dev, "Unable to map BAR2\n");
+		err = -ENODEV;
+		goto cpt_err_release_regions;
+	}
+
+	/* Map AF-PF mailbox memory */
+	cptpf->afpf_mbox_base = ioremap_wc(pci_resource_start(cptpf->pdev,
+					   PCI_MBOX_BAR_NUM),
+					   pci_resource_len(cptpf->pdev,
+					   PCI_MBOX_BAR_NUM));
+	if (!cptpf->afpf_mbox_base) {
+		dev_err(&pdev->dev, "Unable to map BAR4\n");
+		err = -ENODEV;
+		goto cpt_err_release_regions;
+	}
+
+	/* Map VF-PF mailbox memory */
+	vfpf_mbox_base = readq((void __iomem *) ((u64)cptpf->reg_base +
+			       RVU_PF_VF_BAR4_ADDR));
+	if (!vfpf_mbox_base) {
+		dev_err(&pdev->dev, "VF-PF mailbox address not configured\n");
+		err = -ENOMEM;
+		goto cpt_err_iounmap_afpf;
+	}
+	cptpf->vfpf_mbox_base = ioremap_wc(vfpf_mbox_base,
+					   MBOX_SIZE * cptpf->max_vfs);
+	if (!cptpf->vfpf_mbox_base) {
+		dev_err(&pdev->dev,
+			"Mapping of VF-PF mailbox address failed\n");
+		err = -ENOMEM;
+		goto cpt_err_iounmap_afpf;
+	}
+
+	/* Initialize AF-PF mailbox */
+	err = cptpf_afpf_mbox_init(cptpf);
+	if (err)
+		goto cpt_err_iounmap_vfpf;
+
+	/* Initialize VF-PF mailbox */
+	err = cptpf_vfpf_mbox_init(cptpf, cptpf->max_vfs);
+	if (err)
+		goto cpt_err_destroy_afpf_mbox;
+
+	/* Register interrupts */
+	err = cptpf_register_interrupts(cptpf);
+	if (err)
+		goto cpt_err_destroy_vfpf_mbox;
+
+	/* Enable VF FLR interrupts */
+	cptpf_enable_vf_flr_intrs(cptpf);
+
+	/* Enable AF-PF mailbox interrupts */
+	cptpf_enable_afpf_mbox_intrs(cptpf);
+
+	/* Enable VF-PF mailbox interrupts */
+	cptpf_enable_vfpf_mbox_intrs(cptpf, cptpf->max_vfs);
+
+	/* Initialize CPT PF device */
+	err = cptpf_device_init(cptpf);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+	/* Send ready message */
+	err = cpt_send_ready_msg(cptpf->pdev);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+	/* Get available resources count */
+	err = cpt_get_rsrc_cnt(cptpf->pdev);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+	/* Initialize engine groups */
+	err = cpt_init_eng_grps(pdev, &cptpf->eng_grps, CPT_96XX);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+
+	/* Set engine group create/delete handler */
+	cpt_set_eng_grps_plat_hndlr(&cptpf->eng_grps,
+				    cptpf_eng_grp_hndlr);
+	return 0;
+
+cpt_err_unregister_interrupts:
+	cptpf_disable_vfpf_mbox_intrs(cptpf);
+	cptpf_disable_afpf_mbox_intrs(cptpf);
+	cptpf_disable_vf_flr_intrs(cptpf);
+	cptpf_unregister_interrupts(cptpf);
+cpt_err_destroy_vfpf_mbox:
+	cptpf_vfpf_mbox_destroy(cptpf);
+cpt_err_destroy_afpf_mbox:
+	cptpf_afpf_mbox_destroy(cptpf);
+cpt_err_iounmap_vfpf:
+	iounmap(cptpf->vfpf_mbox_base);
+cpt_err_iounmap_afpf:
+	iounmap(cptpf->afpf_mbox_base);
+cpt_err_release_regions:
+	pci_release_regions(pdev);
+cpt_err_set_drvdata:
+	pci_set_drvdata(pdev, NULL);
+	return err;
+}
+
+static void cptpf_remove(struct pci_dev *pdev)
+{
+	struct cptpf_dev *cptpf = pci_get_drvdata(pdev);
+
+	if (!cptpf)
+		return;
+
+	/* Disable SRIOV */
+	pci_disable_sriov(pdev);
+	/* Cleanup engine groups */
+	cpt_cleanup_eng_grps(pdev, &cptpf->eng_grps);
+	/* Disable VF-PF interrupts */
+	cptpf_disable_vfpf_mbox_intrs(cptpf);
+	/* Disable AF-PF mailbox interrupt */
+	cptpf_disable_afpf_mbox_intrs(cptpf);
+	/* Disable VF FLR interrupts */
+	cptpf_disable_vf_flr_intrs(cptpf);
+	/* Unregister CPT interrupts */
+	cptpf_unregister_interrupts(cptpf);
+	/* Destroy AF-PF mbox */
+	cptpf_afpf_mbox_destroy(cptpf);
+	/* Destroy VF-PF mbox */
+	cptpf_vfpf_mbox_destroy(cptpf);
+	/* Unmap VF-PF mailbox memory */
+	iounmap(cptpf->vfpf_mbox_base);
+	/* Unmap AF-PF mailbox memory */
+	iounmap(cptpf->afpf_mbox_base);
+	pci_release_regions(pdev);
+	pci_set_drvdata(pdev, NULL);
+}
+
+/* Supported devices */
+static const struct pci_device_id cpt_id_table[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, CPT_PCI_PF_9X_DEVICE_ID) },
+	{ 0, }  /* end of table */
+};
+
+static struct pci_driver cpt_pci_driver = {
+	.name = DRV_NAME,
+	.id_table = cpt_id_table,
+	.probe = cptpf_probe,
+	.remove = cptpf_remove,
+	.sriov_configure = cptpf_sriov_configure
+};
+
+module_pci_driver(cpt_pci_driver);
+
+MODULE_AUTHOR("Marvell International Ltd.");
+MODULE_DESCRIPTION("Marvell OcteonTX2 CPT Physical Function Driver");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, cpt_id_table);
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_pf_mbox.c b/drivers/crypto/cavium/cpt/9x/cpt9x_pf_mbox.c
new file mode 100644
index 000000000000..4dfb440941f4
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_pf_mbox.c
@@ -0,0 +1,417 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "rvu_reg.h"
+#include "cpt9x_mbox_common.h"
+
+static int forward_to_af(struct cptpf_dev *cptpf, struct cptvf_info *vf,
+			 struct mbox_msghdr *req, int size)
+{
+	struct mbox_msghdr *msg;
+	int ret = 0;
+
+	msg = otx2_mbox_alloc_msg(&cptpf->afpf_mbox, 0, size);
+	if (msg == NULL)
+		return -ENOMEM;
+
+	memcpy((uint8_t *)msg + sizeof(struct mbox_msghdr),
+	       (uint8_t *)req + sizeof(struct mbox_msghdr), size);
+	msg->id = req->id;
+	msg->pcifunc = req->pcifunc;
+	msg->sig = req->sig;
+	msg->ver = req->ver;
+
+	otx2_mbox_msg_send(&cptpf->afpf_mbox, 0);
+	ret = otx2_mbox_wait_for_rsp(&cptpf->afpf_mbox, 0);
+	if (ret == -EIO) {
+		dev_err(&cptpf->pdev->dev, "RVU MBOX timeout.\n");
+		goto error;
+	} else if (ret) {
+		dev_err(&cptpf->pdev->dev, "RVU MBOX error: %d.\n", ret);
+		ret = -EFAULT;
+		goto error;
+	}
+error:
+	return ret;
+}
+
+static int check_attach_rsrcs_req(struct cptpf_dev *cptpf,
+				  struct cptvf_info *vf,
+				  struct mbox_msghdr *req, int size)
+{
+	struct rsrc_attach *rsrc_req = (struct rsrc_attach *)req;
+
+	mutex_lock(&cptpf->vf_limits.lock);
+
+	if (rsrc_req->sso > 0 || rsrc_req->ssow > 0 || rsrc_req->npalf > 0 ||
+	    rsrc_req->timlfs > 0 || rsrc_req->nixlf > 0 ||
+	    rsrc_req->cptlfs > cptpf->vf_limits.cpt->a[vf->vf_id].val) {
+		dev_err(&cptpf->pdev->dev,
+			"Invalid ATTACH_RESOURCES request from %s\n",
+			dev_name(&vf->vf_dev->dev));
+
+		mutex_unlock(&cptpf->vf_limits.lock);
+		return -EINVAL;
+	}
+
+	mutex_unlock(&cptpf->vf_limits.lock);
+	return forward_to_af(cptpf, vf, req, size);
+}
+
+static int reply_free_rsrc_cnt(struct cptpf_dev *cptpf, struct cptvf_info *vf,
+			       struct mbox_msghdr *req)
+{
+	struct free_rsrcs_rsp *rsp;
+
+	rsp = (struct free_rsrcs_rsp *) otx2_mbox_alloc_msg(&cptpf->vfpf_mbox,
+							    vf->vf_id,
+							    sizeof(*rsp));
+	if (rsp == NULL)
+		return -ENOMEM;
+
+	memset(rsp + sizeof(*req), 0, sizeof(*rsp) - sizeof(*req));
+	rsp->hdr.id = MBOX_MSG_FREE_RSRC_CNT;
+	rsp->hdr.pcifunc = req->pcifunc;
+	rsp->hdr.sig = OTX2_MBOX_RSP_SIG;
+
+	mutex_lock(&cptpf->vf_limits.lock);
+	rsp->cpt = cptpf->vf_limits.cpt->a[vf->vf_id].val;
+	mutex_unlock(&cptpf->vf_limits.lock);
+	return 0;
+}
+
+static int reply_ready_msg_ex(struct cptpf_dev *cptpf, struct cptvf_info *vf,
+			      struct mbox_msghdr *req)
+{
+	struct engine_group_info *grp;
+	struct ready_msg_rsp_ex *rsp;
+	int i;
+
+	rsp = (struct ready_msg_rsp_ex *)
+			      otx2_mbox_alloc_msg(&cptpf->vfpf_mbox, vf->vf_id,
+						  sizeof(*rsp));
+	if (!rsp)
+		return -ENOMEM;
+
+	rsp->msg.hdr.id = MBOX_MSG_READY;
+	rsp->msg.hdr.sig = OTX2_MBOX_RSP_SIG;
+	rsp->msg.hdr.pcifunc = req->pcifunc;
+	rsp->eng_grp_num = INVALID_KCRYPTO_ENG_GRP;
+
+	mutex_lock(&cptpf->eng_grps.lock);
+
+	/* Find engine group for kernel crypto functionality, select first
+	 * engine group which is configured and has only SE engines attached
+	 */
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+		grp = &cptpf->eng_grps.grp[i];
+		if (!grp->is_enabled)
+			continue;
+
+		if (cpt_eng_grp_has_eng_type(grp, SE_TYPES) &&
+		    !cpt_eng_grp_has_eng_type(grp, IE_TYPES) &&
+		    !cpt_eng_grp_has_eng_type(grp, AE_TYPES)) {
+			rsp->eng_grp_num = i;
+			break;
+		}
+	}
+
+	mutex_unlock(&cptpf->eng_grps.lock);
+	return 0;
+}
+
+static int cptpf_handle_vf_req(struct cptpf_dev *cptpf, struct cptvf_info *vf,
+			       struct mbox_msghdr *req, int size)
+{
+	int err = 0;
+
+	/* Check if msg is valid, if not reply with an invalid msg */
+	if (req->sig != OTX2_MBOX_REQ_SIG)
+		return otx2_reply_invalid_msg(&cptpf->vfpf_mbox, vf->vf_id,
+					      req->pcifunc, req->id);
+	switch (req->id) {
+	case MBOX_MSG_READY:
+		err = reply_ready_msg_ex(cptpf, vf, req);
+		break;
+
+	case MBOX_MSG_FREE_RSRC_CNT:
+		err = reply_free_rsrc_cnt(cptpf, vf, req);
+		break;
+
+	case MBOX_MSG_ATTACH_RESOURCES:
+		err = check_attach_rsrcs_req(cptpf, vf, req, size);
+		break;
+
+	default:
+		err = forward_to_af(cptpf, vf, req, size);
+		break;
+	}
+
+	return err;
+}
+
+int cptpf_send_crypto_eng_grp_msg(struct cptpf_dev *cptpf, int crypto_eng_grp)
+{
+	struct cpt_set_crypto_grp_req_msg *req;
+	struct pci_dev *pdev = cptpf->pdev;
+	int tmp_crypto_eng_grp;
+	int ret = 0;
+
+	req = (struct cpt_set_crypto_grp_req_msg *)
+			otx2_mbox_alloc_msg_rsp(&cptpf->afpf_mbox, 0,
+						sizeof(*req),
+						sizeof(struct msg_rsp));
+	if (req == NULL) {
+		dev_err(&pdev->dev, "RVU MBOX failed to get message.\n");
+		ret = -EFAULT;
+		goto error;
+	}
+
+	req->hdr.id = MBOX_MSG_CPT_SET_CRYPTO_GRP;
+	req->hdr.sig = OTX2_MBOX_REQ_SIG;
+	req->hdr.pcifunc = RVU_PFFUNC(cptpf->pf_id, 0);
+	req->crypto_eng_grp = crypto_eng_grp;
+
+	tmp_crypto_eng_grp = cptpf->crypto_eng_grp;
+	ret = cpt_send_mbox_msg(pdev);
+	if (ret)
+		goto error;
+
+	if (!cptpf->crypto_eng_grp) {
+		cptpf->crypto_eng_grp = tmp_crypto_eng_grp;
+		ret = -EINVAL;
+	}
+		cptpf->crypto_eng_grp = crypto_eng_grp;
+error:
+	return ret;
+}
+
+irqreturn_t cptpf_afpf_mbox_intr(int irq, void *arg)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) arg;
+	u64 intr;
+
+	/* Read the interrupt bits */
+	intr = cpt_read64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT);
+
+	if (intr & 0x1ULL) {
+		/* Schedule work queue function to process the MBOX request */
+		queue_work(cptpf->afpf_mbox_wq, &cptpf->afpf_mbox_work);
+		/* Clear and ack the interrupt */
+		cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0, RVU_PF_INT,
+			    0x1ULL);
+	}
+	return IRQ_HANDLED;
+}
+
+irqreturn_t cptpf_vfpf_mbox_intr(int irq, void *arg)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) arg;
+	struct cptvf_info *vf;
+	int i, vf_idx;
+	u64 intr;
+
+	/* Check which VF has raised an interrupt and schedule
+	 * corresponding work queue to process the messages
+	 */
+	for (i = 0; i < 2; i++) {
+		/* Read the interrupt bits */
+		intr = cpt_read64(cptpf->reg_base, BLKADDR_RVUM, 0,
+				  RVU_PF_VFPF_MBOX_INTX(i));
+
+		for (vf_idx = i * 64; vf_idx < cptpf->enabled_vfs; vf_idx++) {
+			vf = &cptpf->vf[vf_idx];
+			if (intr & (1ULL << vf->intr_idx)) {
+				queue_work(cptpf->vfpf_mbox_wq,
+					   &vf->vfpf_mbox_work);
+				/* Clear the interrupt */
+				cpt_write64(cptpf->reg_base, BLKADDR_RVUM, 0,
+					    RVU_PF_VFPF_MBOX_INTX(i),
+					    BIT_ULL(vf->intr_idx));
+			}
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+void cptpf_afpf_mbox_handler(struct work_struct *work)
+{
+	struct cpt_set_crypto_grp_req_msg *rsp_set_grp;
+	struct cpt_rd_wr_reg_msg *rsp_rd_wr;
+	struct otx2_mbox *afpf_mbox;
+	struct otx2_mbox *vfpf_mbox;
+	struct mbox_hdr *rsp_hdr;
+	struct mbox_msghdr *msg;
+	struct mbox_msghdr *fwd;
+	struct cptpf_dev *cptpf;
+	int offset, size;
+	int vf_id, i;
+
+	/* Read latest mbox data */
+	smp_rmb();
+
+	cptpf = container_of(work, struct cptpf_dev, afpf_mbox_work);
+	afpf_mbox = &cptpf->afpf_mbox;
+	vfpf_mbox = &cptpf->vfpf_mbox;
+	rsp_hdr = (struct mbox_hdr *)(afpf_mbox->dev->mbase +
+		   afpf_mbox->rx_start);
+	if (rsp_hdr->num_msgs == 0)
+		return;
+	offset = ALIGN(sizeof(struct mbox_hdr), MBOX_MSG_ALIGN);
+
+	for (i = 0; i < rsp_hdr->num_msgs; i++) {
+		msg = (struct mbox_msghdr *)(afpf_mbox->dev->mbase +
+					     afpf_mbox->rx_start + offset);
+		size = msg->next_msgoff - offset;
+
+		if (msg->id >= MBOX_MSG_MAX) {
+			dev_err(&cptpf->pdev->dev,
+				"MBOX msg with unknown ID %d\n", msg->id);
+			goto error;
+		}
+
+		if (msg->sig != OTX2_MBOX_RSP_SIG) {
+			dev_err(&cptpf->pdev->dev,
+				"MBOX msg with wrong signature %x, ID %d\n",
+				msg->sig, msg->id);
+			goto error;
+		}
+
+		offset = msg->next_msgoff;
+		vf_id = (msg->pcifunc >> RVU_PFVF_FUNC_SHIFT) &
+			 RVU_PFVF_FUNC_MASK;
+		if (vf_id > 0) {
+			vf_id--;
+			if (vf_id >= cptpf->enabled_vfs) {
+				dev_err(&cptpf->pdev->dev,
+					"MBOX msg to unknown VF: %d >= %d\n",
+					vf_id, cptpf->enabled_vfs);
+				goto error;
+			}
+			fwd = otx2_mbox_alloc_msg(vfpf_mbox, vf_id, size);
+			if (!fwd) {
+				dev_err(&cptpf->pdev->dev,
+					"Forwarding to VF%d failed.\n", vf_id);
+				goto error;
+			}
+			memcpy((uint8_t *)fwd + sizeof(struct mbox_msghdr),
+			       (uint8_t *)msg + sizeof(struct mbox_msghdr),
+			       size);
+			fwd->id = msg->id;
+			fwd->pcifunc = msg->pcifunc;
+			fwd->sig = msg->sig;
+			fwd->ver = msg->ver;
+			fwd->rc = msg->rc;
+		} else {
+			if (cpt_is_dbg_level_en(CPT_DBG_MBOX_MSGS))
+				dump_mbox_msg(&cptpf->pdev->dev, msg, size);
+
+			switch (msg->id) {
+			case MBOX_MSG_READY:
+				cptpf->pf_id =
+					(msg->pcifunc >> RVU_PFVF_PF_SHIFT) &
+					RVU_PFVF_PF_MASK;
+				break;
+
+			case MBOX_MSG_FREE_RSRC_CNT:
+				memcpy(&cptpf->limits, msg,
+				       sizeof(struct free_rsrcs_rsp));
+				break;
+
+			case MBOX_MSG_CPT_RD_WR_REGISTER:
+				rsp_rd_wr = (struct cpt_rd_wr_reg_msg *) msg;
+				if (msg->rc) {
+					dev_err(&cptpf->pdev->dev,
+						"Reg %llx rd/wr(%d) failed %d",
+						rsp_rd_wr->reg_offset,
+						rsp_rd_wr->is_write,
+						msg->rc);
+					continue;
+				}
+
+				if (!rsp_rd_wr->is_write)
+					*rsp_rd_wr->ret_val = rsp_rd_wr->val;
+				break;
+
+			case MBOX_MSG_CPT_SET_CRYPTO_GRP:
+				rsp_set_grp =
+				    (struct cpt_set_crypto_grp_req_msg *) msg;
+				if (msg->rc) {
+					dev_err(&cptpf->pdev->dev,
+						"Crypto grp %d set failed %d",
+						rsp_set_grp->crypto_eng_grp,
+						msg->rc);
+					cptpf->crypto_eng_grp = 0;
+					continue;
+				} else
+					cptpf->crypto_eng_grp = 1;
+				break;
+
+			default:
+				dev_err(&cptpf->pdev->dev,
+					"Unsupported msg %d received.\n",
+					msg->id);
+				break;
+			}
+		}
+error:
+		afpf_mbox->dev->msgs_acked++;
+	}
+
+	otx2_mbox_reset(afpf_mbox, 0);
+}
+
+void cptpf_vfpf_mbox_handler(struct work_struct *work)
+{
+	struct cptvf_info *vf = container_of(work, struct cptvf_info,
+					      vfpf_mbox_work);
+	struct cptpf_dev *cptpf = vf->cptpf;
+	struct otx2_mbox *mbox = &cptpf->vfpf_mbox;
+	struct otx2_mbox_dev *mdev = &mbox->dev[vf->vf_id];
+	struct mbox_hdr *req_hdr;
+	struct mbox_msghdr *msg;
+	int offset, id, err;
+
+	/* sync with mbox memory region */
+	rmb();
+
+	/* Process received mbox messages */
+	req_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);
+	offset = ALIGN(sizeof(*req_hdr), MBOX_MSG_ALIGN);
+	id = 0;
+	while (id < req_hdr->num_msgs) {
+		while (id < req_hdr->num_msgs) {
+			msg = (struct mbox_msghdr *)(mdev->mbase +
+						     mbox->rx_start + offset);
+
+			/* Set which VF sent this message based on mbox IRQ */
+			msg->pcifunc = ((u16)cptpf->pf_id << RVU_PFVF_PF_SHIFT)
+				| ((vf->vf_id + 1) & RVU_PFVF_FUNC_MASK);
+
+			err = cptpf_handle_vf_req(cptpf, vf, msg,
+						  msg->next_msgoff - offset);
+
+			/* Behave as the AF, drop the msg if there is
+			 * no memory, timeout handling also goes here
+			 */
+			if (err == -ENOMEM ||
+			    err == -EIO)
+				break;
+
+			offset = msg->next_msgoff;
+			id++;
+		}
+
+		/* Send mbox responses to VF */
+		if (mdev->num_msgs)
+			otx2_mbox_msg_send(mbox, vf->vf_id);
+	}
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_quota.c b/drivers/crypto/cavium/cpt/9x/cpt9x_quota.c
new file mode 100644
index 000000000000..42ae3a8a9c59
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_quota.c
@@ -0,0 +1,190 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/pci.h>
+#include <linux/sysfs.h>
+#include "cpt9x_quota.h"
+
+static u64 quotas_get_sum(struct quotas *quotas)
+{
+	u64 lf_sum = 0;
+	int i;
+
+	for (i = 0; i < quotas->cnt; i++)
+		lf_sum += quotas->a[i].val;
+
+	return lf_sum;
+}
+
+static ssize_t quota_show(struct kobject *kobj, struct kobj_attribute *attr,
+			  char *buf)
+{
+	struct quota *quota;
+	int val;
+
+	quota = container_of(attr, struct quota, sysfs);
+
+	if (quota->base->lock)
+		mutex_lock(quota->base->lock);
+	val = quota->val;
+	if (quota->base->lock)
+		mutex_unlock(quota->base->lock);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", val);
+}
+
+static ssize_t quota_store(struct kobject *kobj, struct kobj_attribute *attr,
+			   const char *buf, size_t count)
+{
+	struct quota *quota;
+	struct quotas *base;
+	struct device *dev;
+	int old_val, new_val, res = 0;
+	u64 lf_sum;
+
+	quota = container_of(attr, struct quota, sysfs);
+	dev = quota->dev;
+	base = quota->base;
+
+	if (kstrtoint(buf, 0, &new_val)) {
+		dev_err(dev, "Invalid %s quota: %s", attr->attr.name, buf);
+		return -EIO;
+	}
+	if (new_val <= 0) {
+		dev_err(dev, "Invalid %s quota: %d <= 0", attr->attr.name,
+			new_val);
+		return -EIO;
+	}
+
+	if (new_val > base->max) {
+		dev_err(dev, "Invalid %s quota %d > max allowed %d",
+			attr->attr.name, new_val, base->max);
+		return -EIO;
+	}
+
+	if (base->lock)
+		mutex_lock(base->lock);
+	old_val = quota->val;
+
+	if (base->ops.pre_store)
+		res = base->ops.pre_store(quota->ops_arg, quota, new_val);
+
+	if (res != 0) {
+		res = -EIO;
+		goto unlock;
+	}
+
+	lf_sum = quotas_get_sum(quota->base);
+	if (lf_sum + new_val - quota->val > base->max_sum) {
+		dev_err(dev,
+			"Not enough %s resources, requested %d, avail %lld",
+			attr->attr.name, new_val,
+			base->max_sum - lf_sum + quota->val);
+		res = -EIO;
+		goto unlock;
+	}
+	quota->val = new_val;
+
+	if (base->ops.post_store)
+		base->ops.post_store(quota->ops_arg, quota, old_val);
+
+	res = count;
+
+unlock:
+	if (base->lock)
+		mutex_unlock(base->lock);
+	return res;
+}
+
+struct quotas *quotas_alloc(u32 cnt, u32 max, u64 max_sum,
+			    int init_val, struct mutex *lock,
+			    struct quota_ops *ops)
+{
+	struct quotas *quotas;
+	u64 i;
+
+	if (cnt == 0)
+		return NULL;
+
+	quotas = kzalloc(sizeof(struct quotas) + cnt * sizeof(struct quota),
+			 GFP_KERNEL);
+	if (quotas == NULL)
+		return NULL;
+
+	for (i = 0; i < cnt; i++) {
+		quotas->a[i].base = quotas;
+		quotas->a[i].val = init_val;
+	}
+
+	quotas->cnt = cnt;
+	quotas->max = max;
+	quotas->max_sum = max_sum;
+	if (ops) {
+		quotas->ops.pre_store = ops->pre_store;
+		quotas->ops.post_store = ops->post_store;
+	}
+	quotas->lock = lock;
+
+	return quotas;
+}
+
+void quotas_free(struct quotas *quotas)
+{
+	u64 i;
+
+	if (quotas == NULL)
+		return;
+	WARN_ON(quotas->cnt == 0);
+
+	for (i = 0; i < quotas->cnt; i++)
+		quota_sysfs_destroy(&quotas->a[i]);
+
+	kfree(quotas);
+}
+
+int quota_sysfs_create(const char *name, struct kobject *parent,
+		       struct device *log_dev, struct quota *quota,
+		       void *ops_arg)
+{
+	int err;
+
+	if (name == NULL || quota == NULL || log_dev == NULL)
+		return -EINVAL;
+
+	quota->sysfs.show = quota_show;
+	quota->sysfs.store = quota_store;
+	quota->sysfs.attr.name = name;
+	quota->sysfs.attr.mode = 0664;
+	quota->parent = parent;
+	quota->dev = log_dev;
+	quota->ops_arg = ops_arg;
+
+	sysfs_attr_init(&quota->sysfs.attr);
+	err = sysfs_create_file(quota->parent, &quota->sysfs.attr);
+	if (err) {
+		dev_err(quota->dev,
+			"Failed to create '%s' quota sysfs for '%s'\n",
+			name, kobject_name(quota->parent));
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+int quota_sysfs_destroy(struct quota *quota)
+{
+	if (quota == NULL)
+		return -EINVAL;
+	if (quota->sysfs.attr.mode != 0) {
+		sysfs_remove_file(quota->parent, &quota->sysfs.attr);
+		quota->sysfs.attr.mode = 0;
+	}
+	return 0;
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_quota.h b/drivers/crypto/cavium/cpt/9x/cpt9x_quota.h
new file mode 100644
index 000000000000..fddb56392c03
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_quota.h
@@ -0,0 +1,86 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _CPT9X_QUOTA_H_
+#define _CPT9X_QUOTA_H_
+
+#include <linux/kobject.h>
+#include <linux/mutex.h>
+
+struct quotas;
+
+struct quota {
+	struct kobj_attribute	sysfs;
+	/* Device to scope logs to */
+	struct device		*dev;
+	/* Kobject of the sysfs file */
+	struct kobject		*parent;
+	/* Pointer to base structure */
+	struct quotas		*base;
+	/* Argument passed to the quota_ops when this quota is modified */
+	void			*ops_arg;
+	/* Value of the quota */
+	int			val;
+};
+
+struct quota_ops {
+	/**
+	 * Called before sysfs store(). store() will proceed if returns 0.
+	 * It is called with struct quotas::lock taken.
+	 */
+	int (*pre_store)(void *arg, struct quota *quota, int new_val);
+	/** called after sysfs store(). */
+	void (*post_store)(void *arg, struct quota *quota, int old_val);
+};
+
+struct quotas {
+	struct quota_ops ops;
+	struct mutex *lock;	/* lock taken for each sysfs operation */
+	u32 cnt;		/* number of elements in arr */
+	u32 max;		/* maximum value for a single quota */
+	u64 max_sum;		/* maximum sum of all quotas */
+	struct quota a[0];	/* array of quota assignments */
+};
+
+/**
+ * Allocate and setup quotas structure.
+ *
+ * @p cnt number of quotas to allocate
+ * @p max maximum value of a single quota
+ * @p max_sum maximum sum of all quotas
+ * @p init_val initial value set to all quotas
+ * @p ops callbacks for sysfs manipulation notifications
+ */
+struct quotas *quotas_alloc(u32 cnt, u32 max, u64 max_sum,
+			    int init_val, struct mutex *lock,
+			    struct quota_ops *ops);
+/**
+ * Frees quota array and any sysfs entries associated with it.
+ */
+void quotas_free(struct quotas *quotas);
+
+/**
+ * Create a sysfs entry controling given quota entry.
+ *
+ * File created under parent will read the current value of the quota and
+ * write will take quotas lock and check if new value does not exceed
+ * configured maximum values.
+ *
+ * @return 0 if succeeded, negative error code otherwise.
+ */
+int quota_sysfs_create(const char *name, struct kobject *parent,
+		       struct device *log_dev, struct quota *quota,
+		       void *ops_arg);
+/**
+ * Remove sysfs entry for a given quota if it was created.
+ */
+int quota_sysfs_destroy(struct quota *quota);
+
+#endif /* _CPT9X_QUOTA_H_ */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.c b/drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.c
new file mode 100644
index 000000000000..14989ad4ffe7
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.c
@@ -0,0 +1,161 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt_reqmgr.h"
+#include "cpt9x_mbox_common.h"
+
+inline void fill_cpt_inst(union cpt_inst_s *cptinst,
+			  struct cpt_info_buffer *info,
+			  struct cpt_iq_command *iq_cmd)
+{
+	cptinst->u[0] = 0x0;
+	cptinst->s9x.doneint = true;
+	cptinst->s9x.res_addr = (u64) info->comp_baddr;
+	cptinst->u[2] = 0x0;
+	cptinst->u[3] = 0x0;
+	cptinst->s9x.ei0 = iq_cmd->cmd.u64;
+	cptinst->s9x.ei1 = iq_cmd->dptr;
+	cptinst->s9x.ei2 = iq_cmd->rptr;
+	cptinst->s9x.ei3 = iq_cmd->cptr.u64;
+}
+
+inline int process_ccode(struct pci_dev *pdev, union cpt_res_s *cpt_status,
+			 struct cpt_info_buffer *cpt_info,
+			 struct cpt_request_info *req, u32 *res_code)
+{
+	u8 ccode = cpt_status->s9x.compcode;
+
+	switch (ccode) {
+	case CPT_9X_COMP_E_FAULT:
+		dev_err(&pdev->dev,
+			"Request failed with DMA fault\n");
+		dump_sg_list(pdev, req);
+	break;
+
+	case CPT_9X_COMP_E_HWERR:
+		dev_err(&pdev->dev,
+			"Request failed with hardware error\n");
+		dump_sg_list(pdev, req);
+	break;
+
+	case CPT_9X_COMP_E_INSTERR:
+		dev_err(&pdev->dev,
+			"Request failed with instruction error\n");
+		dump_sg_list(pdev, req);
+	break;
+
+	case COMPLETION_CODE_INIT:
+		/* check for timeout */
+		if (time_after_eq(jiffies,
+				  (cpt_info->time_in +
+				  (CPT_COMMAND_TIMEOUT * HZ)))) {
+			dev_err(&pdev->dev, "Request timed out\n");
+		} else if ((ccode == (COMPLETION_CODE_INIT)) &&
+			   (cpt_info->extra_time <
+			    TIME_IN_RESET_COUNT)) {
+			cpt_info->time_in = jiffies;
+			cpt_info->extra_time++;
+			return 1;
+		}
+	break;
+
+	case CPT_9X_COMP_E_GOOD:
+		/* Check microcode completion code, it is only valid
+		 * when completion code is CPT_COMP_E::GOOD
+		 */
+		if (cpt_status->s9x.uc_compcode) {
+			dev_err(&pdev->dev,
+				"Request failed with software error code 0x%x\n",
+				cpt_status->s9x.uc_compcode);
+			dump_sg_list(pdev, req);
+			break;
+		}
+
+		/* Request has been processed with success */
+		*res_code = 0;
+	break;
+
+	default:
+		dev_err(&pdev->dev,
+			"Request returned invalid status %d\n", ccode);
+	break;
+	}
+
+	return 0;
+}
+
+/*
+ * On 9X platform the parameter insts_num is used as a count of instructions
+ * to be enqueued. The valid values for insts_num are:
+ * 1 - 1 CPT instruction will be enqueued during LMTST operation
+ * 2 - 2 CPT instructions will be enqueued during LMTST operation
+ */
+inline void send_cpt_cmd(union cpt_inst_s *cptinst, u32 insts_num, void *obj)
+{
+	struct cptlf_info *lf = (struct cptlf_info *) obj;
+	void *lmtline = lf->lmtline;
+	void *ioreg = lf->ioreg;
+	long ret;
+
+	do {
+		/* Copy CPT command to LMTLINE */
+		memcpy(lmtline, cptinst, insts_num * CPT_INST_SIZE);
+
+		/* Make sure compiler does not reorder memcpy and ldeor.
+		 * LMTST transactions are always flushed from the write
+		 * buffer immediately, a DMB is not required to push out
+		 * LMTSTs.
+		 */
+		barrier();
+		/* LDEOR initiates atomic transfer to I/O device
+		 * The following will cause the LMTST to fail (the LDEOR
+		 * returns zero):
+		 * - No stores have been performed to the LMTLINE since it was
+		 * last invalidated.
+		 * - The bytes which have been stored to LMTLINE since it was
+		 * last invalidated form a pattern that is non-contiguous, does
+		 * not start at byte 0, or does not end on a 8-byte boundary.
+		 * (i.e.comprises a formation of other than 116 8-byte
+		 * words.)
+		 *
+		 * These rules are designed such that an operating system
+		 * context switch or hypervisor guest switch need have no
+		 * knowledge of the LMTST operations; the switch code does not
+		 * need to store to LMTCANCEL. Also note as LMTLINE data cannot
+		 * be read, there is no information leakage between processes.
+		 */
+		__asm__ volatile(
+			"  .cpu		generic+lse\n"
+			"  ldeor	xzr, %0, [%1]\n"
+			: "=r" (ret) : "r" (ioreg) : "memory");
+	} while (!ret);
+}
+
+inline int cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev)
+{
+	struct cptlfs_info *lfs = get_lfs_info(pdev);
+
+	return lfs->kcrypto_eng_grp_num;
+}
+
+inline void cpt_post_process(struct cptlf_wqe *wqe)
+{
+	process_pending_queue(wqe->lfs->pdev,
+			      &wqe->lfs->lf[wqe->lf_num].pqueue);
+}
+
+inline int cpt_do_request(struct pci_dev *pdev, struct cpt_request_info *req,
+			  int cpu_num)
+{
+	struct cptlfs_info *lfs = get_lfs_info(pdev);
+
+	return process_request(pdev, req, &lfs->lf[cpu_num].pqueue,
+			       &lfs->lf[cpu_num]);
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.h b/drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.h
new file mode 100644
index 000000000000..cb683ba36ffe
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_reqmgr.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_REQUEST_MANAGER_H
+#define __CPT9X_REQUEST_MANAGER_H
+
+void cpt_post_process(struct cptlf_wqe *wqe);
+
+#endif /* __CPT9X_REQUEST_MANAGER_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_ucode.c b/drivers/crypto/cavium/cpt/9x/cpt9x_ucode.c
new file mode 100644
index 000000000000..0331e777738f
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_ucode.c
@@ -0,0 +1,258 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "rvu_reg.h"
+#include "cpt_ucode.h"
+#include "cpt9x_mbox_common.h"
+
+static struct bitmap get_cores_bmap(struct device *dev,
+				    struct engine_group_info *eng_grp)
+{
+	struct bitmap bmap = { 0 };
+	bool found = false;
+	int i;
+
+	if (eng_grp->g->engs_num > CPT_9X_MAX_ENGINES) {
+		dev_err(dev, "9X plat unsupported number of engines %d",
+			eng_grp->g->engs_num);
+		return bmap;
+	}
+
+	for (i = 0; i  < MAX_ENGS_PER_GRP; i++)
+		if (eng_grp->engs[i].type) {
+			bitmap_or(bmap.bits, bmap.bits,
+				  eng_grp->engs[i].bmap,
+				  eng_grp->g->engs_num);
+			bmap.size = eng_grp->g->engs_num;
+			found = true;
+		}
+
+	if (!found)
+		dev_err(dev, "No engines reserved for engine group %d",
+			eng_grp->idx);
+	return bmap;
+}
+
+int cpt_detach_and_disable_cores(struct engine_group_info *eng_grp, void *obj)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) obj;
+	struct bitmap bmap;
+	int i, timeout = 10;
+	int busy, ret = 0;
+	u64 reg;
+
+	bmap = get_cores_bmap(&cptpf->pdev->dev, eng_grp);
+	if (!bmap.size)
+		return -EINVAL;
+
+	/* Detach the cores from group */
+	for_each_set_bit(i, bmap.bits, bmap.size) {
+		ret = cpt_read_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL2(i), &reg);
+		if (ret)
+			goto error;
+
+		if (reg & (1ull << eng_grp->idx)) {
+			eng_grp->g->eng_ref_cnt[i]--;
+			reg &= ~(1ull << eng_grp->idx);
+
+			ret = cpt_write_af_reg(cptpf->pdev,
+					       CPT_AF_EXEX_CTL2(i), reg);
+			if (ret)
+				goto error;
+		}
+	}
+
+	/* Wait for cores to become idle */
+	do {
+		busy = 0;
+		usleep_range(10000, 20000);
+		if (timeout-- < 0)
+			return -EBUSY;
+
+		for_each_set_bit(i, bmap.bits, bmap.size) {
+			ret = cpt_read_af_reg(cptpf->pdev, CPT_AF_EXEX_STS(i),
+					      &reg);
+			if (ret)
+				goto error;
+
+			if (reg & 0x1) {
+				busy = 1;
+				break;
+			}
+		}
+	} while (busy);
+
+	/* Disable the cores only if they are not used anymore */
+	for_each_set_bit(i, bmap.bits, bmap.size)
+		if (!eng_grp->g->eng_ref_cnt[i]) {
+			ret = cpt_write_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL(i),
+					       0x0);
+			if (ret)
+				goto error;
+		}
+error:
+	return ret;
+}
+
+int cpt_set_ucode_base(struct engine_group_info *eng_grp, void *obj)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) obj;
+	struct engines_reserved *engs;
+	dma_addr_t dma_addr;
+	int i, bit, ret = 0;
+
+	/* Set PF number for microcode fetches */
+	ret = cpt_write_af_reg(cptpf->pdev, CPT_AF_PF_FUNC,
+			       cptpf->pf_id << RVU_PFVF_PF_SHIFT);
+	if (ret)
+		goto error;
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		engs = &eng_grp->engs[i];
+		if (!engs->type)
+			continue;
+
+		dma_addr = engs->ucode->align_dma;
+
+		/* Set UCODE_BASE only for the cores which are not used,
+		 * other cores should have already valid UCODE_BASE set
+		 */
+		for_each_set_bit(bit, engs->bmap, eng_grp->g->engs_num)
+			if (!eng_grp->g->eng_ref_cnt[bit]) {
+				ret = cpt_write_af_reg(cptpf->pdev,
+						CPT_AF_EXEX_UCODE_BASE(bit),
+						(u64) dma_addr);
+				if (ret)
+					goto error;
+			}
+	}
+error:
+	return ret;
+}
+
+int cpt_attach_and_enable_cores(struct engine_group_info *eng_grp, void *obj)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) obj;
+	struct bitmap bmap;
+	u64 reg;
+	int i, ret = 0;
+
+	bmap = get_cores_bmap(&cptpf->pdev->dev, eng_grp);
+	if (!bmap.size)
+		return -EINVAL;
+
+	/* Attach the cores to the group */
+	for_each_set_bit(i, bmap.bits, bmap.size) {
+		ret = cpt_read_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL2(i), &reg);
+		if (ret)
+			goto error;
+
+		if (!(reg & (1ull << eng_grp->idx))) {
+			eng_grp->g->eng_ref_cnt[i]++;
+			reg |= 1ull << eng_grp->idx;
+
+			ret = cpt_write_af_reg(cptpf->pdev,
+					       CPT_AF_EXEX_CTL2(i), reg);
+			if (ret)
+				goto error;
+		}
+	}
+
+	/* Enable the cores */
+	for_each_set_bit(i, bmap.bits, bmap.size) {
+		ret = cpt_add_write_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL(i),
+					   0x1);
+		if (ret)
+			goto error;
+	}
+	ret = cpt_send_af_reg_requests(cptpf->pdev);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
+
+void cpt_print_engines_mask(struct engine_group_info *eng_grp, void *obj,
+			    char *buf, int size)
+{
+	struct cptpf_dev *cptpf = (struct cptpf_dev *) obj;
+	struct bitmap bmap;
+	u32 mask[4];
+
+	bmap = get_cores_bmap(&cptpf->pdev->dev, eng_grp);
+	if (!bmap.size) {
+		scnprintf(buf, size, "unknown");
+		return;
+	}
+
+	if (WARN_ON_ONCE(bmap.size > 4 * 32))
+		return;
+
+	bitmap_to_arr32(mask, bmap.bits, bmap.size);
+	scnprintf(buf, size, "%8.8x %8.8x %8.8x %8.8x", mask[3], mask[2],
+		  mask[1], mask[0]);
+}
+
+int cpt_disable_all_cores(struct cptpf_dev *cptpf)
+{
+	int timeout = 10, ret = 0;
+	int i, busy, total_cores;
+	u64 reg;
+
+	total_cores = cptpf->eng_grps.avail.max_se_cnt +
+		      cptpf->eng_grps.avail.max_ie_cnt +
+		      cptpf->eng_grps.avail.max_ae_cnt;
+
+	/* Disengage the cores from groups */
+	for (i = 0; i < total_cores; i++) {
+		ret = cpt_add_write_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL2(i),
+					   0x0);
+		if (ret)
+			goto error;
+
+		cptpf->eng_grps.eng_ref_cnt[i] = 0;
+	}
+	ret = cpt_send_af_reg_requests(cptpf->pdev);
+	if (ret)
+		goto error;
+
+	/* Wait for cores to become idle */
+	do {
+		busy = 0;
+		usleep_range(10000, 20000);
+		if (timeout-- < 0)
+			return -EBUSY;
+
+		for (i = 0; i < total_cores; i++) {
+			ret = cpt_read_af_reg(cptpf->pdev, CPT_AF_EXEX_STS(i),
+					      &reg);
+			if (ret)
+				goto error;
+
+			if (reg & 0x1) {
+				busy = 1;
+				break;
+			}
+		}
+	} while (busy);
+
+	/* Disable the cores */
+	for (i = 0; i < total_cores; i++) {
+		ret = cpt_add_write_af_reg(cptpf->pdev, CPT_AF_EXEX_CTL(i),
+					   0x0);
+		if (ret)
+			goto error;
+	}
+	ret = cpt_send_af_reg_requests(cptpf->pdev);
+	if (ret)
+		goto error;
+error:
+	return ret;
+}
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_vf.h b/drivers/crypto/cavium/cpt/9x/cpt9x_vf.h
new file mode 100644
index 000000000000..291d7531de5f
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_vf.h
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT9X_VF_H
+#define __CPT9X_VF_H
+
+#include "mbox.h"
+#include "cpt9x_lf.h"
+
+struct cptvf_dev {
+	void __iomem *reg_base;		/* Register start address */
+	void __iomem *pfvf_mbox_base;	/* PF-VF mbox start address */
+	struct pci_dev *pdev;		/* PCI device handle */
+	struct cptlfs_info lfs;		/* CPT LFs attached to this VF */
+	struct free_rsrcs_rsp limits;   /* Resource limits for this VF */
+	u8 vf_id;			/* Virtual function index */
+
+	/* PF <=> VF mbox */
+	struct otx2_mbox	pfvf_mbox;
+	struct work_struct	pfvf_mbox_work;
+	struct workqueue_struct *pfvf_mbox_wq;
+};
+
+irqreturn_t cptvf_pfvf_mbox_intr(int irq, void *arg);
+void cptvf_pfvf_mbox_handler(struct work_struct *work);
+
+#endif /* __CPT9X_VF_H */
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_vf_main.c b/drivers/crypto/cavium/cpt/9x/cpt9x_vf_main.c
new file mode 100644
index 000000000000..66742ca8cf7b
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_vf_main.c
@@ -0,0 +1,295 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt9x_mbox_common.h"
+#include "cpt9x_passthrough.h"
+#include "otx2_reg.h"
+#include "rvu_reg.h"
+
+#define DRV_NAME	"octeontx2-cptvf"
+#define DRV_VERSION	"1.0"
+
+DEFINE_CPT_DEBUG_PARM(debug);
+
+static void cptvf_enable_pfvf_mbox_intrs(struct cptvf_dev *cptvf)
+{
+	/* Clear interrupt if any */
+	cpt_write64(cptvf->reg_base, BLKADDR_RVUM, 0, RVU_VF_INT, 0x1ULL);
+
+	/* Enable PF-VF interrupt */
+	cpt_write64(cptvf->reg_base, BLKADDR_RVUM, 0, RVU_VF_INT_ENA_W1S,
+		    0x1ULL);
+}
+
+static void cptvf_disable_pfvf_mbox_intrs(struct cptvf_dev *cptvf)
+{
+	/* Disable PF-VF interrupt */
+	cpt_write64(cptvf->reg_base, BLKADDR_RVUM, 0, RVU_VF_INT_ENA_W1C,
+		    0x1ULL);
+
+	/* Clear interrupt if any */
+	cpt_write64(cptvf->reg_base, BLKADDR_RVUM, 0, RVU_VF_INT, 0x1ULL);
+}
+
+static int cptvf_register_interrupts(struct cptvf_dev *cptvf)
+{
+	u32 num_vec;
+	int ret;
+
+	num_vec = pci_msix_vec_count(cptvf->pdev);
+	if (num_vec <= 0)
+		return -EINVAL;
+
+	/* Enable MSI-X */
+	ret = pci_alloc_irq_vectors(cptvf->pdev, num_vec, num_vec,
+				    PCI_IRQ_MSIX);
+	if (ret < 0) {
+		dev_err(&cptvf->pdev->dev,
+			"Request for %d msix vectors failed\n", num_vec);
+		return ret;
+	}
+
+	/* Register VF-PF mailbox interrupt handler */
+	ret = request_irq(pci_irq_vector(cptvf->pdev,
+			  CPT_9X_VF_INT_VEC_E_MBOX), cptvf_pfvf_mbox_intr,
+			  0, "CPTPFVF Mbox", cptvf);
+	if (ret)
+		goto err;
+	return 0;
+err:
+	dev_err(&cptvf->pdev->dev, "Failed to register interrupts\n");
+	pci_free_irq_vectors(cptvf->pdev);
+	return ret;
+}
+
+static void cptvf_unregister_interrupts(struct cptvf_dev *cptvf)
+{
+	free_irq(pci_irq_vector(cptvf->pdev, CPT_9X_VF_INT_VEC_E_MBOX), cptvf);
+	pci_free_irq_vectors(cptvf->pdev);
+}
+
+static int cptvf_pfvf_mbox_init(struct cptvf_dev *cptvf)
+{
+	int err;
+
+	cptvf->pfvf_mbox_wq = alloc_workqueue("cpt_pfvf_mailbox",
+					      WQ_UNBOUND | WQ_HIGHPRI |
+					      WQ_MEM_RECLAIM, 1);
+	if (!cptvf->pfvf_mbox_wq)
+		return -ENOMEM;
+
+	err = otx2_mbox_init(&cptvf->pfvf_mbox, cptvf->pfvf_mbox_base,
+			     cptvf->pdev, cptvf->reg_base, MBOX_DIR_VFPF, 1);
+	if (err)
+		goto error;
+
+	INIT_WORK(&cptvf->pfvf_mbox_work, cptvf_pfvf_mbox_handler);
+	return 0;
+error:
+	flush_workqueue(cptvf->pfvf_mbox_wq);
+	destroy_workqueue(cptvf->pfvf_mbox_wq);
+	return err;
+}
+
+static void cptvf_pfvf_mbox_destroy(struct cptvf_dev *cptvf)
+{
+	flush_workqueue(cptvf->pfvf_mbox_wq);
+	destroy_workqueue(cptvf->pfvf_mbox_wq);
+	otx2_mbox_destroy(&cptvf->pfvf_mbox);
+}
+
+static ssize_t cptvf_passthrough_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t count)
+{
+	struct pci_dev *pdev;
+	int ret;
+
+	pdev = container_of(dev, struct pci_dev, dev);
+	ret = run_passthrough_test(pdev, buf, count);
+	if (ret != -EINPROGRESS)
+		dev_err(dev, "Passthrough test failed %d\n", ret);
+
+	return strlen(buf);
+}
+
+static DEVICE_ATTR(passthrough_test, 0220, NULL, cptvf_passthrough_store);
+
+static struct attribute *octtx_attrs[] = {
+	&dev_attr_passthrough_test.attr,
+	NULL
+};
+
+static const struct attribute_group octtx_attr_group = {
+	.attrs = octtx_attrs,
+};
+
+static int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct cptvf_dev *cptvf;
+	int err;
+
+	cptvf = devm_kzalloc(dev, sizeof(*cptvf), GFP_KERNEL);
+	if (!cptvf)
+		return -ENOMEM;
+
+	pci_set_drvdata(pdev, cptvf);
+	cptvf->pdev = pdev;
+	cpt_set_dbg_level(debug);
+
+	err = pcim_enable_device(pdev);
+	if (err) {
+		dev_err(dev, "Failed to enable PCI device\n");
+		goto cpt_err_set_drvdata;
+	}
+
+	err = pci_request_regions(pdev, DRV_NAME);
+	if (err) {
+		dev_err(dev, "PCI request regions failed 0x%x\n", err);
+		goto cpt_err_set_drvdata;
+	}
+
+	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "Unable to get usable DMA configuration\n");
+		goto cpt_err_release_regions;
+	}
+
+	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");
+		goto cpt_err_release_regions;
+	}
+
+	/* Map VF's configuration registers */
+	cptvf->reg_base = pcim_iomap(pdev, PCI_PF_REG_BAR_NUM, 0);
+	if (!cptvf->reg_base) {
+		dev_err(dev, "Unable to map BAR2\n");
+		err = -ENOMEM;
+		goto cpt_err_release_regions;
+	}
+
+	/* Map PF-VF mailbox memory */
+	cptvf->pfvf_mbox_base = ioremap_wc(pci_resource_start(cptvf->pdev,
+					   PCI_MBOX_BAR_NUM),
+					   pci_resource_len(cptvf->pdev,
+					   PCI_MBOX_BAR_NUM));
+	if (!cptvf->pfvf_mbox_base) {
+		dev_err(&pdev->dev, "Unable to map BAR4\n");
+		err = -ENODEV;
+		goto cpt_err_release_regions;
+	}
+
+	/* Initialize PF-VF mailbox */
+	err = cptvf_pfvf_mbox_init(cptvf);
+	if (err)
+		goto cpt_err_iounmap;
+
+	/* Register interrupts */
+	err = cptvf_register_interrupts(cptvf);
+	if (err)
+		goto cpt_err_destroy_pfvf_mbox;
+
+	/* Enable PF-VF mailbox interrupts */
+	cptvf_enable_pfvf_mbox_intrs(cptvf);
+
+	/* Send ready message */
+	err = cpt_send_ready_msg(cptvf->pdev);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+	if (cptvf->lfs.kcrypto_eng_grp_num == INVALID_KCRYPTO_ENG_GRP) {
+		dev_err(dev, "Engine group for kernel crypto not available");
+		err = -ENOENT;
+		goto cpt_err_unregister_interrupts;
+	}
+
+	/* Get available CPT LF resources count */
+	err = cpt_get_rsrc_cnt(cptvf->pdev);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+	/* Create sysfs entries */
+	err = sysfs_create_group(&dev->kobj, &octtx_attr_group);
+	if (err)
+		goto cpt_err_remove_sysfs;
+
+	/* Initialize CPT LFs */
+	err = cptlf_init(pdev, cptvf->reg_base, &cptvf->lfs,
+			 cptvf->limits.cpt);
+	if (err)
+		goto cpt_err_unregister_interrupts;
+
+	return 0;
+
+cpt_err_remove_sysfs:
+	sysfs_remove_group(&pdev->dev.kobj, &octtx_attr_group);
+cpt_err_unregister_interrupts:
+	cptvf_disable_pfvf_mbox_intrs(cptvf);
+	cptvf_unregister_interrupts(cptvf);
+cpt_err_destroy_pfvf_mbox:
+	cptvf_pfvf_mbox_destroy(cptvf);
+cpt_err_iounmap:
+	iounmap(cptvf->pfvf_mbox_base);
+cpt_err_release_regions:
+	pci_release_regions(pdev);
+cpt_err_set_drvdata:
+	pci_set_drvdata(pdev, NULL);
+
+	return err;
+}
+
+static void cptvf_remove(struct pci_dev *pdev)
+{
+	struct cptvf_dev *cptvf = pci_get_drvdata(pdev);
+
+	if (!cptvf) {
+		dev_err(&pdev->dev, "Invalid CPT VF device.\n");
+		return;
+	}
+
+	/* Remove sysfs entries */
+	sysfs_remove_group(&pdev->dev.kobj, &octtx_attr_group);
+	/* Shutdown CPT LFs */
+	if (cptlf_shutdown(pdev, &cptvf->lfs))
+		dev_err(&pdev->dev, "CPT LFs shutdown failed.\n");
+	/* Disable PF-VF mailbox interrupt */
+	cptvf_disable_pfvf_mbox_intrs(cptvf);
+	/* Unregister interrupts */
+	cptvf_unregister_interrupts(cptvf);
+	/* Destroy PF-VF mbox */
+	cptvf_pfvf_mbox_destroy(cptvf);
+	/* Unmap PF-VF mailbox memory */
+	iounmap(cptvf->pfvf_mbox_base);
+
+	pci_release_regions(pdev);
+	pci_set_drvdata(pdev, NULL);
+}
+
+/* Supported devices */
+static const struct pci_device_id cptvf_id_table[] = {
+	{PCI_VDEVICE(CAVIUM, CPT_PCI_VF_9X_DEVICE_ID), 0},
+	{ 0, }  /* end of table */
+};
+
+static struct pci_driver cptvf_pci_driver = {
+	.name = DRV_NAME,
+	.id_table = cptvf_id_table,
+	.probe = cptvf_probe,
+	.remove = cptvf_remove,
+};
+
+module_pci_driver(cptvf_pci_driver);
+
+MODULE_AUTHOR("Marvell International Ltd.");
+MODULE_DESCRIPTION("Marvell OcteonTX2 CPT Virtual Function Driver");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, cptvf_id_table);
diff --git a/drivers/crypto/cavium/cpt/9x/cpt9x_vf_mbox.c b/drivers/crypto/cavium/cpt/9x/cpt9x_vf_mbox.c
new file mode 100644
index 000000000000..1bf43d192b67
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/9x/cpt9x_vf_mbox.c
@@ -0,0 +1,134 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt9x_mbox_common.h"
+#include "otx2_reg.h"
+#include "rvu_reg.h"
+
+irqreturn_t cptvf_pfvf_mbox_intr(int irq, void *arg)
+{
+	struct cptvf_dev *cptvf = (struct cptvf_dev *) arg;
+	u64 intr;
+
+	/* Read the interrupt bits */
+	intr = cpt_read64(cptvf->reg_base, BLKADDR_RVUM, 0, RVU_VF_INT);
+
+	if (intr & 0x1ULL) {
+		/* Schedule work queue function to process the MBOX request */
+		queue_work(cptvf->pfvf_mbox_wq, &cptvf->pfvf_mbox_work);
+		/* Clear and ack the interrupt */
+		cpt_write64(cptvf->reg_base, BLKADDR_RVUM, 0, RVU_VF_INT,
+			    0x1ULL);
+	}
+	return IRQ_HANDLED;
+}
+
+void cptvf_pfvf_mbox_handler(struct work_struct *work)
+{
+	struct cpt_rd_wr_reg_msg *rsp_reg;
+	struct msix_offset_rsp *rsp_msix;
+	struct ready_msg_rsp_ex *rsp;
+	struct otx2_mbox *pfvf_mbox;
+	struct mbox_hdr *rsp_hdr;
+	struct mbox_msghdr *msg;
+	struct cptvf_dev *cptvf;
+	int offset, i, j, size;
+
+	/* Read latest mbox data */
+	smp_rmb();
+
+	cptvf = container_of(work, struct cptvf_dev, pfvf_mbox_work);
+	pfvf_mbox = &cptvf->pfvf_mbox;
+	rsp_hdr = (struct mbox_hdr *)(pfvf_mbox->dev->mbase +
+		   pfvf_mbox->rx_start);
+	if (rsp_hdr->num_msgs == 0)
+		return;
+	offset = ALIGN(sizeof(struct mbox_hdr), MBOX_MSG_ALIGN);
+
+	for (i = 0; i < rsp_hdr->num_msgs; i++) {
+		msg = (struct mbox_msghdr *)(pfvf_mbox->dev->mbase +
+					     pfvf_mbox->rx_start + offset);
+		size = msg->next_msgoff - offset;
+
+		if (msg->id >= MBOX_MSG_MAX) {
+			dev_err(&cptvf->pdev->dev,
+				"MBOX msg with unknown ID %d\n", msg->id);
+			goto error;
+		}
+
+		if (msg->sig != OTX2_MBOX_RSP_SIG) {
+			dev_err(&cptvf->pdev->dev,
+				"MBOX msg with wrong signature %x, ID %d\n",
+				msg->sig, msg->id);
+			goto error;
+		}
+
+		if (cpt_is_dbg_level_en(CPT_DBG_MBOX_MSGS))
+			dump_mbox_msg(&cptvf->pdev->dev, msg, size);
+
+		offset = msg->next_msgoff;
+		switch (msg->id) {
+		case MBOX_MSG_READY:
+			rsp = (struct ready_msg_rsp_ex *) msg;
+			cptvf->vf_id =
+				((rsp->msg.hdr.pcifunc >> RVU_PFVF_FUNC_SHIFT)
+				 & RVU_PFVF_FUNC_MASK) - 1;
+			cptvf->lfs.kcrypto_eng_grp_num = rsp->eng_grp_num;
+			break;
+
+		case MBOX_MSG_ATTACH_RESOURCES:
+			/* Check if resources were successfully attached */
+			if (!msg->rc)
+				cptvf->lfs.are_lfs_attached = 1;
+			break;
+
+		case MBOX_MSG_DETACH_RESOURCES:
+			/* Check if resources were successfully detached */
+			if (!msg->rc)
+				cptvf->lfs.are_lfs_attached = 0;
+			break;
+
+		case MBOX_MSG_MSIX_OFFSET:
+			rsp_msix = (struct msix_offset_rsp *) msg;
+			for (j = 0; j < rsp_msix->cptlfs; j++)
+				cptvf->lfs.lf[j].msix_offset =
+						rsp_msix->cptlf_msixoff[j];
+			break;
+
+		case MBOX_MSG_CPT_RD_WR_REGISTER:
+			rsp_reg = (struct cpt_rd_wr_reg_msg *) msg;
+			if (msg->rc) {
+				dev_err(&cptvf->pdev->dev,
+					"Reg %llx rd/wr(%d) failed %d",
+					rsp_reg->reg_offset, rsp_reg->is_write,
+					msg->rc);
+				continue;
+			}
+
+			if (!rsp_reg->is_write)
+				*rsp_reg->ret_val = rsp_reg->val;
+			break;
+
+		case MBOX_MSG_FREE_RSRC_CNT:
+			memcpy(&cptvf->limits, msg,
+			       sizeof(struct free_rsrcs_rsp));
+			break;
+
+		default:
+			dev_err(&cptvf->pdev->dev,
+				"Unsupported msg %d received.\n",
+				msg->id);
+			break;
+		}
+error:
+		pfvf_mbox->dev->msgs_acked++;
+	}
+	otx2_mbox_reset(pfvf_mbox, 0);
+}
diff --git a/drivers/crypto/cavium/cpt/Kconfig b/drivers/crypto/cavium/cpt/Kconfig
index cbd51b1aa046..023d66ce421d 100644
--- a/drivers/crypto/cavium/cpt/Kconfig
+++ b/drivers/crypto/cavium/cpt/Kconfig
@@ -2,16 +2,31 @@
 # Cavium crypto device configuration
 #
 
-config CRYPTO_DEV_CPT
-	tristate
-
-config CAVIUM_CPT
-	tristate "Cavium Cryptographic Accelerator driver"
+choice
+	prompt "Cavium Cryptographic Accelerator (CPT) driver"
 	depends on ARCH_THUNDER || COMPILE_TEST
 	depends on PCI_MSI && 64BIT
-	select CRYPTO_DEV_CPT
+	depends on OCTEONTX2_AF
+	optional
 	help
-	  Support for Cavium CPT block found in octeon-tx series of
-	  processors.
+		Support for Cavium Cryptographic Accelerator (CPT) block
+		found in OcteonTX and OcteonTX2 series of processors.
+
+	config CRYPTO_DEV_OCTEONTX_CPT
+         tristate "OcteonTX CPT driver"
+         help
+			Support for Cavium Cryptographic Accelerator (CPT)
+			block found in OcteonTX series of processors.
+
+			To compile this driver as modules, choose M here:
+			the modules will be called cptpf8x and cptvf8x.
+
+    config CRYPTO_DEV_OCTEONTX2_CPT
+         tristate "OcteonTX2 CPT driver"
+		 help
+			Support for Cavium Cryptographic Accelerator (CPT)
+			block found in OcteonTX2 series of processors.
 
-	  To compile this as a module, choose M here.
+			To compile this driver as modules, choose M here:
+			the modules will be called cptpf9x and cptvf9x.
+endchoice
diff --git a/drivers/crypto/cavium/cpt/Makefile b/drivers/crypto/cavium/cpt/Makefile
deleted file mode 100644
index dbf055e14622..000000000000
--- a/drivers/crypto/cavium/cpt/Makefile
+++ /dev/null
@@ -1,3 +0,0 @@
-obj-$(CONFIG_CAVIUM_CPT) += cptpf.o cptvf.o
-cptpf-objs := cptpf_main.o cptpf_mbox.o
-cptvf-objs := cptvf_main.o cptvf_reqmanager.o cptvf_mbox.o cptvf_algs.o
diff --git a/drivers/crypto/cavium/cpt/common/cpt_algs.c b/drivers/crypto/cavium/cpt/common/cpt_algs.c
new file mode 100644
index 000000000000..403897c9cd8e
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_algs.c
@@ -0,0 +1,1608 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <crypto/aes.h>
+#include <crypto/authenc.h>
+#include <crypto/cryptd.h>
+#include <crypto/des.h>
+#include <crypto/internal/aead.h>
+#include <crypto/sha.h>
+#include <crypto/xts.h>
+#include <linux/rtnetlink.h>
+#include <linux/sort.h>
+#include "cpt_common.h"
+#include "cpt_algs.h"
+
+static DEFINE_MUTEX(mutex);
+static int is_crypto_registered;
+
+/* 8X platform supports 64 VFs for CPT and 9X platform supports 128 VFs
+ * for CPT therefore we take maximum number of supported VFs for CPT
+ */
+#define CPT_MAX_VF_NUM	128
+
+struct cpt_device_desc {
+	enum cpt_pf_type pf_type;
+	struct pci_dev *dev;
+	int num_queues;
+};
+
+struct cpt_device_table {
+	atomic_t count;
+	struct cpt_device_desc desc[CPT_MAX_VF_NUM];
+};
+
+static struct cpt_device_table se_devices = {
+	.count = ATOMIC_INIT(0)
+};
+
+static struct cpt_device_table ae_devices = {
+	.count = ATOMIC_INIT(0)
+};
+
+static inline int get_se_device(struct pci_dev **pdev, int *cpu_num)
+{
+	int count, ret = 0;
+
+	count = atomic_read(&se_devices.count);
+	if (count < 1)
+		return -ENODEV;
+
+	*cpu_num = get_cpu();
+
+	switch (se_devices.desc[0].pf_type) {
+	case CPT_81XX:
+	case CPT_SE_83XX:
+		/* On 8X platform there is one CPT instruction queues
+		 * bound to one VF therefore we need to check if number
+		 * of CPT VF devices is greater than the cpu number from
+		 * which we received a request
+		 */
+		if (*cpu_num >= count) {
+			ret = -ENODEV;
+			goto err;
+		}
+		*pdev = se_devices.desc[*cpu_num].dev;
+	break;
+
+	case CPT_96XX:
+		/* On 9X platform CPT instruction queue is bound to local
+		 * function LF, in turn LFs can be attached to PF or VF
+		 * therefore we always use first device but we need to check
+		 * if number of LFs attached to that device is greater than
+		 * cpu number from which we received a request
+		 */
+		if (*cpu_num >= se_devices.desc[0].num_queues) {
+			ret = -ENODEV;
+			goto err;
+		}
+		*pdev = se_devices.desc[0].dev;
+	break;
+
+	default:
+		pr_err("Unknown PF type %d\n", se_devices.desc[0].pf_type);
+		ret =  -EINVAL;
+	}
+err:
+	put_cpu();
+	return ret;
+}
+
+static inline int validate_hmac_cipher_null(struct cpt_request_info *cpt_req)
+{
+	struct aead_request *req;
+	struct cvm_req_ctx *rctx;
+	struct crypto_aead *tfm;
+
+	req = container_of(cpt_req->callback_arg,
+			   struct aead_request, base);
+	tfm = crypto_aead_reqtfm(req);
+	rctx = aead_request_ctx(req);
+	if (memcmp(rctx->fctx.hmac.s.hmac_calc,
+		   rctx->fctx.hmac.s.hmac_recv,
+		   crypto_aead_authsize(tfm)) != 0)
+		return -EBADMSG;
+
+	return 0;
+}
+
+void cvm_callback(int status, void *arg, void *req)
+{
+	struct crypto_async_request *areq = (struct crypto_async_request *)arg;
+	struct cpt_request_info *cpt_req = (struct cpt_request_info *)req;
+
+	if (!status) {
+		/* When selected cipher is NULL we need to manually
+		 * verify whether calculated hmac value matches
+		 * received hmac value
+		 */
+		if (cpt_req->req_type == AEAD_ENC_DEC_NULL_REQ &&
+		    !cpt_req->is_enc)
+			status = validate_hmac_cipher_null(cpt_req);
+	}
+
+	areq->complete(areq, status);
+}
+
+static inline void update_input_data(struct cpt_request_info *req_info,
+				     struct scatterlist *inp_sg,
+				     u32 nbytes, u32 *argcnt)
+{
+	req_info->req.dlen += nbytes;
+
+	while (nbytes) {
+		u32 len = min(nbytes, inp_sg->length);
+		u8 *ptr = sg_virt(inp_sg);
+
+		req_info->in[*argcnt].vptr = (void *)ptr;
+		req_info->in[*argcnt].size = len;
+		nbytes -= len;
+		++(*argcnt);
+		inp_sg = sg_next(inp_sg);
+	}
+}
+
+static inline void update_output_data(struct cpt_request_info *req_info,
+				      struct scatterlist *outp_sg,
+				      u32 offset, u32 nbytes, u32 *argcnt)
+{
+	req_info->rlen += nbytes;
+
+	while (nbytes) {
+		u32 len = min(nbytes, outp_sg->length - offset);
+		u8 *ptr = sg_virt(outp_sg);
+
+		req_info->out[*argcnt].vptr = (void *) (ptr + offset);
+		req_info->out[*argcnt].size = len;
+		nbytes -= len;
+		++(*argcnt);
+		offset = 0;
+		outp_sg = sg_next(outp_sg);
+	}
+}
+
+static inline u32 create_ctx_hdr(struct ablkcipher_request *req, u32 enc,
+				 u32 *argcnt)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
+	struct cvm_enc_ctx *ctx = crypto_ablkcipher_ctx(tfm);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	struct fc_context *fctx = &rctx->fctx;
+	u64 *ctrl_flags = NULL;
+
+	req_info->ctrl.s.dma_mode = DMA_GATHER_SCATTER;
+	req_info->ctrl.s.se_req = SE_CORE_REQ;
+
+	req_info->req.opcode.s.major = MAJOR_OP_FC |
+					DMA_MODE_FLAG(DMA_GATHER_SCATTER);
+	if (enc)
+		req_info->req.opcode.s.minor = 2;
+	else
+		req_info->req.opcode.s.minor = 3;
+
+	req_info->req.param1 = req->nbytes; /* Encryption Data length */
+	req_info->req.param2 = 0; /*Auth data length */
+
+	fctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;
+	fctx->enc.enc_ctrl.e.aes_key = ctx->key_type;
+	fctx->enc.enc_ctrl.e.iv_source = FROM_CPTR;
+
+	if (ctx->cipher_type == AES_XTS)
+		memcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len * 2);
+	else
+		memcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len);
+
+	memcpy(fctx->enc.encr_iv, req->info, crypto_ablkcipher_ivsize(tfm));
+
+	ctrl_flags = (u64 *)&fctx->enc.enc_ctrl.flags;
+	*ctrl_flags = cpu_to_be64(*ctrl_flags);
+
+	/* Storing  Packet Data Information in offset
+	 * Control Word First 8 bytes
+	 */
+	req_info->in[*argcnt].vptr = (u8 *)&rctx->ctrl_word;
+	req_info->in[*argcnt].size = CONTROL_WORD_LEN;
+	req_info->req.dlen += CONTROL_WORD_LEN;
+	++(*argcnt);
+
+	req_info->in[*argcnt].vptr = (u8 *)fctx;
+	req_info->in[*argcnt].size = sizeof(struct fc_context);
+	req_info->req.dlen += sizeof(struct fc_context);
+
+	++(*argcnt);
+
+	return 0;
+}
+
+static inline u32 create_input_list(struct ablkcipher_request  *req, u32 enc,
+				    u32 enc_iv_len)
+{
+	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	u32 argcnt =  0;
+
+	create_ctx_hdr(req, enc, &argcnt);
+	update_input_data(req_info, req->src, req->nbytes, &argcnt);
+	req_info->incnt = argcnt;
+
+	return 0;
+}
+
+static inline void create_output_list(struct ablkcipher_request *req,
+				      u32 enc_iv_len)
+{
+	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	u32 argcnt = 0;
+
+	/* OUTPUT Buffer Processing
+	 * AES encryption/decryption output would be
+	 * received in the following format
+	 *
+	 * ------IV--------|------ENCRYPTED/DECRYPTED DATA-----|
+	 * [ 16 Bytes/     [   Request Enc/Dec/ DATA Len AES CBC ]
+	 */
+	/* Reading IV information */
+	update_output_data(req_info, req->dst, 0, req->nbytes, &argcnt);
+	req_info->outcnt = argcnt;
+}
+
+static inline int cvm_enc_dec(struct ablkcipher_request *req, u32 enc)
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
+	u32 enc_iv_len = crypto_ablkcipher_ivsize(tfm);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	struct pci_dev *pdev = NULL;
+	int status, cpu_num;
+
+	memset(rctx, 0, sizeof(struct cvm_req_ctx));
+	create_input_list(req, enc, enc_iv_len);
+	create_output_list(req, enc_iv_len);
+
+	status = get_se_device(&pdev, &cpu_num);
+	if (status)
+		return status;
+
+	req_info->callback = (void *)cvm_callback;
+	req_info->callback_arg = (void *)&req->base;
+	req_info->req_type = ENC_DEC_REQ;
+	req_info->ctrl.s.grp = cpt_get_kcrypto_eng_grp_num(pdev);
+
+	status = cpt_do_request(pdev, req_info, cpu_num);
+	/* We perform an asynchronous send and once
+	 * the request is completed the driver would
+	 * intimate through registered call back functions
+	 */
+	return status;
+}
+
+int cvm_encrypt(struct ablkcipher_request *req)
+{
+	return cvm_enc_dec(req, true);
+}
+
+int cvm_decrypt(struct ablkcipher_request *req)
+{
+	return cvm_enc_dec(req, false);
+}
+
+int cvm_xts_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+		   u32 keylen)
+{
+	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);
+	struct cvm_enc_ctx *ctx = crypto_tfm_ctx(tfm);
+	int err;
+	const u8 *key1 = key;
+	const u8 *key2 = key + (keylen / 2);
+
+	err = xts_check_key(tfm, key, keylen);
+	if (err)
+		return err;
+	ctx->key_len = keylen;
+	memcpy(ctx->enc_key, key1, keylen / 2);
+	memcpy(ctx->enc_key + KEY2_OFFSET, key2, keylen / 2);
+	ctx->cipher_type = AES_XTS;
+	switch (ctx->key_len) {
+	case 32:
+		ctx->key_type = AES_128_BIT;
+		break;
+	case 64:
+		ctx->key_type = AES_256_BIT;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int cvm_validate_keylen(struct cvm_enc_ctx *ctx, u32 keylen)
+{
+	if ((keylen == 16) || (keylen == 24) || (keylen == 32)) {
+		ctx->key_len = keylen;
+		switch (ctx->key_len) {
+		case 16:
+			ctx->key_type = AES_128_BIT;
+			break;
+		case 24:
+			ctx->key_type = AES_192_BIT;
+			break;
+		case 32:
+			ctx->key_type = AES_256_BIT;
+			break;
+		default:
+			return -EINVAL;
+		}
+
+		if (ctx->cipher_type == DES3_CBC)
+			ctx->key_type = 0;
+
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+static int cvm_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+		      u32 keylen, u8 cipher_type)
+{
+	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);
+	struct cvm_enc_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	ctx->cipher_type = cipher_type;
+	if (!cvm_validate_keylen(ctx, keylen)) {
+		memcpy(ctx->enc_key, key, keylen);
+		return 0;
+	}
+
+	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+static int cvm_cbc_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+			      u32 keylen)
+{
+	return cvm_setkey(cipher, key, keylen, AES_CBC);
+}
+
+static int cvm_ecb_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+			      u32 keylen)
+{
+	return cvm_setkey(cipher, key, keylen, AES_ECB);
+}
+
+static int cvm_cfb_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+			      u32 keylen)
+{
+	return cvm_setkey(cipher, key, keylen, AES_CFB);
+}
+
+static int cvm_cbc_des3_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+			       u32 keylen)
+{
+	return cvm_setkey(cipher, key, keylen, DES3_CBC);
+}
+
+static int cvm_ecb_des3_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
+			       u32 keylen)
+{
+	return cvm_setkey(cipher, key, keylen, DES3_ECB);
+}
+
+int cvm_enc_dec_init(struct crypto_tfm *tfm)
+{
+	struct cvm_enc_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	memset(ctx, 0, sizeof(*ctx));
+	tfm->crt_ablkcipher.reqsize = sizeof(struct cvm_req_ctx) +
+					sizeof(struct ablkcipher_request);
+	/* Additional memory for ablkcipher_request is
+	 * allocated since the cryptd daemon uses
+	 * this memory for request_ctx information
+	 */
+
+	return 0;
+}
+
+static int cvm_aead_init(struct crypto_aead *tfm, u8 cipher_type, u8 mac_type)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(tfm);
+
+	ctx->cipher_type = cipher_type;
+	ctx->mac_type = mac_type;
+
+	/* When selected cipher is NULL we use HMAC opcode instead of
+	 * FLEXICRYPTO opcode therefore we don't need to use HASH algorithms
+	 * for calculating ipad and opad
+	 */
+	if (ctx->cipher_type != CIPHER_NULL) {
+		switch (ctx->mac_type) {
+		case SHA1:
+			ctx->hashalg = crypto_alloc_shash("sha1", 0,
+							  CRYPTO_ALG_ASYNC);
+			if (IS_ERR(ctx->hashalg))
+				return PTR_ERR(ctx->hashalg);
+			break;
+
+		case SHA256:
+			ctx->hashalg = crypto_alloc_shash("sha256", 0,
+							  CRYPTO_ALG_ASYNC);
+			if (IS_ERR(ctx->hashalg))
+				return PTR_ERR(ctx->hashalg);
+			break;
+
+		case SHA384:
+			ctx->hashalg = crypto_alloc_shash("sha384", 0,
+							  CRYPTO_ALG_ASYNC);
+			if (IS_ERR(ctx->hashalg))
+				return PTR_ERR(ctx->hashalg);
+			break;
+
+		case SHA512:
+			ctx->hashalg = crypto_alloc_shash("sha512", 0,
+							  CRYPTO_ALG_ASYNC);
+			if (IS_ERR(ctx->hashalg))
+				return PTR_ERR(ctx->hashalg);
+			break;
+		}
+	}
+
+	crypto_aead_set_reqsize(tfm, sizeof(struct cvm_req_ctx));
+
+	return 0;
+}
+
+static int cvm_aead_cbc_aes_sha1_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, AES_CBC, SHA1);
+}
+
+static int cvm_aead_cbc_aes_sha256_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, AES_CBC, SHA256);
+}
+
+static int cvm_aead_cbc_aes_sha384_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, AES_CBC, SHA384);
+}
+
+static int cvm_aead_cbc_aes_sha512_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, AES_CBC, SHA512);
+}
+
+static int cvm_aead_ecb_null_sha1_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, CIPHER_NULL, SHA1);
+}
+
+static int cvm_aead_ecb_null_sha256_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, CIPHER_NULL, SHA256);
+}
+
+static int cvm_aead_ecb_null_sha384_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, CIPHER_NULL, SHA384);
+}
+
+static int cvm_aead_ecb_null_sha512_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, CIPHER_NULL, SHA512);
+}
+
+static int cvm_aead_gcm_aes_init(struct crypto_aead *tfm)
+{
+	return cvm_aead_init(tfm, AES_GCM, MAC_NULL);
+}
+
+void cvm_aead_exit(struct crypto_aead *tfm)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(tfm);
+
+	kfree(ctx->ipad);
+	kfree(ctx->opad);
+	if (ctx->hashalg)
+		crypto_free_shash(ctx->hashalg);
+	kfree(ctx->sdesc);
+}
+
+/* This is the Integrity Check Value validation (aka the authentication tag
+ * length)
+ */
+static int cvm_aead_set_authsize(struct crypto_aead *tfm,
+				 unsigned int authsize)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(tfm);
+
+	switch (ctx->mac_type) {
+	case SHA1:
+		if (authsize != SHA1_DIGEST_SIZE &&
+		    authsize != SHA1_TRUNC_DIGEST_SIZE)
+			return -EINVAL;
+		break;
+
+	case SHA256:
+		if (authsize != SHA256_DIGEST_SIZE &&
+		    authsize != SHA256_TRUNC_DIGEST_SIZE)
+			return -EINVAL;
+		break;
+
+	case SHA384:
+		if (authsize != SHA384_DIGEST_SIZE &&
+		    authsize != SHA384_TRUNC_DIGEST_SIZE)
+			return -EINVAL;
+		break;
+
+	case SHA512:
+		if (authsize != SHA512_DIGEST_SIZE &&
+		    authsize != SHA512_TRUNC_DIGEST_SIZE)
+			return -EINVAL;
+		break;
+
+	case MAC_NULL:
+		if (ctx->cipher_type == AES_GCM) {
+			if (authsize != AES_GCM_ICV_SIZE)
+				return -EINVAL;
+		} else
+			return -EINVAL;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	tfm->authsize = authsize;
+	return 0;
+}
+
+static struct sdesc *alloc_sdesc(struct crypto_shash *alg)
+{
+	struct sdesc *sdesc;
+	int size;
+
+	size = sizeof(struct shash_desc) + crypto_shash_descsize(alg);
+	sdesc = kmalloc(size, GFP_KERNEL);
+	if (!sdesc)
+		return NULL;
+
+	sdesc->shash.tfm = alg;
+	sdesc->shash.flags = 0x0;
+
+	return sdesc;
+}
+
+static inline void swap_data32(void *buf, u32 len)
+{
+	u32 *store = (u32 *) buf;
+	int i = 0;
+
+	for (i = 0 ; i < len/sizeof(u32); i++, store++)
+		*store = cpu_to_be32(*store);
+}
+
+static inline void swap_data64(void *buf, u32 len)
+{
+	u64 *store = (u64 *) buf;
+	int i = 0;
+
+	for (i = 0 ; i < len/sizeof(u64); i++, store++)
+		*store = cpu_to_be64(*store);
+}
+
+static int copy_pad(u8 mac_type, u8 *out_pad, u8 *in_pad)
+{
+	struct sha512_state *sha512;
+	struct sha256_state *sha256;
+	struct sha1_state *sha1;
+
+	switch (mac_type) {
+	case SHA1:
+		sha1 = (struct sha1_state *) in_pad;
+		swap_data32(sha1->state, SHA1_DIGEST_SIZE);
+		memcpy(out_pad, &sha1->state, SHA1_DIGEST_SIZE);
+		break;
+
+	case SHA256:
+		sha256 = (struct sha256_state *) in_pad;
+		swap_data32(sha256->state, SHA256_DIGEST_SIZE);
+		memcpy(out_pad, &sha256->state, SHA256_DIGEST_SIZE);
+		break;
+
+	case SHA384:
+	case SHA512:
+		sha512 = (struct sha512_state *) in_pad;
+		swap_data64(sha512->state, SHA512_DIGEST_SIZE);
+		memcpy(out_pad, &sha512->state, SHA512_DIGEST_SIZE);
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int calculateipadopad(struct crypto_aead *cipher)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(cipher);
+	u8 *ipad = NULL, *opad = NULL;
+	int bs = crypto_shash_blocksize(ctx->hashalg);
+	int ds = crypto_shash_digestsize(ctx->hashalg);
+	int state_size = crypto_shash_statesize(ctx->hashalg);
+	int authkeylen = ctx->auth_key_len;
+	int err = 0, icount = 0;
+
+	ctx->sdesc = alloc_sdesc(ctx->hashalg);
+	if (IS_ERR(ctx->sdesc))
+		return -ENOMEM;
+
+	ctx->ipad = kzalloc(bs, GFP_KERNEL);
+	if (!ctx->ipad)
+		goto calc_fail;
+
+	ctx->opad = kzalloc(bs, GFP_KERNEL);
+	if (!ctx->opad)
+		goto calc_fail;
+
+	ipad = kzalloc(state_size, GFP_KERNEL);
+	if (!ipad)
+		goto calc_fail;
+
+	opad = kzalloc(state_size, GFP_KERNEL);
+	if (!opad)
+		goto calc_fail;
+
+	if (authkeylen > bs) {
+		err = crypto_shash_digest(&ctx->sdesc->shash, ctx->key,
+					  authkeylen, ipad);
+		if (err)
+			goto calc_fail;
+
+		authkeylen = ds;
+	} else {
+		memcpy(ipad, ctx->key, authkeylen);
+	}
+
+	memset(ipad + authkeylen, 0, bs - authkeylen);
+	memcpy(opad, ipad, bs);
+
+	for (icount = 0; icount < bs; icount++) {
+		ipad[icount] ^= 0x36;
+		opad[icount] ^= 0x5c;
+	}
+
+	/* Partial Hash calculated from the software
+	 * algorithm is retrieved for IPAD & OPAD
+	 */
+
+	/* IPAD Calculation */
+	crypto_shash_init(&ctx->sdesc->shash);
+	crypto_shash_update(&ctx->sdesc->shash, ipad, bs);
+	crypto_shash_export(&ctx->sdesc->shash, ipad);
+	if (copy_pad(ctx->mac_type, ctx->ipad, ipad) != 0)
+		goto calc_fail;
+
+	/* OPAD Calculation */
+	crypto_shash_init(&ctx->sdesc->shash);
+	crypto_shash_update(&ctx->sdesc->shash, opad, bs);
+	crypto_shash_export(&ctx->sdesc->shash, opad);
+	if (copy_pad(ctx->mac_type, ctx->opad, opad) != 0)
+		goto calc_fail;
+
+	kfree(ipad);
+	kfree(opad);
+	return 0;
+
+calc_fail:
+	kfree(ctx->ipad);
+	kfree(ctx->opad);
+	kfree(ipad);
+	kfree(opad);
+	kfree(ctx->sdesc);
+
+	return -ENOMEM;
+}
+
+int cvm_aead_cbc_aes_sha_setkey(struct crypto_aead *cipher,
+				const unsigned char *key,
+				unsigned int keylen)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(cipher);
+	struct crypto_authenc_key_param *param;
+	struct rtattr *rta = (void *)key;
+	int enckeylen = 0, authkeylen = 0;
+	int status = -EINVAL;
+
+	if (!RTA_OK(rta, keylen))
+		goto badkey;
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)
+		goto badkey;
+
+	if (RTA_PAYLOAD(rta) < sizeof(*param))
+		goto badkey;
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+	if (keylen < enckeylen)
+		goto badkey;
+
+	if (keylen > MAX_KEY_SIZE)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+	memcpy(ctx->key, key, keylen);
+
+	switch (enckeylen) {
+	case AES_KEYSIZE_128:
+		ctx->key_type = AES_128_BIT;
+		break;
+	case AES_KEYSIZE_192:
+		ctx->key_type = AES_192_BIT;
+		break;
+	case AES_KEYSIZE_256:
+		ctx->key_type = AES_256_BIT;
+		break;
+	default:
+		/* Invalid key length */
+		crypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+
+	ctx->enc_key_len = enckeylen;
+	ctx->auth_key_len = authkeylen;
+
+	status = calculateipadopad(cipher);
+	if (status)
+		goto badkey;
+
+	return 0;
+badkey:
+	crypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return status;
+}
+
+int cvm_aead_ecb_null_sha_setkey(struct crypto_aead *cipher,
+				 const unsigned char *key,
+				 unsigned int keylen)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(cipher);
+	struct crypto_authenc_key_param *param;
+	struct rtattr *rta = (void *)key;
+	int enckeylen = 0, authkeylen = 0;
+
+	if (!RTA_OK(rta, keylen))
+		goto badkey;
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)
+		goto badkey;
+
+	if (RTA_PAYLOAD(rta) < sizeof(*param))
+		goto badkey;
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+	if (enckeylen != 0)
+		goto badkey;
+
+	if (keylen > MAX_KEY_SIZE)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+	memcpy(ctx->key, key, keylen);
+	ctx->enc_key_len = enckeylen;
+	ctx->auth_key_len = authkeylen;
+	return 0;
+badkey:
+	crypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+int cvm_aead_gcm_aes_setkey(struct crypto_aead *cipher,
+			    const unsigned char *key,
+			    unsigned int keylen)
+{
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(cipher);
+
+	/* For aes gcm we expect to get encryption key (16, 24, 32 bytes)
+	 * and salt (4 bytes)
+	 */
+	switch (keylen) {
+	case AES_KEYSIZE_128 + AES_GCM_SALT_SIZE:
+		ctx->key_type = AES_128_BIT;
+		ctx->enc_key_len = AES_KEYSIZE_128;
+		break;
+	case AES_KEYSIZE_192 + AES_GCM_SALT_SIZE:
+		ctx->key_type = AES_192_BIT;
+		ctx->enc_key_len = AES_KEYSIZE_192;
+		break;
+	case AES_KEYSIZE_256 + AES_GCM_SALT_SIZE:
+		ctx->key_type = AES_256_BIT;
+		ctx->enc_key_len = AES_KEYSIZE_256;
+		break;
+	default:
+		/* Invalid key and salt length */
+		crypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		return -EINVAL;
+	}
+
+	/* Store encryption key and salt */
+	memcpy(ctx->key, key, keylen);
+
+	return 0;
+}
+
+static inline u32 create_aead_ctx_hdr(struct aead_request *req, u32 enc,
+				      u32 *argcnt)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(tfm);
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	struct fc_context *fctx = &rctx->fctx;
+	int mac_len = crypto_aead_authsize(tfm);
+	int ds;
+
+	rctx->ctrl_word.e.enc_data_offset = req->assoclen;
+
+	switch (ctx->cipher_type) {
+	case AES_CBC:
+		fctx->enc.enc_ctrl.e.iv_source = FROM_CPTR;
+		/* Copy encryption key to context */
+		memcpy(fctx->enc.encr_key, ctx->key + ctx->auth_key_len,
+		       ctx->enc_key_len);
+		/* Copy IV to context */
+		memcpy(fctx->enc.encr_iv, req->iv, crypto_aead_ivsize(tfm));
+
+		ds = crypto_shash_digestsize(ctx->hashalg);
+		if (ctx->mac_type == SHA384)
+			ds = SHA512_DIGEST_SIZE;
+		if (ctx->ipad)
+			memcpy(fctx->hmac.e.ipad, ctx->ipad, ds);
+		if (ctx->opad)
+			memcpy(fctx->hmac.e.opad, ctx->opad, ds);
+		break;
+
+	case AES_GCM:
+		fctx->enc.enc_ctrl.e.iv_source = FROM_DPTR;
+		/* Copy encryption key to context */
+		memcpy(fctx->enc.encr_key, ctx->key, ctx->enc_key_len);
+		/* Copy salt to context */
+		memcpy(fctx->enc.encr_iv, ctx->key + ctx->enc_key_len,
+		       AES_GCM_SALT_SIZE);
+
+		rctx->ctrl_word.e.iv_offset = req->assoclen - AES_GCM_IV_OFFSET;
+		break;
+
+	default:
+		/* Unknown cipher type */
+		return -EINVAL;
+	}
+	rctx->ctrl_word.flags = cpu_to_be64(rctx->ctrl_word.flags);
+
+	req_info->ctrl.s.dma_mode = DMA_GATHER_SCATTER;
+	req_info->ctrl.s.se_req = SE_CORE_REQ;
+	req_info->req.opcode.s.major = MAJOR_OP_FC |
+				 DMA_MODE_FLAG(DMA_GATHER_SCATTER);
+	if (enc) {
+		req_info->req.opcode.s.minor = 2;
+		req_info->req.param1 = req->cryptlen;
+		req_info->req.param2 = req->cryptlen + req->assoclen;
+	} else {
+		req_info->req.opcode.s.minor = 3;
+		req_info->req.param1 = req->cryptlen - mac_len;
+		req_info->req.param2 = req->cryptlen + req->assoclen - mac_len;
+	}
+
+	fctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;
+	fctx->enc.enc_ctrl.e.aes_key = ctx->key_type;
+	fctx->enc.enc_ctrl.e.mac_type = ctx->mac_type;
+	fctx->enc.enc_ctrl.e.mac_len = mac_len;
+	fctx->enc.enc_ctrl.flags = cpu_to_be64(fctx->enc.enc_ctrl.flags);
+
+	/* Storing Packet Data Information in offset
+	 * Control Word First 8 bytes
+	 */
+	req_info->in[*argcnt].vptr = (u8 *)&rctx->ctrl_word;
+	req_info->in[*argcnt].size = CONTROL_WORD_LEN;
+	req_info->req.dlen += CONTROL_WORD_LEN;
+	++(*argcnt);
+
+	req_info->in[*argcnt].vptr = (u8 *)fctx;
+	req_info->in[*argcnt].size = sizeof(struct fc_context);
+	req_info->req.dlen += sizeof(struct fc_context);
+	++(*argcnt);
+
+	return 0;
+}
+
+static inline u32 create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,
+				      u32 enc)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct cvm_aead_ctx *ctx = crypto_aead_ctx(tfm);
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+
+	req_info->ctrl.s.dma_mode = DMA_GATHER_SCATTER;
+	req_info->ctrl.s.se_req = SE_CORE_REQ;
+	req_info->req.opcode.s.major = MAJOR_OP_HMAC |
+				 DMA_MODE_FLAG(DMA_GATHER_SCATTER);
+	req_info->req.opcode.s.minor = 0;
+	req_info->req.param1 = ctx->auth_key_len;
+	req_info->req.param2 = ctx->mac_type << 8;
+
+	/* Add authentication key */
+	req_info->in[*argcnt].vptr = ctx->key;
+	req_info->in[*argcnt].size = ROUNDUP8(ctx->auth_key_len);
+	req_info->req.dlen += ROUNDUP8(ctx->auth_key_len);
+	++(*argcnt);
+
+	return 0;
+}
+
+static inline u32 create_aead_input_list(struct aead_request *req, u32 enc)
+{
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	u32 inputlen =  req->cryptlen + req->assoclen;
+	u32 argcnt = 0;
+
+	create_aead_ctx_hdr(req, enc, &argcnt);
+	update_input_data(req_info, req->src, inputlen, &argcnt);
+	req_info->incnt = argcnt;
+
+	return 0;
+}
+
+static inline u32 create_aead_output_list(struct aead_request *req, u32 enc,
+					  u32 mac_len)
+{
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info =  &rctx->cpt_req;
+	u32 argcnt = 0, outputlen = 0;
+
+	if (enc)
+		outputlen = req->cryptlen +  req->assoclen + mac_len;
+	else
+		outputlen = req->cryptlen + req->assoclen - mac_len;
+
+	update_output_data(req_info, req->dst, 0, outputlen, &argcnt);
+	req_info->outcnt = argcnt;
+
+	return 0;
+}
+
+static inline u32 create_aead_null_input_list(struct aead_request *req,
+					      u32 enc, u32 mac_len)
+{
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	u32 inputlen =  req->cryptlen + req->assoclen;
+	u32 argcnt = 0;
+
+	if (enc)
+		inputlen =  req->cryptlen + req->assoclen;
+	else
+		inputlen =  req->cryptlen + req->assoclen - mac_len;
+
+	create_hmac_ctx_hdr(req, &argcnt, enc);
+	update_input_data(req_info, req->src, inputlen, &argcnt);
+	req_info->incnt = argcnt;
+
+	return 0;
+}
+
+static inline u32 create_aead_null_output_list(struct aead_request *req,
+					       u32 enc, u32 mac_len)
+{
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info =  &rctx->cpt_req;
+	struct scatterlist *dst;
+	u8 *ptr = NULL;
+	int argcnt = 0, status, offset;
+	u32 inputlen;
+
+	if (enc)
+		inputlen =  req->cryptlen + req->assoclen;
+	else
+		inputlen =  req->cryptlen + req->assoclen - mac_len;
+
+	if (req->src != req->dst) {
+		/*
+		 * If source and destination are different
+		 * then copy payload to destination
+		 */
+		ptr = kmalloc(inputlen, GFP_KERNEL);
+		if (!ptr) {
+			status = -ENOMEM;
+			goto error;
+		}
+
+		status = sg_copy_to_buffer(req->src, sg_nents(req->src), ptr,
+					   inputlen);
+		if (status != inputlen) {
+			status = -EINVAL;
+			goto error;
+		}
+		status = sg_copy_from_buffer(req->dst, sg_nents(req->dst), ptr,
+					     inputlen);
+		if (status != inputlen) {
+			status = -EINVAL;
+			goto error;
+		}
+		kfree(ptr);
+	}
+
+	if (enc) {
+		/*
+		 * In an encryption scenario hmac needs
+		 * to be appended after payload
+		 */
+		dst = req->dst;
+		offset = inputlen;
+		while (offset >= dst->length) {
+			offset -= dst->length;
+			dst = sg_next(dst);
+			if (!dst) {
+				status = -ENOENT;
+				goto error;
+			}
+		}
+
+		update_output_data(req_info, dst, offset, mac_len, &argcnt);
+	} else {
+		/*
+		 * In a decryption scenario calculated hmac for received
+		 * payload needs to be compare with hmac received
+		 */
+		status = sg_copy_buffer(req->src, sg_nents(req->src),
+					rctx->fctx.hmac.s.hmac_recv, mac_len,
+					inputlen, true);
+		if (status != mac_len) {
+			status = -EINVAL;
+			goto error;
+		}
+
+		req_info->out[argcnt].vptr = rctx->fctx.hmac.s.hmac_calc;
+		req_info->out[argcnt].size = mac_len;
+		argcnt++;
+	}
+
+	req_info->outcnt = argcnt;
+	return 0;
+error:
+	kfree(ptr);
+	return status;
+}
+
+u32 cvm_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct cvm_req_ctx *rctx = aead_request_ctx(req);
+	struct cpt_request_info *req_info = &rctx->cpt_req;
+	struct pci_dev *pdev = NULL;
+	u32 status, cpu_num;
+
+	memset(rctx, 0, sizeof(struct cvm_req_ctx));
+
+	switch (reg_type) {
+	case AEAD_ENC_DEC_REQ:
+		status = create_aead_input_list(req, enc);
+		if (status)
+			return status;
+		status = create_aead_output_list(req, enc,
+						 crypto_aead_authsize(tfm));
+		if (status)
+			return status;
+		break;
+
+	case AEAD_ENC_DEC_NULL_REQ:
+		status = create_aead_null_input_list(req, enc,
+						     crypto_aead_authsize(tfm));
+		if (status)
+			return status;
+		status = create_aead_null_output_list(req, enc,
+						crypto_aead_authsize(tfm));
+		if (status)
+			return status;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	status = get_se_device(&pdev, &cpu_num);
+	if (status)
+		return status;
+
+	req_info->callback = cvm_callback;
+	req_info->callback_arg = &req->base;
+	req_info->req_type = reg_type;
+	req_info->is_enc = enc;
+	req_info->ctrl.s.grp = cpt_get_kcrypto_eng_grp_num(pdev);
+
+	status = cpt_do_request(pdev, req_info, cpu_num);
+	/* We perform an asynchronous send and once
+	 * the request is completed the driver would
+	 * intimate through registered call back functions
+	 */
+	return status;
+}
+
+static int cvm_aead_encrypt(struct aead_request *req)
+{
+	return cvm_aead_enc_dec(req, AEAD_ENC_DEC_REQ, true);
+}
+
+static int cvm_aead_decrypt(struct aead_request *req)
+{
+	return cvm_aead_enc_dec(req, AEAD_ENC_DEC_REQ, false);
+}
+
+static int cvm_aead_null_encrypt(struct aead_request *req)
+{
+	return cvm_aead_enc_dec(req, AEAD_ENC_DEC_NULL_REQ, true);
+}
+
+static int cvm_aead_null_decrypt(struct aead_request *req)
+{
+	return cvm_aead_enc_dec(req, AEAD_ENC_DEC_NULL_REQ, false);
+}
+
+struct crypto_alg algs[] = { {
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
+	.cra_alignmask = 7,
+	.cra_priority = 4001,
+	.cra_name = "xts(aes)",
+	.cra_driver_name = "cavium-xts-aes",
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_u = {
+		.ablkcipher = {
+			.ivsize = AES_BLOCK_SIZE,
+			.min_keysize = 2 * AES_MIN_KEY_SIZE,
+			.max_keysize = 2 * AES_MAX_KEY_SIZE,
+			.setkey = cvm_xts_setkey,
+			.encrypt = cvm_encrypt,
+			.decrypt = cvm_decrypt,
+		},
+	},
+	.cra_init = cvm_enc_dec_init,
+	.cra_module = THIS_MODULE,
+}, {
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
+	.cra_alignmask = 7,
+	.cra_priority = 4001,
+	.cra_name = "cbc(aes)",
+	.cra_driver_name = "cavium-cbc-aes",
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_u = {
+		.ablkcipher = {
+			.ivsize = AES_BLOCK_SIZE,
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = cvm_cbc_aes_setkey,
+			.encrypt = cvm_encrypt,
+			.decrypt = cvm_decrypt,
+		},
+	},
+	.cra_init = cvm_enc_dec_init,
+	.cra_module = THIS_MODULE,
+}, {
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
+	.cra_alignmask = 7,
+	.cra_priority = 4001,
+	.cra_name = "ecb(aes)",
+	.cra_driver_name = "cavium-ecb-aes",
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_u = {
+		.ablkcipher = {
+			.ivsize = AES_BLOCK_SIZE,
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = cvm_ecb_aes_setkey,
+			.encrypt = cvm_encrypt,
+			.decrypt = cvm_decrypt,
+		},
+	},
+	.cra_init = cvm_enc_dec_init,
+	.cra_module = THIS_MODULE,
+}, {
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
+	.cra_alignmask = 7,
+	.cra_priority = 4001,
+	.cra_name = "cfb(aes)",
+	.cra_driver_name = "cavium-cfb-aes",
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_u = {
+		.ablkcipher = {
+			.ivsize = AES_BLOCK_SIZE,
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = cvm_cfb_aes_setkey,
+			.encrypt = cvm_encrypt,
+			.decrypt = cvm_decrypt,
+		},
+	},
+	.cra_init = cvm_enc_dec_init,
+	.cra_module = THIS_MODULE,
+}, {
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_ctxsize = sizeof(struct cvm_des3_ctx),
+	.cra_alignmask = 7,
+	.cra_priority = 4001,
+	.cra_name = "cbc(des3_ede)",
+	.cra_driver_name = "cavium-cbc-des3_ede",
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES3_EDE_KEY_SIZE,
+			.max_keysize = DES3_EDE_KEY_SIZE,
+			.ivsize = DES_BLOCK_SIZE,
+			.setkey = cvm_cbc_des3_setkey,
+			.encrypt = cvm_encrypt,
+			.decrypt = cvm_decrypt,
+		},
+	},
+	.cra_init = cvm_enc_dec_init,
+	.cra_module = THIS_MODULE,
+}, {
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_ctxsize = sizeof(struct cvm_des3_ctx),
+	.cra_alignmask = 7,
+	.cra_priority = 4001,
+	.cra_name = "ecb(des3_ede)",
+	.cra_driver_name = "cavium-ecb-des3_ede",
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES3_EDE_KEY_SIZE,
+			.max_keysize = DES3_EDE_KEY_SIZE,
+			.ivsize = DES_BLOCK_SIZE,
+			.setkey = cvm_ecb_des3_setkey,
+			.encrypt = cvm_encrypt,
+			.decrypt = cvm_decrypt,
+		},
+	},
+	.cra_init = cvm_enc_dec_init,
+	.cra_module = THIS_MODULE,
+} };
+
+struct aead_alg cvm_aeads[] = { {
+	.base = {
+		.cra_name = "authenc(hmac(sha1),cbc(aes))",
+		.cra_driver_name = "authenc-hmac-sha1-cbc-aes-cavm",
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_cbc_aes_sha1_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_cbc_aes_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_encrypt,
+	.decrypt = cvm_aead_decrypt,
+	.ivsize = AES_BLOCK_SIZE,
+	.maxauthsize = SHA1_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha256),cbc(aes))",
+		.cra_driver_name = "authenc-hmac-sha256-cbc-aes-cavm",
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_cbc_aes_sha256_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_cbc_aes_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_encrypt,
+	.decrypt = cvm_aead_decrypt,
+	.ivsize = AES_BLOCK_SIZE,
+	.maxauthsize = SHA256_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha384),cbc(aes))",
+		.cra_driver_name = "authenc-hmac-sha384-cbc-aes-cavm",
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_cbc_aes_sha384_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_cbc_aes_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_encrypt,
+	.decrypt = cvm_aead_decrypt,
+	.ivsize = AES_BLOCK_SIZE,
+	.maxauthsize = SHA384_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha512),cbc(aes))",
+		.cra_driver_name = "authenc-hmac-sha512-cbc-aes-cavm",
+		.cra_blocksize = AES_BLOCK_SIZE,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_cbc_aes_sha512_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_cbc_aes_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_encrypt,
+	.decrypt = cvm_aead_decrypt,
+	.ivsize = AES_BLOCK_SIZE,
+	.maxauthsize = SHA512_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha1),ecb(cipher_null))",
+		.cra_driver_name = "authenc-hmac-sha1-ecb-null-cavm",
+		.cra_blocksize = 1,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_ecb_null_sha1_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_ecb_null_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_null_encrypt,
+	.decrypt = cvm_aead_null_decrypt,
+	.ivsize = 0,
+	.maxauthsize = SHA1_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha256),ecb(cipher_null))",
+		.cra_driver_name = "authenc-hmac-sha256-ecb-null-cavm",
+		.cra_blocksize = 1,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_ecb_null_sha256_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_ecb_null_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_null_encrypt,
+	.decrypt = cvm_aead_null_decrypt,
+	.ivsize = 0,
+	.maxauthsize = SHA256_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha384),ecb(cipher_null))",
+		.cra_driver_name = "authenc-hmac-sha384-ecb-null-cavm",
+		.cra_blocksize = 1,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_ecb_null_sha384_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_ecb_null_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_null_encrypt,
+	.decrypt = cvm_aead_null_decrypt,
+	.ivsize = 0,
+	.maxauthsize = SHA384_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "authenc(hmac(sha512),ecb(cipher_null))",
+		.cra_driver_name = "authenc-hmac-sha512-ecb-null-cavm",
+		.cra_blocksize = 1,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_ecb_null_sha512_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_ecb_null_sha_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_null_encrypt,
+	.decrypt = cvm_aead_null_decrypt,
+	.ivsize = 0,
+	.maxauthsize = SHA512_DIGEST_SIZE,
+}, {
+	.base = {
+		.cra_name = "rfc4106(gcm(aes))",
+		.cra_driver_name = "rfc4106-gcm-aes-cavm",
+		.cra_blocksize = 1,
+		.cra_ctxsize = sizeof(struct cvm_aead_ctx),
+		.cra_priority = 4001,
+		.cra_alignmask = 0,
+		.cra_module = THIS_MODULE,
+	},
+	.init = cvm_aead_gcm_aes_init,
+	.exit = cvm_aead_exit,
+	.setkey = cvm_aead_gcm_aes_setkey,
+	.setauthsize = cvm_aead_set_authsize,
+	.encrypt = cvm_aead_encrypt,
+	.decrypt = cvm_aead_decrypt,
+	.ivsize = AES_GCM_IV_SIZE,
+	.maxauthsize = AES_GCM_ICV_SIZE,
+} };
+
+static inline int is_any_alg_used(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(algs); i++)
+		if (refcount_read(&algs[i].cra_refcnt) != 1)
+			return true;
+	for (i = 0; i < ARRAY_SIZE(cvm_aeads); i++)
+		if (refcount_read(&cvm_aeads[i].base.cra_refcnt) != 1)
+			return true;
+	return false;
+}
+
+static inline int cav_register_algs(void)
+{
+	int i, err = 0;
+
+	for (i = 0; i < ARRAY_SIZE(algs); i++)
+		algs[i].cra_flags &= ~CRYPTO_ALG_DEAD;
+	err = crypto_register_algs(algs, ARRAY_SIZE(algs));
+	if (err)
+		return err;
+
+	for (i = 0; i < ARRAY_SIZE(cvm_aeads); i++)
+		cvm_aeads[i].base.cra_flags &= ~CRYPTO_ALG_DEAD;
+	err = crypto_register_aeads(cvm_aeads, ARRAY_SIZE(cvm_aeads));
+	if (err) {
+		crypto_unregister_algs(algs, ARRAY_SIZE(algs));
+		return err;
+	}
+
+	return 0;
+}
+
+static inline void cav_unregister_algs(void)
+{
+	crypto_unregister_algs(algs, ARRAY_SIZE(algs));
+	crypto_unregister_aeads(cvm_aeads, ARRAY_SIZE(cvm_aeads));
+}
+
+static int compare_func(const void *lptr, const void *rptr)
+{
+	struct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;
+	struct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;
+
+	if (ldesc->dev->devfn < rdesc->dev->devfn)
+		return -1;
+	if (ldesc->dev->devfn > rdesc->dev->devfn)
+		return 1;
+	return 0;
+}
+
+static void swap_func(void *lptr, void *rptr, int size)
+{
+	struct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;
+	struct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;
+	struct cpt_device_desc desc;
+
+	desc = *ldesc;
+	*ldesc = *rdesc;
+	*rdesc = desc;
+}
+
+int cvm_crypto_init(struct pci_dev *pdev, enum cpt_pf_type pf_type,
+		    enum cpt_vf_type engine_type, int num_queues,
+		    int num_devices)
+{
+	int ret = 0;
+	int count;
+
+	mutex_lock(&mutex);
+	switch (engine_type) {
+	case SE_TYPES:
+		count = atomic_read(&se_devices.count);
+		if (count >= CPT_MAX_VF_NUM) {
+			dev_err(&pdev->dev, "No space to add a new device");
+			ret = -ENOSPC;
+			goto err;
+		}
+		se_devices.desc[count].pf_type = pf_type;
+		se_devices.desc[count].num_queues = num_queues;
+		se_devices.desc[count++].dev = pdev;
+		atomic_inc(&se_devices.count);
+
+		if (atomic_read(&se_devices.count) == num_devices &&
+		    is_crypto_registered == false) {
+			if (cav_register_algs()) {
+				dev_err(&pdev->dev,
+				   "Error in registering crypto algorithms\n");
+				ret =  -EINVAL;
+				goto err;
+			}
+			try_module_get(THIS_MODULE);
+			is_crypto_registered = true;
+		}
+		sort(se_devices.desc, count, sizeof(struct cpt_device_desc),
+		     compare_func, swap_func);
+		break;
+
+	case AE_TYPES:
+		count = atomic_read(&ae_devices.count);
+		if (count >= CPT_MAX_VF_NUM) {
+			dev_err(&pdev->dev, "No space to a add new device");
+			ret = -ENOSPC;
+			goto err;
+		}
+		ae_devices.desc[count].pf_type = pf_type;
+		ae_devices.desc[count].num_queues = num_queues;
+		ae_devices.desc[count++].dev = pdev;
+		atomic_inc(&ae_devices.count);
+		sort(ae_devices.desc, count, sizeof(struct cpt_device_desc),
+		     compare_func, swap_func);
+		break;
+
+	default:
+		dev_err(&pdev->dev, "Unknown VF type %d\n", engine_type);
+	}
+err:
+	mutex_unlock(&mutex);
+	return ret;
+}
+
+void cvm_crypto_exit(struct pci_dev *pdev)
+{
+	bool dev_found = false;
+	int i, j, count;
+
+	mutex_lock(&mutex);
+
+	count = atomic_read(&se_devices.count);
+	for (i = 0; i < count; i++)
+		if (pdev == se_devices.desc[i].dev) {
+			for (j = i; j < count-1; j++)
+				se_devices.desc[j] = se_devices.desc[j+1];
+			dev_found = true;
+			break;
+		}
+
+	if (!dev_found)
+		dev_err(&pdev->dev, "%s device not found", __func__);
+
+	if (atomic_dec_and_test(&se_devices.count) &&
+	    !is_any_alg_used()) {
+		cav_unregister_algs();
+		module_put(THIS_MODULE);
+		is_crypto_registered = false;
+	}
+	mutex_unlock(&mutex);
+}
diff --git a/drivers/crypto/cavium/cpt/common/cpt_algs.h b/drivers/crypto/cavium/cpt/common/cpt_algs.h
new file mode 100644
index 000000000000..8cce0db0ce24
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_algs.h
@@ -0,0 +1,204 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _CPT_ALGS_H_
+#define _CPT_ALGS_H_
+
+#define MAX_ENC_KEY_SIZE 32
+#define MAX_HASH_KEY_SIZE 64
+#define MAX_KEY_SIZE (MAX_ENC_KEY_SIZE + MAX_HASH_KEY_SIZE)
+#define CONTROL_WORD_LEN 8
+#define KEY2_OFFSET 48
+
+#define DMA_MODE_FLAG(dma_mode) \
+	(((dma_mode) == DMA_GATHER_SCATTER) ? (1 << 7) : 0)
+
+/* Size of salt in AES GCM mode */
+#define AES_GCM_SALT_SIZE	4
+/* Size of IV in AES GCM mode */
+#define AES_GCM_IV_SIZE		8
+/* Size of ICV (Integrity Check Value) in AES GCM mode */
+#define AES_GCM_ICV_SIZE	16
+/* Offset of IV in AES GCM mode */
+#define AES_GCM_IV_OFFSET	8
+
+/* Truncated SHA digest size */
+#define SHA1_TRUNC_DIGEST_SIZE		12
+#define SHA256_TRUNC_DIGEST_SIZE	16
+#define SHA384_TRUNC_DIGEST_SIZE	24
+#define SHA512_TRUNC_DIGEST_SIZE	32
+
+#define ROUNDUP8(val) (((val) + 7)&0xfffffff8)
+
+enum request_type {
+	ENC_DEC_REQ		= 0x1,
+	AEAD_ENC_DEC_REQ	= 0x2,
+	AEAD_ENC_DEC_NULL_REQ	= 0x3,
+	PASSTHROUGH_REQ		= 0x4
+};
+
+enum major_opcodes {
+	MAJOR_OP_MISC	= 0x01,
+	MAJOR_OP_FC	= 0x33,
+	MAJOR_OP_HMAC	= 0x35,
+};
+
+enum req_type {
+	AE_CORE_REQ,
+	SE_CORE_REQ,
+};
+
+enum cipher_type {
+	CIPHER_NULL	= 0x0,
+	DES3_CBC	= 0x1,
+	DES3_ECB	= 0x2,
+	AES_CBC		= 0x3,
+	AES_ECB		= 0x4,
+	AES_CFB		= 0x5,
+	AES_CTR		= 0x6,
+	AES_GCM		= 0x7,
+	AES_XTS		= 0x8
+};
+
+enum mac_type {
+	MAC_NULL	= 0x0,
+	MD5		= 0x1,
+	SHA1		= 0x2,
+	SHA224		= 0x3,
+	SHA256		= 0x4,
+	SHA384		= 0x5,
+	SHA512		= 0x6,
+	GMAC		= 0x7
+};
+
+enum aes_key_len {
+	AES_128_BIT = 0x1,
+	AES_192_BIT = 0x2,
+	AES_256_BIT = 0x3
+};
+
+union encr_ctrl {
+	u64 flags;
+	struct {
+#if defined(__BIG_ENDIAN_BITFIELD)
+		u64 enc_cipher:4;
+		u64 reserved1:1;
+		u64 aes_key:2;
+		u64 iv_source:1;
+		u64 mac_type:4;
+		u64 reserved2:3;
+		u64 auth_input_type:1;
+		u64 mac_len:8;
+		u64 reserved3:8;
+		u64 encr_offset:16;
+		u64 iv_offset:8;
+		u64 auth_offset:8;
+#else
+		u64 auth_offset:8;
+		u64 iv_offset:8;
+		u64 encr_offset:16;
+		u64 reserved3:8;
+		u64 mac_len:8;
+		u64 auth_input_type:1;
+		u64 reserved2:3;
+		u64 mac_type:4;
+		u64 iv_source:1;
+		u64 aes_key:2;
+		u64 reserved1:1;
+		u64 enc_cipher:4;
+#endif
+	} e;
+};
+
+struct cvm_cipher {
+	const char *name;
+	u8 value;
+};
+
+struct enc_context {
+	union encr_ctrl enc_ctrl;
+	u8 encr_key[32];
+	u8 encr_iv[16];
+};
+
+union fchmac_context {
+	struct {
+		u8 ipad[64];
+		u8 opad[64]; /* or OPAD */
+	} e;
+	struct {
+		u8 hmac_calc[64]; /* HMAC received */
+		u8 hmac_recv[64]; /* HMAC calculated */
+	} s;
+};
+
+struct fc_context {
+	struct enc_context enc;
+	union fchmac_context hmac;
+};
+
+struct cvm_enc_ctx {
+	u32 key_len;
+	u8 enc_key[MAX_KEY_SIZE];
+	u8 cipher_type;
+	u8 key_type;
+};
+
+struct cvm_des3_ctx {
+	u32 key_len;
+	u8 des3_key[MAX_KEY_SIZE];
+};
+
+union offset_ctrl_word {
+	u64 flags;
+	struct {
+#if defined(__BIG_ENDIAN_BITFIELD)
+		u64 reserved:32;
+		u64 enc_data_offset:16;
+		u64 iv_offset:8;
+		u64 auth_offset:8;
+#else
+		u64 auth_offset:8;
+		u64 iv_offset:8;
+		u64 enc_data_offset:16;
+		u64 reserved:32;
+#endif
+	} e;
+};
+
+struct cvm_req_ctx {
+	struct cpt_request_info cpt_req;
+	union offset_ctrl_word ctrl_word;
+	struct fc_context fctx;
+};
+
+struct sdesc {
+	struct shash_desc shash;
+};
+
+struct cvm_aead_ctx {
+	u8 key[MAX_KEY_SIZE];
+	struct crypto_shash *hashalg;
+	struct sdesc *sdesc;
+	u8 *ipad;
+	u8 *opad;
+	u32 enc_key_len;
+	u32 auth_key_len;
+	u8 cipher_type;
+	u8 mac_type;
+	u8 key_type;
+};
+
+int cpt_do_request(struct pci_dev *pdev, struct cpt_request_info *req,
+		   int cpu_num);
+int cpt_get_kcrypto_eng_grp_num(struct pci_dev *pdev);
+void cvm_callback(int status, void *arg, void *req);
+
+#endif /*_CPT_ALGS_H_*/
diff --git a/drivers/crypto/cavium/cpt/common/cpt_common.h b/drivers/crypto/cavium/cpt/common/cpt_common.h
new file mode 100644
index 000000000000..2247417c705a
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_common.h
@@ -0,0 +1,228 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT_COMMON_H
+#define __CPT_COMMON_H
+
+#include <linux/pci.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include "cpt_debug.h"
+
+/* Delay in us when waiting for a state change */
+#define CSR_DELAY		30
+
+/* Microcode version string maximum length */
+#define CPT_UCODE_VERSION_SZ	32
+#define TIME_IN_RESET_COUNT	5
+
+/* Completion code size and initial value */
+#define COMPLETION_CODE_SIZE	8
+#define COMPLETION_CODE_INIT	0
+
+/* SG list header size in bytes */
+#define SG_LIST_HDR_SIZE	8
+
+/* Maximum number of SG input and output buffers */
+#define MAX_SG_IN_CNT		12
+#define MAX_SG_OUT_CNT		13
+
+/* DMA mode direct or SG */
+#define DMA_DIRECT_DIRECT	0
+#define DMA_GATHER_SCATTER	1
+
+/* Context source CPTR or DPTR */
+#define FROM_CPTR		0
+#define FROM_DPTR		1
+
+/* Maximum number of pending entry input and output buffers */
+#define MAX_BUF_CNT		16
+
+/* CPT instruction queue alignment */
+#define CPT_INST_Q_ALIGNMENT	128
+
+/* Default timeout when waiting for free pending entry in us */
+#define CPT_PENTRY_TIMEOUT	1000
+#define CPT_PENTRY_STEP		50
+
+/* Default threshold for stopping and resuming sender requests */
+#define CPT_IQ_STOP_MARGIN	128
+#define CPT_IQ_RESUME_MARGIN	512
+
+/* Default command timeout in seconds */
+#define CPT_COMMAND_TIMEOUT	4
+#define CPT_TIMER_HOLD		0x03F
+#define CPT_COUNT_HOLD		32
+
+/* Minimum and maximum values for interrupt coalescing */
+#define CPT_COALESC_MIN_TIME_WAIT	0x0
+#define CPT_COALESC_MAX_TIME_WAIT	((1<<16)-1)
+#define CPT_COALESC_MIN_NUM_WAIT	0x0
+#define CPT_COALESC_MAX_NUM_WAIT	((1<<20)-1)
+
+enum cpt_pf_type {
+	CPT_81XX = 1,
+	CPT_AE_83XX = 2,
+	CPT_SE_83XX = 3,
+	CPT_96XX = 4,
+	BAD_CPT_PF_TYPE,
+};
+
+enum cpt_vf_type {
+	AE_TYPES = 1,
+	SE_TYPES = 2,
+	IE_TYPES = 3,
+	BAD_CPT_VF_TYPE,
+};
+
+union opcode_info {
+	u16 flags;
+	struct {
+		u8 major;
+		u8 minor;
+	} s;
+};
+
+struct cptvf_request {
+	union opcode_info opcode;
+	u16 param1;
+	u16 param2;
+	u16 dlen;
+};
+
+struct buf_ptr {
+	u8 *vptr;
+	dma_addr_t dma_addr;
+	u16 size;
+};
+
+union ctrl_info {
+	u32 flags;
+	struct {
+#if defined(__BIG_ENDIAN_BITFIELD)
+		u32 reserved0:26;
+		u32 grp:3; /* Group bits */
+		u32 dma_mode:2; /* DMA mode */
+		u32 se_req:1;/* To SE core */
+#else
+		u32 se_req:1; /* To SE core */
+		u32 dma_mode:2; /* DMA mode */
+		u32 grp:3; /* Group bits */
+		u32 reserved0:26;
+#endif
+	} s;
+};
+
+/*
+ * CPT_INST_S software command definitions
+ * Words EI (0-3)
+ */
+union iq_cmd_word0 {
+	u64 u64;
+	struct {
+		u16 opcode;
+		u16 param1;
+		u16 param2;
+		u16 dlen;
+	} s;
+};
+
+union iq_cmd_word3 {
+	u64 u64;
+	struct {
+#if defined(__BIG_ENDIAN_BITFIELD)
+		u64 grp:3;
+		u64 cptr:61;
+#else
+		u64 cptr:61;
+		u64 grp:3;
+#endif
+	} s;
+};
+
+struct cpt_iq_command {
+	union iq_cmd_word0 cmd;
+	u64 dptr;
+	u64 rptr;
+	union iq_cmd_word3 cptr;
+};
+
+struct sglist_component {
+	union {
+		u64 len;
+		struct {
+			u16 len0;
+			u16 len1;
+			u16 len2;
+			u16 len3;
+		} s;
+	} u;
+	u64 ptr0;
+	u64 ptr1;
+	u64 ptr2;
+	u64 ptr3;
+};
+
+struct pending_entry {
+	u64 *completion_addr;	/* Completion address */
+	void *post_arg;
+	/* Kernel async request callback */
+	void (*callback)(int, void *, void *);
+	void *callback_arg;	/* Kernel async request callback arg */
+	u8 resume_sender;	/* Notify sender to resume sending requests */
+	u8 busy;		/* Entry status (free/busy) */
+};
+
+struct pending_queue {
+	struct pending_entry *head;	/* Head of the queue */
+	u32 front;			/* Process work from here */
+	u32 rear;			/* Append new work here */
+	u32 pending_count;		/* Pending requests count */
+	u32 qlen;			/* Queue length */
+	spinlock_t lock;		/* Queue lock */
+};
+
+struct cpt_request_info {
+	/* Kernel async request callback */
+	void (*callback)(int, void *, void *);
+	void *callback_arg; /* Kernel async request callback arg */
+	struct cptvf_request req; /* Request information (core specific) */
+	union ctrl_info ctrl; /* User control information */
+	struct buf_ptr in[MAX_BUF_CNT];
+	struct buf_ptr out[MAX_BUF_CNT];
+	u16 rlen; /* Output length */
+	u8 incnt; /* Number of input buffers */
+	u8 outcnt; /* Number of output buffers */
+	u8 req_type; /* Type of request */
+	u8 is_enc; /* Is a request an encryption request */
+};
+
+struct cpt_info_buffer {
+	struct pending_entry *pentry;
+	struct cpt_request_info *req;
+	u8 *scatter_components;
+	u8 *gather_components;
+	u64 *completion_addr;
+	u8 *out_buffer;
+	u8 *in_buffer;
+	dma_addr_t dptr_baddr;
+	dma_addr_t rptr_baddr;
+	dma_addr_t comp_baddr;
+	unsigned long time_in;
+	u32 dlen;
+	u8 extra_time;
+};
+
+int cvm_crypto_init(struct pci_dev *pdev, enum cpt_pf_type pf_type,
+		    enum cpt_vf_type engine_type, int num_queues,
+		    int num_devices);
+void cvm_crypto_exit(struct pci_dev *pdev);
+
+#endif /* __CPT_COMMON_H */
diff --git a/drivers/crypto/cavium/cpt/common/cpt_debug.c b/drivers/crypto/cavium/cpt/common/cpt_debug.c
new file mode 100644
index 000000000000..1ca1623209a0
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_debug.c
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/printk.h>
+#include "cpt_debug.h"
+
+static int debug_level;
+
+void cpt_set_dbg_level(int level)
+{
+	if (level >= (2*CPT_DBG_MAX_LEVEL-2))
+		debug_level = -1;
+	else
+		debug_level = level;
+}
+
+int cpt_is_dbg_level_en(int level)
+{
+	return (debug_level & level);
+}
diff --git a/drivers/crypto/cavium/cpt/common/cpt_debug.h b/drivers/crypto/cavium/cpt/common/cpt_debug.h
new file mode 100644
index 000000000000..2e2bf4a5d494
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_debug.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT_DEBUG_H
+#define __CPT_DEBUG_H
+
+#define DEFINE_CPT_DEBUG_PARM(name) \
+static int name; \
+module_param(name, uint, 0000); \
+MODULE_PARM_DESC(name, \
+"Debug level (0=disabled, 1=mbox msgs, 2=enc/dec reqs, 4=engine grps, >6=all)")
+
+enum {
+	CPT_DBG_MBOX_MSGS	= 0x0001, /* Mailbox mesages */
+	CPT_DBG_ENC_DEC_REQS	= 0x0002, /* Encryption/decryption requests */
+	CPT_DBG_ENGINE_GRPS	= 0x0004, /* Engine groups configuration */
+	CPT_DBG_MAX_LEVEL
+};
+
+void cpt_set_dbg_level(int level);
+int cpt_is_dbg_level_en(int level);
+
+#endif /*__CPT_DEBUG_H */
diff --git a/drivers/crypto/cavium/cpt/cpt_hw_types.h b/drivers/crypto/cavium/cpt/common/cpt_hw_types.h
similarity index 53%
rename from drivers/crypto/cavium/cpt/cpt_hw_types.h
rename to drivers/crypto/cavium/cpt/common/cpt_hw_types.h
index 279669494196..ff37300b76de 100644
--- a/drivers/crypto/cavium/cpt/cpt_hw_types.h
+++ b/drivers/crypto/cavium/cpt/common/cpt_hw_types.h
@@ -1,9 +1,11 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
  *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
  */
 
 #ifndef __CPT_HW_TYPES_H
@@ -11,18 +13,223 @@
 
 #include "cpt_common.h"
 
+/* Device IDs */
+#define CPT_PCI_PF_8X_DEVICE_ID 0xa040
+#define CPT_PCI_VF_8X_DEVICE_ID 0xa041
+#define CPT_PCI_PF_9X_DEVICE_ID 0xa0FD
+#define CPT_PCI_VF_9X_DEVICE_ID 0xa0FE
+
+#define CPT_81XX_PCI_PF_SUBSYS_ID 0xa240
+#define CPT_81XX_PCI_VF_SUBSYS_ID 0xa241
+#define CPT_83XX_PCI_PF_SUBSYS_ID 0xa340
+#define CPT_83XX_PCI_VF_SUBSYS_ID 0xa341
+#define CPT_96XX_PCI_PF_SUBSYS_ID 0xb200
+
+/* Configuration and status registers are in BAR0 on 8X platform */
+#define PCI_CPT_PF_8X_CFG_BAR	0
+#define PCI_CPT_VF_8X_CFG_BAR	0
+
+#define CPT_BAR_E_CPTX_VFX_8X_BAR0_OFFSET(a, b) \
+	(0x000020000000ll + 0x1000000000ll * (a) + 0x100000ll * (b))
+#define CPT_BAR_E_CPTX_VFX_8X_BAR0_SIZE	0x400000
+
+/*
+ * Mailbox interrupts offset
+ */
+#define CPT_81XX_PF_MBOX_INT 2
+#define CPT_83XX_PF_MBOX_INT 3
+#define CPT_96XX_PF_MBOX_INT 6
+#define CPT_PF_INT_VEC_E_MBOXX(x, a) ((x) + (a))
+
+/*
+ * Number of MSIX supported in PF
+ */
+#define	CPT_81XX_PF_MSIX_VECTORS 3
+#define	CPT_83XX_PF_MSIX_VECTORS 4
+#define	CPT_96XX_PF_MSIX_VECTORS 7
+
+/*
+ * Maximum supported microcode groups
+ */
+#define CPT_MAX_ENGINE_GROUPS 8
+
+/*
+ * CPT instruction size in bytes
+ */
+#define CPT_INST_SIZE 64
+
+/*
+ * CPT queue next chunk pointer size in bytes
+ */
+#define CPT_NEXT_CHUNK_PTR_SIZE 8
+
+/*
+ * CPT 8X VF MSIX vectors and their offsets
+ */
+#define	CPT_8X_VF_MSIX_VECTORS 2
+#define CPT_8X_VF_INTR_MBOX_MASK BIT(0)
+#define CPT_8X_VF_INTR_DOVF_MASK BIT(1)
+#define CPT_8X_VF_INTR_IRDE_MASK BIT(2)
+#define CPT_8X_VF_INTR_NWRP_MASK BIT(3)
+#define CPT_8X_VF_INTR_SERR_MASK BIT(4)
+
+/*
+ * CPT 9X VF MSIX vectors and their offsets
+ */
+#define	CPT_9X_VF_MSIX_VECTORS 1
+#define CPT_9X_VF_INTR_MBOX_MASK BIT(0)
+
+/*
+ * CPT 9X LF MSIX vectors
+ */
+#define	CPT_9X_LF_MSIX_VECTORS 2
+
+/* 8XXX CPT PF registers */
+#define CPT_PF_CONSTANTS		(0x0ll)
+#define CPT_PF_RESET			(0x100ll)
+#define CPT_PF_DIAG			(0x120ll)
+#define CPT_PF_BIST_STATUS		(0x160ll)
+#define CPT_PF_ECC0_CTL			(0x200ll)
+#define CPT_PF_ECC0_FLIP		(0x210ll)
+#define CPT_PF_ECC0_INT			(0x220ll)
+#define CPT_PF_ECC0_INT_W1S		(0x230ll)
+#define CPT_PF_ECC0_ENA_W1S		(0x240ll)
+#define CPT_PF_ECC0_ENA_W1C		(0x250ll)
+#define CPT_PF_MBOX_INTX(b)		(0x400ll | (u64)(b) << 3)
+#define CPT_PF_MBOX_INT_W1SX(b)		(0x420ll | (u64)(b) << 3)
+#define CPT_PF_MBOX_ENA_W1CX(b)		(0x440ll | (u64)(b) << 3)
+#define CPT_PF_MBOX_ENA_W1SX(b)		(0x460ll | (u64)(b) << 3)
+#define CPT_PF_EXEC_INT			(0x500ll)
+#define CPT_PF_EXEC_INT_W1S		(0x520ll)
+#define CPT_PF_EXEC_ENA_W1C		(0x540ll)
+#define CPT_PF_EXEC_ENA_W1S		(0x560ll)
+#define CPT_PF_GX_EN(b)			(0x600ll | (u64)(b) << 3)
+#define CPT_PF_EXEC_INFO		(0x700ll)
+#define CPT_PF_EXEC_BUSY		(0x800ll)
+#define CPT_PF_EXEC_INFO0		(0x900ll)
+#define CPT_PF_EXEC_INFO1		(0x910ll)
+#define CPT_PF_INST_REQ_PC		(0x10000ll)
+#define CPT_PF_INST_LATENCY_PC		(0x10020ll)
+#define CPT_PF_RD_REQ_PC		(0x10040ll)
+#define CPT_PF_RD_LATENCY_PC		(0x10060ll)
+#define CPT_PF_RD_UC_PC			(0x10080ll)
+#define CPT_PF_ACTIVE_CYCLES_PC		(0x10100ll)
+#define CPT_PF_EXE_CTL			(0x4000000ll)
+#define CPT_PF_EXE_STATUS		(0x4000008ll)
+#define CPT_PF_EXE_CLK			(0x4000010ll)
+#define CPT_PF_EXE_DBG_CTL		(0x4000018ll)
+#define CPT_PF_EXE_DBG_DATA		(0x4000020ll)
+#define CPT_PF_EXE_BIST_STATUS		(0x4000028ll)
+#define CPT_PF_EXE_REQ_TIMER		(0x4000030ll)
+#define CPT_PF_EXE_MEM_CTL		(0x4000038ll)
+#define CPT_PF_EXE_PERF_CTL		(0x4001000ll)
+#define CPT_PF_EXE_DBG_CNTX(b)		(0x4001100ll | (u64)(b) << 3)
+#define CPT_PF_EXE_PERF_EVENT_CNT	(0x4001180ll)
+#define CPT_PF_EXE_EPCI_INBX_CNT(b)	(0x4001200ll | (u64)(b) << 3)
+#define CPT_PF_EXE_EPCI_OUTBX_CNT(b)	(0x4001240ll | (u64)(b) << 3)
+#define CPT_PF_ENGX_UCODE_BASE(b)	(0x4002000ll | (u64)(b) << 3)
+#define CPT_PF_QX_CTL(b)		(0x8000000ll | (u64)(b) << 20)
+#define CPT_PF_QX_GMCTL(b)		(0x8000020ll | (u64)(b) << 20)
+#define CPT_PF_QX_CTL2(b)		(0x8000100ll | (u64)(b) << 20)
+#define CPT_PF_VFX_MBOXX(b, c)	(0x8001000ll | (u64)(b) << 20 | (u64)(c) << 8)
+
+/* 8XXX CPT VF registers */
+#define CPT_VQX_CTL(b)		(0x100ll | (u64)(b) << 20)
+#define CPT_VQX_SADDR(b)	(0x200ll | (u64)(b) << 20)
+#define CPT_VQX_DONE_WAIT(b)	(0x400ll | (u64)(b) << 20)
+#define CPT_VQX_INPROG(b)	(0x410ll | (u64)(b) << 20)
+#define CPT_VQX_DONE(b)		(0x420ll | (u64)(b) << 20)
+#define CPT_VQX_DONE_ACK(b)	(0x440ll | (u64)(b) << 20)
+#define CPT_VQX_DONE_INT_W1S(b) (0x460ll | (u64)(b) << 20)
+#define CPT_VQX_DONE_INT_W1C(b) (0x468ll | (u64)(b) << 20)
+#define CPT_VQX_DONE_ENA_W1S(b) (0x470ll | (u64)(b) << 20)
+#define CPT_VQX_DONE_ENA_W1C(b) (0x478ll | (u64)(b) << 20)
+#define CPT_VQX_MISC_INT(b)	(0x500ll | (u64)(b) << 20)
+#define CPT_VQX_MISC_INT_W1S(b) (0x508ll | (u64)(b) << 20)
+#define CPT_VQX_MISC_ENA_W1S(b) (0x510ll | (u64)(b) << 20)
+#define CPT_VQX_MISC_ENA_W1C(b) (0x518ll | (u64)(b) << 20)
+#define CPT_VQX_DOORBELL(b)	(0x600ll | (u64)(b) << 20)
+#define CPT_VFX_PF_MBOXX(b, c)	(0x1000ll | ((b) << 20) | ((c) << 3))
+
+/* 9XXX CPT LF registers */
+#define CPT_LF_CTL                      (0x10ull)
+#define CPT_LF_DONE_WAIT                (0x30ull)
+#define CPT_LF_INPROG                   (0x40ull)
+#define CPT_LF_DONE                     (0x50ull)
+#define CPT_LF_DONE_ACK                 (0x60ull)
+#define CPT_LF_DONE_INT_ENA_W1S         (0x90ull)
+#define CPT_LF_DONE_INT_ENA_W1C         (0xa0ull)
+#define CPT_LF_MISC_INT                 (0xb0ull)
+#define CPT_LF_MISC_INT_W1S             (0xc0ull)
+#define CPT_LF_MISC_INT_ENA_W1S         (0xd0ull)
+#define CPT_LF_MISC_INT_ENA_W1C         (0xe0ull)
+#define CPT_LF_Q_BASE                   (0xf0ull)
+#define CPT_LF_Q_SIZE                   (0x100ull)
+#define CPT_LF_Q_INST_PTR               (0x110ull)
+#define CPT_LF_Q_GRP_PTR                (0x120ull)
+#define CPT_LF_NQX(a)                   (0x400ull | (u64)(a) << 3)
+
 /**
- * Enumeration cpt_comp_e
+ * Enumeration cpt_8x_comp_e
  *
- * CPT Completion Enumeration
+ * CPT 8X Completion Enumeration
  * Enumerates the values of CPT_RES_S[COMPCODE].
  */
-enum cpt_comp_e {
-	CPT_COMP_E_NOTDONE = 0x00,
-	CPT_COMP_E_GOOD = 0x01,
-	CPT_COMP_E_FAULT = 0x02,
-	CPT_COMP_E_SWERR = 0x03,
-	CPT_COMP_E_LAST_ENTRY = 0xFF
+enum cpt_8x_comp_e {
+	CPT_8X_COMP_E_NOTDONE = 0x00,
+	CPT_8X_COMP_E_GOOD = 0x01,
+	CPT_8X_COMP_E_FAULT = 0x02,
+	CPT_8X_COMP_E_SWERR = 0x03,
+	CPT_8X_COMP_E_HWERR = 0x04,
+	CPT_8X_COMP_E_LAST_ENTRY = 0x05
+};
+
+ /**
+  * Enumeration cpt_9x_comp_e
+  *
+  * CPT 9X Completion Enumeration
+  * Enumerates the values of CPT_RES_S[COMPCODE].
+  */
+enum cpt_9x_comp_e {
+	CPT_9X_COMP_E_NOTDONE = 0x00,
+	CPT_9X_COMP_E_GOOD = 0x01,
+	CPT_9X_COMP_E_FAULT = 0x02,
+	CPT_9X_RESERVED = 0x03,
+	CPT_9X_COMP_E_HWERR = 0x04,
+	CPT_9X_COMP_E_INSTERR = 0x05,
+	CPT_9X_COMP_E_LAST_ENTRY = 0x06
+};
+
+/**
+ * Enumeration cpt_8x_vf_int_vec_e
+ *
+ * CPT 8X VF MSI-X Vector Enumeration
+ * Enumerates the MSI-X interrupt vectors.
+ */
+enum cpt_8x_vf_int_vec_e {
+	CPT_8X_VF_INT_VEC_E_MISC = 0x00,
+	CPT_8X_VF_INT_VEC_E_DONE = 0x01
+};
+
+/**
+ * Enumeration cpt_9x_vf_int_vec_e
+ *
+ * CPT 9X VF MSI-X Vector Enumeration
+ * Enumerates the MSI-X interrupt vectors.
+ */
+enum cpt_9x_vf_int_vec_e {
+	CPT_9X_VF_INT_VEC_E_MBOX = 0x00
+};
+
+/**
+ * Enumeration cpt_9x_lf_int_vec_e
+ *
+ * CPT 9X LF MSI-X Vector Enumeration
+ * Enumerates the MSI-X interrupt vectors.
+ */
+enum cpt_9x_lf_int_vec_e {
+	CPT_9X_LF_INT_VEC_E_MISC = 0x00,
+	CPT_9X_LF_INT_VEC_E_DONE = 0x01
 };
 
 /**
@@ -75,11 +282,12 @@ enum cpt_comp_e {
  */
 union cpt_inst_s {
 	u64 u[8];
-	struct cpt_inst_s_s {
+
+	struct cpt_inst_s_8s {
 #if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
 		u64 reserved_17_63:47;
 		u64 doneint:1;
-		u64 reserved_0_1:16;
+		u64 reserved_0_15:16;
 #else /* Word 0 - Little Endian */
 		u64 reserved_0_15:16;
 		u64 doneint:1;
@@ -87,7 +295,7 @@ union cpt_inst_s {
 #endif /* Word 0 - End */
 		u64 res_addr;
 #if defined(__BIG_ENDIAN_BITFIELD) /* Word 2 - Big Endian */
-		u64 reserved_172_19:20;
+		u64 reserved_172_191:20;
 		u64 grp:10;
 		u64 tt:2;
 		u64 tag:32;
@@ -102,7 +310,46 @@ union cpt_inst_s {
 		u64 ei1;
 		u64 ei2;
 		u64 ei3;
-	} s;
+	} s8x;
+
+	struct cpt_inst_s_9s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		u64 nixtx_addr:60;
+		u64 doneint:1;
+		u64 nixtxl:3;
+#else /* Word 0 - Little Endian */
+		u64 nixtxl:3;
+		u64 doneint:1;
+		u64 nixtx_addr:60;
+#endif /* Word 0 - End */
+		u64 res_addr;
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 2 - Big Endian */
+		u64 rvu_pf_func:16;
+		u64 reserved_172_175:4;
+		u64 grp:10;
+		u64 tt:2;
+		u64 tag:32;
+#else /* Word 2 - Little Endian */
+		u64 tag:32;
+		u64 tt:2;
+		u64 grp:10;
+		u64 reserved_172_175:4;
+		u64 rvu_pf_func:16;
+#endif /* Word 2 - End */
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 3 - Big Endian */
+		u64 wq_ptr:61;
+		u64 reserved_194_193:2;
+		u64 qord:1;
+#else /* Word 3 - Little Endian */
+		u64 qord:1;
+		u64 reserved_194_193:2;
+		u64 wq_ptr:61;
+#endif /* Word 3 - End */
+		u64 ei0;
+		u64 ei1;
+		u64 ei2;
+		u64 ei3;
+	} s9x;
 };
 
 /**
@@ -132,7 +379,7 @@ union cpt_inst_s {
  */
 union cpt_res_s {
 	u64 u[2];
-	struct cpt_res_s_s {
+	struct cpt_res_s_8s {
 #if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
 		u64 reserved_17_63:47;
 		u64 doneint:1;
@@ -145,7 +392,22 @@ union cpt_res_s {
 		u64 reserved_17_63:47;
 #endif /* Word 0 - End */
 		u64 reserved_64_127;
-	} s;
+	} s8x;
+
+	struct cpt_res_s_9s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		u64 reserved_17_63:47;
+		u64 doneint:1;
+		u64 uc_compcode:8;
+		u64 compcode:8;
+#else /* Word 0 - Little Endian */
+		u64 compcode:8;
+		u64 uc_compcode:8;
+		u64 doneint:1;
+		u64 reserved_17_63:47;
+#endif /* Word 0 - End */
+		u64 reserved_64_127;
+	} s9x;
 };
 
 /**
@@ -655,4 +917,299 @@ union cptx_vqx_ctl {
 #endif /* Word 0 - End */
 	} s;
 };
+
+/**
+ * Register (NCB) cpt#_pf_q#_gmctl
+ *
+ * CPT Queue Guest Machine Control Register
+ * This register configures queues. This register should be changed only when
+ * quiescent, (see CPT()_VQ()_INPROG[INFLIGHT]).
+ *
+ * Word0
+ *  [23:16](R/W) Low 8 bits of the SMMU stream identifier to use when issuing
+ *  requests. Stream 0x0 corresponds to the PF, and VFs start at 0x1.
+ *  Reset such that VF0/index 0 is 0x1, VF1/index 1 is 0x2, etc.
+ *  Maximum legal value is 64.
+ *  [15:0](R/W) Guest machine identifier. The GMID to send to FPA for all
+ *  buffer free, or to SSO for all submit work operations initiated by this
+ *  queue. Must be nonzero or FPA/SSO will drop requests; see FPA_PF_MAP()
+ *  and SSO_PF_MAP().
+ */
+union cptx_pf_qx_gmctl {
+	u64 u;
+	struct cptx_pf_qx_gmctl_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t reserved_24_63			: 40;
+		uint64_t strm				: 8;
+		uint64_t gmid				: 16;
+#else /* Word 0 - Little Endian */
+		uint64_t gmid				: 16;
+		uint64_t strm				: 8;
+		uint64_t reserved_24_63			: 40;
+#endif /* Word 0 - End */
+	} s;
+};
+
+/**
+ * Error Address/Error Codes
+ *
+ * In the event of a severe error, microcode writes an 8-byte Error Code
+ * value (ECODE) to host memory at the Rptr address specified by the host
+ * system (in the 64-byte request).
+ *
+ * Word0
+ *  [63:56](R) 8-bit completion code
+ *  [55:48](R) Number of the core that reported the severe error
+ *  [47:0] Lower 6 bytes of MInst word2. Used to assist in uniquely
+ *  identifying which specific instruction caused the error. This assumes
+ *  that each instruction has a unique result location (RPTR), at least
+ *  for a given period of time.
+ */
+union error_code {
+	u64 u;
+	struct error_code_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t ccode				: 8;
+		uint64_t coreid				: 8;
+		uint64_t rptr6				: 48;
+#else /* Word 0 - Little Endian */
+		uint64_t rptr6				: 48;
+		uint64_t coreid				: 8;
+		uint64_t ccode				: 8;
+#endif /* Word 0 - End */
+	} s;
+};
+
+/**
+ * Register (RVU_PF_BAR0) cpt#_af_constants1
+ *
+ * CPT AF Constants Register
+ * This register contains implementation-related parameters of CPT.
+ */
+union cptx_af_constants1 {
+	uint64_t u;
+	struct cptx_af_constants1_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t reserved_48_63        : 16;
+		uint64_t ae                    : 16;
+		uint64_t ie                    : 16;
+		uint64_t se                    : 16;
+#else /* Word 0 - Little Endian */
+		uint64_t se                    : 16;
+		uint64_t ie                    : 16;
+		uint64_t ae                    : 16;
+		uint64_t reserved_48_63        : 16;
+#endif /* Word 0 - End */
+	} s;
+};
+
+/**
+ * RVU_PFVF_BAR2 - cpt_lf_misc_int
+ *
+ * This register contain the per-queue miscellaneous interrupts.
+ *
+ */
+union cptx_lf_misc_int {
+	uint64_t u;
+	struct cptx_lf_misc_int_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t reserved_7_63               : 57;
+		uint64_t fault                       :  1;
+		uint64_t hwerr                       :  1;
+		uint64_t reserved_4_4                :  1;
+		uint64_t nwrp                        :  1;
+		uint64_t irde                        :  1;
+		uint64_t nqerr                       :  1;
+		uint64_t reserved_0_0                :  1;
+#else /* Word 0 - Little Endian */
+		uint64_t reserved_0_0                :  1;
+		uint64_t nqerr                       :  1;
+		uint64_t irde                        :  1;
+		uint64_t nwrp                        :  1;
+		uint64_t reserved_4_4                :  1;
+		uint64_t hwerr                       :  1;
+		uint64_t fault                       :  1;
+		uint64_t reserved_7_63               : 57;
+#endif
+	} s;
+};
+
+/**
+ * RVU_PFVF_BAR2 - cpt_lf_misc_int_ena_w1s
+ *
+ * This register sets interrupt enable bits.
+ *
+ */
+union cptx_lf_misc_int_ena_w1s {
+	uint64_t u;
+	struct cptx_lf_misc_int_ena_w1s_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t reserved_7_63               : 57;
+		uint64_t fault                       :  1;
+		uint64_t hwerr                       :  1;
+		uint64_t reserved_4_4                :  1;
+		uint64_t nwrp                        :  1;
+		uint64_t irde                        :  1;
+		uint64_t nqerr                       :  1;
+		uint64_t reserved_0_0                :  1;
+#else /* Word 0 - Little Endian */
+		uint64_t reserved_0_0                :  1;
+		uint64_t nqerr                       :  1;
+		uint64_t irde                        :  1;
+		uint64_t nwrp                        :  1;
+		uint64_t reserved_4_4                :  1;
+		uint64_t hwerr                       :  1;
+		uint64_t fault                       :  1;
+		uint64_t reserved_7_63               : 57;
+#endif
+	} s;
+};
+
+/**
+ * RVU_PFVF_BAR2 - cpt_lf_ctl
+ *
+ * This register configures the queue.
+ *
+ * When the queue is not execution-quiescent (see CPT_LF_INPROG[EENA,INFLIGHT]),
+ * software must only write this register with [ENA]=0.
+ */
+union cptx_lf_ctl {
+	uint64_t u;
+	struct cptx_lf_ctl_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t reserved_8_63               : 56;
+		uint64_t fc_hyst_bits                :  4;
+		uint64_t reserved_3_3                :  1;
+		uint64_t fc_up_crossing              :  1;
+		uint64_t fc_ena                      :  1;
+		uint64_t ena                         :  1;
+#else /* Word 0 - Little Endian */
+		uint64_t ena                         :  1;
+		uint64_t fc_ena                      :  1;
+		uint64_t fc_up_crossing              :  1;
+		uint64_t reserved_3_3                :  1;
+		uint64_t fc_hyst_bits                :  4;
+		uint64_t reserved_8_63               : 56;
+#endif
+	} s;
+};
+
+/**
+ * RVU_PFVF_BAR2 - cpt_lf_inprog
+ *
+ * These registers contain the per-queue instruction in flight registers.
+ *
+ */
+union cptx_lf_inprog {
+	uint64_t u;
+	struct cptx_lf_inprog_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+		uint64_t reserved_48_63              : 16;
+		uint64_t gwb_cnt                     :  8;
+		uint64_t grb_cnt                     :  8;
+		uint64_t grb_partial                 :  1;
+		uint64_t reserved_18_30              : 13;
+		uint64_t grp_drp                     :  1;
+		uint64_t eena                        :  1;
+		uint64_t reserved_9_15               :  7;
+		uint64_t inflight                    :  9;
+#else /* Word 0 - Little Endian */
+		uint64_t inflight                    :	9;
+		uint64_t reserved_9_15               :	7;
+		uint64_t eena                        :	1;
+		uint64_t grp_drp                     :	1;
+		uint64_t reserved_18_30              :	13;
+		uint64_t grb_partial                 :	1;
+		uint64_t grb_cnt                     :	8;
+		uint64_t gwb_cnt                     :	8;
+		uint64_t reserved_48_63		     :	16;
+#endif
+	} s;
+};
+
+/**
+ * RVU_PFVF_BAR2 - cpt_lf_q_base
+ *
+ * CPT initializes these CSR fields to these values on any CPT_LF_Q_BASE write:
+ * _ CPT_LF_Q_INST_PTR[XQ_XOR]=0.
+ * _ CPT_LF_Q_INST_PTR[NQ_PTR]=2.
+ * _ CPT_LF_Q_INST_PTR[DQ_PTR]=2.
+ * _ CPT_LF_Q_GRP_PTR[XQ_XOR]=0.
+ * _ CPT_LF_Q_GRP_PTR[NQ_PTR]=1.
+ * _ CPT_LF_Q_GRP_PTR[DQ_PTR]=1.
+ */
+union cptx_lf_q_base {
+	uint64_t u;
+	struct cptx_lf_q_base_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+	uint64_t reserved_53_63              : 11;
+	uint64_t addr                        : 46;
+	uint64_t reserved_1_6                :  6;
+	uint64_t fault                       :  1;
+#else /* Word 0 - Little Endian */
+	uint64_t fault                       :  1;
+	uint64_t reserved_1_6                :  6;
+	uint64_t addr                        : 46;
+	uint64_t reserved_53_63              : 11;
+#endif
+	} s;
+};
+
+/**
+ * RVU_PFVF_BAR2 - cpt_lf_q_size
+ *
+ * CPT initializes these CSR fields to these values on any CPT_LF_Q_SIZE write:
+ * _ CPT_LF_Q_INST_PTR[XQ_XOR]=0.
+ * _ CPT_LF_Q_INST_PTR[NQ_PTR]=2.
+ * _ CPT_LF_Q_INST_PTR[DQ_PTR]=2.
+ * _ CPT_LF_Q_GRP_PTR[XQ_XOR]=0.
+ * _ CPT_LF_Q_GRP_PTR[NQ_PTR]=1.
+ * _ CPT_LF_Q_GRP_PTR[DQ_PTR]=1.
+ */
+union cptx_lf_q_size {
+	uint64_t u;
+	struct cptx_lf_q_size_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+	uint64_t reserved_15_63              : 49;
+	uint64_t size_div40                  : 15;
+#else /* Word 0 - Little Endian */
+	uint64_t size_div40                  : 15;
+	uint64_t reserved_15_63              : 49;
+#endif
+	} s;
+};
+
+/**
+ * RVU_PF_BAR0 - cpt_af_lf_ctl
+ *
+ * This register configures queues. This register should be written only
+ * when the queue is execution-quiescent (see CPT_LF_INPROG[INFLIGHT]).
+ */
+union cptx_af_lf_ctrl {
+	uint64_t u;
+	struct cptx_af_lf_ctrl_s {
+#if defined(__BIG_ENDIAN_BITFIELD) /* Word 0 - Big Endian */
+	uint64_t reserved_56_63              :	8;
+	uint64_t grp                         :	8;
+	uint64_t reserved_17_47              : 31;
+	uint64_t nixtx_en                    :	1;
+	uint64_t reserved_11_15              :	5;
+	uint64_t cont_err                    :	1;
+	uint64_t pf_func_inst                :	1;
+	uint64_t reserved_1_8                :	8;
+	uint64_t pri                         :	1;
+#else /* Word 0 - Little Endian */
+	uint64_t pri                         :	1;
+	uint64_t reserved_1_8                :	8;
+	uint64_t pf_func_inst                :	1;
+	uint64_t cont_err                    :	1;
+	uint64_t reserved_11_15              :	5;
+	uint64_t nixtx_en                    :	1;
+	uint64_t reserved_17_47              :	31;
+	uint64_t grp                         :	8;
+	uint64_t reserved_56_63              :	8;
+#endif
+	} s;
+};
+
 #endif /*__CPT_HW_TYPES_H*/
diff --git a/drivers/crypto/cavium/cpt/common/cpt_reqmgr.c b/drivers/crypto/cavium/cpt/common/cpt_reqmgr.c
new file mode 100644
index 000000000000..56f9229e36fa
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_reqmgr.c
@@ -0,0 +1,41 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "cpt_common.h"
+#include "cpt_reqmgr.h"
+
+void dump_sg_list(struct pci_dev *pdev, struct cpt_request_info *req)
+{
+	int i;
+
+	dev_info(&pdev->dev, "Gather list size %d\n", req->incnt);
+	for (i = 0; i < req->incnt; i++) {
+		dev_info(&pdev->dev,
+			 "Buffer %d size %d, vptr 0x%p, dmaptr 0x%p\n", i,
+			 req->in[i].size, req->in[i].vptr,
+			 (void *) req->in[i].dma_addr);
+		dev_info(&pdev->dev, "Buffer hexdump (%d bytes)\n",
+			 req->in[i].size);
+		print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+			       req->in[i].vptr, req->in[i].size, false);
+	}
+
+	dev_info(&pdev->dev, "Scatter list size %d\n", req->outcnt);
+	for (i = 0; i < req->outcnt; i++) {
+		dev_info(&pdev->dev,
+			 "Buffer %d size %d, vptr 0x%p, dmaptr 0x%p\n", i,
+			 req->out[i].size, req->out[i].vptr,
+			 (void *) req->out[i].dma_addr);
+		dev_info(&pdev->dev, "Buffer hexdump (%d bytes)\n",
+			 req->out[i].size);
+		print_hex_dump(KERN_INFO, "", DUMP_PREFIX_NONE, 16, 1,
+			       req->out[i].vptr, req->out[i].size, false);
+	}
+}
diff --git a/drivers/crypto/cavium/cpt/common/cpt_reqmgr.h b/drivers/crypto/cavium/cpt/common/cpt_reqmgr.h
new file mode 100644
index 000000000000..33c054b8635d
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_reqmgr.h
@@ -0,0 +1,534 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT_REQUEST_MANAGER_H
+#define __CPT_REQUEST_MANAGER_H
+
+#include <linux/delay.h>
+#include "cpt_hw_types.h"
+
+void send_cpt_cmd(union cpt_inst_s *cptinst, u32 val, void *obj);
+void send_cpt_cmds_in_batch(union cpt_inst_s *cptinst, u32 num, void *obj);
+void send_cpt_cmds_for_speed_test(union cpt_inst_s *cptinst, u32 num,
+				  void *obj);
+void dump_sg_list(struct pci_dev *pdev, struct cpt_request_info *req);
+void fill_cpt_inst(union cpt_inst_s *cptinst, struct cpt_info_buffer *info,
+		   struct cpt_iq_command *iq_cmd);
+int process_ccode(struct pci_dev *pdev, union cpt_res_s *cpt_status,
+		  struct cpt_info_buffer *cpt_info,
+		  struct cpt_request_info *req, u32 *res_code);
+static inline
+struct pending_entry *get_free_pending_entry(struct pending_queue *q, int qlen)
+{
+	struct pending_entry *ent = NULL;
+
+	ent = &q->head[q->rear];
+	if (unlikely(ent->busy)) {
+		ent = NULL;
+		goto no_free_entry;
+	}
+
+	q->rear++;
+	if (unlikely(q->rear == qlen))
+		q->rear = 0;
+
+no_free_entry:
+	return ent;
+}
+
+static inline u32 modulo_inc(u32 index, u32 length, u32 inc)
+{
+	if (WARN_ON(inc > length))
+		inc = length;
+
+	index += inc;
+	if (unlikely(index >= length))
+		index -= length;
+
+	return index;
+}
+
+static inline void free_pentry(struct pending_entry *pentry)
+{
+	pentry->completion_addr = NULL;
+	pentry->post_arg = NULL;
+	pentry->callback = NULL;
+	pentry->callback_arg = NULL;
+	pentry->resume_sender = false;
+	pentry->busy = false;
+}
+
+static inline int setup_sgio_components(struct pci_dev *pdev,
+					struct buf_ptr *list,
+					int buf_count, u8 *buffer)
+{
+	struct sglist_component *sg_ptr = NULL;
+	int ret = 0, i, j;
+	int components;
+
+	if (unlikely(!list)) {
+		dev_err(&pdev->dev, "Input list pointer is NULL\n");
+		return -EFAULT;
+	}
+
+	for (i = 0; i < buf_count; i++) {
+		if (likely(list[i].vptr)) {
+			list[i].dma_addr = dma_map_single(&pdev->dev,
+							  list[i].vptr,
+							  list[i].size,
+							  DMA_BIDIRECTIONAL);
+			if (unlikely(dma_mapping_error(&pdev->dev,
+						       list[i].dma_addr))) {
+				dev_err(&pdev->dev, "Dma mapping failed\n");
+				ret = -EIO;
+				goto sg_cleanup;
+			}
+		}
+	}
+
+	components = buf_count / 4;
+	sg_ptr = (struct sglist_component *)buffer;
+	for (i = 0; i < components; i++) {
+		sg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);
+		sg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);
+		sg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);
+		sg_ptr->u.s.len3 = cpu_to_be16(list[i * 4 + 3].size);
+		sg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);
+		sg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);
+		sg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);
+		sg_ptr->ptr3 = cpu_to_be64(list[i * 4 + 3].dma_addr);
+		sg_ptr++;
+	}
+	components = buf_count % 4;
+
+	switch (components) {
+	case 3:
+		sg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);
+		sg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);
+		/* Fall through */
+	case 2:
+		sg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);
+		sg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);
+		/* Fall through */
+	case 1:
+		sg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);
+		sg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);
+		break;
+	default:
+		break;
+	}
+
+	return ret;
+
+sg_cleanup:
+	for (j = 0; j < i; j++) {
+		if (list[j].dma_addr) {
+			dma_unmap_single(&pdev->dev, list[i].dma_addr,
+					 list[i].size, DMA_BIDIRECTIONAL);
+		}
+
+		list[j].dma_addr = 0;
+	}
+
+	return ret;
+}
+
+static inline int setup_sgio_list(struct pci_dev *pdev,
+				  struct cpt_info_buffer *info,
+				  struct cpt_request_info *req)
+{
+	u16 g_sz_bytes = 0, s_sz_bytes = 0;
+	int ret = 0;
+
+	if (unlikely(req->incnt > MAX_SG_IN_CNT ||
+		     req->outcnt > MAX_SG_OUT_CNT)) {
+		dev_err(&pdev->dev, "Error too many sg components\n");
+		ret = -EINVAL;
+		goto  scatter_gather_clean;
+	}
+
+	/* Setup gather (input) components */
+	g_sz_bytes = ((req->incnt + 3) / 4) * sizeof(struct sglist_component);
+	info->gather_components = kzalloc(g_sz_bytes, GFP_KERNEL);
+	if (unlikely(!info->gather_components)) {
+		dev_err(&pdev->dev, "Memory allocation failed\n");
+		ret = -ENOMEM;
+		goto  scatter_gather_clean;
+	}
+
+	ret = setup_sgio_components(pdev, req->in, req->incnt,
+				    info->gather_components);
+	if (unlikely(ret)) {
+		dev_err(&pdev->dev, "Failed to setup gather list\n");
+		ret = -EFAULT;
+		goto  scatter_gather_clean;
+	}
+
+	/* Setup scatter (output) components */
+	s_sz_bytes = ((req->outcnt + 3) / 4) * sizeof(struct sglist_component);
+	info->scatter_components = kzalloc(s_sz_bytes, GFP_KERNEL);
+	if (unlikely(!info->scatter_components)) {
+		dev_err(&pdev->dev, "Memory allocation failed\n");
+		ret = -ENOMEM;
+		goto  scatter_gather_clean;
+	}
+
+	ret = setup_sgio_components(pdev, req->out, req->outcnt,
+				    info->scatter_components);
+	if (unlikely(ret)) {
+		dev_err(&pdev->dev, "Failed to setup scatter list\n");
+		ret = -EFAULT;
+		goto  scatter_gather_clean;
+	}
+
+	/* Create and initialize DPTR */
+	info->dlen = g_sz_bytes + s_sz_bytes + SG_LIST_HDR_SIZE;
+	info->in_buffer = kzalloc(info->dlen, GFP_KERNEL);
+	if (unlikely(!info->in_buffer)) {
+		dev_err(&pdev->dev, "Memory allocation failed\n");
+		ret = -ENOMEM;
+		goto  scatter_gather_clean;
+	}
+
+	((u16 *)info->in_buffer)[0] = req->outcnt;
+	((u16 *)info->in_buffer)[1] = req->incnt;
+	((u16 *)info->in_buffer)[2] = 0;
+	((u16 *)info->in_buffer)[3] = 0;
+	*(u64 *)info->in_buffer = cpu_to_be64p((u64 *)info->in_buffer);
+
+	memcpy(&info->in_buffer[8], info->gather_components, g_sz_bytes);
+	memcpy(&info->in_buffer[8 + g_sz_bytes], info->scatter_components,
+	       s_sz_bytes);
+	info->dptr_baddr = dma_map_single(&pdev->dev,
+					  (void *)info->in_buffer,
+					  info->dlen,
+					  DMA_BIDIRECTIONAL);
+	if (unlikely(dma_mapping_error(&pdev->dev, info->dptr_baddr))) {
+		dev_err(&pdev->dev, "Mapping dptr failed %d\n", info->dlen);
+		ret = -EIO;
+		goto  scatter_gather_clean;
+	}
+
+	/* Create and initialize RPTR */
+	info->out_buffer = kzalloc(COMPLETION_CODE_SIZE, GFP_KERNEL);
+	if (unlikely(!info->out_buffer)) {
+		dev_err(&pdev->dev, "Memory allocation failed\n");
+		ret = -ENOMEM;
+		goto scatter_gather_clean;
+	}
+
+	*((u64 *) info->out_buffer) = ~((u64) COMPLETION_CODE_INIT);
+	info->rptr_baddr = dma_map_single(&pdev->dev,
+					  (void *)info->out_buffer,
+					  COMPLETION_CODE_SIZE,
+					  DMA_BIDIRECTIONAL);
+	if (unlikely(dma_mapping_error(&pdev->dev, info->rptr_baddr))) {
+		dev_err(&pdev->dev, "Mapping rptr failed %d\n",
+			COMPLETION_CODE_SIZE);
+		ret = -EIO;
+		goto  scatter_gather_clean;
+	}
+
+	return 0;
+
+scatter_gather_clean:
+	return ret;
+}
+
+static inline void do_request_cleanup(struct pci_dev *pdev,
+				      struct cpt_info_buffer *info)
+{
+	struct cpt_request_info *req;
+	int i;
+
+	if (info->dptr_baddr)
+		dma_unmap_single(&pdev->dev, info->dptr_baddr,
+				 info->dlen, DMA_BIDIRECTIONAL);
+
+	if (info->rptr_baddr)
+		dma_unmap_single(&pdev->dev, info->rptr_baddr,
+				 COMPLETION_CODE_SIZE, DMA_BIDIRECTIONAL);
+
+	if (info->comp_baddr)
+		dma_unmap_single(&pdev->dev, info->comp_baddr,
+				 sizeof(union cpt_res_s), DMA_BIDIRECTIONAL);
+
+	if (info->req) {
+		req = info->req;
+		for (i = 0; i < req->outcnt; i++) {
+			if (req->out[i].dma_addr)
+				dma_unmap_single(&pdev->dev,
+						 req->out[i].dma_addr,
+						 req->out[i].size,
+						 DMA_BIDIRECTIONAL);
+		}
+
+		for (i = 0; i < req->incnt; i++) {
+			if (req->in[i].dma_addr)
+				dma_unmap_single(&pdev->dev,
+						 req->in[i].dma_addr,
+						 req->in[i].size,
+						 DMA_BIDIRECTIONAL);
+		}
+	}
+
+	if (info->scatter_components)
+		kzfree(info->scatter_components);
+
+	if (info->gather_components)
+		kzfree(info->gather_components);
+
+	if (info->out_buffer)
+		kzfree(info->out_buffer);
+
+	if (info->in_buffer)
+		kzfree(info->in_buffer);
+
+	if (info->completion_addr)
+		kzfree((void *)info->completion_addr);
+
+	kzfree(info);
+}
+
+static inline void process_pending_queue(struct pci_dev *pdev,
+					 struct pending_queue *pqueue)
+{
+	struct pending_entry *resume_pentry = NULL;
+	struct cpt_info_buffer *cpt_info = NULL;
+	void (*callback)(int, void *, void *);
+	struct pending_entry *pentry = NULL;
+	struct cpt_request_info *req = NULL;
+	union cpt_res_s *cpt_status = NULL;
+	void *callback_arg;
+	u32 res_code, resume_index;
+
+	while (1) {
+		spin_lock_bh(&pqueue->lock);
+		pentry = &pqueue->head[pqueue->front];
+
+		if (WARN_ON(!pentry)) {
+			spin_unlock_bh(&pqueue->lock);
+			break;
+		}
+
+		res_code = -EINVAL;
+		if (unlikely(!pentry->busy)) {
+			spin_unlock_bh(&pqueue->lock);
+			break;
+		}
+
+		if (unlikely(!pentry->callback) ||
+			unlikely(!pentry->callback_arg)) {
+			dev_err(&pdev->dev, "Callback or callback arg NULL\n");
+			goto process_pentry;
+		}
+
+		cpt_info = (struct cpt_info_buffer *) pentry->post_arg;
+		if (unlikely(!cpt_info)) {
+			dev_err(&pdev->dev, "Pending entry post arg NULL\n");
+			goto process_pentry;
+		}
+
+		req = cpt_info->req;
+		if (unlikely(!req)) {
+			dev_err(&pdev->dev, "Request NULL\n");
+			goto process_pentry;
+		}
+
+		cpt_status = (union cpt_res_s *) pentry->completion_addr;
+		if (unlikely(!cpt_status)) {
+			dev_err(&pdev->dev, "Completion address NULL\n");
+			goto process_pentry;
+		}
+
+		/* Process completion code */
+		if (process_ccode(pdev, cpt_status, cpt_info, req,
+				  &res_code)) {
+			spin_unlock_bh(&pqueue->lock);
+			return;
+		}
+
+process_pentry:
+		/*
+		 * Check if we should inform sending side to resume
+		 * We do it CPT_IQ_RESUME_MARGIN elements in advance before
+		 * pending queue becomes empty
+		 */
+		resume_index = modulo_inc(pqueue->front, pqueue->qlen,
+					  CPT_IQ_RESUME_MARGIN);
+		resume_pentry = &pqueue->head[resume_index];
+		if (resume_pentry &&
+		    resume_pentry->resume_sender) {
+			resume_pentry->resume_sender = false;
+			callback = resume_pentry->callback;
+			callback_arg = resume_pentry->callback_arg;
+
+			if (callback && callback_arg) {
+				spin_unlock_bh(&pqueue->lock);
+				/*
+				 * EINPROGRESS is an indication for sending
+				 * side that it can resume sending requests
+				 */
+				callback(-EINPROGRESS, callback_arg, req);
+				spin_lock_bh(&pqueue->lock);
+			}
+		}
+
+		callback = pentry->callback;
+		callback_arg = pentry->callback_arg;
+		free_pentry(pentry);
+
+		pqueue->pending_count--;
+		pqueue->front = modulo_inc(pqueue->front, pqueue->qlen, 1);
+		spin_unlock_bh(&pqueue->lock);
+
+		/*
+		 * Call callback after current pending entry has been been
+		 * processed we don't do it if the callback pointer or
+		 * argument pointer is invalid
+		 */
+		if (callback && callback_arg)
+			callback(res_code, callback_arg, req);
+
+		if (cpt_info)
+			do_request_cleanup(pdev, cpt_info);
+	}
+}
+
+static inline int process_request(struct pci_dev *pdev,
+				  struct cpt_request_info *req,
+				  struct pending_queue *pqueue,
+				  void *obj)
+{
+	struct cptvf_request *cpt_req = &req->req;
+	struct cpt_info_buffer *info = NULL;
+	struct pending_entry *pentry = NULL;
+	union ctrl_info *ctrl = &req->ctrl;
+	union cpt_res_s *result = NULL;
+	struct cpt_iq_command iq_cmd;
+	union cpt_inst_s cptinst;
+	int retry, ret = 0;
+	u8 resume_sender;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (unlikely(!info)) {
+		dev_err(&pdev->dev, "Memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	ret = setup_sgio_list(pdev, info, req);
+	if (unlikely(ret)) {
+		dev_err(&pdev->dev, "Setting up SG list failed");
+		goto request_cleanup;
+	}
+	cpt_req->dlen = info->dlen;
+
+	/*
+	 * Get buffer for union cpt_res_s response
+	 * structure and its physical address
+	 */
+	info->completion_addr = kzalloc(sizeof(union cpt_res_s), GFP_KERNEL);
+	if (unlikely(!info->completion_addr)) {
+		dev_err(&pdev->dev, "memory allocation failed\n");
+		goto request_cleanup;
+	}
+
+	result = (union cpt_res_s *) info->completion_addr;
+	result->s9x.compcode = COMPLETION_CODE_INIT;
+	info->comp_baddr = dma_map_single(&pdev->dev,
+					  (void *) info->completion_addr,
+					  sizeof(union cpt_res_s),
+					  DMA_BIDIRECTIONAL);
+	if (unlikely(dma_mapping_error(&pdev->dev, info->comp_baddr))) {
+		dev_err(&pdev->dev, "Dma mapping failed\n");
+		ret = -EFAULT;
+		goto request_cleanup;
+	}
+
+	spin_lock_bh(&pqueue->lock);
+	pentry = get_free_pending_entry(pqueue, pqueue->qlen);
+	retry = CPT_PENTRY_TIMEOUT / CPT_PENTRY_STEP;
+	while (unlikely(!pentry) && retry--) {
+		spin_unlock_bh(&pqueue->lock);
+		udelay(CPT_PENTRY_STEP);
+
+		spin_lock_bh(&pqueue->lock);
+		pentry = get_free_pending_entry(pqueue, pqueue->qlen);
+	}
+
+	if (unlikely(!pentry)) {
+		ret = -ENOSPC;
+		spin_unlock_bh(&pqueue->lock);
+		goto request_cleanup;
+	}
+
+	/*
+	 * Check if we are close to filling in entire pending queue,
+	 * if so then tell the sender to stop by returning -EBUSY
+	 */
+	if (pqueue->pending_count > (pqueue->qlen - CPT_IQ_STOP_MARGIN))
+		pentry->resume_sender = true;
+	else
+		pentry->resume_sender = false;
+	resume_sender = pentry->resume_sender;
+	pqueue->pending_count++;
+
+	pentry->completion_addr = info->completion_addr;
+	pentry->post_arg = (void *) info;
+	pentry->callback = req->callback;
+	pentry->callback_arg = req->callback_arg;
+	pentry->busy = true;
+	info->pentry = pentry;
+	info->time_in = jiffies;
+	info->req = req;
+
+	/* Fill in the command */
+	iq_cmd.cmd.u64 = 0;
+	iq_cmd.cmd.s.opcode = cpu_to_be16(cpt_req->opcode.flags);
+	iq_cmd.cmd.s.param1 = cpu_to_be16(cpt_req->param1);
+	iq_cmd.cmd.s.param2 = cpu_to_be16(cpt_req->param2);
+	iq_cmd.cmd.s.dlen   = cpu_to_be16(cpt_req->dlen);
+
+	/* 64-bit swap for microcode data reads, not needed for addresses*/
+	iq_cmd.cmd.u64 = cpu_to_be64(iq_cmd.cmd.u64);
+	iq_cmd.dptr = info->dptr_baddr;
+	iq_cmd.rptr = info->rptr_baddr;
+	iq_cmd.cptr.u64 = 0;
+	iq_cmd.cptr.s.grp = ctrl->s.grp;
+
+	/* Fill in the CPT_INST_S type command for HW interpretation */
+	fill_cpt_inst(&cptinst, info, &iq_cmd);
+
+	/* Print debug info if enabled */
+	if (cpt_is_dbg_level_en(CPT_DBG_ENC_DEC_REQS)) {
+		dump_sg_list(pdev, req);
+		dev_info(&pdev->dev, "Cpt_inst_s hexdump (%d bytes)\n",
+			 CPT_INST_SIZE);
+		print_hex_dump(KERN_INFO, "", 0, 16, 1, &cptinst,
+			       CPT_INST_SIZE, false);
+		dev_info(&pdev->dev, "Dptr hexdump (%d bytes)\n",
+			 cpt_req->dlen);
+		print_hex_dump(KERN_INFO, "", 0, 16, 1, info->in_buffer,
+			       cpt_req->dlen, false);
+	}
+
+	/* Send CPT command */
+	send_cpt_cmd(&cptinst, 1, obj);
+	spin_unlock_bh(&pqueue->lock);
+
+	ret = resume_sender ? -EBUSY : -EINPROGRESS;
+	return ret;
+
+request_cleanup:
+	do_request_cleanup(pdev, info);
+	return ret;
+}
+
+#endif /* __CPT_REQUEST_MANAGER_H */
diff --git a/drivers/crypto/cavium/cpt/common/cpt_ucode.c b/drivers/crypto/cavium/cpt/common/cpt_ucode.c
new file mode 100644
index 000000000000..2f19ea00ff8e
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_ucode.c
@@ -0,0 +1,1678 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include <linux/firmware.h>
+#include "cpt_ucode.h"
+
+static int is_eng_type(int val, int eng_type)
+{
+	return val & (1 << eng_type);
+}
+
+static int dev_supports_eng_type(struct engine_groups *eng_grps, int eng_type)
+{
+	return is_eng_type(eng_grps->eng_types_supported, eng_type);
+}
+
+static int is_2nd_ucode_used(struct engine_group_info *eng_grp)
+{
+	if (eng_grp->ucode[1].type)
+		return true;
+	else
+		return false;
+}
+
+static void set_ucode_filename(struct microcode *ucode, const char *filename)
+{
+	strlcpy(ucode->filename, filename, NAME_LENGTH);
+}
+
+static char *get_eng_type_str(int eng_type)
+{
+	char *str = "unknown";
+
+	switch (eng_type) {
+	case SE_TYPES:
+		str = "SE";
+	break;
+
+	case IE_TYPES:
+		str = "IE";
+	break;
+
+	case AE_TYPES:
+		str = "AE";
+	break;
+	}
+
+	return str;
+}
+
+static char *get_ucode_type_str(int ucode_type)
+{
+	char *str = "unknown";
+
+	switch (ucode_type) {
+	case (1 << SE_TYPES):
+		str = "SE";
+	break;
+
+	case (1 << IE_TYPES):
+		str = "IE";
+	break;
+
+	case (1 << AE_TYPES):
+		str = "AE";
+	break;
+
+	case (1 << SE_TYPES | 1 << IE_TYPES):
+		str = "SE+IPSEC";
+	break;
+	}
+
+	return str;
+}
+
+static void swap_engines(struct engines *engsl, struct engines *engsr)
+{
+	struct engines engs;
+
+	engs = *engsl;
+	*engsl = *engsr;
+	*engsr = engs;
+}
+
+static void swap_ucodes(struct microcode *ucodel, struct microcode *ucoder)
+{
+	struct microcode ucode;
+
+	ucode = *ucodel;
+	*ucodel = *ucoder;
+	*ucoder = ucode;
+}
+
+static int get_ucode_type(char *ucode_ver_str, int *ucode_type)
+{
+	char tmp_ver_str[CPT_UCODE_VERSION_SZ];
+	int i, val = 0;
+
+	strlcpy(tmp_ver_str, ucode_ver_str, CPT_UCODE_VERSION_SZ);
+	for (i = 0; i < strlen(tmp_ver_str); i++)
+		tmp_ver_str[i] = tolower(tmp_ver_str[i]);
+
+	if (strnstr(tmp_ver_str, "se-", CPT_UCODE_VERSION_SZ))
+		val |= 1 << SE_TYPES;
+	if (strnstr(tmp_ver_str, "ipsec", CPT_UCODE_VERSION_SZ))
+		val |= 1 << IE_TYPES;
+	if (strnstr(tmp_ver_str, "ae", CPT_UCODE_VERSION_SZ))
+		val |= 1 << AE_TYPES;
+
+	*ucode_type = val;
+
+	if (!val)
+		return -EINVAL;
+	if (is_eng_type(val, AE_TYPES) && (is_eng_type(val, SE_TYPES) ||
+	    is_eng_type(val, IE_TYPES)))
+		return -EINVAL;
+	return 0;
+}
+
+static int is_mem_zero(const char *ptr, int size)
+{
+	int i;
+
+	for (i = 0; i < size; i++)
+		if (ptr[i])
+			return 0;
+	return 1;
+}
+
+static int process_tar_file(struct device *dev,
+			    struct tar_arch_info_t *tar_arch, char *filename,
+			    const u8 *data, int size)
+{
+	struct tar_ucode_info_t *tar_ucode_info;
+	struct microcode_hdr *ucode_hdr;
+	int ucode_type, ucode_size;
+
+	/* If size is less than microcode header size then don't report
+	 * an error because it might not be microcode file, just process
+	 * next file from archive
+	 */
+	if (size < sizeof(struct microcode_hdr))
+		return 0;
+
+	ucode_hdr = (struct microcode_hdr *) data;
+	/* If microcode version can't be found don't report an error
+	 * because it might not be microcode file, just process next file
+	 */
+	if (get_ucode_type(ucode_hdr->version, &ucode_type))
+		return 0;
+
+	ucode_size = ntohl(ucode_hdr->code_length) * 2;
+	if (size < ROUNDUP16(ucode_size) + sizeof(struct microcode_hdr)
+	    + CPT_UCODE_SIGN_LEN) {
+		dev_err(dev, "Ucode %s invalid size", filename);
+		return -EINVAL;
+	}
+
+	tar_ucode_info = kzalloc(sizeof(struct tar_ucode_info_t), GFP_KERNEL);
+	if (!tar_ucode_info)
+		return -ENOMEM;
+
+	tar_ucode_info->ucode_ptr = data;
+	set_ucode_filename(&tar_ucode_info->ucode, filename);
+	memcpy(tar_ucode_info->ucode.version, (u8 *) ucode_hdr->version,
+	       CPT_UCODE_VERSION_SZ);
+	tar_ucode_info->ucode.type = ucode_type;
+	tar_ucode_info->ucode.size = ucode_size;
+	list_add_tail(&tar_ucode_info->list, &tar_arch->ucodes);
+
+	return 0;
+}
+
+static void release_tar_archive(struct tar_arch_info_t *tar_arch)
+{
+	struct tar_ucode_info_t *curr, *temp;
+
+	if (!tar_arch)
+		return;
+
+	list_for_each_entry_safe(curr, temp, &tar_arch->ucodes, list) {
+		list_del(&curr->list);
+		kfree(curr);
+	}
+
+	if (tar_arch->fw)
+		release_firmware(tar_arch->fw);
+	kfree(tar_arch);
+}
+
+static struct tar_ucode_info_t *get_uc_from_tar_archive(
+					struct tar_arch_info_t *tar_arch,
+					int ucode_type)
+{
+	struct tar_ucode_info_t *curr;
+
+	list_for_each_entry(curr, &tar_arch->ucodes, list)
+		if (ucode_type == IE_TYPES) {
+			if (is_eng_type(curr->ucode.type, IE_TYPES) &&
+				!is_eng_type(curr->ucode.type, SE_TYPES))
+				return curr;
+		} else if (is_eng_type(curr->ucode.type, ucode_type))
+			return curr;
+	return NULL;
+}
+
+static void print_tar_dbg_info(struct device *dev,
+			       struct tar_arch_info_t *tar_arch,
+			       char *tar_filename)
+{
+	struct tar_ucode_info_t *curr;
+
+	dev_info(dev, "Tar archive filename %s", tar_filename);
+	dev_info(dev, "Tar archive pointer %p, size %ld", tar_arch->fw->data,
+		 tar_arch->fw->size);
+	list_for_each_entry(curr, &tar_arch->ucodes, list) {
+		dev_info(dev, "Ucode filename %s", curr->ucode.filename);
+		dev_info(dev, "Ucode version %s", &curr->ucode.version[1]);
+		dev_info(dev, "Ucode type (%d) %s", curr->ucode.type,
+			 get_ucode_type_str(curr->ucode.type));
+		dev_info(dev, "Ucode size %d", curr->ucode.size);
+		dev_info(dev, "Ucode ptr %p", curr->ucode_ptr);
+		dev_info(dev, "\n");
+	}
+	dev_info(dev, "\n");
+}
+
+static struct tar_arch_info_t *load_tar_archive(struct device *dev,
+						char *tar_filename)
+{
+	struct tar_arch_info_t *tar_arch = NULL;
+	struct tar_blk_t *tar_blk;
+	unsigned int cur_size;
+	size_t tar_offs = 0;
+	size_t tar_size;
+	int ret;
+
+	tar_arch = kzalloc(sizeof(struct tar_arch_info_t), GFP_KERNEL);
+	if (!tar_arch)
+		goto err;
+
+	INIT_LIST_HEAD(&tar_arch->ucodes);
+
+	/* Load tar archive */
+	ret = request_firmware(&tar_arch->fw, tar_filename, dev);
+	if (ret)
+		goto err;
+
+	if (tar_arch->fw->size < TAR_BLOCK_LEN) {
+		dev_err(dev, "Invalid tar archive %s ", tar_filename);
+		goto err;
+	}
+
+	tar_size = tar_arch->fw->size;
+	tar_blk = (struct tar_blk_t *) tar_arch->fw->data;
+	if (strncmp(tar_blk->hdr.magic, TAR_MAGIC, TAR_MAGIC_LEN - 1)) {
+		dev_err(dev, "Unsupported format of tar archive %s",
+			tar_filename);
+		goto err;
+	}
+
+	while (1) {
+		/* Read current file size */
+		ret = kstrtouint(tar_blk->hdr.size, 8, &cur_size);
+		if (ret)
+			goto err;
+
+		if (tar_offs + cur_size > tar_size ||
+		    tar_offs + 2*TAR_BLOCK_LEN > tar_size) {
+			dev_err(dev, "Invalid tar archive %s ", tar_filename);
+			goto err;
+		}
+
+		tar_offs += TAR_BLOCK_LEN;
+		if (tar_blk->hdr.typeflag == REGTYPE ||
+		    tar_blk->hdr.typeflag == AREGTYPE) {
+			ret = process_tar_file(dev, tar_arch,
+					       tar_blk->hdr.name,
+					       &tar_arch->fw->data[tar_offs],
+					       cur_size);
+			if (ret)
+				goto err;
+		}
+
+		tar_offs += (cur_size/TAR_BLOCK_LEN) * TAR_BLOCK_LEN;
+		if (cur_size % TAR_BLOCK_LEN)
+			tar_offs += TAR_BLOCK_LEN;
+
+		/* Check for the end of the archive */
+		if (tar_offs + 2*TAR_BLOCK_LEN > tar_size) {
+			dev_err(dev, "Invalid tar archive %s ", tar_filename);
+			goto err;
+		}
+
+		if (is_mem_zero(&tar_arch->fw->data[tar_offs],
+		    2*TAR_BLOCK_LEN))
+			break;
+
+		/* Read next block from tar archive */
+		tar_blk = (struct tar_blk_t *) &tar_arch->fw->data[tar_offs];
+	}
+
+	if (cpt_is_dbg_level_en(CPT_DBG_ENGINE_GRPS))
+		print_tar_dbg_info(dev, tar_arch, tar_filename);
+	return tar_arch;
+err:
+	release_tar_archive(tar_arch);
+	return NULL;
+}
+
+static struct engines_reserved *find_engines_by_type(
+					struct engine_group_info *eng_grp,
+					int eng_type)
+{
+	int i;
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		if (!eng_grp->engs[i].type)
+			continue;
+
+		if (eng_grp->engs[i].type == eng_type)
+			return &eng_grp->engs[i];
+	}
+
+	return NULL;
+}
+
+int cpt_uc_supports_eng_type(struct microcode *ucode, int eng_type)
+{
+	return is_eng_type(ucode->type, eng_type);
+}
+
+int cpt_eng_grp_has_eng_type(struct engine_group_info *eng_grp, int eng_type)
+{
+	struct engines_reserved *engs;
+
+	engs = find_engines_by_type(eng_grp, eng_type);
+
+	return (engs != NULL ? 1 : 0);
+}
+
+static void print_ucode_info(struct engine_group_info *eng_grp,
+			     char *buf, int size)
+{
+	int len, ucode_type;
+
+	if (eng_grp->mirror.is_ena) {
+		ucode_type =
+			eng_grp->g->grp[eng_grp->mirror.idx].ucode[0].type;
+		scnprintf(buf, size, "%s (shared with engine_group%d)",
+			  get_ucode_type_str(ucode_type), eng_grp->mirror.idx);
+	} else {
+		scnprintf(buf, size, "%s",
+			  get_ucode_type_str(eng_grp->ucode[0].type));
+	}
+
+	if (is_2nd_ucode_used(eng_grp)) {
+		len = strlen(buf);
+		scnprintf(buf + len, size - len, ", %s (used by IE engines)",
+			  get_ucode_type_str(eng_grp->ucode[1].type));
+	}
+}
+
+static void print_engs_info(struct engine_group_info *eng_grp,
+			    char *buf, int size, int idx)
+{
+	struct engines_reserved *mirrored_engs = NULL;
+	struct engines_reserved *engs;
+	int len, i;
+
+	buf[0] = '\0';
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		engs = &eng_grp->engs[i];
+		if (!engs->type)
+			continue;
+		if (idx != -1 &&
+		    idx != i)
+			continue;
+
+		if (eng_grp->mirror.is_ena)
+			mirrored_engs = find_engines_by_type(
+					&eng_grp->g->grp[eng_grp->mirror.idx],
+					engs->type);
+		if (i > 0 &&
+		    idx == -1) {
+			len = strlen(buf);
+			scnprintf(buf+len, size-len, ", ");
+		}
+
+		len = strlen(buf);
+		scnprintf(buf+len, size-len, "%d %s ", mirrored_engs ?
+			  engs->count + mirrored_engs->count : engs->count,
+			  get_eng_type_str(engs->type));
+		if (mirrored_engs) {
+			len = strlen(buf);
+			scnprintf(buf+len, size-len,
+				  "(%d shared with engine_group%d) ",
+				  engs->count <= 0 ? engs->count +
+				  mirrored_engs->count : mirrored_engs->count,
+				  eng_grp->mirror.idx);
+		}
+	}
+}
+
+static void print_ucode_dbg_info(struct device *dev, struct microcode *ucode)
+{
+	dev_info(dev, "\n");
+	dev_info(dev, "Ucode info");
+	dev_info(dev, "Ucode version %s", &ucode->version[1]);
+	dev_info(dev, "Ucode type %s", get_ucode_type_str(ucode->type));
+	dev_info(dev, "Ucode size %d", ucode->size);
+	dev_info(dev, "Ucode virt address %16.16llx", (u64)ucode->align_va);
+	dev_info(dev, "Ucode phys address %16.16llx", ucode->align_dma);
+	dev_info(dev, "\n");
+}
+
+static void print_dbg_info(struct device *dev,
+			   struct engine_groups *eng_grps)
+{
+	struct engine_group_info *mirrored_grp;
+	struct engine_group_info *grp;
+	struct engines_reserved *engs;
+	char engs_info[2*NAME_LENGTH];
+	char engs_mask[NAME_LENGTH];
+	u32 mask[4];
+	int i, j;
+
+	dev_info(dev, "\n");
+	dev_info(dev, "Engine groups global info");
+	dev_info(dev, "max SE %d, max IE %d, max AE %d",
+		 eng_grps->avail.max_se_cnt, eng_grps->avail.max_ie_cnt,
+		 eng_grps->avail.max_ae_cnt);
+	dev_info(dev, "free SE %d", eng_grps->avail.se_cnt);
+	dev_info(dev, "free IE %d", eng_grps->avail.ie_cnt);
+	dev_info(dev, "free AE %d", eng_grps->avail.ae_cnt);
+	dev_info(dev, "\n");
+
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+		grp = &eng_grps->grp[i];
+		dev_info(dev, "engine_group%d, state %s", i, grp->is_enabled ?
+			 "enabled" : "disabled");
+		if (grp->is_enabled) {
+			mirrored_grp = &eng_grps->grp[grp->mirror.idx];
+			dev_info(dev, "Ucode0 filename %s, version %s",
+				 grp->mirror.is_ena ?
+				 mirrored_grp->ucode[0].filename :
+				 grp->ucode[0].filename,
+				 grp->mirror.is_ena ?
+				 &mirrored_grp->ucode[0].version[1] :
+				 &grp->ucode[0].version[1]);
+			if (is_2nd_ucode_used(grp))
+				dev_info(dev,
+					 "Ucode1 filename %s, version %s",
+					 grp->ucode[1].filename,
+					 &grp->ucode[1].version[1]);
+			else
+				dev_info(dev, "Ucode1 not used");
+		}
+
+		for (j = 0; j < MAX_ENGS_PER_GRP; j++) {
+			engs = &grp->engs[j];
+			if (engs->type) {
+				print_engs_info(grp, engs_info, 2*NAME_LENGTH,
+						j);
+				dev_info(dev, "Slot%d: %s", j, engs_info);
+				if (WARN_ON_ONCE(eng_grps->engs_num > 4 * 32))
+					return;
+				bitmap_to_arr32(mask, engs->bmap,
+						eng_grps->engs_num);
+				dev_info(dev, "Mask:  %8.8x %8.8x %8.8x %8.8x",
+					 mask[3], mask[2], mask[1], mask[0]);
+			} else
+				dev_info(dev, "Slot%d not used", j);
+		}
+		if (grp->is_enabled) {
+			cpt_print_engines_mask(grp, eng_grps->obj, engs_mask,
+					       NAME_LENGTH);
+			dev_info(dev, "Cmask: %s", engs_mask);
+		}
+		dev_info(dev, "\n");
+	}
+}
+
+static int update_engines_avail_count(struct device *dev,
+				      struct engines_available *avail,
+				      struct engines_reserved *engs, int val)
+{
+	switch (engs->type) {
+	case SE_TYPES:
+		avail->se_cnt += val;
+	break;
+
+	case IE_TYPES:
+		avail->ie_cnt += val;
+	break;
+
+	case AE_TYPES:
+		avail->ae_cnt += val;
+	break;
+
+	default:
+		dev_err(dev, "Invalid engine type %d\n", engs->type);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int update_engines_offset(struct device *dev,
+				 struct engines_available *avail,
+				 struct engines_reserved *engs)
+{
+	switch (engs->type) {
+	case SE_TYPES:
+		engs->offset = 0;
+	break;
+
+	case IE_TYPES:
+		engs->offset = avail->max_se_cnt;
+	break;
+
+	case AE_TYPES:
+		engs->offset = avail->max_se_cnt + avail->max_ie_cnt;
+	break;
+
+	default:
+		dev_err(dev, "Invalid engine type %d\n", engs->type);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int release_engines(struct device *dev, struct engine_group_info *grp)
+{
+	int i, ret = 0;
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		if (!grp->engs[i].type)
+			continue;
+
+		if (grp->engs[i].count > 0) {
+			ret = update_engines_avail_count(dev, &grp->g->avail,
+							 &grp->engs[i],
+							 grp->engs[i].count);
+			if (ret)
+				return ret;
+		}
+
+		grp->engs[i].type = 0;
+		grp->engs[i].count = 0;
+		grp->engs[i].offset = 0;
+		grp->engs[i].ucode = NULL;
+		bitmap_zero(grp->engs[i].bmap, grp->g->engs_num);
+	}
+
+	return 0;
+}
+
+static int do_reserve_engines(struct device *dev,
+			      struct engine_group_info *grp,
+			      struct engines *req_engs)
+{
+	struct engines_reserved *engs = NULL;
+	int i, ret = 0;
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		if (!grp->engs[i].type) {
+			engs = &grp->engs[i];
+			break;
+		}
+	}
+
+	if (!engs)
+		return -ENOMEM;
+
+	engs->type = req_engs->type;
+	engs->count = req_engs->count;
+
+	ret = update_engines_offset(dev, &grp->g->avail, engs);
+	if (ret)
+		return ret;
+
+	if (engs->count > 0) {
+		ret = update_engines_avail_count(dev, &grp->g->avail, engs,
+						 -engs->count);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int check_engines_availability(struct device *dev,
+				      struct engine_group_info *grp,
+				      struct engines *req_eng)
+{
+	int avail_cnt = 0;
+
+	switch (req_eng->type) {
+	case SE_TYPES:
+		avail_cnt = grp->g->avail.se_cnt;
+	break;
+
+	case IE_TYPES:
+		avail_cnt = grp->g->avail.ie_cnt;
+	break;
+
+	case AE_TYPES:
+		avail_cnt = grp->g->avail.ae_cnt;
+	break;
+
+	default:
+		dev_err(dev, "Invalid engine type %d\n", req_eng->type);
+		return -EINVAL;
+	}
+
+	if (avail_cnt < req_eng->count) {
+		dev_err(dev,
+			"Error available %s engines %d < than requested %d",
+			get_eng_type_str(req_eng->type),
+			avail_cnt, req_eng->count);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static int reserve_engines(struct device *dev, struct engine_group_info *grp,
+			   struct engines *req_engs, int req_cnt)
+{
+	int i, ret = 0;
+
+	/* Validate if a number of requested engines is available */
+	for (i = 0; i < req_cnt; i++) {
+		ret = check_engines_availability(dev, grp, &req_engs[i]);
+		if (ret)
+			goto err;
+	}
+
+	/* Reserve requested engines for this engine group */
+	for (i = 0; i < req_cnt; i++) {
+		ret = do_reserve_engines(dev, grp, &req_engs[i]);
+		if (ret)
+			goto err;
+	}
+
+
+err:
+	return ret;
+}
+
+static ssize_t eng_grp_info_show(struct device *dev,
+				 struct device_attribute *attr,
+				 char *buf)
+{
+	struct engine_group_info *eng_grp;
+	char engs_info[2*NAME_LENGTH];
+	char ucode_info[NAME_LENGTH];
+	char engs_mask[NAME_LENGTH];
+	int ret = 0;
+
+	eng_grp = container_of(attr, struct engine_group_info, info_attr);
+	mutex_lock(&eng_grp->g->lock);
+
+	print_engs_info(eng_grp, engs_info, 2*NAME_LENGTH, -1);
+	print_ucode_info(eng_grp, ucode_info, NAME_LENGTH);
+	cpt_print_engines_mask(eng_grp, eng_grp->g, engs_mask, NAME_LENGTH);
+	ret = scnprintf(buf, PAGE_SIZE,
+			"Microcode type: %s\nEngines: %s\nEngines mask: %s\n",
+			ucode_info, engs_info, engs_mask);
+
+	mutex_unlock(&eng_grp->g->lock);
+	return ret;
+
+	return 0;
+}
+
+static int create_sysfs_eng_grps_info(struct device *dev,
+				      struct engine_group_info *eng_grp)
+{
+	int ret = 0;
+
+	eng_grp->info_attr.show = eng_grp_info_show;
+	eng_grp->info_attr.store = NULL;
+	eng_grp->info_attr.attr.name = eng_grp->sysfs_info_name;
+	eng_grp->info_attr.attr.mode = 0440;
+	sysfs_attr_init(&eng_grp->info_attr.attr);
+	ret = device_create_file(dev, &eng_grp->info_attr);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void ucode_unload(struct device *dev, struct microcode *ucode)
+{
+	if (ucode->va) {
+		dma_free_coherent(dev, ucode->size + CPT_UCODE_ALIGNMENT,
+				  ucode->va, ucode->dma);
+		ucode->va = NULL;
+		ucode->align_va = NULL;
+		ucode->dma = 0;
+		ucode->align_dma = 0;
+		ucode->size = 0;
+	}
+
+	ucode->version[0] = '\0';
+	ucode->version[1] = '\0';
+	set_ucode_filename(ucode, "");
+	ucode->type = 0;
+}
+
+static int copy_ucode_to_dma_mem(struct device *dev, struct microcode *ucode,
+				 const u8 *ucode_data)
+{
+	int i;
+
+	/*  Allocate DMAable space */
+	ucode->va = dma_zalloc_coherent(dev, ucode->size + CPT_UCODE_ALIGNMENT,
+					&ucode->dma, GFP_KERNEL);
+	if (!ucode->va) {
+		dev_err(dev, "Unable to allocate space for microcode");
+		return -ENOMEM;
+	}
+	ucode->align_va = PTR_ALIGN(ucode->va, CPT_UCODE_ALIGNMENT);
+	ucode->align_dma = PTR_ALIGN(ucode->dma, CPT_UCODE_ALIGNMENT);
+
+	memcpy((void *) ucode->align_va, (void *) ucode_data +
+	       sizeof(struct microcode_hdr), ucode->size);
+
+	/* Byte swap 64-bit */
+	for (i = 0; i < (ucode->size / 8); i++)
+		((u64 *)ucode->align_va)[i] =
+				cpu_to_be64(((u64 *)ucode->align_va)[i]);
+	/*  Ucode needs 16-bit swap */
+	for (i = 0; i < (ucode->size / 2); i++)
+		((u16 *)ucode->align_va)[i] =
+				cpu_to_be16(((u16 *)ucode->align_va)[i]);
+	return 0;
+}
+
+static int ucode_load(struct device *dev, struct microcode *ucode,
+		      const char *ucode_filename)
+{
+	struct microcode_hdr *ucode_hdr;
+	const struct firmware *fw;
+	int ret = 0;
+
+	set_ucode_filename(ucode, ucode_filename);
+	ret = request_firmware(&fw, ucode->filename, dev);
+	if (ret)
+		return ret;
+
+	ucode_hdr = (struct microcode_hdr *) fw->data;
+	memcpy(ucode->version, ucode_hdr->version, CPT_UCODE_VERSION_SZ);
+	ucode->size = ntohl(ucode_hdr->code_length) * 2;
+	if (!ucode->size) {
+		ret = -EINVAL;
+		goto err;
+	}
+
+	ret = get_ucode_type(ucode_hdr->version, &ucode->type);
+	if (ret) {
+		dev_err(dev, "Microcode %s unknown type 0x%x", ucode->filename,
+			ucode->type);
+		goto err;
+	}
+
+	ret = copy_ucode_to_dma_mem(dev, ucode, fw->data);
+	if (ret)
+		goto err;
+
+	if (cpt_is_dbg_level_en(CPT_DBG_ENGINE_GRPS))
+		print_ucode_dbg_info(dev, ucode);
+err:
+	release_firmware(fw);
+	return ret;
+}
+
+static int enable_eng_grp(struct engine_group_info *eng_grp,
+			  void *obj)
+{
+	int ret = 0;
+
+	/* Point microcode to each core of the group */
+	ret = cpt_set_ucode_base(eng_grp, obj);
+	if (ret)
+		goto err;
+
+	/* Attach the cores to the group and enable them */
+	ret = cpt_attach_and_enable_cores(eng_grp, obj);
+	if (ret)
+		goto err;
+err:
+	return ret;
+}
+
+static int disable_eng_grp(struct device *dev,
+			   struct engine_group_info *eng_grp,
+			   void *obj)
+{
+	int i, ret = 0;
+
+	/* Disable all engines used by this group */
+	ret = cpt_detach_and_disable_cores(eng_grp, obj);
+	if (ret)
+		goto err;
+
+	/* Unload ucode used by this engine group */
+	ucode_unload(dev, &eng_grp->ucode[0]);
+	ucode_unload(dev, &eng_grp->ucode[1]);
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		if (!eng_grp->engs[i].type)
+			continue;
+
+		eng_grp->engs[i].ucode = &eng_grp->ucode[0];
+	}
+
+	/* Clear UCODE_BASE register for each engine used by this group */
+	ret = cpt_set_ucode_base(eng_grp, obj);
+	if (ret)
+		goto err;
+err:
+	return ret;
+}
+
+static void setup_eng_grp_mirroring(struct engine_group_info *dst_grp,
+				    struct engine_group_info *src_grp)
+{
+	/* Setup fields for engine group which is mirrored */
+	src_grp->mirror.is_ena = false;
+	src_grp->mirror.idx = 0;
+	src_grp->mirror.ref_count++;
+
+	/* Setup fields for mirroring engine group */
+	dst_grp->mirror.is_ena = true;
+	dst_grp->mirror.idx = src_grp->idx;
+	dst_grp->mirror.ref_count = 0;
+}
+
+static void remove_eng_grp_mirroring(struct engine_group_info *dst_grp)
+{
+	struct engine_group_info *src_grp;
+
+	if (!dst_grp->mirror.is_ena)
+		return;
+
+	src_grp = &dst_grp->g->grp[dst_grp->mirror.idx];
+
+	src_grp->mirror.ref_count--;
+	dst_grp->mirror.is_ena = false;
+	dst_grp->mirror.idx = 0;
+	dst_grp->mirror.ref_count = 0;
+}
+
+static void update_requested_engs(struct engine_group_info *mirrored_eng_grp,
+				  struct engines *engs, int engs_cnt)
+{
+	struct engines_reserved *mirrored_engs;
+	int i;
+
+	for (i = 0; i < engs_cnt; i++) {
+		mirrored_engs = find_engines_by_type(mirrored_eng_grp,
+						     engs[i].type);
+		if (!mirrored_engs)
+			continue;
+
+		/* If mirrored group has this type of engines attached then
+		 * there are 3 scenarios possible:
+		 * 1) mirrored_engs.count == engs[i].count then all engines
+		 * from mirrored engine group will be shared with this engine
+		 * group
+		 * 2) mirrored_engs.count > engs[i].count then only a subset of
+		 * engines from mirrored engine group will be shared with this
+		 * engine group
+		 * 3) mirrored_engs.count < engs[i].count then all engines
+		 * from mirrored engine group will be shared with this group
+		 * and additional engines will be reserved for exclusively use
+		 * by this engine group
+		 */
+		engs[i].count -= mirrored_engs->count;
+	}
+}
+
+struct engine_group_info *find_mirrored_eng_grp(struct engine_group_info *grp)
+{
+	struct engine_groups *eng_grps = grp->g;
+	int i;
+
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+		if (!eng_grps->grp[i].is_enabled)
+			continue;
+		if (eng_grps->grp[i].ucode[0].type &&
+		    eng_grps->grp[i].ucode[1].type)
+			continue;
+		if (grp->idx == i)
+			continue;
+		if (!strncasecmp(eng_grps->grp[i].ucode[0].version,
+				 grp->ucode[0].version, CPT_UCODE_VERSION_SZ))
+			return &eng_grps->grp[i];
+	}
+
+	return NULL;
+}
+
+struct engine_group_info *find_unused_eng_grp(struct engine_groups *eng_grps)
+{
+	int i;
+
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++)
+		if (!eng_grps->grp[i].is_enabled)
+			return &eng_grps->grp[i];
+	return NULL;
+}
+
+static int eng_grp_update_masks(struct device *dev,
+				struct engine_group_info *eng_grp)
+{
+	struct engines_reserved *engs, *mirrored_engs;
+	int i, j, cnt, max_cnt, ret = 0;
+	struct bitmap tmp_bmap = { 0 };
+	int bit;
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		engs = &eng_grp->engs[i];
+		if (!engs->type)
+			continue;
+		if (engs->count <= 0)
+			continue;
+
+		switch (engs->type) {
+		case SE_TYPES:
+			max_cnt = eng_grp->g->avail.max_se_cnt;
+		break;
+
+		case IE_TYPES:
+			max_cnt = eng_grp->g->avail.max_ie_cnt;
+		break;
+
+		case AE_TYPES:
+			max_cnt = eng_grp->g->avail.max_ae_cnt;
+		break;
+
+		default:
+			dev_err(dev, "Invalid engine type %d", engs->type);
+			ret = -EINVAL;
+			goto end;
+		}
+
+		cnt = engs->count;
+		WARN_ON(engs->offset + max_cnt > CPT_MAX_ENGINES);
+		bitmap_zero(tmp_bmap.bits, eng_grp->g->engs_num);
+		for (j = engs->offset; j < engs->offset + max_cnt; j++) {
+			if (!eng_grp->g->eng_ref_cnt[j]) {
+				bitmap_set(tmp_bmap.bits, j, 1);
+				cnt--;
+				if (!cnt)
+					break;
+			}
+		}
+
+		if (cnt) {
+			ret = -ENOSPC;
+			goto end;
+		}
+
+		bitmap_copy(engs->bmap, tmp_bmap.bits, eng_grp->g->engs_num);
+	}
+
+	if (!eng_grp->mirror.is_ena)
+		goto end;
+
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		engs = &eng_grp->engs[i];
+		if (!engs->type)
+			continue;
+
+		mirrored_engs = find_engines_by_type(
+					&eng_grp->g->grp[eng_grp->mirror.idx],
+					engs->type);
+		WARN_ON(!mirrored_engs && engs->count <= 0);
+		if (!mirrored_engs)
+			continue;
+
+		bitmap_copy(tmp_bmap.bits, mirrored_engs->bmap,
+			    eng_grp->g->engs_num);
+		if (engs->count < 0) {
+			bit = find_first_bit(mirrored_engs->bmap,
+					     eng_grp->g->engs_num);
+			bitmap_clear(tmp_bmap.bits, bit, -engs->count);
+		}
+		bitmap_or(engs->bmap, engs->bmap, tmp_bmap.bits,
+			  eng_grp->g->engs_num);
+	}
+end:
+	return ret;
+}
+
+static int delete_engine_group(struct device *dev,
+			       struct engine_group_info *eng_grp)
+{
+	int i, ret = 0;
+
+	if (!eng_grp->is_enabled)
+		return -EINVAL;
+
+	if (eng_grp->mirror.ref_count) {
+		dev_err(dev, "Can't delete engine_group%d as it is used by:",
+			eng_grp->idx);
+		for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+			if (eng_grp->g->grp[i].mirror.is_ena &&
+			    eng_grp->g->grp[i].mirror.idx == eng_grp->idx)
+				dev_err(dev, "engine_group%d", i);
+		}
+		return -EINVAL;
+	}
+
+	/* Removing engine group mirroring if enabled */
+	remove_eng_grp_mirroring(eng_grp);
+
+	/* Disable engine group */
+	ret = disable_eng_grp(dev, eng_grp, eng_grp->g->obj);
+	if (ret)
+		goto err;
+
+	/* Release all engines held by this engine group */
+	ret = release_engines(dev, eng_grp);
+	if (ret)
+		goto err;
+
+	device_remove_file(dev, &eng_grp->info_attr);
+	eng_grp->is_enabled = false;
+	if (eng_grp->g->plat_hndlr)
+		eng_grp->g->plat_hndlr(eng_grp->g->obj);
+err:
+	return ret;
+}
+
+static int validate_2_ucodes_scenario(struct device *dev,
+				      struct engine_group_info *eng_grp)
+{
+	struct microcode *se_ucode = NULL, *ie_ucode = NULL;
+	struct microcode *ucode;
+	int i, ret = 0;
+
+	/* Find ucode which supports SE engines and ucode which supports
+	 * IE engines only
+	 */
+	for (i = 0; i < MAX_ENGS_PER_GRP; i++) {
+		ucode = &eng_grp->ucode[i];
+		if (cpt_uc_supports_eng_type(ucode, SE_TYPES))
+			se_ucode = ucode;
+		else if (cpt_uc_supports_eng_type(ucode, IE_TYPES) &&
+			 !cpt_uc_supports_eng_type(ucode, SE_TYPES))
+			ie_ucode = ucode;
+	}
+
+	if (!se_ucode || !ie_ucode) {
+		dev_err(dev,
+			"Only combination of SE+IE microcodes is supported.");
+		ret = -EINVAL;
+		goto err;
+	}
+
+	/* Keep SE ucode at index 0 */
+	if (cpt_uc_supports_eng_type(&eng_grp->ucode[1], SE_TYPES))
+		swap_ucodes(&eng_grp->ucode[0], &eng_grp->ucode[1]);
+err:
+	return ret;
+}
+
+static int validate_1_ucode_scenario(struct device *dev,
+				     struct engine_group_info *eng_grp,
+				     struct engines *engs, int engs_cnt)
+{
+	int i, ret = 0;
+
+	/* Verify that ucode loaded supports requested engine types */
+	for (i = 0; i < engs_cnt; i++) {
+		if (cpt_uc_supports_eng_type(&eng_grp->ucode[0], SE_TYPES) &&
+		    engs[i].type == IE_TYPES) {
+			dev_err(dev,
+				"IE engines can't be used with SE microcode.");
+			ret = -EINVAL;
+			goto err;
+		}
+
+		if (!cpt_uc_supports_eng_type(&eng_grp->ucode[0],
+					      engs[i].type)) {
+			/* Exception to this rule is the case
+			 * where IPSec ucode can use SE engines
+			 */
+			if (cpt_uc_supports_eng_type(&eng_grp->ucode[0],
+						     IE_TYPES) &&
+			    engs[i].type == SE_TYPES)
+				continue;
+
+			dev_err(dev,
+				"Microcode %s does not support %s engines",
+				eng_grp->ucode[0].filename,
+				get_eng_type_str(engs[i].type));
+			ret = -EINVAL;
+			goto err;
+		}
+	}
+err:
+	return ret;
+}
+
+static void update_ucode_ptrs(struct engine_group_info *eng_grp)
+{
+	struct microcode *ucode;
+
+	if (eng_grp->mirror.is_ena)
+		ucode = &eng_grp->g->grp[eng_grp->mirror.idx].ucode[0];
+	else
+		ucode = &eng_grp->ucode[0];
+	WARN_ON(!eng_grp->engs[0].type);
+	eng_grp->engs[0].ucode = ucode;
+
+	if (eng_grp->engs[1].type) {
+		if (is_2nd_ucode_used(eng_grp))
+			eng_grp->engs[1].ucode = &eng_grp->ucode[1];
+		else
+			eng_grp->engs[1].ucode = ucode;
+	}
+}
+
+static int create_engine_group(struct device *dev,
+			       struct engine_groups *eng_grps,
+			       struct engines *engs, int engs_cnt,
+			       void *ucode_data[], int ucodes_cnt,
+			       bool use_uc_from_tar_arch)
+{
+	struct engine_group_info *mirrored_eng_grp;
+	struct tar_ucode_info_t *tar_ucode_info;
+	struct engine_group_info *eng_grp;
+	int i, ret = 0;
+
+	if (ucodes_cnt > MAX_ENGS_PER_GRP)
+		goto err;
+
+	/* Validate if requested engine types are supported by this device */
+	for (i = 0; i < engs_cnt; i++)
+		if (!dev_supports_eng_type(eng_grps, engs[i].type)) {
+			dev_err(dev, "Device does not support %s engines",
+				get_eng_type_str(engs[i].type));
+			return -EPERM;
+		}
+
+	/* Find engine group which is not used*/
+	eng_grp = find_unused_eng_grp(eng_grps);
+	if (!eng_grp) {
+		dev_err(dev, "Error all engine groups are being used");
+		return -ENOSPC;
+	}
+
+	/* Load ucode */
+	for (i = 0; i < ucodes_cnt; i++) {
+		if (use_uc_from_tar_arch) {
+			tar_ucode_info =
+				     (struct tar_ucode_info_t *) ucode_data[i];
+			eng_grp->ucode[i] = tar_ucode_info->ucode;
+			ret = copy_ucode_to_dma_mem(dev, &eng_grp->ucode[i],
+						    tar_ucode_info->ucode_ptr);
+		} else
+			ret = ucode_load(dev, &eng_grp->ucode[i],
+					 (char *) ucode_data[i]);
+		if (ret)
+			goto err_ucode_unload;
+	}
+
+	if (ucodes_cnt > 1) {
+		/* Validate scenario where 2 ucodes are used - this
+		 * is only allowed for combination of SE+IE ucodes
+		 */
+		ret = validate_2_ucodes_scenario(dev, eng_grp);
+		if (ret)
+			goto err_ucode_unload;
+	} else {
+		/* Validate scenario where 1 ucode is used */
+		ret = validate_1_ucode_scenario(dev, eng_grp, engs, engs_cnt);
+		if (ret)
+			goto err_ucode_unload;
+	}
+
+	/* Check if this group mirrors another existing engine group */
+	mirrored_eng_grp = find_mirrored_eng_grp(eng_grp);
+	if (mirrored_eng_grp) {
+		/* Setup mirroring */
+		setup_eng_grp_mirroring(eng_grp, mirrored_eng_grp);
+
+		/* Update count of requested engines because some
+		 * of them might be shared with mirrored group
+		 */
+		update_requested_engs(mirrored_eng_grp, engs, engs_cnt);
+	}
+
+	/* Reserve engines */
+	ret = reserve_engines(dev, eng_grp, engs, engs_cnt);
+	if (ret)
+		goto err;
+
+	/* Update ucode pointers used by engines */
+	update_ucode_ptrs(eng_grp);
+
+	/* Update engine masks used by this group */
+	ret = eng_grp_update_masks(dev, eng_grp);
+	if (ret)
+		goto err_release_engs;
+
+	/* Create sysfs entry for engine group info */
+	ret = create_sysfs_eng_grps_info(dev, eng_grp);
+	if (ret)
+		goto err_release_engs;
+
+	/* Enable engine group */
+	ret = enable_eng_grp(eng_grp, eng_grps->obj);
+	if (ret)
+		goto err_release_engs;
+
+	/* If this engine group mirrors another engine group
+	 * then we need to unload ucode as we will use ucode
+	 * from mirrored engine group
+	 */
+	if (eng_grp->mirror.is_ena)
+		ucode_unload(dev, &eng_grp->ucode[0]);
+
+	eng_grp->is_enabled = true;
+	if (eng_grp->mirror.is_ena)
+		dev_info(dev,
+			 "Engine_group%d: reuse microcode %s from group %d",
+			 eng_grp->idx, &mirrored_eng_grp->ucode[0].version[1],
+			 mirrored_eng_grp->idx);
+	else
+		dev_info(dev, "Engine_group%d: microcode loaded %s",
+			 eng_grp->idx, &eng_grp->ucode[0].version[1]);
+	if (is_2nd_ucode_used(eng_grp))
+		dev_info(dev, "Engine_group%d: microcode loaded %s",
+			 eng_grp->idx, &eng_grp->ucode[1].version[1]);
+
+	if (eng_grps->plat_hndlr)
+		eng_grps->plat_hndlr(eng_grps->obj);
+	return 0;
+
+err_release_engs:
+	release_engines(dev, eng_grp);
+err_ucode_unload:
+	ucode_unload(dev, &eng_grp->ucode[0]);
+	ucode_unload(dev, &eng_grp->ucode[1]);
+err:
+	return ret;
+}
+
+static ssize_t ucode_load_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	struct engines engs[MAX_ENGS_PER_GRP] = { 0 };
+	char *ucode_filename[MAX_ENGS_PER_GRP];
+	char tmp_buf[NAME_LENGTH] = { 0 };
+	char *start, *val, *err_msg, *tmp;
+	struct engine_groups *eng_grps;
+	int grp_idx = 0, ret = -EINVAL;
+	int del_grp_idx = -1;
+	int ucode_idx = 0;
+	bool has_se, has_ie, has_ae;
+
+	if (strlen(buf) > NAME_LENGTH)
+		return -EINVAL;
+
+	eng_grps = container_of(attr, struct engine_groups, ucode_load_attr);
+	err_msg = "Invalid engine group format";
+	strlcpy(tmp_buf, buf, NAME_LENGTH);
+	start = tmp_buf;
+
+	has_se = has_ie = has_ae = false;
+
+	for (;;) {
+		val = strsep(&start, ";");
+		if (!val)
+			break;
+		val = strim(val);
+		if (!*val)
+			continue;
+
+		if (!strncasecmp(val, "engine_group", 12)) {
+			if (del_grp_idx != -1)
+				goto err_print;
+			tmp = strim(strsep(&val, ":"));
+			if (!val)
+				goto err_print;
+			if (strlen(tmp) != 13)
+				goto err_print;
+			if (kstrtoint((tmp + 12), 10, &del_grp_idx))
+				goto err_print;
+			val = strim(val);
+			if (strncasecmp(val, "null", 4))
+				goto err_print;
+			if (strlen(val) != 4)
+				goto err_print;
+		} else if (!strncasecmp(val, "se", 2)) {
+			if (has_se || ucode_idx)
+				goto err_print;
+			tmp = strim(strsep(&val, ":"));
+			if (!val)
+				goto err_print;
+			if (strlen(tmp) != 2)
+				goto err_print;
+			if (kstrtoint(strim(val), 10, &engs[grp_idx].count))
+				goto err_print;
+			engs[grp_idx++].type = SE_TYPES;
+			has_se = true;
+		} else if (!strncasecmp(strim(val), "ae", 2)) {
+			if (has_ae || ucode_idx)
+				goto err_print;
+			tmp = strim(strsep(&val, ":"));
+			if (!val)
+				goto err_print;
+			if (strlen(tmp) != 2)
+				goto err_print;
+			if (kstrtoint(strim(val), 10, &engs[grp_idx].count))
+				goto err_print;
+			engs[grp_idx++].type = AE_TYPES;
+			has_ae = true;
+		} else if (!strncasecmp(val, "ie", 2)) {
+			if (has_ie || ucode_idx)
+				goto err_print;
+			tmp = strim(strsep(&val, ":"));
+			if (!val)
+				goto err_print;
+			if (strlen(tmp) != 2)
+				goto err_print;
+			if (kstrtoint(strim(val), 10, &engs[grp_idx].count))
+				goto err_print;
+			engs[grp_idx++].type = IE_TYPES;
+			has_ie = true;
+		} else {
+			if (ucode_idx > 1)
+				goto err_print;
+			if (!strlen(val))
+				goto err_print;
+			if (strnstr(val, " ", strlen(val)))
+				goto err_print;
+			ucode_filename[ucode_idx++] = val;
+		}
+	}
+
+	/* Validate input parameters */
+	if (del_grp_idx == -1) {
+		if (!(grp_idx && ucode_idx))
+			goto err_print;
+
+		if (ucode_idx > 1 && grp_idx < 2)
+			goto err_print;
+
+		if (grp_idx > MAX_ENGS_PER_GRP) {
+			err_msg = "Error max 2 engine types can be attached";
+			goto err_print;
+		}
+
+		if (grp_idx > 1) {
+			if ((engs[0].type + engs[1].type) !=
+			    (SE_TYPES + IE_TYPES)) {
+				err_msg =
+				"Only combination of SE+IE engines is allowed";
+				goto err_print;
+			}
+
+			/* Keep SE engines at zero index */
+			if (engs[1].type == SE_TYPES)
+				swap_engines(&engs[0], &engs[1]);
+		}
+
+	} else {
+		if (del_grp_idx < 0 || del_grp_idx >= CPT_MAX_ENGINE_GROUPS) {
+			dev_err(dev, "Invalid engine group index %d",
+				del_grp_idx);
+			goto err;
+		}
+
+		if (!eng_grps->grp[del_grp_idx].is_enabled) {
+			dev_err(dev, "Error engine_group%d is not configured",
+				del_grp_idx);
+			ret = -EINVAL;
+			goto err;
+		}
+
+		if (grp_idx || ucode_idx)
+			goto err_print;
+	}
+
+	mutex_lock(&eng_grps->lock);
+
+	if (eng_grps->is_rdonly) {
+		dev_err(dev, "Disable VFs before modifying engine groups\n");
+		ret = -EACCES;
+		goto err_unlock;
+	}
+
+	if (del_grp_idx == -1)
+		/* create engine group */
+		ret = create_engine_group(dev, eng_grps, engs, grp_idx,
+					  (void **) ucode_filename,
+					  ucode_idx, false);
+	else
+		/* delete engine group */
+		ret = delete_engine_group(dev, &eng_grps->grp[del_grp_idx]);
+	if (ret)
+		goto err_unlock;
+
+	if (cpt_is_dbg_level_en(CPT_DBG_ENGINE_GRPS))
+		print_dbg_info(dev, eng_grps);
+err_unlock:
+	mutex_unlock(&eng_grps->lock);
+	return ret ? ret : count;
+err_print:
+	dev_err(dev, "%s\n", err_msg);
+err:
+	return ret;
+}
+
+int cpt_try_create_default_eng_grps(struct pci_dev *pdev,
+				    struct engine_groups *eng_grps,
+				    int pf_type)
+{
+	struct tar_ucode_info_t *tar_ucode_info[MAX_ENGS_PER_GRP] = { 0 };
+	struct engines engs[MAX_ENGS_PER_GRP] = { 0 };
+	struct tar_arch_info_t *tar_arch = NULL;
+	char *tar_filename;
+	int i, ret = 0;
+
+	mutex_lock(&eng_grps->lock);
+
+	/* We don't create engine group for kernel crypto if attempt to create
+	 * it was already made (when user enabled VFs for the first time)
+	 */
+	if (eng_grps->is_first_try)
+		goto err;
+	eng_grps->is_first_try = true;
+
+	/* We create group for kcrypto only if no groups are configured */
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++)
+		if (eng_grps->grp[i].is_enabled)
+			goto err;
+
+	switch (pf_type) {
+	case CPT_81XX:
+	case CPT_SE_83XX:
+		tar_filename = CPT_8X_TAR_FILE_NAME;
+	break;
+
+	case CPT_96XX:
+		tar_filename = CPT_9X_TAR_FILE_NAME;
+	break;
+
+	default:
+		dev_err(&pdev->dev, "Unknown PF type %d\n", pf_type);
+		ret = -EINVAL;
+		goto err;
+	}
+
+	tar_arch = load_tar_archive(&pdev->dev, tar_filename);
+	if (!tar_arch)
+		goto err;
+
+	/* If device supports SE engines and there is SE microcode in tar
+	 * archive try to create engine group with SE engines for kernel
+	 * crypto functionality (symmetric crypto)
+	 */
+	tar_ucode_info[0] = get_uc_from_tar_archive(tar_arch, SE_TYPES);
+	if (tar_ucode_info[0] && dev_supports_eng_type(eng_grps, SE_TYPES)) {
+
+		engs[0].type = SE_TYPES;
+		engs[0].count = eng_grps->avail.max_se_cnt;
+
+		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+					  (void **) tar_ucode_info, 1, true);
+		if (ret)
+			goto err;
+	}
+
+	/* If device supports SE+IE engines and there is SE and IE microcode in
+	 * tar archive try to create engine group with SE+IE engines for IPSec.
+	 * All SE engines will be shared with engine group 0. This case applies
+	 * only to 9X platform.
+	 */
+	tar_ucode_info[0] = get_uc_from_tar_archive(tar_arch, SE_TYPES);
+	tar_ucode_info[1] = get_uc_from_tar_archive(tar_arch, IE_TYPES);
+	if (tar_ucode_info[0] && tar_ucode_info[1] &&
+	    dev_supports_eng_type(eng_grps, SE_TYPES) &&
+	    dev_supports_eng_type(eng_grps, IE_TYPES)) {
+
+		engs[0].type = SE_TYPES;
+		engs[0].count = eng_grps->avail.max_se_cnt;
+		engs[1].type = IE_TYPES;
+		engs[1].count = eng_grps->avail.max_ie_cnt;
+
+		ret = create_engine_group(&pdev->dev, eng_grps, engs, 2,
+					  (void **) tar_ucode_info, 2, true);
+		if (ret)
+			goto err;
+	}
+
+	/* If device supports AE engines and there is AE microcode in tar
+	 * archive try to create engine group with AE engines for asymmetric
+	 * crypto functionality.
+	 */
+	tar_ucode_info[0] = get_uc_from_tar_archive(tar_arch, AE_TYPES);
+	if (tar_ucode_info[0] && dev_supports_eng_type(eng_grps, AE_TYPES)) {
+
+		engs[0].type = AE_TYPES;
+		engs[0].count = eng_grps->avail.max_ae_cnt;
+
+		ret = create_engine_group(&pdev->dev, eng_grps, engs, 1,
+					  (void **) tar_ucode_info, 1, true);
+		if (ret)
+			goto err;
+	}
+
+	if (cpt_is_dbg_level_en(CPT_DBG_ENGINE_GRPS))
+		print_dbg_info(&pdev->dev, eng_grps);
+err:
+	release_tar_archive(tar_arch);
+	mutex_unlock(&eng_grps->lock);
+	return ret;
+}
+
+void cpt_set_eng_grps_plat_hndlr(struct engine_groups *eng_grps,
+				 void (*plat_hndlr)(void *obj))
+{
+	mutex_lock(&eng_grps->lock);
+
+	eng_grps->plat_hndlr = plat_hndlr;
+
+	mutex_unlock(&eng_grps->lock);
+}
+
+void cpt_set_eng_grps_is_rdonly(struct engine_groups *eng_grps, bool is_rdonly)
+{
+	mutex_lock(&eng_grps->lock);
+
+	eng_grps->is_rdonly = is_rdonly;
+
+	mutex_unlock(&eng_grps->lock);
+}
+
+void cpt_cleanup_eng_grps(struct pci_dev *pdev,
+			  struct engine_groups *eng_grps)
+{
+	struct engine_group_info *grp;
+	int i, j;
+
+	mutex_lock(&eng_grps->lock);
+	if (eng_grps->is_ucode_load_created) {
+		device_remove_file(&pdev->dev,
+				   &eng_grps->ucode_load_attr);
+		eng_grps->is_ucode_load_created = false;
+	}
+
+	/* First delete all mirroring engine groups */
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++)
+		if (eng_grps->grp[i].mirror.is_ena)
+			delete_engine_group(&pdev->dev, &eng_grps->grp[i]);
+
+	/* Delete remaining engine groups */
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++)
+		delete_engine_group(&pdev->dev, &eng_grps->grp[i]);
+
+	/* Release memory */
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+		grp = &eng_grps->grp[i];
+		for (j = 0; j < MAX_ENGS_PER_GRP; j++) {
+			kfree(grp->engs[j].bmap);
+			grp->engs[j].bmap = NULL;
+		}
+	}
+
+	mutex_unlock(&eng_grps->lock);
+}
+
+int cpt_init_eng_grps(struct pci_dev *pdev, struct engine_groups *eng_grps,
+		      int pf_type)
+{
+	struct engine_group_info *grp;
+	int i, j, ret = 0;
+
+	mutex_init(&eng_grps->lock);
+	eng_grps->obj = pci_get_drvdata(pdev);
+	eng_grps->avail.se_cnt = eng_grps->avail.max_se_cnt;
+	eng_grps->avail.ie_cnt = eng_grps->avail.max_ie_cnt;
+	eng_grps->avail.ae_cnt = eng_grps->avail.max_ae_cnt;
+
+	eng_grps->engs_num = eng_grps->avail.max_se_cnt +
+			     eng_grps->avail.max_ie_cnt +
+			     eng_grps->avail.max_ae_cnt;
+	if (eng_grps->engs_num > CPT_MAX_ENGINES) {
+		dev_err(&pdev->dev,
+			"Number of engines %d > than max supported %d",
+			eng_grps->engs_num, CPT_MAX_ENGINES);
+		ret = -EINVAL;
+		goto err;
+	}
+
+	for (i = 0; i < CPT_MAX_ENGINE_GROUPS; i++) {
+		grp = &eng_grps->grp[i];
+		grp->g = eng_grps;
+		grp->idx = i;
+
+		snprintf(grp->sysfs_info_name, NAME_LENGTH,
+			 "engine_group%d", i);
+		for (j = 0; j < MAX_ENGS_PER_GRP; j++) {
+			grp->engs[j].bmap =
+				kcalloc(BITS_TO_LONGS(eng_grps->engs_num),
+					sizeof(long), GFP_KERNEL);
+			if (!grp->engs[j].bmap) {
+				ret = -ENOMEM;
+				goto err;
+			}
+		}
+	}
+
+	switch (pf_type) {
+	case CPT_81XX:
+		/* 81XX CPT PF has SE and AE engines attached */
+		eng_grps->eng_types_supported = 1 << SE_TYPES | 1 << AE_TYPES;
+	break;
+
+	case CPT_SE_83XX:
+		/* 83XX SE CPT PF has only SE engines attached */
+		eng_grps->eng_types_supported = 1 << SE_TYPES;
+	break;
+
+	case CPT_AE_83XX:
+		/* 83XX AE CPT PF has only AE engines attached */
+		eng_grps->eng_types_supported = 1 << AE_TYPES;
+	break;
+
+	case CPT_96XX:
+		/* 96XX CPT PF has SE, IE and AE engines attached */
+		eng_grps->eng_types_supported = 1 << SE_TYPES | 1 << IE_TYPES |
+						1 << AE_TYPES;
+	break;
+
+	default:
+		dev_err(&pdev->dev, "Unknown PF type %d\n", pf_type);
+		ret = -EINVAL;
+		goto err;
+	}
+
+	eng_grps->ucode_load_attr.show = NULL;
+	eng_grps->ucode_load_attr.store = ucode_load_store;
+	eng_grps->ucode_load_attr.attr.name = "ucode_load";
+	eng_grps->ucode_load_attr.attr.mode = 0220;
+	sysfs_attr_init(&eng_grps->ucode_load_attr.attr);
+	ret = device_create_file(&pdev->dev,
+				 &eng_grps->ucode_load_attr);
+	if (ret)
+		goto err;
+	eng_grps->is_ucode_load_created = true;
+
+	if (cpt_is_dbg_level_en(CPT_DBG_ENGINE_GRPS))
+		print_dbg_info(&pdev->dev, eng_grps);
+	return ret;
+err:
+	cpt_cleanup_eng_grps(pdev, eng_grps);
+	return ret;
+}
diff --git a/drivers/crypto/cavium/cpt/common/cpt_ucode.h b/drivers/crypto/cavium/cpt/common/cpt_ucode.h
new file mode 100644
index 000000000000..5a834bb608f1
--- /dev/null
+++ b/drivers/crypto/cavium/cpt/common/cpt_ucode.h
@@ -0,0 +1,200 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __CPT_UCODE_H
+#define __CPT_UCODE_H
+
+#include "cpt_hw_types.h"
+
+/* Name maximum length */
+#define NAME_LENGTH		64
+
+/* On 8x platform only one type of engines is allowed to be attached
+ * to an engine group. On 9x platform we have one exception from this
+ * rule because IPSec ucode can use both IE and SE engines therefore
+ * IE and SE engines can be attached to the same engine group.
+ */
+#define MAX_ENGS_PER_GRP	2
+
+/* Default tar archive file names */
+#define CPT_8X_TAR_FILE_NAME	"cpt8x-mc.tar"
+#define CPT_9X_TAR_FILE_NAME	"cpt9x-mc.tar"
+
+/* CPT ucode alignment */
+#define CPT_UCODE_ALIGNMENT	128
+
+/* CPT ucode signature size */
+#define CPT_UCODE_SIGN_LEN	256
+
+/* Maximum number of supported engines/cores on 8X platform */
+#define CPT_8X_MAX_ENGINES	64
+
+/* Maximum number of supported engines/cores on 9X platform */
+#define CPT_9X_MAX_ENGINES	128
+
+#define CPT_MAX_ENGINES		CPT_9X_MAX_ENGINES
+
+#define CPT_ENGS_BITMASK_LEN	(CPT_MAX_ENGINES/(BITS_PER_BYTE * \
+				 sizeof(unsigned long)))
+
+#define ROUNDUP16(val) ((((int) val) + 15) & 0xFFFFFFF0)
+
+/* Tar archive defines */
+#define TAR_MAGIC		"ustar"
+#define TAR_MAGIC_LEN		6
+#define TAR_BLOCK_LEN		512
+#define REGTYPE			'0'
+#define AREGTYPE		'\0'
+
+/* tar header as defined in POSIX 1003.1-1990.  */
+struct tar_hdr_t {
+	char name[100];
+	char mode[8];
+	char uid[8];
+	char gid[8];
+	char size[12];
+	char mtime[12];
+	char chksum[8];
+	char typeflag;
+	char linkname[100];
+	char magic[6];
+	char version[2];
+	char uname[32];
+	char gname[32];
+	char devmajor[8];
+	char devminor[8];
+	char prefix[155];
+};
+
+struct tar_blk_t {
+	union {
+		struct tar_hdr_t hdr;
+		char block[TAR_BLOCK_LEN];
+	};
+};
+
+struct tar_arch_info_t {
+	struct list_head ucodes;
+	const struct firmware *fw;
+};
+
+struct bitmap {
+	unsigned long bits[CPT_ENGS_BITMASK_LEN];
+	int size;
+};
+
+struct engines {
+	int type;
+	int count;
+};
+
+/* Microcode header size should be 48 bytes according to CNT8x-MC-IPSEC-0002 */
+struct microcode_hdr {
+	u8 version[CPT_UCODE_VERSION_SZ];
+	u32 code_length;
+	u32 padding[3];
+};
+
+struct microcode {
+	char filename[NAME_LENGTH];	/* ucode filename */
+	u8 version[CPT_UCODE_VERSION_SZ];/* ucode version in readable format */
+	dma_addr_t dma;		/* phys address of ucode image */
+	dma_addr_t align_dma;	/* aligned phys address of ucode image */
+	void *va;		/* virt address of ucode image */
+	void *align_va;		/* aligned virt address of ucode image */
+	u32 size;		/* ucode image size */
+	int type;		/* ucode image type SE, IE, AE or SE+IE */
+};
+
+struct tar_ucode_info_t {
+	struct list_head list;
+	struct microcode ucode;	/* microcode information */
+	const u8 *ucode_ptr;	/* pointer to microcode in tar archive */
+};
+
+/* Maximum and current number of engines available for all engine groups */
+struct engines_available {
+	int max_se_cnt;
+	int max_ie_cnt;
+	int max_ae_cnt;
+	int se_cnt;
+	int ie_cnt;
+	int ae_cnt;
+};
+
+/* Engines reserved to an engine group */
+struct engines_reserved {
+	int type;	/* engine type */
+	int count;	/* number of engines attached */
+	int offset;     /* constant offset of engine type in the bitmap */
+	unsigned long *bmap;		/* attached engines bitmap */
+	struct microcode *ucode;	/* ucode used by these engines */
+};
+
+struct mirror_info {
+	int is_ena;	/* is mirroring enabled, it is set only for engine
+			 * group which mirrors another engine group
+			 */
+	int idx;	/* index of engine group which is mirrored by this
+			 * group, set only for engine group which mirrors
+			 * another group
+			 */
+	int ref_count;	/* number of times this engine group is mirrored by
+			 * other groups, this is set only for engine group
+			 * which is mirrored by other group(s)
+			 */
+};
+
+struct engine_group_info {
+	struct engine_groups *g; /* pointer to engine_groups structure */
+	struct device_attribute info_attr; /* group info entry attr */
+	struct engines_reserved engs[MAX_ENGS_PER_GRP];	/* engines attached */
+	struct microcode ucode[MAX_ENGS_PER_GRP]; /* ucodes information */
+	char sysfs_info_name[NAME_LENGTH]; /* sysfs info entry name */
+	struct mirror_info mirror; /* engine group mirroring information */
+	int idx;	 /* engine group index */
+	bool is_enabled; /* is engine group enabled, engine group is enabled
+			  * when it has engines attached and ucode loaded
+			  */
+};
+
+struct engine_groups {
+	struct engine_group_info grp[CPT_MAX_ENGINE_GROUPS];
+	struct device_attribute ucode_load_attr;	/* ucode load attr */
+	struct engines_available avail;
+	struct mutex lock;
+	void (*plat_hndlr)(void *obj);	/* 8x/9x hndlr for create/delete grp */
+	void *obj;			/* 8x/9x platform specific data */
+	int engs_num;			/* total number of engines supported */
+	int eng_types_supported;	/* engine types supported SE, IE, AE */
+	u8 eng_ref_cnt[CPT_MAX_ENGINES];/* engines reference count */
+	bool is_ucode_load_created;	/* is ucode_load sysfs entry created */
+	bool is_first_try; /* is this first try to create kcrypto engine grp */
+	bool is_rdonly;	/* do engine groups configuration can be modified */
+};
+
+int cpt_init_eng_grps(struct pci_dev *pdev, struct engine_groups *eng_grps,
+		      int pf_type);
+void cpt_cleanup_eng_grps(struct pci_dev *pdev,
+			  struct engine_groups *eng_grps);
+int cpt_try_create_default_eng_grps(struct pci_dev *pdev,
+				    struct engine_groups *eng_grps,
+				    int pf_type);
+int cpt_detach_and_disable_cores(struct engine_group_info *eng_grp, void *obj);
+int cpt_set_ucode_base(struct engine_group_info *eng_grp, void *obj);
+int cpt_attach_and_enable_cores(struct engine_group_info *eng_grp, void *obj);
+void cpt_print_engines_mask(struct engine_group_info *eng_grp, void *obj,
+			    char *buf, int size);
+void cpt_set_eng_grps_is_rdonly(struct engine_groups *eng_grps, bool is_rdonly);
+int cpt_uc_supports_eng_type(struct microcode *ucode, int eng_type);
+int cpt_eng_grp_has_eng_type(struct engine_group_info *eng_grp, int eng_type);
+void cpt_set_eng_grps_plat_hndlr(struct engine_groups *eng_grps,
+				 void (*plat_hdnlr)(void *obj));
+#endif /* __CPT_UCODE_H */
diff --git a/drivers/crypto/cavium/cpt/cpt_common.h b/drivers/crypto/cavium/cpt/cpt_common.h
deleted file mode 100644
index 225078d03773..000000000000
--- a/drivers/crypto/cavium/cpt/cpt_common.h
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-
-#ifndef __CPT_COMMON_H
-#define __CPT_COMMON_H
-
-#include <asm/byteorder.h>
-#include <linux/delay.h>
-#include <linux/pci.h>
-
-#include "cpt_hw_types.h"
-
-/* Device ID */
-#define CPT_81XX_PCI_PF_DEVICE_ID 0xa040
-#define CPT_81XX_PCI_VF_DEVICE_ID 0xa041
-
-/* flags to indicate the features supported */
-#define CPT_FLAG_SRIOV_ENABLED BIT(1)
-#define CPT_FLAG_VF_DRIVER BIT(2)
-#define CPT_FLAG_DEVICE_READY BIT(3)
-
-#define cpt_sriov_enabled(cpt) ((cpt)->flags & CPT_FLAG_SRIOV_ENABLED)
-#define cpt_vf_driver(cpt) ((cpt)->flags & CPT_FLAG_VF_DRIVER)
-#define cpt_device_ready(cpt) ((cpt)->flags & CPT_FLAG_DEVICE_READY)
-
-#define CPT_MBOX_MSG_TYPE_ACK 1
-#define CPT_MBOX_MSG_TYPE_NACK 2
-#define CPT_MBOX_MSG_TIMEOUT 2000
-#define VF_STATE_DOWN 0
-#define VF_STATE_UP 1
-
-/*
- * CPT Registers map for 81xx
- */
-
-/* PF registers */
-#define CPTX_PF_CONSTANTS(a) (0x0ll + ((u64)(a) << 36))
-#define CPTX_PF_RESET(a) (0x100ll + ((u64)(a) << 36))
-#define CPTX_PF_DIAG(a) (0x120ll + ((u64)(a) << 36))
-#define CPTX_PF_BIST_STATUS(a) (0x160ll + ((u64)(a) << 36))
-#define CPTX_PF_ECC0_CTL(a) (0x200ll + ((u64)(a) << 36))
-#define CPTX_PF_ECC0_FLIP(a) (0x210ll + ((u64)(a) << 36))
-#define CPTX_PF_ECC0_INT(a) (0x220ll + ((u64)(a) << 36))
-#define CPTX_PF_ECC0_INT_W1S(a) (0x230ll + ((u64)(a) << 36))
-#define CPTX_PF_ECC0_ENA_W1S(a)	(0x240ll + ((u64)(a) << 36))
-#define CPTX_PF_ECC0_ENA_W1C(a)	(0x250ll + ((u64)(a) << 36))
-#define CPTX_PF_MBOX_INTX(a, b)	\
-	(0x400ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_MBOX_INT_W1SX(a, b) \
-	(0x420ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_MBOX_ENA_W1CX(a, b) \
-	(0x440ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_MBOX_ENA_W1SX(a, b) \
-	(0x460ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_EXEC_INT(a) (0x500ll + 0x1000000000ll * ((a) & 0x1))
-#define CPTX_PF_EXEC_INT_W1S(a)	(0x520ll + ((u64)(a) << 36))
-#define CPTX_PF_EXEC_ENA_W1C(a)	(0x540ll + ((u64)(a) << 36))
-#define CPTX_PF_EXEC_ENA_W1S(a)	(0x560ll + ((u64)(a) << 36))
-#define CPTX_PF_GX_EN(a, b) \
-	(0x600ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_EXEC_INFO(a) (0x700ll + ((u64)(a) << 36))
-#define CPTX_PF_EXEC_BUSY(a) (0x800ll + ((u64)(a) << 36))
-#define CPTX_PF_EXEC_INFO0(a) (0x900ll + ((u64)(a) << 36))
-#define CPTX_PF_EXEC_INFO1(a) (0x910ll + ((u64)(a) << 36))
-#define CPTX_PF_INST_REQ_PC(a) (0x10000ll + ((u64)(a) << 36))
-#define CPTX_PF_INST_LATENCY_PC(a) \
-	(0x10020ll + ((u64)(a) << 36))
-#define CPTX_PF_RD_REQ_PC(a) (0x10040ll + ((u64)(a) << 36))
-#define CPTX_PF_RD_LATENCY_PC(a) (0x10060ll + ((u64)(a) << 36))
-#define CPTX_PF_RD_UC_PC(a) (0x10080ll + ((u64)(a) << 36))
-#define CPTX_PF_ACTIVE_CYCLES_PC(a) (0x10100ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_CTL(a) (0x4000000ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_STATUS(a) (0x4000008ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_CLK(a) (0x4000010ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_DBG_CTL(a) (0x4000018ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_DBG_DATA(a)	(0x4000020ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_BIST_STATUS(a) (0x4000028ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_REQ_TIMER(a) (0x4000030ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_MEM_CTL(a) (0x4000038ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_PERF_CTL(a)	(0x4001000ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_DBG_CNTX(a, b) \
-	(0x4001100ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_EXE_PERF_EVENT_CNT(a) (0x4001180ll + ((u64)(a) << 36))
-#define CPTX_PF_EXE_EPCI_INBX_CNT(a, b) \
-	(0x4001200ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_EXE_EPCI_OUTBX_CNT(a, b) \
-	(0x4001240ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_ENGX_UCODE_BASE(a, b) \
-	(0x4002000ll + ((u64)(a) << 36) + ((b) << 3))
-#define CPTX_PF_QX_CTL(a, b) \
-	(0x8000000ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_PF_QX_GMCTL(a, b) \
-	(0x8000020ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_PF_QX_CTL2(a, b) \
-	(0x8000100ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_PF_VFX_MBOXX(a, b, c) \
-	(0x8001000ll + ((u64)(a) << 36) + ((b) << 20) + ((c) << 8))
-
-/* VF registers */
-#define CPTX_VQX_CTL(a, b) (0x100ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_SADDR(a, b) (0x200ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE_WAIT(a, b) (0x400ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_INPROG(a, b) (0x410ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE(a, b) (0x420ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE_ACK(a, b) (0x440ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE_INT_W1S(a, b) (0x460ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE_INT_W1C(a, b) (0x468ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE_ENA_W1S(a, b) (0x470ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DONE_ENA_W1C(a, b) (0x478ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_MISC_INT(a, b)	(0x500ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_MISC_INT_W1S(a, b) (0x508ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_MISC_ENA_W1S(a, b) (0x510ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_MISC_ENA_W1C(a, b) (0x518ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VQX_DOORBELL(a, b) (0x600ll + ((u64)(a) << 36) + ((b) << 20))
-#define CPTX_VFX_PF_MBOXX(a, b, c) \
-	(0x1000ll + ((u64)(a) << 36) + ((b) << 20) + ((c) << 3))
-
-enum vftype {
-	AE_TYPES = 1,
-	SE_TYPES = 2,
-	BAD_CPT_TYPES,
-};
-
-/* Max CPT devices supported */
-enum cpt_mbox_opcode {
-	CPT_MSG_VF_UP = 1,
-	CPT_MSG_VF_DOWN,
-	CPT_MSG_READY,
-	CPT_MSG_QLEN,
-	CPT_MSG_QBIND_GRP,
-	CPT_MSG_VQ_PRIORITY,
-};
-
-/* CPT mailbox structure */
-struct cpt_mbox {
-	u64 msg; /* Message type MBOX[0] */
-	u64 data;/* Data         MBOX[1] */
-};
-
-/* Register read/write APIs */
-static inline void cpt_write_csr64(u8 __iomem *hw_addr, u64 offset,
-				   u64 val)
-{
-	writeq(val, hw_addr + offset);
-}
-
-static inline u64 cpt_read_csr64(u8 __iomem *hw_addr, u64 offset)
-{
-	return readq(hw_addr + offset);
-}
-#endif /* __CPT_COMMON_H */
diff --git a/drivers/crypto/cavium/cpt/cptpf.h b/drivers/crypto/cavium/cpt/cptpf.h
deleted file mode 100644
index c0556c5f63c9..000000000000
--- a/drivers/crypto/cavium/cpt/cptpf.h
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-
-#ifndef __CPTPF_H
-#define __CPTPF_H
-
-#include "cpt_common.h"
-
-#define CSR_DELAY 30
-#define CPT_MAX_CORE_GROUPS 8
-#define CPT_MAX_SE_CORES 10
-#define CPT_MAX_AE_CORES 6
-#define CPT_MAX_TOTAL_CORES (CPT_MAX_SE_CORES + CPT_MAX_AE_CORES)
-#define CPT_MAX_VF_NUM 16
-#define	CPT_PF_MSIX_VECTORS 3
-#define CPT_PF_INT_VEC_E_MBOXX(a) (0x02 + (a))
-#define CPT_UCODE_VERSION_SZ 32
-struct cpt_device;
-
-struct microcode {
-	u8 is_mc_valid;
-	u8 is_ae;
-	u8 group;
-	u8 num_cores;
-	u32 code_size;
-	u64 core_mask;
-	u8 version[CPT_UCODE_VERSION_SZ];
-	/* Base info */
-	dma_addr_t phys_base;
-	void *code;
-};
-
-struct cpt_vf_info {
-	u8 state;
-	u8 priority;
-	u8 id;
-	u32 qlen;
-};
-
-/**
- * cpt device structure
- */
-struct cpt_device {
-	u16 flags;	/* Flags to hold device status bits */
-	u8 num_vf_en; /* Number of VFs enabled (0...CPT_MAX_VF_NUM) */
-	struct cpt_vf_info vfinfo[CPT_MAX_VF_NUM]; /* Per VF info */
-
-	void __iomem *reg_base; /* Register start address */
-	struct pci_dev *pdev; /* pci device handle */
-
-	struct microcode mcode[CPT_MAX_CORE_GROUPS];
-	u8 next_mc_idx; /* next microcode index */
-	u8 next_group;
-	u8 max_se_cores;
-	u8 max_ae_cores;
-};
-
-void cpt_mbox_intr_handler(struct cpt_device *cpt, int mbx);
-#endif /* __CPTPF_H */
diff --git a/drivers/crypto/cavium/cpt/cptpf_mbox.c b/drivers/crypto/cavium/cpt/cptpf_mbox.c
deleted file mode 100644
index 20f2c6ee46a5..000000000000
--- a/drivers/crypto/cavium/cpt/cptpf_mbox.c
+++ /dev/null
@@ -1,163 +0,0 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-#include <linux/module.h>
-#include "cptpf.h"
-
-static void cpt_send_msg_to_vf(struct cpt_device *cpt, int vf,
-			       struct cpt_mbox *mbx)
-{
-	/* Writing mbox(0) causes interrupt */
-	cpt_write_csr64(cpt->reg_base, CPTX_PF_VFX_MBOXX(0, vf, 1),
-			mbx->data);
-	cpt_write_csr64(cpt->reg_base, CPTX_PF_VFX_MBOXX(0, vf, 0), mbx->msg);
-}
-
-/* ACKs VF's mailbox message
- * @vf: VF to which ACK to be sent
- */
-static void cpt_mbox_send_ack(struct cpt_device *cpt, int vf,
-			      struct cpt_mbox *mbx)
-{
-	mbx->data = 0ull;
-	mbx->msg = CPT_MBOX_MSG_TYPE_ACK;
-	cpt_send_msg_to_vf(cpt, vf, mbx);
-}
-
-static void cpt_clear_mbox_intr(struct cpt_device *cpt, u32 vf)
-{
-	/* W1C for the VF */
-	cpt_write_csr64(cpt->reg_base, CPTX_PF_MBOX_INTX(0, 0), (1 << vf));
-}
-
-/*
- *  Configure QLEN/Chunk sizes for VF
- */
-static void cpt_cfg_qlen_for_vf(struct cpt_device *cpt, int vf, u32 size)
-{
-	union cptx_pf_qx_ctl pf_qx_ctl;
-
-	pf_qx_ctl.u = cpt_read_csr64(cpt->reg_base, CPTX_PF_QX_CTL(0, vf));
-	pf_qx_ctl.s.size = size;
-	pf_qx_ctl.s.cont_err = true;
-	cpt_write_csr64(cpt->reg_base, CPTX_PF_QX_CTL(0, vf), pf_qx_ctl.u);
-}
-
-/*
- * Configure VQ priority
- */
-static void cpt_cfg_vq_priority(struct cpt_device *cpt, int vf, u32 pri)
-{
-	union cptx_pf_qx_ctl pf_qx_ctl;
-
-	pf_qx_ctl.u = cpt_read_csr64(cpt->reg_base, CPTX_PF_QX_CTL(0, vf));
-	pf_qx_ctl.s.pri = pri;
-	cpt_write_csr64(cpt->reg_base, CPTX_PF_QX_CTL(0, vf), pf_qx_ctl.u);
-}
-
-static int cpt_bind_vq_to_grp(struct cpt_device *cpt, u8 q, u8 grp)
-{
-	struct microcode *mcode = cpt->mcode;
-	union cptx_pf_qx_ctl pf_qx_ctl;
-	struct device *dev = &cpt->pdev->dev;
-
-	if (q >= CPT_MAX_VF_NUM) {
-		dev_err(dev, "Queues are more than cores in the group");
-		return -EINVAL;
-	}
-	if (grp >= CPT_MAX_CORE_GROUPS) {
-		dev_err(dev, "Request group is more than possible groups");
-		return -EINVAL;
-	}
-	if (grp >= cpt->next_mc_idx) {
-		dev_err(dev, "Request group is higher than available functional groups");
-		return -EINVAL;
-	}
-	pf_qx_ctl.u = cpt_read_csr64(cpt->reg_base, CPTX_PF_QX_CTL(0, q));
-	pf_qx_ctl.s.grp = mcode[grp].group;
-	cpt_write_csr64(cpt->reg_base, CPTX_PF_QX_CTL(0, q), pf_qx_ctl.u);
-	dev_dbg(dev, "VF %d TYPE %s", q, (mcode[grp].is_ae ? "AE" : "SE"));
-
-	return mcode[grp].is_ae ? AE_TYPES : SE_TYPES;
-}
-
-/* Interrupt handler to handle mailbox messages from VFs */
-static void cpt_handle_mbox_intr(struct cpt_device *cpt, int vf)
-{
-	struct cpt_vf_info *vfx = &cpt->vfinfo[vf];
-	struct cpt_mbox mbx = {};
-	int vftype;
-	struct device *dev = &cpt->pdev->dev;
-	/*
-	 * MBOX[0] contains msg
-	 * MBOX[1] contains data
-	 */
-	mbx.msg  = cpt_read_csr64(cpt->reg_base, CPTX_PF_VFX_MBOXX(0, vf, 0));
-	mbx.data = cpt_read_csr64(cpt->reg_base, CPTX_PF_VFX_MBOXX(0, vf, 1));
-	dev_dbg(dev, "%s: Mailbox msg 0x%llx from VF%d", __func__, mbx.msg, vf);
-	switch (mbx.msg) {
-	case CPT_MSG_VF_UP:
-		vfx->state = VF_STATE_UP;
-		try_module_get(THIS_MODULE);
-		cpt_mbox_send_ack(cpt, vf, &mbx);
-		break;
-	case CPT_MSG_READY:
-		mbx.msg  = CPT_MSG_READY;
-		mbx.data = vf;
-		cpt_send_msg_to_vf(cpt, vf, &mbx);
-		break;
-	case CPT_MSG_VF_DOWN:
-		/* First msg in VF teardown sequence */
-		vfx->state = VF_STATE_DOWN;
-		module_put(THIS_MODULE);
-		cpt_mbox_send_ack(cpt, vf, &mbx);
-		break;
-	case CPT_MSG_QLEN:
-		vfx->qlen = mbx.data;
-		cpt_cfg_qlen_for_vf(cpt, vf, vfx->qlen);
-		cpt_mbox_send_ack(cpt, vf, &mbx);
-		break;
-	case CPT_MSG_QBIND_GRP:
-		vftype = cpt_bind_vq_to_grp(cpt, vf, (u8)mbx.data);
-		if ((vftype != AE_TYPES) && (vftype != SE_TYPES))
-			dev_err(dev, "Queue %d binding to group %llu failed",
-				vf, mbx.data);
-		else {
-			dev_dbg(dev, "Queue %d binding to group %llu successful",
-				vf, mbx.data);
-			mbx.msg = CPT_MSG_QBIND_GRP;
-			mbx.data = vftype;
-			cpt_send_msg_to_vf(cpt, vf, &mbx);
-		}
-		break;
-	case CPT_MSG_VQ_PRIORITY:
-		vfx->priority = mbx.data;
-		cpt_cfg_vq_priority(cpt, vf, vfx->priority);
-		cpt_mbox_send_ack(cpt, vf, &mbx);
-		break;
-	default:
-		dev_err(&cpt->pdev->dev, "Invalid msg from VF%d, msg 0x%llx\n",
-			vf, mbx.msg);
-		break;
-	}
-}
-
-void cpt_mbox_intr_handler (struct cpt_device *cpt, int mbx)
-{
-	u64 intr;
-	u8  vf;
-
-	intr = cpt_read_csr64(cpt->reg_base, CPTX_PF_MBOX_INTX(0, 0));
-	dev_dbg(&cpt->pdev->dev, "PF interrupt Mbox%d 0x%llx\n", mbx, intr);
-	for (vf = 0; vf < CPT_MAX_VF_NUM; vf++) {
-		if (intr & (1ULL << vf)) {
-			dev_dbg(&cpt->pdev->dev, "Intr from VF %d\n", vf);
-			cpt_handle_mbox_intr(cpt, vf);
-			cpt_clear_mbox_intr(cpt, vf);
-		}
-	}
-}
diff --git a/drivers/crypto/cavium/cpt/cptvf_algs.c b/drivers/crypto/cavium/cpt/cptvf_algs.c
deleted file mode 100644
index df21d996db7e..000000000000
--- a/drivers/crypto/cavium/cpt/cptvf_algs.c
+++ /dev/null
@@ -1,524 +0,0 @@
-
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-
-#include <crypto/aes.h>
-#include <crypto/algapi.h>
-#include <crypto/authenc.h>
-#include <crypto/cryptd.h>
-#include <crypto/crypto_wq.h>
-#include <crypto/des.h>
-#include <crypto/xts.h>
-#include <linux/crypto.h>
-#include <linux/err.h>
-#include <linux/list.h>
-#include <linux/scatterlist.h>
-
-#include "cptvf.h"
-#include "cptvf_algs.h"
-
-struct cpt_device_handle {
-	void *cdev[MAX_DEVICES];
-	u32 dev_count;
-};
-
-static struct cpt_device_handle dev_handle;
-
-static void cvm_callback(u32 status, void *arg)
-{
-	struct crypto_async_request *req = (struct crypto_async_request *)arg;
-
-	req->complete(req, !status);
-}
-
-static inline void update_input_iv(struct cpt_request_info *req_info,
-				   u8 *iv, u32 enc_iv_len,
-				   u32 *argcnt)
-{
-	/* Setting the iv information */
-	req_info->in[*argcnt].vptr = (void *)iv;
-	req_info->in[*argcnt].size = enc_iv_len;
-	req_info->req.dlen += enc_iv_len;
-
-	++(*argcnt);
-}
-
-static inline void update_output_iv(struct cpt_request_info *req_info,
-				    u8 *iv, u32 enc_iv_len,
-				    u32 *argcnt)
-{
-	/* Setting the iv information */
-	req_info->out[*argcnt].vptr = (void *)iv;
-	req_info->out[*argcnt].size = enc_iv_len;
-	req_info->rlen += enc_iv_len;
-
-	++(*argcnt);
-}
-
-static inline void update_input_data(struct cpt_request_info *req_info,
-				     struct scatterlist *inp_sg,
-				     u32 nbytes, u32 *argcnt)
-{
-	req_info->req.dlen += nbytes;
-
-	while (nbytes) {
-		u32 len = min(nbytes, inp_sg->length);
-		u8 *ptr = sg_virt(inp_sg);
-
-		req_info->in[*argcnt].vptr = (void *)ptr;
-		req_info->in[*argcnt].size = len;
-		nbytes -= len;
-
-		++(*argcnt);
-		++inp_sg;
-	}
-}
-
-static inline void update_output_data(struct cpt_request_info *req_info,
-				      struct scatterlist *outp_sg,
-				      u32 nbytes, u32 *argcnt)
-{
-	req_info->rlen += nbytes;
-
-	while (nbytes) {
-		u32 len = min(nbytes, outp_sg->length);
-		u8 *ptr = sg_virt(outp_sg);
-
-		req_info->out[*argcnt].vptr = (void *)ptr;
-		req_info->out[*argcnt].size = len;
-		nbytes -= len;
-		++(*argcnt);
-		++outp_sg;
-	}
-}
-
-static inline u32 create_ctx_hdr(struct ablkcipher_request *req, u32 enc,
-				 u32 *argcnt)
-{
-	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
-	struct cvm_enc_ctx *ctx = crypto_ablkcipher_ctx(tfm);
-	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
-	struct fc_context *fctx = &rctx->fctx;
-	u64 *offset_control = &rctx->control_word;
-	u32 enc_iv_len = crypto_ablkcipher_ivsize(tfm);
-	struct cpt_request_info *req_info = &rctx->cpt_req;
-	u64 *ctrl_flags = NULL;
-
-	req_info->ctrl.s.grp = 0;
-	req_info->ctrl.s.dma_mode = DMA_GATHER_SCATTER;
-	req_info->ctrl.s.se_req = SE_CORE_REQ;
-
-	req_info->req.opcode.s.major = MAJOR_OP_FC |
-					DMA_MODE_FLAG(DMA_GATHER_SCATTER);
-	if (enc)
-		req_info->req.opcode.s.minor = 2;
-	else
-		req_info->req.opcode.s.minor = 3;
-
-	req_info->req.param1 = req->nbytes; /* Encryption Data length */
-	req_info->req.param2 = 0; /*Auth data length */
-
-	fctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;
-	fctx->enc.enc_ctrl.e.aes_key = ctx->key_type;
-	fctx->enc.enc_ctrl.e.iv_source = FROM_DPTR;
-
-	if (ctx->cipher_type == AES_XTS)
-		memcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len * 2);
-	else
-		memcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len);
-	ctrl_flags = (u64 *)&fctx->enc.enc_ctrl.flags;
-	*ctrl_flags = cpu_to_be64(*ctrl_flags);
-
-	*offset_control = cpu_to_be64(((u64)(enc_iv_len) << 16));
-	/* Storing  Packet Data Information in offset
-	 * Control Word First 8 bytes
-	 */
-	req_info->in[*argcnt].vptr = (u8 *)offset_control;
-	req_info->in[*argcnt].size = CONTROL_WORD_LEN;
-	req_info->req.dlen += CONTROL_WORD_LEN;
-	++(*argcnt);
-
-	req_info->in[*argcnt].vptr = (u8 *)fctx;
-	req_info->in[*argcnt].size = sizeof(struct fc_context);
-	req_info->req.dlen += sizeof(struct fc_context);
-
-	++(*argcnt);
-
-	return 0;
-}
-
-static inline u32 create_input_list(struct ablkcipher_request  *req, u32 enc,
-				    u32 enc_iv_len)
-{
-	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
-	struct cpt_request_info *req_info = &rctx->cpt_req;
-	u32 argcnt =  0;
-
-	create_ctx_hdr(req, enc, &argcnt);
-	update_input_iv(req_info, req->info, enc_iv_len, &argcnt);
-	update_input_data(req_info, req->src, req->nbytes, &argcnt);
-	req_info->incnt = argcnt;
-
-	return 0;
-}
-
-static inline void store_cb_info(struct ablkcipher_request *req,
-				 struct cpt_request_info *req_info)
-{
-	req_info->callback = (void *)cvm_callback;
-	req_info->callback_arg = (void *)&req->base;
-}
-
-static inline void create_output_list(struct ablkcipher_request *req,
-				      u32 enc_iv_len)
-{
-	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
-	struct cpt_request_info *req_info = &rctx->cpt_req;
-	u32 argcnt = 0;
-
-	/* OUTPUT Buffer Processing
-	 * AES encryption/decryption output would be
-	 * received in the following format
-	 *
-	 * ------IV--------|------ENCRYPTED/DECRYPTED DATA-----|
-	 * [ 16 Bytes/     [   Request Enc/Dec/ DATA Len AES CBC ]
-	 */
-	/* Reading IV information */
-	update_output_iv(req_info, req->info, enc_iv_len, &argcnt);
-	update_output_data(req_info, req->dst, req->nbytes, &argcnt);
-	req_info->outcnt = argcnt;
-}
-
-static inline int cvm_enc_dec(struct ablkcipher_request *req, u32 enc)
-{
-	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
-	struct cvm_req_ctx *rctx = ablkcipher_request_ctx(req);
-	u32 enc_iv_len = crypto_ablkcipher_ivsize(tfm);
-	struct fc_context *fctx = &rctx->fctx;
-	struct cpt_request_info *req_info = &rctx->cpt_req;
-	void *cdev = NULL;
-	int status;
-
-	memset(req_info, 0, sizeof(struct cpt_request_info));
-	memset(fctx, 0, sizeof(struct fc_context));
-	create_input_list(req, enc, enc_iv_len);
-	create_output_list(req, enc_iv_len);
-	store_cb_info(req, req_info);
-	cdev = dev_handle.cdev[smp_processor_id()];
-	status = cptvf_do_request(cdev, req_info);
-	/* We perform an asynchronous send and once
-	 * the request is completed the driver would
-	 * intimate through  registered call back functions
-	 */
-
-	if (status)
-		return status;
-	else
-		return -EINPROGRESS;
-}
-
-static int cvm_encrypt(struct ablkcipher_request *req)
-{
-	return cvm_enc_dec(req, true);
-}
-
-static int cvm_decrypt(struct ablkcipher_request *req)
-{
-	return cvm_enc_dec(req, false);
-}
-
-static int cvm_xts_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-		   u32 keylen)
-{
-	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);
-	struct cvm_enc_ctx *ctx = crypto_tfm_ctx(tfm);
-	int err;
-	const u8 *key1 = key;
-	const u8 *key2 = key + (keylen / 2);
-
-	err = xts_check_key(tfm, key, keylen);
-	if (err)
-		return err;
-	ctx->key_len = keylen;
-	memcpy(ctx->enc_key, key1, keylen / 2);
-	memcpy(ctx->enc_key + KEY2_OFFSET, key2, keylen / 2);
-	ctx->cipher_type = AES_XTS;
-	switch (ctx->key_len) {
-	case 32:
-		ctx->key_type = AES_128_BIT;
-		break;
-	case 64:
-		ctx->key_type = AES_256_BIT;
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	return 0;
-}
-
-static int cvm_validate_keylen(struct cvm_enc_ctx *ctx, u32 keylen)
-{
-	if ((keylen == 16) || (keylen == 24) || (keylen == 32)) {
-		ctx->key_len = keylen;
-		switch (ctx->key_len) {
-		case 16:
-			ctx->key_type = AES_128_BIT;
-			break;
-		case 24:
-			ctx->key_type = AES_192_BIT;
-			break;
-		case 32:
-			ctx->key_type = AES_256_BIT;
-			break;
-		default:
-			return -EINVAL;
-		}
-
-		if (ctx->cipher_type == DES3_CBC)
-			ctx->key_type = 0;
-
-		return 0;
-	}
-
-	return -EINVAL;
-}
-
-static int cvm_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-		      u32 keylen, u8 cipher_type)
-{
-	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);
-	struct cvm_enc_ctx *ctx = crypto_tfm_ctx(tfm);
-
-	ctx->cipher_type = cipher_type;
-	if (!cvm_validate_keylen(ctx, keylen)) {
-		memcpy(ctx->enc_key, key, keylen);
-		return 0;
-	} else {
-		crypto_ablkcipher_set_flags(cipher,
-					    CRYPTO_TFM_RES_BAD_KEY_LEN);
-		return -EINVAL;
-	}
-}
-
-static int cvm_cbc_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-			      u32 keylen)
-{
-	return cvm_setkey(cipher, key, keylen, AES_CBC);
-}
-
-static int cvm_ecb_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-			      u32 keylen)
-{
-	return cvm_setkey(cipher, key, keylen, AES_ECB);
-}
-
-static int cvm_cfb_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-			      u32 keylen)
-{
-	return cvm_setkey(cipher, key, keylen, AES_CFB);
-}
-
-static int cvm_cbc_des3_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-			       u32 keylen)
-{
-	return cvm_setkey(cipher, key, keylen, DES3_CBC);
-}
-
-static int cvm_ecb_des3_setkey(struct crypto_ablkcipher *cipher, const u8 *key,
-			       u32 keylen)
-{
-	return cvm_setkey(cipher, key, keylen, DES3_ECB);
-}
-
-static int cvm_enc_dec_init(struct crypto_tfm *tfm)
-{
-	struct cvm_enc_ctx *ctx = crypto_tfm_ctx(tfm);
-
-	memset(ctx, 0, sizeof(*ctx));
-	tfm->crt_ablkcipher.reqsize = sizeof(struct cvm_req_ctx) +
-					sizeof(struct ablkcipher_request);
-	/* Additional memory for ablkcipher_request is
-	 * allocated since the cryptd daemon uses
-	 * this memory for request_ctx information
-	 */
-
-	return 0;
-}
-
-struct crypto_alg algs[] = { {
-	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
-	.cra_blocksize = AES_BLOCK_SIZE,
-	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
-	.cra_alignmask = 7,
-	.cra_priority = 4001,
-	.cra_name = "xts(aes)",
-	.cra_driver_name = "cavium-xts-aes",
-	.cra_type = &crypto_ablkcipher_type,
-	.cra_u = {
-		.ablkcipher = {
-			.ivsize = AES_BLOCK_SIZE,
-			.min_keysize = 2 * AES_MIN_KEY_SIZE,
-			.max_keysize = 2 * AES_MAX_KEY_SIZE,
-			.setkey = cvm_xts_setkey,
-			.encrypt = cvm_encrypt,
-			.decrypt = cvm_decrypt,
-		},
-	},
-	.cra_init = cvm_enc_dec_init,
-	.cra_module = THIS_MODULE,
-}, {
-	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
-	.cra_blocksize = AES_BLOCK_SIZE,
-	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
-	.cra_alignmask = 7,
-	.cra_priority = 4001,
-	.cra_name = "cbc(aes)",
-	.cra_driver_name = "cavium-cbc-aes",
-	.cra_type = &crypto_ablkcipher_type,
-	.cra_u = {
-		.ablkcipher = {
-			.ivsize = AES_BLOCK_SIZE,
-			.min_keysize = AES_MIN_KEY_SIZE,
-			.max_keysize = AES_MAX_KEY_SIZE,
-			.setkey = cvm_cbc_aes_setkey,
-			.encrypt = cvm_encrypt,
-			.decrypt = cvm_decrypt,
-		},
-	},
-	.cra_init = cvm_enc_dec_init,
-	.cra_module = THIS_MODULE,
-}, {
-	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
-	.cra_blocksize = AES_BLOCK_SIZE,
-	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
-	.cra_alignmask = 7,
-	.cra_priority = 4001,
-	.cra_name = "ecb(aes)",
-	.cra_driver_name = "cavium-ecb-aes",
-	.cra_type = &crypto_ablkcipher_type,
-	.cra_u = {
-		.ablkcipher = {
-			.ivsize = AES_BLOCK_SIZE,
-			.min_keysize = AES_MIN_KEY_SIZE,
-			.max_keysize = AES_MAX_KEY_SIZE,
-			.setkey = cvm_ecb_aes_setkey,
-			.encrypt = cvm_encrypt,
-			.decrypt = cvm_decrypt,
-		},
-	},
-	.cra_init = cvm_enc_dec_init,
-	.cra_module = THIS_MODULE,
-}, {
-	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
-	.cra_blocksize = AES_BLOCK_SIZE,
-	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
-	.cra_alignmask = 7,
-	.cra_priority = 4001,
-	.cra_name = "cfb(aes)",
-	.cra_driver_name = "cavium-cfb-aes",
-	.cra_type = &crypto_ablkcipher_type,
-	.cra_u = {
-		.ablkcipher = {
-			.ivsize = AES_BLOCK_SIZE,
-			.min_keysize = AES_MIN_KEY_SIZE,
-			.max_keysize = AES_MAX_KEY_SIZE,
-			.setkey = cvm_cfb_aes_setkey,
-			.encrypt = cvm_encrypt,
-			.decrypt = cvm_decrypt,
-		},
-	},
-	.cra_init = cvm_enc_dec_init,
-	.cra_module = THIS_MODULE,
-}, {
-	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
-	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
-	.cra_ctxsize = sizeof(struct cvm_des3_ctx),
-	.cra_alignmask = 7,
-	.cra_priority = 4001,
-	.cra_name = "cbc(des3_ede)",
-	.cra_driver_name = "cavium-cbc-des3_ede",
-	.cra_type = &crypto_ablkcipher_type,
-	.cra_u = {
-		.ablkcipher = {
-			.min_keysize = DES3_EDE_KEY_SIZE,
-			.max_keysize = DES3_EDE_KEY_SIZE,
-			.ivsize = DES_BLOCK_SIZE,
-			.setkey = cvm_cbc_des3_setkey,
-			.encrypt = cvm_encrypt,
-			.decrypt = cvm_decrypt,
-		},
-	},
-	.cra_init = cvm_enc_dec_init,
-	.cra_module = THIS_MODULE,
-}, {
-	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
-	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
-	.cra_ctxsize = sizeof(struct cvm_des3_ctx),
-	.cra_alignmask = 7,
-	.cra_priority = 4001,
-	.cra_name = "ecb(des3_ede)",
-	.cra_driver_name = "cavium-ecb-des3_ede",
-	.cra_type = &crypto_ablkcipher_type,
-	.cra_u = {
-		.ablkcipher = {
-			.min_keysize = DES3_EDE_KEY_SIZE,
-			.max_keysize = DES3_EDE_KEY_SIZE,
-			.ivsize = DES_BLOCK_SIZE,
-			.setkey = cvm_ecb_des3_setkey,
-			.encrypt = cvm_encrypt,
-			.decrypt = cvm_decrypt,
-		},
-	},
-	.cra_init = cvm_enc_dec_init,
-	.cra_module = THIS_MODULE,
-} };
-
-static inline int cav_register_algs(void)
-{
-	int err = 0;
-
-	err = crypto_register_algs(algs, ARRAY_SIZE(algs));
-	if (err)
-		return err;
-
-	return 0;
-}
-
-static inline void cav_unregister_algs(void)
-{
-	crypto_unregister_algs(algs, ARRAY_SIZE(algs));
-}
-
-int cvm_crypto_init(struct cpt_vf *cptvf)
-{
-	struct pci_dev *pdev = cptvf->pdev;
-	u32 dev_count;
-
-	dev_count = dev_handle.dev_count;
-	dev_handle.cdev[dev_count] = cptvf;
-	dev_handle.dev_count++;
-
-	if (dev_count == 3) {
-		if (cav_register_algs()) {
-			dev_err(&pdev->dev, "Error in registering crypto algorithms\n");
-			return -EINVAL;
-		}
-	}
-
-	return 0;
-}
-
-void cvm_crypto_exit(void)
-{
-	u32 dev_count;
-
-	dev_count = --dev_handle.dev_count;
-	if (!dev_count)
-		cav_unregister_algs();
-}
diff --git a/drivers/crypto/cavium/cpt/cptvf_algs.h b/drivers/crypto/cavium/cpt/cptvf_algs.h
deleted file mode 100644
index 902f25751123..000000000000
--- a/drivers/crypto/cavium/cpt/cptvf_algs.h
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-
-#ifndef _CPTVF_ALGS_H_
-#define _CPTVF_ALGS_H_
-
-#include "request_manager.h"
-
-#define MAX_DEVICES 16
-#define MAJOR_OP_FC 0x33
-#define MAX_ENC_KEY_SIZE 32
-#define MAX_HASH_KEY_SIZE 64
-#define MAX_KEY_SIZE (MAX_ENC_KEY_SIZE + MAX_HASH_KEY_SIZE)
-#define CONTROL_WORD_LEN 8
-#define KEY2_OFFSET 48
-
-#define DMA_MODE_FLAG(dma_mode) \
-	(((dma_mode) == DMA_GATHER_SCATTER) ? (1 << 7) : 0)
-
-enum req_type {
-	AE_CORE_REQ,
-	SE_CORE_REQ,
-};
-
-enum cipher_type {
-	DES3_CBC = 0x1,
-	DES3_ECB = 0x2,
-	AES_CBC = 0x3,
-	AES_ECB = 0x4,
-	AES_CFB = 0x5,
-	AES_CTR = 0x6,
-	AES_GCM = 0x7,
-	AES_XTS = 0x8
-};
-
-enum aes_type {
-	AES_128_BIT = 0x1,
-	AES_192_BIT = 0x2,
-	AES_256_BIT = 0x3
-};
-
-union encr_ctrl {
-	u64 flags;
-	struct {
-#if defined(__BIG_ENDIAN_BITFIELD)
-		u64 enc_cipher:4;
-		u64 reserved1:1;
-		u64 aes_key:2;
-		u64 iv_source:1;
-		u64 hash_type:4;
-		u64 reserved2:3;
-		u64 auth_input_type:1;
-		u64 mac_len:8;
-		u64 reserved3:8;
-		u64 encr_offset:16;
-		u64 iv_offset:8;
-		u64 auth_offset:8;
-#else
-		u64 auth_offset:8;
-		u64 iv_offset:8;
-		u64 encr_offset:16;
-		u64 reserved3:8;
-		u64 mac_len:8;
-		u64 auth_input_type:1;
-		u64 reserved2:3;
-		u64 hash_type:4;
-		u64 iv_source:1;
-		u64 aes_key:2;
-		u64 reserved1:1;
-		u64 enc_cipher:4;
-#endif
-	} e;
-};
-
-struct cvm_cipher {
-	const char *name;
-	u8 value;
-};
-
-struct enc_context {
-	union encr_ctrl enc_ctrl;
-	u8 encr_key[32];
-	u8 encr_iv[16];
-};
-
-struct fchmac_context {
-	u8 ipad[64];
-	u8 opad[64]; /* or OPAD */
-};
-
-struct fc_context {
-	struct enc_context enc;
-	struct fchmac_context hmac;
-};
-
-struct cvm_enc_ctx {
-	u32 key_len;
-	u8 enc_key[MAX_KEY_SIZE];
-	u8 cipher_type:4;
-	u8 key_type:2;
-};
-
-struct cvm_des3_ctx {
-	u32 key_len;
-	u8 des3_key[MAX_KEY_SIZE];
-};
-
-struct cvm_req_ctx {
-	struct cpt_request_info cpt_req;
-	u64 control_word;
-	struct fc_context fctx;
-};
-
-int cptvf_do_request(void *cptvf, struct cpt_request_info *req);
-#endif /*_CPTVF_ALGS_H_*/
diff --git a/drivers/crypto/cavium/cpt/cptvf_reqmanager.c b/drivers/crypto/cavium/cpt/cptvf_reqmanager.c
deleted file mode 100644
index b0ba4331944b..000000000000
--- a/drivers/crypto/cavium/cpt/cptvf_reqmanager.c
+++ /dev/null
@@ -1,594 +0,0 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-
-#include "cptvf.h"
-#include "request_manager.h"
-
-/**
- * get_free_pending_entry - get free entry from pending queue
- * @param pqinfo: pending_qinfo structure
- * @param qno: queue number
- */
-static struct pending_entry *get_free_pending_entry(struct pending_queue *q,
-						    int qlen)
-{
-	struct pending_entry *ent = NULL;
-
-	ent = &q->head[q->rear];
-	if (unlikely(ent->busy)) {
-		ent = NULL;
-		goto no_free_entry;
-	}
-
-	q->rear++;
-	if (unlikely(q->rear == qlen))
-		q->rear = 0;
-
-no_free_entry:
-	return ent;
-}
-
-static inline void pending_queue_inc_front(struct pending_qinfo *pqinfo,
-					   int qno)
-{
-	struct pending_queue *queue = &pqinfo->queue[qno];
-
-	queue->front++;
-	if (unlikely(queue->front == pqinfo->qlen))
-		queue->front = 0;
-}
-
-static int setup_sgio_components(struct cpt_vf *cptvf, struct buf_ptr *list,
-				 int buf_count, u8 *buffer)
-{
-	int ret = 0, i, j;
-	int components;
-	struct sglist_component *sg_ptr = NULL;
-	struct pci_dev *pdev = cptvf->pdev;
-
-	if (unlikely(!list)) {
-		dev_err(&pdev->dev, "Input List pointer is NULL\n");
-		return -EFAULT;
-	}
-
-	for (i = 0; i < buf_count; i++) {
-		if (likely(list[i].vptr)) {
-			list[i].dma_addr = dma_map_single(&pdev->dev,
-							  list[i].vptr,
-							  list[i].size,
-							  DMA_BIDIRECTIONAL);
-			if (unlikely(dma_mapping_error(&pdev->dev,
-						       list[i].dma_addr))) {
-				dev_err(&pdev->dev, "DMA map kernel buffer failed for component: %d\n",
-					i);
-				ret = -EIO;
-				goto sg_cleanup;
-			}
-		}
-	}
-
-	components = buf_count / 4;
-	sg_ptr = (struct sglist_component *)buffer;
-	for (i = 0; i < components; i++) {
-		sg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);
-		sg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);
-		sg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);
-		sg_ptr->u.s.len3 = cpu_to_be16(list[i * 4 + 3].size);
-		sg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);
-		sg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);
-		sg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);
-		sg_ptr->ptr3 = cpu_to_be64(list[i * 4 + 3].dma_addr);
-		sg_ptr++;
-	}
-
-	components = buf_count % 4;
-
-	switch (components) {
-	case 3:
-		sg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);
-		sg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);
-		/* Fall through */
-	case 2:
-		sg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);
-		sg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);
-		/* Fall through */
-	case 1:
-		sg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);
-		sg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);
-		break;
-	default:
-		break;
-	}
-
-	return ret;
-
-sg_cleanup:
-	for (j = 0; j < i; j++) {
-		if (list[j].dma_addr) {
-			dma_unmap_single(&pdev->dev, list[i].dma_addr,
-					 list[i].size, DMA_BIDIRECTIONAL);
-		}
-
-		list[j].dma_addr = 0;
-	}
-
-	return ret;
-}
-
-static inline int setup_sgio_list(struct cpt_vf *cptvf,
-				  struct cpt_info_buffer *info,
-				  struct cpt_request_info *req)
-{
-	u16 g_sz_bytes = 0, s_sz_bytes = 0;
-	int ret = 0;
-	struct pci_dev *pdev = cptvf->pdev;
-
-	if (req->incnt > MAX_SG_IN_CNT || req->outcnt > MAX_SG_OUT_CNT) {
-		dev_err(&pdev->dev, "Request SG components are higher than supported\n");
-		ret = -EINVAL;
-		goto  scatter_gather_clean;
-	}
-
-	/* Setup gather (input) components */
-	g_sz_bytes = ((req->incnt + 3) / 4) * sizeof(struct sglist_component);
-	info->gather_components = kzalloc(g_sz_bytes, GFP_KERNEL);
-	if (!info->gather_components) {
-		ret = -ENOMEM;
-		goto  scatter_gather_clean;
-	}
-
-	ret = setup_sgio_components(cptvf, req->in,
-				    req->incnt,
-				    info->gather_components);
-	if (ret) {
-		dev_err(&pdev->dev, "Failed to setup gather list\n");
-		ret = -EFAULT;
-		goto  scatter_gather_clean;
-	}
-
-	/* Setup scatter (output) components */
-	s_sz_bytes = ((req->outcnt + 3) / 4) * sizeof(struct sglist_component);
-	info->scatter_components = kzalloc(s_sz_bytes, GFP_KERNEL);
-	if (!info->scatter_components) {
-		ret = -ENOMEM;
-		goto  scatter_gather_clean;
-	}
-
-	ret = setup_sgio_components(cptvf, req->out,
-				    req->outcnt,
-				    info->scatter_components);
-	if (ret) {
-		dev_err(&pdev->dev, "Failed to setup gather list\n");
-		ret = -EFAULT;
-		goto  scatter_gather_clean;
-	}
-
-	/* Create and initialize DPTR */
-	info->dlen = g_sz_bytes + s_sz_bytes + SG_LIST_HDR_SIZE;
-	info->in_buffer = kzalloc(info->dlen, GFP_KERNEL);
-	if (!info->in_buffer) {
-		ret = -ENOMEM;
-		goto  scatter_gather_clean;
-	}
-
-	((u16 *)info->in_buffer)[0] = req->outcnt;
-	((u16 *)info->in_buffer)[1] = req->incnt;
-	((u16 *)info->in_buffer)[2] = 0;
-	((u16 *)info->in_buffer)[3] = 0;
-	*(u64 *)info->in_buffer = cpu_to_be64p((u64 *)info->in_buffer);
-
-	memcpy(&info->in_buffer[8], info->gather_components,
-	       g_sz_bytes);
-	memcpy(&info->in_buffer[8 + g_sz_bytes],
-	       info->scatter_components, s_sz_bytes);
-
-	info->dptr_baddr = dma_map_single(&pdev->dev,
-					  (void *)info->in_buffer,
-					  info->dlen,
-					  DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(&pdev->dev, info->dptr_baddr)) {
-		dev_err(&pdev->dev, "Mapping DPTR Failed %d\n", info->dlen);
-		ret = -EIO;
-		goto  scatter_gather_clean;
-	}
-
-	/* Create and initialize RPTR */
-	info->out_buffer = kzalloc(COMPLETION_CODE_SIZE, GFP_KERNEL);
-	if (!info->out_buffer) {
-		ret = -ENOMEM;
-		goto scatter_gather_clean;
-	}
-
-	*((u64 *)info->out_buffer) = ~((u64)COMPLETION_CODE_INIT);
-	info->alternate_caddr = (u64 *)info->out_buffer;
-	info->rptr_baddr = dma_map_single(&pdev->dev,
-					  (void *)info->out_buffer,
-					  COMPLETION_CODE_SIZE,
-					  DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(&pdev->dev, info->rptr_baddr)) {
-		dev_err(&pdev->dev, "Mapping RPTR Failed %d\n",
-			COMPLETION_CODE_SIZE);
-		ret = -EIO;
-		goto  scatter_gather_clean;
-	}
-
-	return 0;
-
-scatter_gather_clean:
-	return ret;
-}
-
-int send_cpt_command(struct cpt_vf *cptvf, union cpt_inst_s *cmd,
-		     u32 qno)
-{
-	struct pci_dev *pdev = cptvf->pdev;
-	struct command_qinfo *qinfo = NULL;
-	struct command_queue *queue;
-	struct command_chunk *chunk;
-	u8 *ent;
-	int ret = 0;
-
-	if (unlikely(qno >= cptvf->nr_queues)) {
-		dev_err(&pdev->dev, "Invalid queue (qno: %d, nr_queues: %d)\n",
-			qno, cptvf->nr_queues);
-		return -EINVAL;
-	}
-
-	qinfo = &cptvf->cqinfo;
-	queue = &qinfo->queue[qno];
-	/* lock commad queue */
-	spin_lock(&queue->lock);
-	ent = &queue->qhead->head[queue->idx * qinfo->cmd_size];
-	memcpy(ent, (void *)cmd, qinfo->cmd_size);
-
-	if (++queue->idx >= queue->qhead->size / 64) {
-		struct hlist_node *node;
-
-		hlist_for_each(node, &queue->chead) {
-			chunk = hlist_entry(node, struct command_chunk,
-					    nextchunk);
-			if (chunk == queue->qhead) {
-				continue;
-			} else {
-				queue->qhead = chunk;
-				break;
-			}
-		}
-		queue->idx = 0;
-	}
-	/* make sure all memory stores are done before ringing doorbell */
-	smp_wmb();
-	cptvf_write_vq_doorbell(cptvf, 1);
-	/* unlock command queue */
-	spin_unlock(&queue->lock);
-
-	return ret;
-}
-
-void do_request_cleanup(struct cpt_vf *cptvf,
-			struct cpt_info_buffer *info)
-{
-	int i;
-	struct pci_dev *pdev = cptvf->pdev;
-	struct cpt_request_info *req;
-
-	if (info->dptr_baddr)
-		dma_unmap_single(&pdev->dev, info->dptr_baddr,
-				 info->dlen, DMA_BIDIRECTIONAL);
-
-	if (info->rptr_baddr)
-		dma_unmap_single(&pdev->dev, info->rptr_baddr,
-				 COMPLETION_CODE_SIZE, DMA_BIDIRECTIONAL);
-
-	if (info->comp_baddr)
-		dma_unmap_single(&pdev->dev, info->comp_baddr,
-				 sizeof(union cpt_res_s), DMA_BIDIRECTIONAL);
-
-	if (info->req) {
-		req = info->req;
-		for (i = 0; i < req->outcnt; i++) {
-			if (req->out[i].dma_addr)
-				dma_unmap_single(&pdev->dev,
-						 req->out[i].dma_addr,
-						 req->out[i].size,
-						 DMA_BIDIRECTIONAL);
-		}
-
-		for (i = 0; i < req->incnt; i++) {
-			if (req->in[i].dma_addr)
-				dma_unmap_single(&pdev->dev,
-						 req->in[i].dma_addr,
-						 req->in[i].size,
-						 DMA_BIDIRECTIONAL);
-		}
-	}
-
-	if (info->scatter_components)
-		kzfree(info->scatter_components);
-
-	if (info->gather_components)
-		kzfree(info->gather_components);
-
-	if (info->out_buffer)
-		kzfree(info->out_buffer);
-
-	if (info->in_buffer)
-		kzfree(info->in_buffer);
-
-	if (info->completion_addr)
-		kzfree((void *)info->completion_addr);
-
-	kzfree(info);
-}
-
-void do_post_process(struct cpt_vf *cptvf, struct cpt_info_buffer *info)
-{
-	struct pci_dev *pdev = cptvf->pdev;
-
-	if (!info) {
-		dev_err(&pdev->dev, "incorrect cpt_info_buffer for post processing\n");
-		return;
-	}
-
-	do_request_cleanup(cptvf, info);
-}
-
-static inline void process_pending_queue(struct cpt_vf *cptvf,
-					 struct pending_qinfo *pqinfo,
-					 int qno)
-{
-	struct pci_dev *pdev = cptvf->pdev;
-	struct pending_queue *pqueue = &pqinfo->queue[qno];
-	struct pending_entry *pentry = NULL;
-	struct cpt_info_buffer *info = NULL;
-	union cpt_res_s *status = NULL;
-	unsigned char ccode;
-
-	while (1) {
-		spin_lock_bh(&pqueue->lock);
-		pentry = &pqueue->head[pqueue->front];
-		if (unlikely(!pentry->busy)) {
-			spin_unlock_bh(&pqueue->lock);
-			break;
-		}
-
-		info = (struct cpt_info_buffer *)pentry->post_arg;
-		if (unlikely(!info)) {
-			dev_err(&pdev->dev, "Pending Entry post arg NULL\n");
-			pending_queue_inc_front(pqinfo, qno);
-			spin_unlock_bh(&pqueue->lock);
-			continue;
-		}
-
-		status = (union cpt_res_s *)pentry->completion_addr;
-		ccode = status->s.compcode;
-		if ((status->s.compcode == CPT_COMP_E_FAULT) ||
-		    (status->s.compcode == CPT_COMP_E_SWERR)) {
-			dev_err(&pdev->dev, "Request failed with %s\n",
-				(status->s.compcode == CPT_COMP_E_FAULT) ?
-				"DMA Fault" : "Software error");
-			pentry->completion_addr = NULL;
-			pentry->busy = false;
-			atomic64_dec((&pqueue->pending_count));
-			pentry->post_arg = NULL;
-			pending_queue_inc_front(pqinfo, qno);
-			do_request_cleanup(cptvf, info);
-			spin_unlock_bh(&pqueue->lock);
-			break;
-		} else if (status->s.compcode == COMPLETION_CODE_INIT) {
-			/* check for timeout */
-			if (time_after_eq(jiffies,
-					  (info->time_in +
-					  (CPT_COMMAND_TIMEOUT * HZ)))) {
-				dev_err(&pdev->dev, "Request timed out");
-				pentry->completion_addr = NULL;
-				pentry->busy = false;
-				atomic64_dec((&pqueue->pending_count));
-				pentry->post_arg = NULL;
-				pending_queue_inc_front(pqinfo, qno);
-				do_request_cleanup(cptvf, info);
-				spin_unlock_bh(&pqueue->lock);
-				break;
-			} else if ((*info->alternate_caddr ==
-				(~COMPLETION_CODE_INIT)) &&
-				(info->extra_time < TIME_IN_RESET_COUNT)) {
-				info->time_in = jiffies;
-				info->extra_time++;
-				spin_unlock_bh(&pqueue->lock);
-				break;
-			}
-		}
-
-		pentry->completion_addr = NULL;
-		pentry->busy = false;
-		pentry->post_arg = NULL;
-		atomic64_dec((&pqueue->pending_count));
-		pending_queue_inc_front(pqinfo, qno);
-		spin_unlock_bh(&pqueue->lock);
-
-		do_post_process(info->cptvf, info);
-		/*
-		 * Calling callback after we find
-		 * that the request has been serviced
-		 */
-		pentry->callback(ccode, pentry->callback_arg);
-	}
-}
-
-int process_request(struct cpt_vf *cptvf, struct cpt_request_info *req)
-{
-	int ret = 0, clear = 0, queue = 0;
-	struct cpt_info_buffer *info = NULL;
-	struct cptvf_request *cpt_req = NULL;
-	union ctrl_info *ctrl = NULL;
-	union cpt_res_s *result = NULL;
-	struct pending_entry *pentry = NULL;
-	struct pending_queue *pqueue = NULL;
-	struct pci_dev *pdev = cptvf->pdev;
-	u8 group = 0;
-	struct cpt_vq_command vq_cmd;
-	union cpt_inst_s cptinst;
-
-	info = kzalloc(sizeof(*info), GFP_KERNEL);
-	if (unlikely(!info)) {
-		dev_err(&pdev->dev, "Unable to allocate memory for info_buffer\n");
-		return -ENOMEM;
-	}
-
-	cpt_req = (struct cptvf_request *)&req->req;
-	ctrl = (union ctrl_info *)&req->ctrl;
-
-	info->cptvf = cptvf;
-	group = ctrl->s.grp;
-	ret = setup_sgio_list(cptvf, info, req);
-	if (ret) {
-		dev_err(&pdev->dev, "Setting up SG list failed");
-		goto request_cleanup;
-	}
-
-	cpt_req->dlen = info->dlen;
-	/*
-	 * Get buffer for union cpt_res_s response
-	 * structure and its physical address
-	 */
-	info->completion_addr = kzalloc(sizeof(union cpt_res_s), GFP_KERNEL);
-	if (unlikely(!info->completion_addr)) {
-		dev_err(&pdev->dev, "Unable to allocate memory for completion_addr\n");
-		ret = -ENOMEM;
-		goto request_cleanup;
-	}
-
-	result = (union cpt_res_s *)info->completion_addr;
-	result->s.compcode = COMPLETION_CODE_INIT;
-	info->comp_baddr = dma_map_single(&pdev->dev,
-					       (void *)info->completion_addr,
-					       sizeof(union cpt_res_s),
-					       DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(&pdev->dev, info->comp_baddr)) {
-		dev_err(&pdev->dev, "mapping compptr Failed %lu\n",
-			sizeof(union cpt_res_s));
-		ret = -EFAULT;
-		goto  request_cleanup;
-	}
-
-	/* Fill the VQ command */
-	vq_cmd.cmd.u64 = 0;
-	vq_cmd.cmd.s.opcode = cpu_to_be16(cpt_req->opcode.flags);
-	vq_cmd.cmd.s.param1 = cpu_to_be16(cpt_req->param1);
-	vq_cmd.cmd.s.param2 = cpu_to_be16(cpt_req->param2);
-	vq_cmd.cmd.s.dlen   = cpu_to_be16(cpt_req->dlen);
-
-	/* 64-bit swap for microcode data reads, not needed for addresses*/
-	vq_cmd.cmd.u64 = cpu_to_be64(vq_cmd.cmd.u64);
-	vq_cmd.dptr = info->dptr_baddr;
-	vq_cmd.rptr = info->rptr_baddr;
-	vq_cmd.cptr.u64 = 0;
-	vq_cmd.cptr.s.grp = group;
-	/* Get Pending Entry to submit command */
-	/* Always queue 0, because 1 queue per VF */
-	queue = 0;
-	pqueue = &cptvf->pqinfo.queue[queue];
-
-	if (atomic64_read(&pqueue->pending_count) > PENDING_THOLD) {
-		dev_err(&pdev->dev, "pending threshold reached\n");
-		process_pending_queue(cptvf, &cptvf->pqinfo, queue);
-	}
-
-get_pending_entry:
-	spin_lock_bh(&pqueue->lock);
-	pentry = get_free_pending_entry(pqueue, cptvf->pqinfo.qlen);
-	if (unlikely(!pentry)) {
-		spin_unlock_bh(&pqueue->lock);
-		if (clear == 0) {
-			process_pending_queue(cptvf, &cptvf->pqinfo, queue);
-			clear = 1;
-			goto get_pending_entry;
-		}
-		dev_err(&pdev->dev, "Get free entry failed\n");
-		dev_err(&pdev->dev, "queue: %d, rear: %d, front: %d\n",
-			queue, pqueue->rear, pqueue->front);
-		ret = -EFAULT;
-		goto request_cleanup;
-	}
-
-	pentry->completion_addr = info->completion_addr;
-	pentry->post_arg = (void *)info;
-	pentry->callback = req->callback;
-	pentry->callback_arg = req->callback_arg;
-	info->pentry = pentry;
-	pentry->busy = true;
-	atomic64_inc(&pqueue->pending_count);
-
-	/* Send CPT command */
-	info->pentry = pentry;
-	info->time_in = jiffies;
-	info->req = req;
-
-	/* Create the CPT_INST_S type command for HW intrepretation */
-	cptinst.s.doneint = true;
-	cptinst.s.res_addr = (u64)info->comp_baddr;
-	cptinst.s.tag = 0;
-	cptinst.s.grp = 0;
-	cptinst.s.wq_ptr = 0;
-	cptinst.s.ei0 = vq_cmd.cmd.u64;
-	cptinst.s.ei1 = vq_cmd.dptr;
-	cptinst.s.ei2 = vq_cmd.rptr;
-	cptinst.s.ei3 = vq_cmd.cptr.u64;
-
-	ret = send_cpt_command(cptvf, &cptinst, queue);
-	spin_unlock_bh(&pqueue->lock);
-	if (unlikely(ret)) {
-		dev_err(&pdev->dev, "Send command failed for AE\n");
-		ret = -EFAULT;
-		goto request_cleanup;
-	}
-
-	return 0;
-
-request_cleanup:
-	dev_dbg(&pdev->dev, "Failed to submit CPT command\n");
-	do_request_cleanup(cptvf, info);
-
-	return ret;
-}
-
-void vq_post_process(struct cpt_vf *cptvf, u32 qno)
-{
-	struct pci_dev *pdev = cptvf->pdev;
-
-	if (unlikely(qno > cptvf->nr_queues)) {
-		dev_err(&pdev->dev, "Request for post processing on invalid pending queue: %u\n",
-			qno);
-		return;
-	}
-
-	process_pending_queue(cptvf, &cptvf->pqinfo, qno);
-}
-
-int cptvf_do_request(void *vfdev, struct cpt_request_info *req)
-{
-	struct cpt_vf *cptvf = (struct cpt_vf *)vfdev;
-	struct pci_dev *pdev = cptvf->pdev;
-
-	if (!cpt_device_ready(cptvf)) {
-		dev_err(&pdev->dev, "CPT Device is not ready");
-		return -ENODEV;
-	}
-
-	if ((cptvf->vftype == SE_TYPES) && (!req->ctrl.s.se_req)) {
-		dev_err(&pdev->dev, "CPTVF-%d of SE TYPE got AE request",
-			cptvf->vfid);
-		return -EINVAL;
-	} else if ((cptvf->vftype == AE_TYPES) && (req->ctrl.s.se_req)) {
-		dev_err(&pdev->dev, "CPTVF-%d of AE TYPE got SE request",
-			cptvf->vfid);
-		return -EINVAL;
-	}
-
-	return process_request(cptvf, req);
-}
diff --git a/drivers/crypto/cavium/cpt/request_manager.h b/drivers/crypto/cavium/cpt/request_manager.h
deleted file mode 100644
index 80ee074c6e0c..000000000000
--- a/drivers/crypto/cavium/cpt/request_manager.h
+++ /dev/null
@@ -1,147 +0,0 @@
-/*
- * Copyright (C) 2016 Cavium, Inc.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of version 2 of the GNU General Public License
- * as published by the Free Software Foundation.
- */
-
-#ifndef __REQUEST_MANAGER_H
-#define __REQUEST_MANAGER_H
-
-#include "cpt_common.h"
-
-#define TIME_IN_RESET_COUNT  5
-#define COMPLETION_CODE_SIZE 8
-#define COMPLETION_CODE_INIT 0
-#define PENDING_THOLD  100
-#define MAX_SG_IN_CNT 12
-#define MAX_SG_OUT_CNT 13
-#define SG_LIST_HDR_SIZE  8
-#define MAX_BUF_CNT	16
-
-union ctrl_info {
-	u32 flags;
-	struct {
-#if defined(__BIG_ENDIAN_BITFIELD)
-		u32 reserved0:26;
-		u32 grp:3; /* Group bits */
-		u32 dma_mode:2; /* DMA mode */
-		u32 se_req:1;/* To SE core */
-#else
-		u32 se_req:1; /* To SE core */
-		u32 dma_mode:2; /* DMA mode */
-		u32 grp:3; /* Group bits */
-		u32 reserved0:26;
-#endif
-	} s;
-};
-
-union opcode_info {
-	u16 flags;
-	struct {
-		u8 major;
-		u8 minor;
-	} s;
-};
-
-struct cptvf_request {
-	union opcode_info opcode;
-	u16 param1;
-	u16 param2;
-	u16 dlen;
-};
-
-struct buf_ptr {
-	u8 *vptr;
-	dma_addr_t dma_addr;
-	u16 size;
-};
-
-struct cpt_request_info {
-	u8 incnt; /* Number of input buffers */
-	u8 outcnt; /* Number of output buffers */
-	u16 rlen; /* Output length */
-	union ctrl_info ctrl; /* User control information */
-	struct cptvf_request req; /* Request Information (Core specific) */
-
-	struct buf_ptr in[MAX_BUF_CNT];
-	struct buf_ptr out[MAX_BUF_CNT];
-
-	void (*callback)(int, void *); /* Kernel ASYNC request callabck */
-	void *callback_arg; /* Kernel ASYNC request callabck arg */
-};
-
-struct sglist_component {
-	union {
-		u64 len;
-		struct {
-			u16 len0;
-			u16 len1;
-			u16 len2;
-			u16 len3;
-		} s;
-	} u;
-	u64 ptr0;
-	u64 ptr1;
-	u64 ptr2;
-	u64 ptr3;
-};
-
-struct cpt_info_buffer {
-	struct cpt_vf *cptvf;
-	unsigned long time_in;
-	u8 extra_time;
-
-	struct cpt_request_info *req;
-	dma_addr_t dptr_baddr;
-	u32 dlen;
-	dma_addr_t rptr_baddr;
-	dma_addr_t comp_baddr;
-	u8 *in_buffer;
-	u8 *out_buffer;
-	u8 *gather_components;
-	u8 *scatter_components;
-
-	struct pending_entry *pentry;
-	volatile u64 *completion_addr;
-	volatile u64 *alternate_caddr;
-};
-
-/*
- * CPT_INST_S software command definitions
- * Words EI (0-3)
- */
-union vq_cmd_word0 {
-	u64 u64;
-	struct {
-		u16 opcode;
-		u16 param1;
-		u16 param2;
-		u16 dlen;
-	} s;
-};
-
-union vq_cmd_word3 {
-	u64 u64;
-	struct {
-#if defined(__BIG_ENDIAN_BITFIELD)
-		u64 grp:3;
-		u64 cptr:61;
-#else
-		u64 cptr:61;
-		u64 grp:3;
-#endif
-	} s;
-};
-
-struct cpt_vq_command {
-	union vq_cmd_word0 cmd;
-	u64 dptr;
-	u64 rptr;
-	union vq_cmd_word3 cptr;
-};
-
-void vq_post_process(struct cpt_vf *cptvf, u32 qno);
-int process_request(struct cpt_vf *cptvf, struct cpt_request_info *req);
-#endif /* __REQUEST_MANAGER_H */
-- 
2.17.1

