From 5b75ff1723cc90b5bc68aa2fe8928ffbf3dfe460 Mon Sep 17 00:00:00 2001
From: Po Liu <Po.Liu@nxp.com>
Date: Thu, 8 Mar 2018 02:05:16 +0800
Subject: [PATCH 116/706] enetc-tsn: add enetc tsn function support

Support m0169. Base on enetc BG v86.

Signed-off-by: Po Liu <Po.Liu@nxp.com>
(cherry picked from commit b9817ef89b2a59177904dabef7fd57a18cf0fee9)
Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 drivers/net/ethernet/freescale/enetc/Kconfig  |   10 +
 drivers/net/ethernet/freescale/enetc/Makefile |    1 +
 drivers/net/ethernet/freescale/enetc/enetc.h  |    6 +
 .../net/ethernet/freescale/enetc/enetc_hw.h   |  499 +++++
 .../net/ethernet/freescale/enetc/enetc_pf.c   |    2 +
 .../net/ethernet/freescale/enetc/enetc_tsn.c  | 1711 +++++++++++++++++
 6 files changed, 2229 insertions(+)
 create mode 100644 drivers/net/ethernet/freescale/enetc/enetc_tsn.c

diff --git a/drivers/net/ethernet/freescale/enetc/Kconfig b/drivers/net/ethernet/freescale/enetc/Kconfig
index d0c2a1e31995..201fcfd3e5a6 100644
--- a/drivers/net/ethernet/freescale/enetc/Kconfig
+++ b/drivers/net/ethernet/freescale/enetc/Kconfig
@@ -9,3 +9,13 @@ config FSL_ENETC_VF
 	depends on PCI
 	---help---
 	  TBD
+
+if FSL_ENETC
+
+config ENETC_TSN
+	bool "TSN Support for NXP ENETC driver"
+	default n
+	depends on TSN
+	---help---
+	  This driver supports TSN on Freescale ENETC driver.
+endif
diff --git a/drivers/net/ethernet/freescale/enetc/Makefile b/drivers/net/ethernet/freescale/enetc/Makefile
index 139acdee4fd8..7450801a692d 100644
--- a/drivers/net/ethernet/freescale/enetc/Makefile
+++ b/drivers/net/ethernet/freescale/enetc/Makefile
@@ -1,5 +1,6 @@
 obj-$(CONFIG_FSL_ENETC) += fsl-enetc.o
 fsl-enetc-$(CONFIG_FSL_ENETC) += enetc.o enetc_cbdr.o enetc_ethtool.o
+obj-$(CONFIG_ENETC_TSN) += enetc_tsn.o
 fsl-enetc-$(CONFIG_PCI_IOV) += enetc_msg.o
 fsl-enetc-objs := enetc_pf.o $(fsl-enetc-y)
 
diff --git a/drivers/net/ethernet/freescale/enetc/enetc.h b/drivers/net/ethernet/freescale/enetc/enetc.h
index f2a6476e7c77..451741be5c7f 100644
--- a/drivers/net/ethernet/freescale/enetc/enetc.h
+++ b/drivers/net/ethernet/freescale/enetc/enetc.h
@@ -253,3 +253,9 @@ void enetc_sync_mac_filters(struct enetc_si *si, struct enetc_mac_filter *tbl,
 int enetc_set_fs_entry(struct enetc_si *si, struct enetc_cmd_rfse *rfse,
 		       int index);
 int enetc_set_rss_table(struct enetc_si *si, u16 *table, int len);
+
+#ifdef CONFIG_ENETC_TSN
+void enetc_tsn_init(struct enetc_si *si);
+#else
+#define enetc_tsn_init(si) (void)0
+#endif
diff --git a/drivers/net/ethernet/freescale/enetc/enetc_hw.h b/drivers/net/ethernet/freescale/enetc/enetc_hw.h
index 77652a3ef295..9dc73f64392a 100644
--- a/drivers/net/ethernet/freescale/enetc/enetc_hw.h
+++ b/drivers/net/ethernet/freescale/enetc/enetc_hw.h
@@ -393,6 +393,7 @@ union enetc_rx_bd {
 #define EMETC_MAC_ADDR_FILT_RES	3 /* # of reserved entries at the beginning */
 #define ENETC_MAX_NUM_VFS	2
 
+#ifndef CONFIG_ENETC_TSN
 struct enetc_cbd {
 	union {
 		struct {
@@ -408,6 +409,7 @@ struct enetc_cbd {
 	u8 _res;
 	u8 status_flags;
 };
+#endif
 
 #define ENETC_CBD_FLAGS_SF	BIT(7) /* short format */
 #define ENETC_CBD_FLAGS_IE	BIT(6) /* interrupt enable */
@@ -507,3 +509,500 @@ static inline void enetc_enable_txvlan(struct enetc_hw *hw, int si_idx,
 	val = (val & ~ENETC_TBMR_VIH) | (en ? ENETC_TBMR_VIH : 0);
 	enetc_txbdr_wr(hw, si_idx, ENETC_TBMR, val);
 }
+
+#ifdef CONFIG_ENETC_TSN
+
+#define ENETC_BG_V86
+enum bdcr_cmd_class {
+	BDCR_CMD_UNSPEC = 0,
+	BDCR_CMD_MAC_FILTER,
+	BDCR_CMD_VLAN_FILTER,
+	BDCR_CMD_RSS,
+	BDCR_CMD_RFS,
+	BDCR_CMD_PORT_GCL,
+	BDCR_CMD_RECV_CLASSIFIER,
+	BDCR_CMD_STREAM_IDENTIFY,
+	BDCR_CMD_STREAM_FILTER,
+	BDCR_CMD_STREAM_GCL,
+	BDCR_CMD_FLOW_METER,
+	__BDCR_CMD_MAX_LEN,
+	BDCR_CMD_MAX_LEN = __BDCR_CMD_MAX_LEN - 1,
+};
+
+/* class 7, command 0, Stream Identity Entry Configuration */
+struct streamid_conf {
+	__le32	stream_handle;	/* init gate value */
+	__le32	ifac_iports;
+		u8	id_type;
+		u8	oui[3];
+		u8	res[3];
+		u8	en;
+};
+
+#define ENETC_CBDR_SID_VID_MASK 0xfff
+#define ENETC_CBDR_SID_VIDM BIT(12)
+#define ENETC_CBDR_SID_TG_MASK 0xc000
+/* streamid_conf address point to this data space */
+struct null_streamid_data {
+	u8	dmac[6];
+	u16	vid_vidm_tg;
+};
+
+struct smac_streamid_data {
+	u8	smac[6];
+	u16	vid_vidm_tg;
+};
+
+/* class 7, command 1, query config , long format */
+/* No need structure define */
+
+#define ENETC_CDBR_SID_ENABLE	BIT(7)
+/*  Stream ID Query Response Data Buffer */
+struct streamid_query_resp {
+	u32	stream_handle;
+	u32	input_ports;
+	u8	id_type;
+	u8	oui[3];
+	u8	mac[6];
+	u16	vid_vidm_tg;
+	u8	res[3];
+	u8  en;
+};
+
+/* class 7, command 2, qeury status count, Stream ID query long format */
+struct streamid_stat_query {
+	u8	res[12];
+	__le32 input_ports;
+};
+
+/* Stream Identity Statistics Query */
+struct streamid_stat_query_resp {
+	u32	psinl;
+	u32	psinh;
+	u64	pspi[32];
+};
+
+#define ENETC_CBDR_SFI_PRI_MASK 0x7
+#define ENETC_CBDR_SFI_PRIM		BIT(3)
+#define ENETC_CBDR_SFI_BLOV		BIT(4)
+#define ENETC_CBDR_SFI_BLEN		BIT(5)
+#define ENETC_CBDR_SFI_MSDUEN	BIT(6)
+#define ENETC_CBDR_SFI_FMITEN	BIT(7)
+#define ENETC_CBDR_SFI_ENABLE	BIT(7)
+/* class 8, command 0, Stream Filter Instance, Short Format */
+struct sfi_conf {
+	__le32	stream_handle;
+		u8	multi;
+		u8	res[2];
+		u8	sthm;
+	/*
+	 * Max Service Data Unit or Flow Meter Instance Table index.
+	 * Depending on the value of FLT this represents either Max
+	 * Service Data Unit (max frame size) allowed by the filter
+	 * entry or is an index into the Flow Meter Instance table
+	 * index identifying the policer which will be used to police
+	 * it.
+	 */
+	__le16	fm_inst_table_index;
+	__le16	msdu;
+	__le16	sg_inst_table_index;
+		u8	res1[2];
+	__le32	input_ports;
+		u8	res2[3];
+		u8	en;
+};
+
+/* class 8, command 1, Stream Filter Instance, write back, short Format */
+struct sfi_query {
+		u32	stream_handle;
+		u8	multi;
+		u8	res[2];
+		u8	sthm;
+		u16	fm_inst_table_index;
+		u16	msdu;
+		u16	sg_inst_table_index;
+		u8	res1[2];
+		u32	input_ports;
+		u8	res2[3];
+		u8	en;
+};
+
+/* class 8, command 2 stream Filter Instance status query short format */
+/* command no need structure define */
+
+/* Stream Filter Instance Query Statistics Response data */
+struct sfi_query_stat_resp {
+	u32	match_filter_countl;
+	u32	match_filter_counth;
+	u32	sdu_filter_drop_countl;
+	u32	sdu_filter_drop_counth;
+	u32	sdu_filter_pass_countl;
+	u32	sdu_filter_pass_counth;
+};
+
+struct sfi_counter_data {
+	u32 matchl;
+	u32 matchh;
+	u32 msdu_dropl;
+	u32 msdu_droph;
+	u32 stream_gate_dropl;
+	u32 stream_gate_droph;
+	u32 flow_meter_dropl;
+	u32 flow_meter_droph;
+};
+
+#define ENETC_CBDR_SGI_OIPV_MASK 0x7
+#define ENETC_CBDR_SGI_OIPV_EN	BIT(3)
+#define ENETC_CBDR_SGI_CGTST	BIT(6)
+#define ENETC_CBDR_SGI_OGTST	BIT(7)
+#define ENETC_CBDR_SGI_CFG_CHG  BIT(1)
+#define ENETC_CBDR_SGI_CFG_PND  BIT(2)
+#define ENETC_CBDR_SGI_OEX		BIT(4)
+#define ENETC_CBDR_SGI_OEXEN	BIT(5)
+#define ENETC_CBDR_SGI_IRX		BIT(6)
+#define ENETC_CBDR_SGI_IRXEN	BIT(7)
+#define ENETC_CBDR_SGI_ACLLEN_MASK 0x3
+#define ENETC_CBDR_SGI_OCLLEN_MASK 0xc
+#define	ENETC_CBDR_SGI_EN		BIT(7)
+/* class 9, command 0, Stream Gate Instance Table, Short Format */
+/* class 9, command 2, Stream Gate Instance Table entry query write back ,Short Format */
+struct sgi_table {
+	u8	res[8];
+	u8	oipv;
+	u8	res0[2];
+	u8	ocgtst;
+	u8	res1[7];
+	u8	gset;
+	u8	oacl_len;
+	u8	res2[2];
+	u8	en;
+};
+
+#define ENETC_CBDR_SGI_AIPV_MASK 0x7
+#define ENETC_CBDR_SGI_AIPV_EN	BIT(3)
+#define ENETC_CBDR_SGI_AGTST	BIT(7)
+
+/* class 9, command 1, Stream Gate Control List, Long Format */
+struct sgcl_conf {
+	u8	aipv;
+	u8	res[2];
+	u8	agtst;
+	u8	res1[4];
+	union {
+		struct {
+			u8 res2[4];
+			u8 acl_len;
+			u8 res3[3];
+		};
+		u8 cct[8]; /* Config change time */
+	};
+};
+
+/* stream control list class 9 , cmd 1 data buffer */
+struct sgcl_data {
+	u32	btl;
+	u32 bth;
+	u32	ct;
+	u32	cte;
+#ifdef ENETC_BG_V86
+	u32 cctl;
+	u32 ccth;
+#endif
+	/*struct sgce	*sgcl;*/
+};
+
+/* class 9, command 2, stream gate instant table enery query, short format */
+/* write back see struct sgi_table. Do not need define.*/
+
+/* class 9, command 3 Stream Gate Control List Query Descriptor - Long Format
+ * ocl_len or acl_len to be 0, oper or admin would not show in the data space
+ * true len will be write back in the space.
+ * */
+struct sgcl_query {
+	u8 res[12];
+	u8 oacl_len;
+	u8 res1[3];
+};
+
+/* define for 'stat' */
+#define ENETC_CBDR_SGIQ_AIPV_MASK 0x7
+#define ENETC_CBDR_SGIQ_AIPV_EN	BIT(3)
+#define ENETC_CBDR_SGIQ_AGTST	BIT(4)
+#define ENETC_CBDR_SGIQ_ACL_LEN_MASK 0x60
+#define ENETC_CBDR_SGIQ_OIPV_MASK 0x380
+#define ENETC_CBDR_SGIQ_OIPV_EN	BIT(10)
+#define ENETC_CBDR_SGIQ_OGTST	BIT(11)
+#define ENETC_CBDR_SGIQ_OCL_LEN_MASK 0x3000
+/* class 9, command 3 data space */
+struct sgcl_query_resp {
+#ifndef ENETC_BG_V86
+	u16 stat;
+	u16 res;
+#endif
+	u32	abtl;
+	u32 abth;
+	u32	act;
+	u32	acte;
+	u32	cctl;
+	u32 ccth;
+	u32	obtl;
+	u32 obth;
+	u32	oct;
+	u32	octe;
+/*	u32	ccel;
+	u32 cceh;
+	*/
+};
+
+/* class 9, command 4 Stream Gate Instance Table Query Statistics Response
+ * short command, write back, no command define
+ */
+struct sgi_query_stat_resp {
+	u32	pgcl;
+	u32 pgch;
+	u32 dgcl;
+	u32 dgch;
+	u16	msdu_avail;
+	u8 res[6];
+};
+
+#define ENETC_CBDR_FMI_MR	BIT(0)
+#define ENETC_CBDR_FMI_MREN BIT(1)
+#define ENETC_CBDR_FMI_DOY	BIT(2)
+#define	ENETC_CBDR_FMI_CM	BIT(3)
+#define ENETC_CBDR_FMI_CF	BIT(4)
+#define ENETC_CBDR_FMI_NDOR BIT(5)
+#define ENETC_CBDR_FMI_OALEN BIT(6)
+#define ENETC_CBDR_FMI_IRFPP_MASK 0x1f
+/* class 10: command 0/1, Flow Meter Instance Set, short Format */
+struct fmi_conf {
+	__le32	cir;
+	__le32	cbs;
+	__le32	eir;
+	__le32	ebs;
+		u8	conf;
+		u8	res1;
+		u8	ir_fpp;
+		u8	res2[4];
+		u8	en;
+};
+
+/* class:10, command:2, Flow Meter Instance Statistics Query Response */
+struct fmi_query_stat_resp {
+	u32	bcl;
+	u32 bch;
+	u32 dfl;
+	u32 dfh;
+	u32 d0gfl;
+	u32 d0gfh;
+	u32 d1gfl;
+	u32 d1gfh;
+	u32 dyfl;
+	u32 dyfh;
+	u32 ryfl;
+	u32 ryfh;
+	u32 drfl;
+	u32 drfh;
+	u32 rrfl;
+	u32 rrfh;
+	u32 lts;
+	u32 bci;
+	u32 bcf;
+	u32 bei;
+	u32 bef;
+};
+
+/* class 5, command 0 */
+struct tgs_gcl_conf {
+	u8	atc;	/* init gate value */
+	u8	res[7];
+	union {
+		struct {
+			u8	res1[4];
+			__le16 acl_len;
+			u8 res2[2];
+		};
+		struct {
+			u32 cctl;
+			u32 ccth;
+		};
+	};
+};
+
+#define ENETC_CBDR_SGL_IOMEN	BIT(0)
+#define ENETC_CBDR_SGL_IPVEN	BIT(3)
+#define ENETC_CBDR_SGL_GTST		BIT(4)
+#define ENETC_CBDR_SGL_IPV_MASK 0xe
+/* Stream Gate Control List Entry */
+struct sgce {
+	u32	interval;
+	u8	msdu[3];
+	u8	multi;
+};
+
+/* gate control list entry */
+struct gce {
+	u32	period;
+	u8	gate;
+	u8	res[3];
+};
+
+/* tgs_gcl_conf address point to this data space */
+struct tgs_gcl_data {
+	u32	btl;
+	u32 bth;
+	u32 ct;
+	u32 cte;
+#ifdef ENETC_BG_V86
+	u32 cctl;
+	u32 ccth;
+#endif
+};
+
+/* class 5, command 1 */
+struct tgs_gcl_query {
+		u8	res[12];
+		union {
+			struct {
+				__le16	acl_len; /* admin list length */
+				__le16	ocl_len; /* operation list length */
+			};
+			struct {
+				u16 admin_list_len;
+				u16 oper_list_len;
+			};
+		};
+
+};
+
+/* tgs_gcl_query command response data format */
+struct tgs_gcl_resp {
+	u32	abtl;	/* base time */
+	u32 abth;
+	u32	act;	/* cycle time */
+	u32	acte;	/* cycle time extend */
+	u32	cctl;	/* config change time */
+	u32 ccth;
+	u32 obtl;	/* operation base time */
+	u32 obth;
+	u32	oct;	/* operation cycle time */
+	u32	octe;	/* operation cycle time extend */
+	u32	ccel;	/* config change error */
+	u32 cceh;
+	/*struct gce	*gcl;*/
+};
+
+struct enetc_cbd {
+	union{
+		struct sfi_conf sfi_conf; /* Just for Stream Filter Instance Entry */
+		struct sfi_query_stat_resp sfi_query_stat_resp;
+		struct sgi_table sgi_table; /* Just for Stream Gate Instance table set */
+	/*	struct sgi_query_resp sgi_query_resp;*/
+		struct sgi_query_stat_resp sgi_query_stat_resp;
+		struct fmi_conf fmi_conf;
+	/*	struct fmi_query_stat_resp fmi_query_stat_resp;*/
+		struct {
+			__le32	addr[2];
+			union {
+				__le32	opt[4];
+				struct tgs_gcl_conf		gcl_conf;
+				struct tgs_gcl_query	gcl_query;
+				struct streamid_conf		sid_set;
+				struct streamid_stat_query	sid_stat;
+				struct sgcl_conf		sgcl_conf;
+				struct sgcl_query		sgcl_query;
+			};
+		};			/* Long format */
+		__le32 data[6];
+	};
+	__le16 index;
+	__le16 length;
+	u8 cmd;
+	u8 cls;
+	u8 _res;
+	u8 status_flags;
+};
+
+#define ENETC_SICTR		0x18
+#define ENETC_SIPCAPR0	0x20
+#define ENETC_SIPCAPR1	0x24
+#define ENETC_SITGTGR	0x30
+
+/* Port capability register 0 */
+#define ENETC_PCAPR0_PSFPM BIT(10)
+#define ENETC_PCAPR0_PSFP BIT(9)
+#define ENETC_PCAPR0_TSN BIT(4)
+#define ENETC_PCAPR0_QBU BIT(3)
+
+/* port time gating control register */
+#define QBV_PTGCR_OFFSET 0x11a00
+#define QBV_TGE		0x80000000
+
+/* Port time gating capability register */
+#define QBV_PTGCAPR_OFFSET 0x11a08
+#define QBV_MAX_GCL_LEN_MASK	0xffff
+
+/* Port time gating tick granularity register */
+#define QBV_PTGTGR_OFFSET 0x11a0c
+#define QBV_TICK_GRAN_MASK 0xffffffff
+
+/* Port time gating admin gate list status register */
+#define QBV_PTGAGLSR_OFFSET 0x11a10
+
+#define QBV_CFG_PEND_MASK 0x00000002
+
+/* Port time gating admin gate list length register */
+#define QBV_PTGAGLLR_OFFSET 0x11a14
+#define QBV_ADMIN_GATE_LIST_LENGTH_MASK 0xffff
+
+/* Port time gating operational gate list status register */
+#define QBV_PTGOGLSR_OFFSET 0x11a18
+#define QBV_HTA_POS_MASK 0xffff0000
+
+#define QBV_CURR_POS_MASK 0x0000ffff
+
+/* Port time gating operational gate list length register */
+#define QBV_PTGOGLLR_OFFSET 0x11a1c
+#define QBV_OPER_GATE_LIST_LENGTH_MASK 0xffff
+
+/* Port time gating current time register */
+#define QBV_PTGCTR_OFFSET 0x11a20
+#define QBV_CURR_TIME_MASK 0xffffffffffffffff
+
+/* Port traffic class a time gating control register */
+#define QBV_PTC0TGCR_OFFSET  0x11a40
+#define QBV_PTC1TGCR_OFFSET  0x11a50
+#define QBV_PTC2TGCR_OFFSET  0x11a60
+#define QBV_PTC3TGCR_OFFSET  0x11a70
+#define QBV_PTC4TGCR_OFFSET  0x11a80
+#define QBV_PTC5TGCR_OFFSET  0x11a90
+#define QBV_PTC6TGCR_OFFSET  0x11aa0
+#define QBV_PTC7TGCR_OFFSET  0x11ab0
+
+/* Maximum Service Data Unit. */
+#define ENETC_PTC0MSDUR 0x12020
+#define ENETC_PTC1MSDUR 0x12024
+#define ENETC_PTC2MSDUR 0x12028
+#define ENETC_PTC3MSDUR 0x1202c
+#define ENETC_PTC4MSDUR 0x12030
+#define ENETC_PTC5MSDUR 0x12034
+#define ENETC_PTC6MSDUR 0x12038
+#define ENETC_PTC7MSDUR 0x1203c
+
+#define QBV_MAXSDU_MASK 0xffff
+
+/* Port traffic class a time gating status register */
+#define QBV_PTC0TGSR_OFFSET  0x11a44
+#define QBV_HTA_STATE_MASK  0x10000
+#define QBV_CURR_STATE_MASK 0x1
+
+/* Port traffic class a time gating transmission overrun counter register*/
+#define QBV_PTC0TGTOCR_OFFSET 0x11a48
+#define QBV_TX_OVERRUN_MASK 0xffffffffffffffff
+#define ENETC_TGLSTR 0xa200
+#define ENETC_TGS_MIN_DIS_MASK 0x80000000
+#define ENETC_MIN_LOOKAHEAD_MASK 0xffff
+
+#define ENETC_PPSFPMR 0x11b00
+#define ENETC_PSFPEN 0x1
+#endif
diff --git a/drivers/net/ethernet/freescale/enetc/enetc_pf.c b/drivers/net/ethernet/freescale/enetc/enetc_pf.c
index 65627b1fcea2..acc17995a28e 100644
--- a/drivers/net/ethernet/freescale/enetc/enetc_pf.c
+++ b/drivers/net/ethernet/freescale/enetc/enetc_pf.c
@@ -533,6 +533,8 @@ static int enetc_pf_probe(struct pci_dev *pdev,
 		goto err_alloc_msix;
 	}
 
+	enetc_tsn_init(si);
+
 	err = register_netdev(ndev);
 	if (err)
 		goto err_reg_netdev;
diff --git a/drivers/net/ethernet/freescale/enetc/enetc_tsn.c b/drivers/net/ethernet/freescale/enetc/enetc_tsn.c
new file mode 100644
index 000000000000..ee415aa7da12
--- /dev/null
+++ b/drivers/net/ethernet/freescale/enetc/enetc_tsn.c
@@ -0,0 +1,1711 @@
+/*
+ * Copyright 2017 NXP
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the names of the above-listed copyright holders nor the
+ *       names of any contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifdef CONFIG_ENETC_TSN
+#include "enetc.h"
+
+#include <net/tsn.h>
+#include <linux/module.h>
+
+void DUMP_CBDR(struct enetc_cbd *cbdr)
+{
+	int i;
+
+	printk("addrl: %04x", cbdr->addr[0]);
+	printk("addrh: %04x\n", cbdr->addr[1]);
+
+	char *data = (char *)cbdr;
+
+	for (i = 0; i < 8; i++) {
+		printk("%02x %02x %02x %02x\n",
+				*(data + i*4 + 3), *(data + i*4 + 2), *(data + i*4 + 1), *(data + i*4));
+	}
+	printk("\n");
+}
+
+void DUMP_DATA(char *data, int size)
+{
+	int i;
+
+	printk("data memory: \n");
+
+	for (i = 0; i < size / 4; i++) {
+		printk("%02x %02x %02x %02x\n",
+				*(data + i*4 + 3), *(data + i*4 + 2), *(data + i*4 + 1), *(data + i*4));
+	}
+
+	printk("\n");
+}
+
+static int alloc_cbdr(struct enetc_si *si, struct enetc_cbd **curr_cbd)
+{
+	struct enetc_cbdr *ring = &si->cbd_ring;
+	int i;
+
+	i = ring->next_to_use;
+	*curr_cbd = ENETC_CBD(*ring, i);
+	printk("cbd: %p\n", *curr_cbd);
+
+	memset(*curr_cbd, 0, sizeof(struct enetc_cbd));
+	return i;
+}
+
+/* Transmit the BD control ring by writing the ccir register.
+ * Update the counters maintained by software.
+ */
+static int xmit_cbdr(struct enetc_si *si, int i)
+{
+	struct enetc_cbdr *ring = &si->cbd_ring;
+	struct enetc_cbd *dest_cbd;
+	int nc;
+
+	i = (i + 1) % ring->bd_count;
+
+	ring->next_to_use = i;
+	/* let H/W know BD ring has been updated */
+	enetc_wr_reg(ring->cir, i);
+
+	int timeout = ENETC_CBDR_TIMEOUT;
+
+	do {
+		if (enetc_rd_reg(ring->cisr) == i)
+			break;
+		udelay(10);
+		timeout -= 10;
+	} while (timeout);
+
+	if (!timeout)
+		return ENETC_CMD_TIMEOUT;
+#if 0
+	enetc_clean_cbdr(si);
+#endif
+	nc = ring->next_to_clean;
+
+	while (enetc_rd_reg(ring->cisr) != nc) {
+		dest_cbd = ENETC_CBD(*ring, nc);
+		if (dest_cbd->status_flags & ENETC_CBD_STATUS_MASK)
+			WARN_ON(1);
+
+		/*memset(dest_cbd, 0, sizeof(*dest_cbd));*/
+
+		nc = (nc + 1) % ring->bd_count;
+	}
+
+	ring->next_to_clean = nc;
+
+	return ENETC_CMD_OK;
+}
+
+int enetc_qci_sgi_counters_get(struct net_device *ndev, u32 index,
+		struct sgi_query_stat_resp *counters)
+{
+	struct enetc_cbd *cbdr;
+	struct sgi_query_stat_resp *sgi_data;
+	int curr_cbd;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 4;
+	cbdr->cls = BDCR_CMD_STREAM_GCL;
+	cbdr->status_flags = 0x80;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	sgi_data = &cbdr->sgi_query_stat_resp;
+	memcpy(counters, sgi_data, sizeof(struct sgi_query_stat_resp));
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+
+/* Class 10: Flow Meter Instance Statistics Query Descriptor - Long Format */
+int enetc_qci_fmi_counters_get(struct net_device *ndev, u32 index,
+			struct fmi_query_stat_resp *counters)
+{
+	struct enetc_cbd *cbdr;
+	struct fmi_query_stat_resp *fmi_data;
+	dma_addr_t dma;
+	u16 data_size, dma_size;
+	int curr_cbd;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16((u16)index);
+	cbdr->cmd = 2;
+	cbdr->cls = BDCR_CMD_FLOW_METER;
+	cbdr->status_flags = 0;
+
+	data_size = sizeof(struct fmi_query_stat_resp);
+
+	fmi_data = (struct fmi_query_stat_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (fmi_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+
+	dma = dma_map_single(&priv->si->pdev->dev, fmi_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		kfree(fmi_data);
+		return -ENOMEM;
+	}
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)fmi_data, data_size);
+
+	memcpy(counters, fmi_data, sizeof(struct fmi_query_stat_resp));
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	kfree(fmi_data);
+	return 0;
+}
+
+u16 enetc_get_max_gcl_len(struct enetc_hw *hw)
+{
+	return (enetc_rd(hw, QBV_PTGCAPR_OFFSET) & QBV_MAX_GCL_LEN_MASK);
+}
+
+void testcbdr(struct enetc_si *si)
+{
+	u16 *table;
+	struct enetc_cbd *cbd;
+	dma_addr_t dma;
+	int num;
+
+	table = (u16 *)kzalloc(0x80, __GFP_DMA | GFP_KERNEL);
+	table[0] = 0x10; table[1] = 0x11; table[2] = 0x12; table[3] = 0x13;
+	//enetc_set_rss_table(si, table, 0x80);
+
+	/* fill up the "set" descriptor */
+	num = alloc_cbdr(si, &cbd);
+
+	cbd->cmd = 1;
+	cbd->cls = 3;
+	cbd->length = cpu_to_le16(0x80);
+
+	dma = dma_map_single(&si->pdev->dev, table, 0x80, DMA_TO_DEVICE);
+	if (dma_mapping_error(&si->pdev->dev, dma)) {
+		netdev_err(si->ndev, "DMA mapping of RSS table failed!\n");
+		return;
+	}
+
+	cbd->addr[0] = lower_32_bits(dma);
+	cbd->addr[1] = upper_32_bits(dma);
+	DUMP_CBDR(cbd);
+	xmit_cbdr(si, num);
+	DUMP_CBDR(cbd);
+	DUMP_DATA((char *)table, 0x80);
+	dma_unmap_single(&si->pdev->dev, dma, 0x80, DMA_TO_DEVICE);
+	memset(cbd, 0, sizeof(*cbd));
+
+	memset(table, 0, 0x80);
+
+	/* fill up the "set" descriptor */
+	num = alloc_cbdr(si, &cbd);
+	cbd->cmd = 2;
+	cbd->cls = 3;
+	cbd->length = cpu_to_le16(0x80);
+	dma = dma_map_single(&si->pdev->dev, table, 0x80, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&si->pdev->dev, dma)) {
+		netdev_err(si->ndev, "DMA mapping of RSS table failed!\n");
+		return;
+	}
+
+	cbd->addr[0] = lower_32_bits(dma);
+	cbd->addr[1] = upper_32_bits(dma);
+	DUMP_CBDR(cbd);
+	xmit_cbdr(si, num);
+	DUMP_CBDR(cbd);
+	DUMP_DATA((char *)table, 0x80);
+	dma_unmap_single(&si->pdev->dev, dma, 0x80, DMA_FROM_DEVICE);
+
+	memset(cbd, 0, sizeof(*cbd));
+	kfree(table);
+}
+
+/*
+ * CBD Class 5: Time Gated Scheduling Gate Control List configuration
+ * Descriptor - Long Format
+ */
+int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
+{
+	struct enetc_cbd *cbdr;
+	struct tgs_gcl_data *gcl_data;
+	struct tgs_gcl_conf *gcl_config;
+	struct gce *gce;
+	u16 gcl_len;
+	u16 data_size;
+	int i;
+	dma_addr_t dma;
+	int curr_cbd;
+	struct tsn_qbv_basic *admin_basic = &admin_conf->admin;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	u32 temp;
+
+	gcl_len = admin_basic->control_list_length;
+	if (gcl_len > enetc_get_max_gcl_len(&priv->si->hw))
+		return -EINVAL;
+
+	temp = enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET);
+	if (admin_conf->gate_enabled && !(temp & QBV_TGE)) {
+		enetc_wr(&priv->si->hw, QBV_PTGCR_OFFSET, temp | QBV_TGE);
+	} else if (!admin_conf->gate_enabled) {
+		enetc_wr(&priv->si->hw, QBV_PTGCR_OFFSET, temp & (~QBV_TGE));
+		return 0;
+	}
+
+	/*
+	 * Set the maximum frame size for each traffic class index
+	 * PTCaMSDUR[MAXSDU]. The maximum frame size cannot exceed
+	 * 9,600 bytes (0x2580). Frames that exceed the limit are
+	 * discarded.
+	 */
+	if (admin_conf->maxsdu) {
+		enetc_wr(&priv->si->hw, ENETC_PTC0MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC1MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC2MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC3MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC4MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC5MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC6MSDUR, admin_conf->maxsdu);
+		enetc_wr(&priv->si->hw, ENETC_PTC7MSDUR, admin_conf->maxsdu);
+	}
+
+	/*
+	 * Configure the (administrative) gate control list using the
+	 * control BD descriptor.
+	 */
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	gcl_config = &cbdr->gcl_conf;
+
+	data_size = sizeof(struct tgs_gcl_data) + gcl_len * sizeof(struct gce);
+
+	gcl_data = (struct tgs_gcl_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (gcl_data == NULL)
+		return -ENOMEM;
+
+	gce = (struct gce *)(gcl_data + 1);
+
+	gcl_config->atc = admin_basic->gate_states;
+	gcl_config->acl_len = cpu_to_le16(gcl_len);
+
+	gcl_data->btl = cpu_to_le32(lower_32_bits(admin_basic->base_time));
+	gcl_data->bth = cpu_to_le32(upper_32_bits(admin_basic->base_time));
+	gcl_data->ct = cpu_to_le32(admin_basic->cycle_time);
+	gcl_data->cte = cpu_to_le32(admin_basic->cycle_time_extension);
+
+	for (i = 0; i < gcl_len; i++) {
+		struct gce *temp_gce = gce + i;
+		struct tsn_qbv_entry *temp_entry = admin_basic->control_list + i;
+
+		temp_gce->gate = temp_entry->gate_state;
+		temp_gce->period = cpu_to_le32(temp_entry->time_interval);
+	}
+
+	cbdr->length = cpu_to_le16(data_size);
+	cbdr->status_flags = 0; /* long format command no ie */
+
+	dma = dma_map_single(&priv->si->pdev->dev, gcl_data, data_size, DMA_TO_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		kfree(gcl_data);
+		return -ENOMEM;
+	}
+
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+	cbdr->cmd = 0;
+	cbdr->cls = BDCR_CMD_PORT_GCL;
+
+	/*
+	 * Updated by ENETC on completion of the configuration
+	 * command. A zero value indicates success.
+	 */
+	cbdr->status_flags = 0;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	 /* config change time could be read in the, but up layer could not get it
+	 * */
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)gcl_data, data_size);
+	memset(cbdr, 0, sizeof(struct enetc_cbd));
+	dma_unmap_single(&priv->si->pdev->dev, dma, data_size, DMA_TO_DEVICE);
+	kfree(gcl_data);
+
+	return 0;
+}
+
+/*
+ * CBD Class 5: Time Gated Scheduling Gate Control List query
+ * Descriptor - Long Format
+ */
+int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
+{
+	struct enetc_cbd *cbdr;
+	struct tgs_gcl_resp *gcl_data;
+	struct tgs_gcl_query *gcl_query;
+	struct gce *gce;
+
+	struct tsn_qbv_basic *admin_basic = &admin_conf->admin;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	dma_addr_t dma;
+	int curr_cbd;
+	u16 maxlen;
+	u16 data_size, dma_size;
+	u16 admin_len;
+	u16 oper_len;
+	u64 temp;
+	int i;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	gcl_query =  &cbdr->gcl_query;
+
+	maxlen = enetc_get_max_gcl_len(&priv->si->hw);
+
+	data_size = sizeof(struct tgs_gcl_resp) + sizeof(struct gce) * 2 * maxlen;
+
+	gcl_data = (struct tgs_gcl_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (gcl_data == NULL)
+		return -ENOMEM;
+
+	gce = (struct gce *)(gcl_data + 1);
+
+	gcl_query->acl_len = cpu_to_le16(maxlen);
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+	cbdr->status_flags = 0; /* long format command no ie */
+
+	dma = dma_map_single(&priv->si->pdev->dev, gcl_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		kfree(gcl_data);
+		return -ENOMEM;
+	}
+
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+	cbdr->cmd = 1;
+	cbdr->cls = BDCR_CMD_PORT_GCL;
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	dma_unmap_single(&priv->si->pdev->dev, dma, data_size, DMA_FROM_DEVICE);
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)gcl_data, data_size);
+
+	/* since cbdr already passed to free, below could be get wrong */
+	admin_len = le16_to_cpu(gcl_query->admin_list_len);
+	oper_len = le16_to_cpu(gcl_query->oper_list_len);
+
+	admin_basic->control_list_length = admin_len;
+
+	temp = ((u64)le32_to_cpu(gcl_data->abth)) << 32;
+	admin_basic->base_time = le32_to_cpu(gcl_data->abtl) + temp;
+
+	admin_basic->cycle_time = le32_to_cpu(gcl_data->act);
+	admin_basic->cycle_time_extension = le32_to_cpu(gcl_data->acte);
+
+	admin_basic->control_list =
+		kzalloc(admin_len *	sizeof(*(admin_basic->control_list)), GFP_KERNEL);
+	if (admin_basic->control_list == NULL) {
+		memset(cbdr, 0, sizeof(*cbdr));
+		kfree(gcl_data);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < admin_len; i++) {
+		struct gce *temp_gce = gce + i;
+		struct tsn_qbv_entry *temp_entry = admin_basic->control_list + i;
+
+		temp_entry->gate_state = temp_gce->gate;
+		temp_entry->time_interval = le32_to_cpu(temp_gce->period);
+	}
+
+	if (enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET) & QBV_TGE)
+		admin_conf->gate_enabled = true;
+	else
+		admin_conf->gate_enabled = false;
+
+	/* Updated by ENETC on completion of the configuration
+	 * command. A zero value indicates success.
+	 */
+	admin_conf->config_change = true;
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	kfree(gcl_data);
+
+	return 0;
+}
+
+int enetc_qbv_get_status(struct net_device *ndev,
+							struct tsn_qbv_status *status)
+{
+	struct enetc_cbd *cbdr;
+	struct tgs_gcl_resp *gcl_data;
+	struct tgs_gcl_query *gcl_query;
+	struct gce *gce;
+	struct tsn_qbv_basic *oper_basic;
+	struct enetc_ndev_priv *priv;
+	dma_addr_t dma;
+	int curr_cbd;
+	u16 maxlen;
+	u16 data_size, dma_size;
+	u16 admin_len;
+	u16 oper_len;
+	u64 temp;
+	int i;
+
+	if (!ndev)
+		return -EINVAL;
+
+	if (!status)
+		return -EINVAL;
+
+	oper_basic = &status->oper;
+	priv = netdev_priv(ndev);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	gcl_query = &cbdr->gcl_query;
+
+	maxlen = enetc_get_max_gcl_len(&priv->si->hw);
+
+	data_size = sizeof(struct tgs_gcl_resp) + sizeof(struct gce) * 2 * maxlen;
+
+	gcl_data = (struct tgs_gcl_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (gcl_data == NULL)
+		return -ENOMEM;
+
+	gce = (struct gce *)(gcl_data + 1);
+
+	gcl_query->acl_len = cpu_to_le16(maxlen);
+	gcl_query->ocl_len = cpu_to_le16(maxlen);
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+	cbdr->status_flags = 0; /* long format command no ie */
+
+	dma = dma_map_single(&priv->si->pdev->dev, gcl_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		kfree(gcl_data);
+		return -ENOMEM;
+	}
+
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+	cbdr->cmd = 1;
+	cbdr->cls = BDCR_CMD_PORT_GCL;
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	dma_unmap_single(&priv->si->pdev->dev, dma, data_size, DMA_FROM_DEVICE);
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)gcl_data, data_size);
+
+	/* since cbdr already passed to free, below could be get wrong */
+	admin_len = le16_to_cpu(gcl_query->admin_list_len);
+	oper_len = le16_to_cpu(gcl_query->oper_list_len);
+
+	gce += admin_len;
+
+	if (enetc_rd(&priv->si->hw, QBV_PTGAGLSR_OFFSET) &
+						QBV_CFG_PEND_MASK) {
+		status->config_pending = true;
+		goto exit;
+	}
+
+	/* The Oper and Admin timing fields exist in the response buffer even
+	 * if no valid corresponding lists exists. These fields are considered
+	 * invalid if the corresponding list does not exist.
+	 */
+	status->config_pending = false;
+	temp = ((u64)le32_to_cpu(gcl_data->ccth)) << 32;
+	status->config_change_time = le32_to_cpu(gcl_data->cctl) + temp;
+
+	temp = ((u64)le32_to_cpu(gcl_data->cceh)) << 32;
+	status->config_change_error = le32_to_cpu(gcl_data->ccel) + temp;
+
+	/* changed to SITGTGR */
+	status->tick_granularity = enetc_rd(&priv->si->hw, ENETC_SITGTGR);
+
+	/* current time */
+	temp = ((u64)enetc_rd(&priv->si->hw, ENETC_SICTR + 4)) << 32;
+	status->current_time = enetc_rd(&priv->si->hw, ENETC_SICTR) + temp;
+
+	status->supported_list_max = maxlen;
+
+	/* status->oper.gate_states , no init oper/admin gate state */
+	status->oper.control_list_length = oper_len;
+	temp = ((u64)le32_to_cpu(gcl_data->obth)) << 32;
+	status->oper.base_time = le32_to_cpu(gcl_data->obtl) + temp;
+	status->oper.cycle_time = le32_to_cpu(gcl_data->oct);
+	status->oper.cycle_time_extension = le32_to_cpu(gcl_data->octe);
+
+
+	oper_basic->control_list =
+		kzalloc(oper_len * sizeof(*(oper_basic->control_list)), GFP_KERNEL);
+	if (oper_basic->control_list == NULL) {
+		memset(cbdr, 0, sizeof(*cbdr));
+		kfree(gcl_data);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < oper_len; i++) {
+		struct gce *temp_gce = gce + i;
+		struct tsn_qbv_entry *temp_entry = oper_basic->control_list + i;
+
+		temp_entry->gate_state = temp_gce->gate;
+		temp_entry->time_interval = le32_to_cpu(temp_gce->period);
+	}
+
+exit:
+	memset(cbdr, 0, sizeof(*cbdr));
+	kfree(gcl_data);
+	return 0;
+}
+
+/* CBD Class 7: Stream Identity Entry Set Descriptor - Long Format */
+int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
+				bool en, struct tsn_cb_streamid *streamid)
+{
+	struct enetc_cbd *cbdr;
+	void *si_data;
+	struct null_streamid_data *si_data1;
+	struct smac_streamid_data *si_data2;
+	struct streamid_conf *si_conf;
+	struct enetc_ndev_priv *priv;
+	dma_addr_t dma;
+	u16 data_size, dma_size;
+	int curr_cbd;
+
+	if (!ndev)
+		return -EINVAL;
+
+	priv = netdev_priv(ndev);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16((u16)index);
+	cbdr->cmd = 0;
+	cbdr->cls = BDCR_CMD_STREAM_IDENTIFY;
+	cbdr->status_flags = 0;
+
+	if (!en) {
+		cbdr->length = cpu_to_le16(8);
+		u64 zero = 0x8000000000000000;
+
+		dma = dma_map_single(&priv->si->pdev->dev, &zero, 8, DMA_FROM_DEVICE);
+		if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+			netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+			kfree(si_data);
+			return -ENOMEM;
+		}
+
+		cbdr->addr[0] = lower_32_bits(dma);
+		cbdr->addr[1] = upper_32_bits(dma);
+
+		si_conf = &cbdr->sid_set;
+		si_conf->ifac_iports = 0xffff;
+		si_conf->id_type = 1;
+		si_conf->oui[2] = 0x0;
+		si_conf->oui[1] = 0x80;
+		si_conf->oui[0] = 0xC2;
+
+		xmit_cbdr(priv->si, curr_cbd);
+		DUMP_CBDR(cbdr);
+		DUMP_DATA((char *)(&zero), 8);
+		memset(cbdr, 0, sizeof(*cbdr));
+		return 0;
+	}
+
+	si_conf = &cbdr->sid_set;
+	si_conf->en = 0x80;
+	si_conf->stream_handle = cpu_to_le32(streamid->handle);
+	if (streamid->ifac_iport)
+		si_conf->ifac_iports = cpu_to_le32(streamid->ifac_iport);
+	else
+		si_conf->ifac_iports = 0xffff;
+	si_conf->id_type = streamid->type;
+	si_conf->oui[2] = 0x0;
+	si_conf->oui[1] = 0x80;
+	si_conf->oui[0] = 0xC2;
+
+	if (si_conf->id_type == 1) {
+		data_size = sizeof(struct null_streamid_data);
+		si_data = (struct null_streamid_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	} else if (si_conf->id_type == 2) {
+		data_size = sizeof(struct smac_streamid_data);
+		si_data = (struct smac_streamid_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	} else
+		return -EINVAL;
+
+	if (si_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+	cbdr->status_flags = 0; /* long format command no ie */
+
+	dma = dma_map_single(&priv->si->pdev->dev, si_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		memset(cbdr, 0, sizeof(*cbdr));
+		kfree(si_data);
+		return -ENOMEM;
+	}
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+
+	/* VIDM default to be 1.
+	 * VID Match. If set (b1) then the VID must match, otherwise
+	 * any VID is considered a match. VIDM setting is only used
+	 * when TG is set to b01. */
+	if (si_conf->id_type == 1) {
+		si_data1 = (struct null_streamid_data *)si_data;
+		si_data1->dmac[0] = streamid->para.nid.dmac & 0xFF;
+		si_data1->dmac[1] = (streamid->para.nid.dmac >> 8) & 0xFF;
+		si_data1->dmac[2] = (streamid->para.nid.dmac >> 16) & 0xFF;
+		si_data1->dmac[3] = (streamid->para.nid.dmac >> 24) & 0xFF;
+		si_data1->dmac[4] = (streamid->para.nid.dmac >> 32) & 0xFF;
+		si_data1->dmac[5] = (streamid->para.nid.dmac >> 40) & 0xFF;
+		si_data1->vid_vidm_tg =
+		cpu_to_le16((streamid->para.nid.vid & ENETC_CBDR_SID_VID_MASK) +
+			((((u16)(streamid->para.nid.tagged) & 0x3) << 14) | ENETC_CBDR_SID_VIDM));
+	} else if (si_conf->id_type == 2) {
+		si_data2 = (struct smac_streamid_data *)si_data;
+		si_data2->smac[0] = streamid->para.sid.smac & 0xFF;
+		si_data2->smac[1] = (streamid->para.sid.smac >> 8) & 0xFF;
+		si_data2->smac[2] = (streamid->para.sid.smac >> 16) & 0xFF;
+		si_data2->smac[3] = (streamid->para.sid.smac >> 24) & 0xFF;
+		si_data2->smac[4] = (streamid->para.sid.smac >> 32) & 0xFF;
+		si_data2->smac[5] = (streamid->para.sid.smac >> 40) & 0xFF;
+		si_data2->vid_vidm_tg =
+		cpu_to_le16((streamid->para.sid.vid & ENETC_CBDR_SID_VID_MASK) +
+			((((u16)(streamid->para.sid.tagged) & 0x3) << 14) | ENETC_CBDR_SID_VIDM));
+	}
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)si_data, data_size);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	kfree(si_data);
+
+	return 0;
+}
+
+/* CBD Class 7: Stream Identity Entry Query Descriptor - Long Format */
+int enetc_cb_streamid_get(struct net_device *ndev, u32 index,
+							struct tsn_cb_streamid *streamid)
+{
+	struct enetc_cbd *cbdr;
+	struct streamid_query_resp *si_data;
+	struct enetc_ndev_priv *priv;
+	dma_addr_t dma;
+	u16 data_size, dma_size;
+	int curr_cbd;
+
+	if (!ndev)
+		return -EINVAL;
+
+	priv = netdev_priv(ndev);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le32(index);
+	cbdr->cmd = 1;
+	cbdr->cls = BDCR_CMD_STREAM_IDENTIFY;
+	cbdr->status_flags = 0;
+
+	data_size = sizeof(struct streamid_query_resp);
+	si_data = (struct streamid_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (si_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+	cbdr->status_flags = 0; /* long format command no ie */
+
+	dma = dma_map_single(&priv->si->pdev->dev, si_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		kfree(si_data);
+		return -ENOMEM;
+	}
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)si_data, data_size);
+
+	streamid->type = si_data->id_type;
+
+	if (streamid->type == 1) {
+		streamid->para.nid.dmac = si_data->mac[0] + ((u64)si_data->mac[1] << 8)
+			+ ((u64)si_data->mac[2] << 16) + ((u64)si_data->mac[3] << 24)
+			+ ((u64)si_data->mac[4] << 32) + ((u64)si_data->mac[5] << 40);
+		/* VID Match. If set (b1) then the VID must match, otherwise
+		 * any VID is considered a match.
+		*/
+		/* if (si_data->vid_vidm_tg & ENETC_CBDR_SID_VIDM) */
+		streamid->para.nid.vid =
+				le16_to_cpu(si_data->vid_vidm_tg & ENETC_CBDR_SID_VID_MASK);
+		streamid->para.nid.tagged =
+				le16_to_cpu(si_data->vid_vidm_tg >> 14 & 0x3);
+	} else if (streamid->type == 2) {
+		streamid->para.sid.smac = si_data->mac[0] + ((u64)si_data->mac[1] << 8)
+			+ ((u64)si_data->mac[2] << 16) + ((u64)si_data->mac[3] << 24)
+			+ ((u64)si_data->mac[4] << 32) + ((u64)si_data->mac[5] << 40);
+		/* VID Match. If set (b1) then the VID must match, otherwise
+		 * any VID is considered a match.
+		 */
+		/* if (si_data->vid_vidm_tg & ENETC_CBDR_SID_VIDM) */
+		streamid->para.sid.vid =
+				le16_to_cpu(si_data->vid_vidm_tg & ENETC_CBDR_SID_VID_MASK);
+		streamid->para.sid.tagged =
+				le16_to_cpu(si_data->vid_vidm_tg >> 14 & 0x3);
+	}
+
+	streamid->handle = le32_to_cpu(si_data->stream_handle);
+	streamid->ifac_iport = le32_to_cpu(si_data->input_ports);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	kfree(si_data);
+
+	return 0;
+}
+
+/*  CBD Class 7: Stream Identity Statistics Query Descriptor - Long Format */
+int enetc_cb_streamid_counters_get(struct net_device *ndev, u32 index,
+				struct tsn_cb_streamid_counters *counters)
+{
+	struct enetc_cbd *cbdr;
+	struct streamid_stat_query_resp *si_data;
+	struct streamid_stat_query *query;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	dma_addr_t dma;
+	u16 data_size, dma_size;
+	int curr_cbd, i;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 2;
+	cbdr->cls = BDCR_CMD_STREAM_IDENTIFY;
+	cbdr->status_flags = 0;
+
+	query = &cbdr->sid_stat;
+
+	query->input_ports = 0xffff;
+
+	data_size = sizeof(struct streamid_stat_query_resp);
+	si_data = (struct streamid_stat_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (si_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+	cbdr->status_flags = 0; /* long format command no ie */
+
+	dma = dma_map_single(&priv->si->pdev->dev, si_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		memset(cbdr, 0, sizeof(*cbdr));
+		kfree(si_data);
+		return -ENOMEM;
+	}
+
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	counters->per_stream.input = ((u64)le32_to_cpu(si_data->psinh) << 32)
+				+ le32_to_cpu(si_data->psinl);
+
+	for (i = 0; i < 16; i++)
+		counters->per_streamport[i].input = si_data->pspi[i];
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	kfree(si_data);
+	return 0;
+}
+
+void enetc_qci_enable(struct enetc_hw *hw)
+{
+	enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR) | ENETC_PSFPEN);
+}
+
+void enetc_qci_disable(struct enetc_hw *hw)
+{
+	enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR) & ~ENETC_PSFPEN);
+}
+
+/* CBD Class 8: Stream Filter Instance Set Descriptor - Short Format */
+int enetc_qci_sfi_set(struct net_device *ndev, u32 index, bool en,
+		struct tsn_qci_psfp_sfi_conf *tsn_qci_sfi)
+{
+	struct enetc_cbd *cbdr;
+	struct sfi_conf *sfi_config;
+
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int curr_cbd;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 0;
+	cbdr->cls = BDCR_CMD_STREAM_FILTER;
+	cbdr->status_flags = 0x80;
+	cbdr->length = cpu_to_le16(1);
+
+	sfi_config = &cbdr->sfi_conf;
+	if (en)
+		sfi_config->en = 0x80;
+
+	if (tsn_qci_sfi->stream_handle_spec >= 0) {
+		sfi_config->stream_handle =
+			cpu_to_le32(tsn_qci_sfi->stream_handle_spec);
+		sfi_config->sthm |= 0x80;
+	}
+
+	sfi_config->sg_inst_table_index =
+		cpu_to_le16(tsn_qci_sfi->stream_gate_instance_id);
+	sfi_config->input_ports = 0xFFFF;
+
+	/* The priority value which may be matched against the
+	 * frameâ€™s priority value to determine a match for this entry.
+	 */
+	if (tsn_qci_sfi->priority_spec >= 0)
+		sfi_config->multi |= (tsn_qci_sfi->priority_spec & 0x7) | 0x8;
+
+	/* Filter Type. Identifies the contents of the MSDU/FM_INST_INDEX
+	 * field as being either an MSDU value or an index into the Flow
+	 * Meter Instance table.
+	 */
+	if (tsn_qci_sfi->stream_filter.maximum_sdu_size != 0) {
+		sfi_config->msdu =
+		cpu_to_le16(tsn_qci_sfi->stream_filter.maximum_sdu_size);
+		sfi_config->multi |= 0x40;
+	}
+
+	if (tsn_qci_sfi->stream_filter.flow_meter_instance_id >= 0) {
+		sfi_config->fm_inst_table_index =
+		cpu_to_le16(tsn_qci_sfi->stream_filter.flow_meter_instance_id);
+		sfi_config->multi |= 0x80;
+	}
+
+	/* Stream blocked due to oversized frame enable. TRUE or FALSE */
+	if (tsn_qci_sfi->block_oversize_enable)
+		sfi_config->multi |= 0x20;
+
+	/* Stream blocked due to oversized frame. TRUE or FALSE */
+	if (tsn_qci_sfi->block_oversize)
+		sfi_config->multi |= 0x10;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+
+/* CBD Class 8: Stream Filter Instance Query Descriptor - Short Format */
+int enetc_qci_sfi_get(struct net_device *ndev, u32 index,
+						struct tsn_qci_psfp_sfi_conf *tsn_qci_sfi)
+{
+	struct enetc_cbd *cbdr;
+	struct sfi_conf *sfi_config;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int curr_cbd;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 1;
+	cbdr->cls = BDCR_CMD_STREAM_FILTER;
+	cbdr->status_flags = 0x80;
+
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	sfi_config = &cbdr->sfi_conf;
+	if (sfi_config->sthm & 0x80)
+		tsn_qci_sfi->stream_handle_spec =
+			le32_to_cpu(sfi_config->stream_handle);
+	else
+		tsn_qci_sfi->stream_handle_spec = -1;
+
+	tsn_qci_sfi->stream_gate_instance_id =
+		le16_to_cpu(sfi_config->sg_inst_table_index);
+
+	if (sfi_config->multi & 0x8)
+		tsn_qci_sfi->priority_spec = le16_to_cpu(sfi_config->multi & 0x7);
+	else
+		tsn_qci_sfi->priority_spec = -1;
+
+	/* Filter Type. Identifies the contents of the MSDU/FM_INST_INDEX
+	 * field as being either an MSDU value or an index into the Flow
+	 * Meter Instance table.
+	 */
+	if (sfi_config->multi & 0x80)
+		tsn_qci_sfi->stream_filter.flow_meter_instance_id =
+			le16_to_cpu(sfi_config->fm_inst_table_index);
+	else
+		tsn_qci_sfi->stream_filter.flow_meter_instance_id = -1;
+
+	if (sfi_config->multi & 0x40)
+		tsn_qci_sfi->stream_filter.maximum_sdu_size =
+			le16_to_cpu(sfi_config->msdu);
+
+	/* Stream blocked due to oversized frame enable. TRUE or FALSE */
+	if (sfi_config->multi & 0x20)
+		tsn_qci_sfi->block_oversize_enable = true;
+	/* Stream blocked due to oversized frame. TRUE or FALSE */
+	if (sfi_config->multi & 0x10)
+		tsn_qci_sfi->block_oversize = true;
+
+	if (sfi_config->en & 0x80) {
+		memset(cbdr, 0, sizeof(*cbdr));
+		return 1;
+	}
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+
+#ifdef ENETC_BG_V86
+/* CBD Class 8: Stream Filter Instance Query Statistics
+ * Descriptor - Short Format
+ */
+int enetc_qci_sfi_counters_get(struct net_device *ndev, u32 index,
+								struct tsn_qci_psfp_sfi_counters *counters)
+{
+	struct enetc_cbd *cbdr;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct tsn_qci_psfp_sfi_conf tsn_qci_sfi;
+	struct fmi_query_stat_resp fmi_counter;
+	struct sgi_query_stat_resp sgi_counter;
+	int curr_cbd;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16((u16)index);
+	cbdr->cmd = 2;
+	cbdr->cls = BDCR_CMD_STREAM_FILTER;
+	cbdr->status_flags = 0x80;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	memset(&tsn_qci_sfi, 0, sizeof(struct tsn_qci_psfp_sfi_conf));
+	enetc_qci_sfi_get(ndev, index, &tsn_qci_sfi);
+
+	counters->matching_frames_count =
+			((u64)le32_to_cpu(cbdr->sfi_query_stat_resp.match_filter_counth) << 32)
+			+ cbdr->sfi_query_stat_resp.match_filter_countl;
+	counters->not_passing_sdu_count =
+			((u64)le32_to_cpu(cbdr->sfi_query_stat_resp.sdu_filter_drop_counth) << 32)
+			+ cbdr->sfi_query_stat_resp.sdu_filter_drop_countl;
+	counters->passing_sdu_count =
+			((u64)le32_to_cpu(cbdr->sfi_query_stat_resp.sdu_filter_pass_counth) << 32)
+			+ cbdr->sfi_query_stat_resp.sdu_filter_pass_countl;
+
+	memset(&sgi_counter, 0, sizeof(struct sgi_query_stat_resp));
+	enetc_qci_sgi_counters_get(ndev,
+			tsn_qci_sfi.stream_gate_instance_id, &sgi_counter);
+
+	if (tsn_qci_sfi.stream_filter.flow_meter_instance_id >= 0) {
+		memset(&fmi_counter, 0, sizeof(struct fmi_query_stat_resp));
+		enetc_qci_fmi_counters_get(ndev,
+				tsn_qci_sfi.stream_filter.flow_meter_instance_id, &fmi_counter);
+	}
+
+	counters->not_passing_frames_count = ((u64)le32_to_cpu(sgi_counter.dgch) << 32) +
+					le32_to_cpu(sgi_counter.dgcl);
+	counters->passing_frames_count = ((u64)le32_to_cpu(sgi_counter.pgch) << 32) +
+					le32_to_cpu(sgi_counter.pgcl);
+	counters->red_frames_count = ((u64)le32_to_cpu(fmi_counter.rrfh) << 32) +
+					le32_to_cpu(fmi_counter.rrfl);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+#else
+/* CBD Class 8: Stream Filter Instance Query Statistics
+ * Descriptor - Long Format
+ */
+int enetc_qci_sfi_counters_get(struct net_device *ndev, u32 index,
+							struct tsn_qci_psfp_sfi_counters *counters)
+{
+	struct enetc_cbd *cbdr;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int curr_cbd;
+	struct sfi_counter_data *sfi_counter_data;
+	dma_addr_t dma;
+	u16 data_size, dma_size;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16((u16)index);
+	cbdr->cmd = 2;
+	cbdr->cls = BDCR_CMD_STREAM_FILTER;
+	cbdr->status_flags = 0;
+
+	data_size = sizeof(struct sfi_counter_data);
+	sfi_counter_data = (struct sfi_counter_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (sfi_counter_data == NULL)
+		return -ENOMEM;
+
+	dma = dma_map_single(&priv->si->pdev->dev, sfi_counter_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		kfree(sfi_counter_data);
+		return -ENOMEM;
+	}
+	cbdr->addr[0] = lower_32_bits(dma);
+	cbdr->addr[1] = upper_32_bits(dma);
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr->length = dma_size;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+	DUMP_DATA((char *)sfi_counter_data, data_size);
+
+	counters->matching_frames_count =
+			((u64)le32_to_cpu(sfi_counter_data->matchh) << 32)
+			+ sfi_counter_data->matchl;
+	counters->not_passing_sdu_count =
+			((u64)le32_to_cpu(sfi_counter_data->msdu_droph) << 32)
+			+ sfi_counter_data->msdu_dropl;
+	counters->passing_sdu_count = counters->matching_frames_count - counters->not_passing_sdu_count;
+
+	counters->not_passing_frames_count = ((u64)le32_to_cpu(sfi_counter_data->stream_gate_droph) << 32)
+				+ le32_to_cpu(sfi_counter_data->stream_gate_dropl);
+
+	counters->passing_frames_count = counters->matching_frames_count - counters->not_passing_sdu_count
+				- counters->not_passing_frames_count;
+
+	counters->red_frames_count = ((u64)le32_to_cpu(sfi_counter_data->flow_meter_droph) << 32)
+				+ le32_to_cpu(sfi_counter_data->flow_meter_dropl);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+#endif
+/*
+ * CBD Class 9: Stream Gate Instance Table Entry Set
+ * Descriptor - Short Format
+ */
+int enetc_qci_sgi_set(struct net_device *ndev, u32 index,
+				struct tsn_qci_psfp_sgi_conf *tsn_qci_sgi)
+{
+	struct enetc_cbd *cbdr, *cbdr_sgcl;
+	struct sgi_table *sgi_config;
+	struct sgcl_conf *sgcl_config;
+	struct sgcl_data *sgcl_data;
+	struct sgce *sgce;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	dma_addr_t dma;
+	u16 data_size, dma_size;
+	int curr_cbd, i;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 0;
+	cbdr->cls = BDCR_CMD_STREAM_GCL;
+	cbdr->status_flags = 0x80;
+
+	sgi_config = &cbdr->sgi_table;
+	sgi_config->ocgtst |= 0x80;
+
+	if (!tsn_qci_sgi->gate_enabled) {
+		xmit_cbdr(priv->si, curr_cbd);
+		memset(cbdr, 0, sizeof(*cbdr));
+		return 0;
+	}
+	sgi_config->en = 0x80;
+
+	if (tsn_qci_sgi->block_invalid_rx_enable)
+		sgi_config->gset |= 0x80;
+	if (tsn_qci_sgi->block_invalid_rx)
+		sgi_config->gset |= 0x40;
+	if (tsn_qci_sgi->block_octets_exceeded)
+		sgi_config->gset |= 0x10;
+	if (tsn_qci_sgi->block_octets_exceeded_enable)
+		sgi_config->gset |= 0x20;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr_sgcl);
+
+	cbdr_sgcl->index = cpu_to_le16(index);
+	cbdr_sgcl->cmd = 1;
+	cbdr_sgcl->cls = BDCR_CMD_STREAM_GCL;
+	cbdr_sgcl->status_flags = 0;
+
+	sgcl_config = &cbdr_sgcl->sgcl_conf;
+	if (tsn_qci_sgi->admin.control_list_length > 4)
+		return -EINVAL;
+	else if (tsn_qci_sgi->admin.control_list_length)
+		sgcl_config->acl_len = (tsn_qci_sgi->admin.control_list_length - 1) & 0x3;
+	else
+		sgcl_config->acl_len = 0;
+
+	data_size = sizeof(struct sgcl_data) +
+		(sgcl_config->acl_len + 1) * sizeof(struct sgce);
+
+	sgcl_data = (struct sgcl_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (sgcl_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr_sgcl->length = dma_size;
+
+	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		memset(cbdr, 0, sizeof(*cbdr));
+		memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+		kfree(sgcl_data);
+		return -ENOMEM;
+	}
+	cbdr_sgcl->addr[0] = lower_32_bits(dma);
+	cbdr_sgcl->addr[1] = upper_32_bits(dma);
+
+	sgce = (struct sgce *)(sgcl_data + 1);
+
+	if (tsn_qci_sgi->admin.gate_states)
+		sgcl_config->agtst = 0x80;
+
+	sgcl_data->ct = cpu_to_le32(tsn_qci_sgi->admin.cycle_time);
+	sgcl_data->cte = cpu_to_le32(tsn_qci_sgi->admin.cycle_time_extension);
+	sgcl_data->bth = cpu_to_le32(upper_32_bits(tsn_qci_sgi->admin.base_time));
+	sgcl_data->btl = cpu_to_le32(lower_32_bits(tsn_qci_sgi->admin.base_time));
+
+	if (tsn_qci_sgi->admin.init_ipv >= 0)
+		sgcl_config->aipv = (tsn_qci_sgi->admin.init_ipv & 0x7) | 0x8;
+
+	for (i = 0; i < tsn_qci_sgi->admin.control_list_length; i++) {
+		struct tsn_qci_psfp_gcl *temp_sgcl = tsn_qci_sgi->admin.gcl + i;
+		struct sgce *temp_entry = (struct sgce *)(sgce + i);
+
+		if (temp_sgcl->gate_state)
+			temp_entry->multi |= 0x10;
+
+		if (temp_sgcl->ipv >= 0)
+			temp_entry->multi |= ((temp_sgcl->ipv & 0x7) << 5) | 0x08;
+
+		if (temp_sgcl->octet_max)
+			temp_entry->multi |= 0x01;
+
+		temp_entry->interval = cpu_to_le32(temp_sgcl->time_interval);
+		temp_entry->msdu[0] = temp_sgcl->octet_max & 0xFF;
+		temp_entry->msdu[1] = (temp_sgcl->octet_max >> 8) & 0xFF;
+		temp_entry->msdu[2] = (temp_sgcl->octet_max >> 16) & 0xFF;
+	}
+
+	/* Workaround for length = 0, then period = 0 , then hang at simulator */
+	if (!tsn_qci_sgi->admin.control_list_length) {
+		sgce->interval = 10;
+		sgce->multi |= (sgcl_config->agtst >> 3);
+		sgcl_data->ct = 10;
+	}
+
+	DUMP_CBDR(cbdr_sgcl);
+	DUMP_DATA((char *)sgcl_data, data_size);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr_sgcl);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+	kfree(sgcl_data);
+	return 0;
+}
+
+/* CBD Class 9: Stream Gate Instance Table Entry Query
+ * Descriptor - Short Format
+ */
+int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
+				struct tsn_qci_psfp_sgi_conf *tsn_qci_sgi)
+{
+	struct enetc_cbd *cbdr, *cbdr_sgcl;
+	struct sgi_table *sgi_config;
+	struct sgcl_query *sgcl_query;
+	struct sgcl_query_resp *sgcl_data;
+	struct sgce *sgce;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	dma_addr_t dma;
+	u16 data_size, dma_size, gcl_data_stat = 0;
+	u8 admin_len = 0;
+	int curr_cbd, i;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 2;
+	cbdr->cls = BDCR_CMD_STREAM_GCL;
+	cbdr->status_flags = 0x80;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	sgi_config = &cbdr->sgi_table;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr_sgcl);
+
+	cbdr_sgcl->index = cpu_to_le16(index);
+	cbdr_sgcl->cmd = 3;
+	cbdr_sgcl->cls = BDCR_CMD_STREAM_GCL;
+	cbdr_sgcl->status_flags = 0;
+
+	data_size = sizeof(struct sgcl_query_resp) + 4 * sizeof(struct sgce); /* Max is 4 */
+
+	sgcl_data = (struct sgcl_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (sgcl_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr_sgcl->length = dma_size;
+	cbdr_sgcl->status_flags = 0;
+
+	sgcl_query = &cbdr_sgcl->sgcl_query;
+
+#ifdef ENETC_BG_V86
+	sgcl_query->oacl_len = 0x3; /* Get the admin control list */
+#else
+	sgcl_query->oacl_len = 0x10;
+#endif
+
+	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		memset(cbdr, 0, sizeof(*cbdr));
+		memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+		kfree(sgcl_data);
+		return -ENOMEM;
+	}
+	cbdr_sgcl->addr[0] = lower_32_bits(dma);
+	cbdr_sgcl->addr[1] = upper_32_bits(dma);
+
+	DUMP_CBDR(cbdr_sgcl);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr_sgcl);
+	DUMP_DATA((char *)sgcl_data, data_size);
+
+	if (sgi_config->en & 0x80)
+		tsn_qci_sgi->gate_enabled = true;
+	if (sgi_config->gset & 0x80)
+		tsn_qci_sgi->block_invalid_rx_enable = true;
+	if (sgi_config->gset & 0x40)
+		tsn_qci_sgi->block_invalid_rx = true;
+	if (sgi_config->gset & 0x20)
+		tsn_qci_sgi->block_octets_exceeded_enable = true;
+	if (sgi_config->gset & 0x10)
+		tsn_qci_sgi->block_octets_exceeded = true;
+
+	sgce = (struct sgce *)(sgcl_data + 1);
+#ifndef ENETC_BG_V86
+	gcl_data_stat = le16_to_cpu(sgcl_data->stat);
+	if (gcl_data_stat & 0x10)
+		tsn_qci_sgi->admin.gate_states = true;
+
+	if (gcl_data_stat & 0x80)
+		tsn_qci_sgi->admin.init_ipv = gcl_data_stat & 0x7;
+	else
+		tsn_qci_sgi->admin.init_ipv = -1;
+#endif
+	/* admin_len can also get from gcl_data_stat bit 5,6 OR sgi_config->oacl_len */
+	admin_len = (sgcl_query->oacl_len & 0x3) + 1;
+	tsn_qci_sgi->admin.control_list_length = admin_len;
+	tsn_qci_sgi->admin.cycle_time = le32_to_cpu(sgcl_data->act);
+	tsn_qci_sgi->admin.cycle_time_extension = le32_to_cpu(sgcl_data->acte);
+	tsn_qci_sgi->admin.base_time = ((u64)(le32_to_cpu(sgcl_data->abth)) << 32)
+						+ le32_to_cpu(sgcl_data->abtl);
+
+	tsn_qci_sgi->admin.gcl =
+		kzalloc(admin_len * sizeof(struct tsn_qci_psfp_gcl), GFP_KERNEL);
+	if (tsn_qci_sgi->admin.gcl == NULL) {
+		kfree(sgcl_data);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < admin_len; i++) {
+		struct tsn_qci_psfp_gcl *temp_sgcl = tsn_qci_sgi->admin.gcl + i;
+		struct sgce *temp_entry = (struct sgce *)(sgce + i);
+
+		if (temp_entry->multi & 0x10)
+			temp_sgcl->gate_state = true;
+
+		if (temp_entry->multi & 0x08)
+			temp_sgcl->ipv = temp_entry->multi >> 5;
+		else
+			temp_sgcl->ipv = -1;
+
+		temp_sgcl->time_interval = le32_to_cpu(temp_entry->interval);
+
+		if (temp_entry->multi & 0x01)
+			temp_sgcl->octet_max = (temp_entry->msdu[0] & 0xff)
+							| (((u32)temp_entry->msdu[1] << 8) & 0xff00)
+							| (((u32)temp_entry->msdu[1] << 16) & 0xff0000);
+		else
+			temp_sgcl->octet_max = 0;
+	}
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+
+	kfree(sgcl_data);
+	return 0;
+}
+
+/* CBD Class 9: Stream Gate Instance Table Entry Query Descriptor - Short Format */
+/* CBD Class 9: Stream Gate Control List Query Descriptor - Long Format */
+int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
+				struct tsn_psfp_sgi_status *status)
+{
+	struct enetc_cbd *cbdr_sgi, *cbdr_sgcl;
+	struct sgi_table *sgi_config;
+	struct sgcl_query *sgcl_query;
+	struct sgcl_query_resp *sgcl_data;
+	struct sgce *sgce;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	dma_addr_t dma;
+	u16 data_size, dma_size, gcl_data_stat = 0;
+	u8 oper_len = 0;
+	u64 temp;
+	int curr_cbd, i;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr_sgi);
+
+	cbdr_sgi->index = cpu_to_le16(index);
+	cbdr_sgi->cmd = 2;
+	cbdr_sgi->cls = BDCR_CMD_STREAM_GCL;
+	cbdr_sgi->status_flags = 0x80;
+
+	sgi_config = &cbdr_sgi->sgi_table;
+
+	DUMP_CBDR(cbdr_sgi);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr_sgi);
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr_sgcl);
+
+	cbdr_sgcl->index = cpu_to_le16(index);
+	cbdr_sgcl->cmd = 3;
+	cbdr_sgcl->cls = BDCR_CMD_STREAM_GCL;
+	cbdr_sgcl->status_flags = 0;
+
+	data_size = sizeof(struct sgcl_query_resp) + 4 * sizeof(struct sgce); /* Max is 4 */
+
+	sgcl_data = (struct sgcl_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (sgcl_data == NULL)
+		return -ENOMEM;
+
+	dma_size = cpu_to_le16(data_size);
+	cbdr_sgcl->length = dma_size;
+	cbdr_sgcl->status_flags = 0;
+
+	sgcl_query = &cbdr_sgcl->sgcl_query;
+#ifdef ENETC_BG_V86
+	sgcl_query->oacl_len = 0x0c; /* Get the oper control list */
+#else
+	sgcl_query->oacl_len = 0x20;
+#endif
+
+	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data, data_size, DMA_FROM_DEVICE);
+	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
+		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
+		memset(cbdr_sgi, 0, sizeof(*cbdr_sgi));
+		memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+		kfree(sgcl_data);
+		return -ENOMEM;
+	}
+	cbdr_sgcl->addr[0] = lower_32_bits(dma);
+	cbdr_sgcl->addr[1] = upper_32_bits(dma);
+
+	DUMP_CBDR(cbdr_sgcl);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr_sgcl);
+	DUMP_DATA((char *)sgcl_data, data_size);
+
+	sgce = (struct sgce *)(sgcl_data + 1);
+
+	/* oper_len can also get from gcl_data_stat bit 5,6 OR sgi_config->oacl_len */
+	oper_len = ((sgcl_query->oacl_len & 0x0c) >> 2) + 1;
+
+	/* Get Stream Gate Control List */
+	status->oper.cycle_time = le32_to_cpu(sgcl_data->oct);
+	status->oper.cycle_time_extension = le32_to_cpu(sgcl_data->octe);
+	status->oper.base_time = le32_to_cpu(sgcl_data->obtl) + ((u64)le32_to_cpu(sgcl_data->obth) << 32);
+	status->oper.control_list_length = oper_len;
+#ifndef ENETC_BG_V86
+	gcl_data_stat = le16_to_cpu(sgcl_data->stat);
+	if (gcl_data_stat & 0x400)
+		status->oper.init_ipv = gcl_data_stat & 0x38 >> 7;
+	else
+		status->oper.init_ipv = -1;
+
+	if (gcl_data_stat & 0x800)
+		status->oper.gate_states = true;
+#endif
+	status->oper.gcl =
+		kzalloc(oper_len * sizeof(struct tsn_qci_psfp_gcl), GFP_KERNEL);
+	if (status->oper.gcl == NULL) {
+		memset(cbdr_sgi, 0, sizeof(*cbdr_sgi));
+		memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+		kfree(sgcl_data);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < oper_len; i++) {
+		struct tsn_qci_psfp_gcl *temp_sgcl = status->oper.gcl + i;
+		struct sgce *temp_entry = (struct sgce *)(sgce + i);
+
+		if (temp_entry->multi & 0x10)
+			temp_sgcl->gate_state = true;
+
+		if (temp_entry->multi & 0x08)
+			temp_sgcl->ipv = temp_entry->multi >> 5;
+		else
+			temp_sgcl->ipv = -1;
+
+		temp_sgcl->time_interval = le32_to_cpu(temp_entry->interval);
+
+		if (temp_entry->multi & 0x01)
+			temp_sgcl->octet_max = temp_entry->msdu[0]
+					| ((((u32)temp_entry->msdu[1]) << 8) & 0xff00)
+					| ((((u32)temp_entry->msdu[2]) << 16) & 0xff0000);
+		else
+			temp_sgcl->octet_max = 0;
+	}
+
+	if (sgi_config->gset & 0x4)
+		status->config_pending = true;
+
+	/* changed to SITGTGR */
+	status->tick_granularity = enetc_rd(&priv->si->hw, ENETC_SITGTGR);
+
+	/* current time */
+	temp = ((u64)enetc_rd(&priv->si->hw, ENETC_SICTR + 4)) << 32;
+	status->current_time = enetc_rd(&priv->si->hw, ENETC_SICTR) + temp;
+
+	status->config_change_time = le32_to_cpu(sgcl_data->cctl) + ((u64)le32_to_cpu(sgcl_data->ccth) << 32);
+
+	memset(cbdr_sgi, 0, sizeof(*cbdr_sgi));
+	memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
+
+	return 0;
+}
+
+/* CBD Class 10: Flow Meter Instance Set Descriptor - Short Format */
+int enetc_qci_fmi_set(struct net_device *ndev, u32 index,
+				struct tsn_qci_psfp_fmi *tsn_qci_fmi)
+{
+	struct enetc_cbd *cbdr;
+	struct fmi_conf *fmi_config;
+
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int curr_cbd;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16((u16)index);
+	cbdr->cmd = 0;
+	cbdr->cls = BDCR_CMD_FLOW_METER;
+	cbdr->status_flags = 0x80;
+
+	fmi_config = &cbdr->fmi_conf;
+	fmi_config->cir = cpu_to_le32(tsn_qci_fmi->cir);
+	fmi_config->cbs = cpu_to_le32(tsn_qci_fmi->cbs);
+	fmi_config->eir = cpu_to_le32(tsn_qci_fmi->eir);
+	fmi_config->ebs = cpu_to_le32(tsn_qci_fmi->ebs);
+	if ((tsn_qci_fmi->cir == 0) && (tsn_qci_fmi->cbs == 0)
+			&& (tsn_qci_fmi->eir == 0) && (tsn_qci_fmi->ebs == 0))
+		fmi_config->en = 0;
+	else
+		fmi_config->en = 0x80;
+
+	if (tsn_qci_fmi->mark_red)
+		fmi_config->conf |= 0x1;
+
+	if (tsn_qci_fmi->mark_red_enable)
+		fmi_config->conf |= 0x2;
+
+	if (tsn_qci_fmi->drop_on_yellow)
+		fmi_config->conf |= 0x4;
+
+	if (tsn_qci_fmi->cm)
+		fmi_config->conf |= 0x8;
+
+	if (tsn_qci_fmi->cf)
+		fmi_config->conf |= 0x10;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+
+/* CBD Class 10: Flow Meter Instance Query Descriptor - Short Format */
+int enetc_qci_fmi_get(struct net_device *ndev, u32 index,
+						struct tsn_qci_psfp_fmi *tsn_qci_fmi)
+{
+	struct enetc_cbd *cbdr;
+	struct fmi_conf *fmi_config;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int curr_cbd;
+
+	curr_cbd = alloc_cbdr(priv->si, &cbdr);
+
+	cbdr->index = cpu_to_le16(index);
+	cbdr->cmd = 1;
+	cbdr->cls = BDCR_CMD_FLOW_METER;
+	cbdr->status_flags = 0x80;
+
+	DUMP_CBDR(cbdr);
+	xmit_cbdr(priv->si, curr_cbd);
+	DUMP_CBDR(cbdr);
+
+	fmi_config = &cbdr->fmi_conf;
+	tsn_qci_fmi->cir = le32_to_cpu(fmi_config->cir);
+	tsn_qci_fmi->cbs = le32_to_cpu(fmi_config->cbs);
+	tsn_qci_fmi->eir = le32_to_cpu(fmi_config->eir);
+	tsn_qci_fmi->ebs = le32_to_cpu(fmi_config->ebs);
+
+	if (fmi_config->conf & 0x1)
+		tsn_qci_fmi->mark_red = true;
+
+	if (fmi_config->conf & 0x2)
+		tsn_qci_fmi->mark_red_enable = true;
+
+	if (fmi_config->conf & 0x4)
+		tsn_qci_fmi->drop_on_yellow = true;
+
+	if (fmi_config->conf & 0x8)
+		tsn_qci_fmi->cm = true;
+
+	if (fmi_config->conf & 0x10)
+		tsn_qci_fmi->cf = true;
+
+	memset(cbdr, 0, sizeof(*cbdr));
+	return 0;
+}
+
+u32 __enetc_tsn_get_cap(struct enetc_si *si)
+{
+	u32 reg = 0;
+	u32 cap = 0;
+
+	reg = enetc_port_rd(&si->hw, ENETC_PCAPR0);
+
+	if (reg & ENETC_PCAPR0_PSFP)
+		cap |= TSN_CAP_QCI;
+	else if (reg & ENETC_PCAPR0_TSN)
+		cap |= TSN_CAP_QBV;
+	else if (reg & ENETC_PCAPR0_QBU)
+		cap |= TSN_CAP_QBU;
+
+	cap |= TSN_CAP_CBS;
+	cap |= TSN_CAP_TBS;
+
+	return cap;
+}
+
+u32 enetc_tsn_get_capability(struct net_device *ndev)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	return __enetc_tsn_get_cap(priv->si);
+}
+
+static struct tsn_ops enetc_tsn_ops = {
+	.qbv_set = enetc_qbv_set,
+	.qbv_get = enetc_qbv_get,
+	.qbv_get_status = enetc_qbv_get_status,
+	.cb_streamid_set = enetc_cb_streamid_set,
+	.cb_streamid_get = enetc_cb_streamid_get,
+	.cb_streamid_counters_get = enetc_cb_streamid_counters_get,
+	.qci_sfi_set = enetc_qci_sfi_set,
+	.qci_sfi_get = enetc_qci_sfi_get,
+	.qci_sfi_counters_get = enetc_qci_sfi_counters_get,
+	.qci_sgi_set = enetc_qci_sgi_set,
+	.qci_sgi_get = enetc_qci_sgi_get,
+	.qci_sgi_status_get = enetc_qci_sgi_status_get,
+	.qci_fmi_set = enetc_qci_fmi_set,
+	.qci_fmi_get = enetc_qci_fmi_get,
+};
+
+void enetc_tsn_init(struct enetc_si *si)
+{
+	si->ndev->tsn_ops = &enetc_tsn_ops;
+#ifdef ENETC_BG_V86
+	enetc_wr(&si->hw, ENETC_SITGTGR, 0x1);
+#endif
+
+	enetc_qci_enable(&si->hw);
+
+	dev_info(&si->pdev->dev, "%s: setup done\n", __func__);
+}
+#endif	/* #if IS_ENABLED(CONFIG_ENETC_TSN) */
-- 
2.17.1

