From c08b7100f160ecde7014f3527dcce9d402a1470d Mon Sep 17 00:00:00 2001
From: Stanislaw Kardach <skardach@marvell.com>
Date: Fri, 1 Feb 2019 14:19:51 +0530
Subject: [PATCH 102/255] octeontx2-af: Add resource partitioning config

commit 03edea00f9751eeabc12eed3012d06ea1a153e0f from
git@git.assembla.com:cavium/WindRiver.linux.git

This commit adds facilities for configuring and tracking resource
partitioning between RVU PF devices (and indirectly their related
RVU VFs).
The AF keeps track of the PF level resource partitioning only because
partitioning between VFs is a job of each RVU PF and only RVU PF driver
knows the moment and number of VFs enabled.

For each tracked resource (currently RVU LFs), user can set a limit
for each RVU PF on how many resources may be attached to it via
MBOX (MBOX_MSG_ATTACH_RESOURCES msg for LFs).
This limit value K for PF X means that the sum of attach requests
for that resource coming from all pcifunc belonging to PF x (called
PF x's family) cannot exceed K.
The sum of limit values for a particular resource always has to be lower
or equal to the total amount of those resources on the system.

The limits are modifiable for PFx until resource attach message is
received. At that point they become read-only until all resources are
detached from that device.

The default values for the limits are defined as follows:
* RVU AF (for its future VF handling)
  - NIX: pci_sriov_get_totalvfs(pdev)
  - NPA: pci_sriov_get_totalvfs(pdev)
* networking RVU PF
  - NIX: 1 + pci_sriov_get_totalvfs(pdev)
  - NPA: 1 + pci_sriov_get_totalvfs(pdev)
  - rest: 0
* SSO/TIM RVU PF
  - NPA: pci_sriov_get_totalvfs(pdev)
  - SSO: N(SSO_LFS) / N(SSO_PFS)
  - SSOW: N(SSOW_LFS) / N(SSO_PFS)
  - TIM: N(TIM_LFS) / N(SSO_PFS)
  - CPT: num_online_cpus() / N(SSO_PFS)
  - rest: 0
* NPA RVU PF
  - NPA: pci_sriov_get_totalvfs(pdev)
  - rest: 0
* CPT RVU PF
  - CPT: num_online_cpus()
  - NPA: 1
  - rest: 0

Change-Id: I602ddc19f87c8d863e92eb05c4d933dd39af3d48
Signed-off-by: Stanislaw Kardach <skardach@marvell.com>
Signed-off-by: Krzysztof Garczynski <kgarczynski@marvell.com>
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 .../ethernet/marvell/octeontx2/af/Makefile    |   2 +-
 .../net/ethernet/marvell/octeontx2/af/rvu.c   |  16 +-
 .../net/ethernet/marvell/octeontx2/af/rvu.h   |   7 +
 .../marvell/octeontx2/af/rvu_validation.c     | 549 ++++++++++++++++++
 .../marvell/octeontx2/af/rvu_validation.h     |  64 ++
 5 files changed, 636 insertions(+), 2 deletions(-)
 create mode 100644 drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.c
 create mode 100644 drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.h

diff --git a/drivers/net/ethernet/marvell/octeontx2/af/Makefile b/drivers/net/ethernet/marvell/octeontx2/af/Makefile
index 06329acf9c2c..b6a792a25533 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/Makefile
+++ b/drivers/net/ethernet/marvell/octeontx2/af/Makefile
@@ -8,4 +8,4 @@ obj-$(CONFIG_OCTEONTX2_AF) += octeontx2_af.o
 
 octeontx2_mbox-y := mbox.o
 octeontx2_af-y := cgx.o rvu.o rvu_cgx.o rvu_npa.o rvu_nix.o \
-		  rvu_reg.o rvu_npc.o
+		  rvu_reg.o rvu_npc.o rvu_validation.o
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu.c
index e581091c09c4..1ae41370e379 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu.c
@@ -886,7 +886,7 @@ static int rvu_mbox_handler_ready(struct rvu *rvu, struct msg_req *req,
 /* Get current count of a RVU block's LF/slots
  * provisioned to a given RVU func.
  */
-static u16 rvu_get_rsrc_mapcount(struct rvu_pfvf *pfvf, int blktype)
+u16 rvu_get_rsrc_mapcount(struct rvu_pfvf *pfvf, int blktype)
 {
 	switch (blktype) {
 	case BLKTYPE_NPA:
@@ -1076,6 +1076,12 @@ static int rvu_check_rsrc_availability(struct rvu *rvu,
 	struct rvu_block *block;
 	int free_lfs, mappedlfs;
 
+	if (rvu_check_rsrc_policy(rvu, req, pcifunc)) {
+		dev_err(rvu->dev, "Func 0x%x: Resource policy check failed\n",
+			pcifunc);
+		return -EINVAL;
+	}
+
 	/* Only one NPA LF can be attached */
 	if (req->npalf && !rvu_get_rsrc_mapcount(pfvf, BLKTYPE_NPA)) {
 		block = &hw->block[BLKADDR_NPA];
@@ -2456,7 +2462,14 @@ static int rvu_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (err)
 		goto err_irq;
 
+	err = rvu_policy_init(rvu);
+	if (err)
+		goto err_sriov;
+
 	return 0;
+
+err_sriov:
+	rvu_disable_sriov(rvu);
 err_irq:
 	rvu_unregister_interrupts(rvu);
 err_flr:
@@ -2482,6 +2495,7 @@ static void rvu_remove(struct pci_dev *pdev)
 {
 	struct rvu *rvu = pci_get_drvdata(pdev);
 
+	rvu_policy_destroy(rvu);
 	rvu_unregister_interrupts(rvu);
 	rvu_flr_wq_destroy(rvu);
 	rvu_cgx_exit(rvu);
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu.h b/drivers/net/ethernet/marvell/octeontx2/af/rvu.h
index c9d60b0554c0..91213e75c178 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu.h
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu.h
@@ -15,6 +15,7 @@
 #include "rvu_struct.h"
 #include "common.h"
 #include "mbox.h"
+#include "rvu_validation.h"
 
 /* PCI device IDs */
 #define	PCI_DEVID_OCTEONTX2_RVU_AF		0xA065
@@ -147,6 +148,10 @@ struct rvu_pfvf {
 	u16			bcast_mce_idx;
 	struct nix_mce_list	bcast_mce_list;
 
+	/* For resource limits */
+	struct pci_dev	*pdev;
+	struct kobject	*limits_kobj;
+
 	/* VLAN offload */
 	struct mcam_entry entry;
 	int rxvlan_index;
@@ -229,6 +234,7 @@ struct rvu {
 	struct rvu_hwinfo       *hw;
 	struct rvu_pfvf		*pf;
 	struct rvu_pfvf		*hwvf;
+	struct rvu_limits	pf_limits;
 	struct mutex		rsrc_lock; /* Serialize resource alloc/free */
 	int			vfs; /* Number of VFs attached to RVU */
 
@@ -307,6 +313,7 @@ void rvu_free_rsrc(struct rsrc_bmap *rsrc, int id);
 int rvu_rsrc_free_count(struct rsrc_bmap *rsrc);
 int rvu_alloc_rsrc_contig(struct rsrc_bmap *rsrc, int nrsrc);
 bool rvu_rsrc_check_contig(struct rsrc_bmap *rsrc, int nrsrc);
+u16 rvu_get_rsrc_mapcount(struct rvu_pfvf *pfvf, int blktype);
 int rvu_get_pf(u16 pcifunc);
 struct rvu_pfvf *rvu_get_pfvf(struct rvu *rvu, int pcifunc);
 void rvu_get_pf_numvfs(struct rvu *rvu, int pf, int *numvfs, int *hwvf);
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.c
new file mode 100644
index 000000000000..fa9c7a8c045f
--- /dev/null
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.c
@@ -0,0 +1,549 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/types.h>
+#include <linux/pci.h>
+#include "rvu.h"
+
+#define	PCI_DEVID_OCTEONTX2_RVU_PF	0xA063
+#define	PCI_DEVID_OCTEONTX2_SSO_RVU_PF	0xA0F9
+#define	PCI_DEVID_OCTEONTX2_NPA_RVU_PF	0xA0FB
+#define	PCI_DEVID_OCTEONTX2_CPT_RVU_PF	0xA0FD
+
+static u64 quotas_get_sum(struct rvu_quotas *quotas)
+{
+	u64 lf_sum = 0;
+	int i;
+
+	for (i = 0; i < quotas->cnt; i++)
+		lf_sum += quotas->a[i].val;
+
+	return lf_sum;
+}
+
+static ssize_t quota_show(struct kobject *kobj, struct kobj_attribute *attr,
+			  char *buf)
+{
+	struct rvu_quota *quota;
+	int val;
+
+	quota = container_of(attr, struct rvu_quota, sysfs);
+
+	if (quota->base->lock)
+		mutex_lock(quota->base->lock);
+	val = quota->val;
+	if (quota->base->lock)
+		mutex_unlock(quota->base->lock);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", val);
+}
+
+static ssize_t quota_store(struct kobject *kobj, struct kobj_attribute *attr,
+			   const char *buf, size_t count)
+{
+	int old_val, new_val, res = 0;
+	struct rvu_quota *quota;
+	struct rvu_quotas *base;
+	struct device *dev;
+	u64 lf_sum;
+
+	quota = container_of(attr, struct rvu_quota, sysfs);
+	dev = quota->dev;
+	base = quota->base;
+
+	if (kstrtoint(buf, 0, &new_val)) {
+		dev_err(dev, "Invalid %s quota: %s\n", attr->attr.name, buf);
+		return -EIO;
+	}
+	if (new_val < 0) {
+		dev_err(dev, "Invalid %s quota: %d < 0\n", attr->attr.name,
+			new_val);
+		return -EIO;
+	}
+
+	if (new_val > base->max) {
+		dev_err(dev, "Invalid %s quota: %d > %d\n", attr->attr.name,
+			new_val, base->max);
+		return -EIO;
+	}
+
+	if (base->lock)
+		mutex_lock(base->lock);
+	old_val = quota->val;
+
+	if (base->ops.pre_store)
+		res = base->ops.pre_store(quota->ops_arg, quota, new_val);
+
+	if (res != 0) {
+		res = -EIO;
+		goto unlock;
+	}
+
+	lf_sum = quotas_get_sum(quota->base);
+
+	if (lf_sum + new_val - quota->val > base->max_sum) {
+		dev_err(dev,
+			"Not enough resources for %s quota. Used: %lld, Max: %lld\n",
+			attr->attr.name, lf_sum, base->max_sum);
+		res = -EIO;
+		goto unlock;
+	}
+	quota->val = new_val;
+
+	if (base->ops.post_store)
+		base->ops.post_store(quota->ops_arg, quota, old_val);
+
+	res = count;
+
+unlock:
+	if (base->lock)
+		mutex_unlock(base->lock);
+	return res;
+}
+
+static int quota_sysfs_destroy(struct rvu_quota *quota)
+{
+	if (quota == NULL)
+		return -EINVAL;
+	if (quota->sysfs.attr.mode != 0) {
+		sysfs_remove_file(quota->parent, &quota->sysfs.attr);
+		quota->sysfs.attr.mode = 0;
+	}
+	return 0;
+}
+
+static struct rvu_quotas *quotas_alloc(u32 cnt, u32 max, u64 max_sum,
+				   int init_val, struct mutex *lock,
+				   struct rvu_quota_ops *ops)
+{
+	struct rvu_quotas *quotas;
+	u64 i;
+
+	if (cnt == 0)
+		return NULL;
+
+	quotas = kzalloc(sizeof(struct rvu_quotas) +
+			 cnt * sizeof(struct rvu_quota), GFP_KERNEL);
+	if (quotas == NULL)
+		return NULL;
+
+	for (i = 0; i < cnt; i++) {
+		quotas->a[i].base = quotas;
+		quotas->a[i].val = init_val;
+	}
+
+	quotas->cnt = cnt;
+	quotas->max = max;
+	quotas->max_sum = max_sum;
+	if (ops) {
+		quotas->ops.pre_store = ops->pre_store;
+		quotas->ops.post_store = ops->post_store;
+	}
+	quotas->lock = lock;
+
+	return quotas;
+}
+
+static void quotas_free(struct rvu_quotas *quotas)
+{
+	u64 i;
+
+	if (quotas == NULL)
+		return;
+	WARN_ON(quotas->cnt == 0);
+
+	for (i = 0; i < quotas->cnt; i++)
+		quota_sysfs_destroy(&quotas->a[i]);
+
+	kfree(quotas);
+}
+
+static int quota_sysfs_create(const char *name, struct kobject *parent,
+			      struct device *log_dev, struct rvu_quota *quota,
+			      void *ops_arg)
+{
+	int err;
+
+	if (name == NULL || quota == NULL || log_dev == NULL)
+		return -EINVAL;
+
+	quota->sysfs.show = quota_show;
+	quota->sysfs.store = quota_store;
+	quota->sysfs.attr.name = name;
+	quota->sysfs.attr.mode = 0644;
+	quota->parent = parent;
+	quota->dev = log_dev;
+	quota->ops_arg = ops_arg;
+
+	sysfs_attr_init(&quota->sysfs.attr);
+	err = sysfs_create_file(quota->parent, &quota->sysfs.attr);
+	if (err) {
+		dev_err(quota->dev,
+			"Failed to create '%s' quota sysfs for '%s'\n",
+			name, kobject_name(quota->parent));
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static int rvu_blk_count_rsrc(struct rvu_block *block, u16 pcifunc, u8 rshift)
+{
+	int count = 0, lf;
+
+	for (lf = 0; lf < block->lf.max; lf++)
+		if ((block->fn_map[lf] >> rshift) == (pcifunc >> rshift))
+			count++;
+
+	return count;
+}
+
+int rvu_check_rsrc_policy(struct rvu *rvu, struct rsrc_attach *req,
+			  u16 pcifunc)
+{
+	struct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);
+	int free_lfs, mappedlfs, familylfs, limit, delta;
+	struct rvu_hwinfo *hw = rvu->hw;
+	int pf = rvu_get_pf(pcifunc);
+	struct rvu_block *block;
+
+	/* Only one NPA LF can be attached */
+	if (req->npalf) {
+		block = &hw->block[BLKADDR_NPA];
+		free_lfs = rvu_rsrc_free_count(&block->lf);
+		limit = rvu->pf_limits.npa->a[pf].val;
+		familylfs = rvu_blk_count_rsrc(block, pcifunc,
+					       RVU_PFVF_PF_SHIFT);
+		if (!free_lfs || (limit == familylfs))
+			goto fail;
+	}
+
+	/* Only one NIX LF can be attached */
+	if (req->nixlf) {
+		block = &hw->block[BLKADDR_NIX0];
+		free_lfs = rvu_rsrc_free_count(&block->lf);
+		limit = rvu->pf_limits.nix->a[pf].val;
+		familylfs = rvu_blk_count_rsrc(block, pcifunc,
+					       RVU_PFVF_PF_SHIFT);
+		if (!free_lfs || (limit == familylfs))
+			goto fail;
+	}
+
+	if (req->sso) {
+		block = &hw->block[BLKADDR_SSO];
+		mappedlfs = rvu_get_rsrc_mapcount(pfvf, block->type);
+		free_lfs = rvu_rsrc_free_count(&block->lf);
+		limit = rvu->pf_limits.sso->a[pf].val;
+		familylfs = rvu_blk_count_rsrc(block, pcifunc,
+					       RVU_PFVF_PF_SHIFT);
+		/* Check if additional resources are available */
+		delta = req->sso - mappedlfs;
+		if ((delta > 0) && /* always allow usage decrease */
+		    ((limit < familylfs + delta) ||
+		     (delta > free_lfs)))
+			goto fail;
+	}
+
+	if (req->ssow) {
+		block = &hw->block[BLKADDR_SSOW];
+		mappedlfs = rvu_get_rsrc_mapcount(pfvf, block->type);
+		free_lfs = rvu_rsrc_free_count(&block->lf);
+		limit = rvu->pf_limits.ssow->a[pf].val;
+		familylfs = rvu_blk_count_rsrc(block, pcifunc,
+					       RVU_PFVF_PF_SHIFT);
+		/* Check if additional resources are available */
+		delta = req->ssow - mappedlfs;
+		if ((delta > 0) && /* always allow usage decrease */
+		    ((limit < familylfs + delta) ||
+		     (delta > free_lfs)))
+			goto fail;
+	}
+
+	if (req->timlfs) {
+		block = &hw->block[BLKADDR_TIM];
+		mappedlfs = rvu_get_rsrc_mapcount(pfvf, block->type);
+		free_lfs = rvu_rsrc_free_count(&block->lf);
+		limit = rvu->pf_limits.tim->a[pf].val;
+		familylfs = rvu_blk_count_rsrc(block, pcifunc,
+					       RVU_PFVF_PF_SHIFT);
+		/* Check if additional resources are available */
+		delta = req->timlfs - mappedlfs;
+		if ((delta > 0) && /* always allow usage decrease */
+		    ((limit < familylfs + delta) ||
+		     (delta > free_lfs)))
+			goto fail;
+	}
+
+	if (req->cptlfs) {
+		block = &hw->block[BLKADDR_CPT0];
+		mappedlfs = rvu_get_rsrc_mapcount(pfvf, block->type);
+		free_lfs = rvu_rsrc_free_count(&block->lf);
+		limit = rvu->pf_limits.cpt->a[pf].val;
+		familylfs = rvu_blk_count_rsrc(block, pcifunc,
+					       RVU_PFVF_PF_SHIFT);
+		/* Check if additional resources are available */
+		delta = req->cptlfs - mappedlfs;
+		if ((delta > 0) && /* always allow usage decrease */
+		    ((limit < familylfs + delta) ||
+		     (delta > free_lfs)))
+			goto fail;
+	}
+
+	return 0;
+
+fail:
+	dev_info(rvu->dev, "Request for %s failed\n", block->name);
+	return -ENOSPC;
+}
+
+static int check_mapped_rsrcs(void *arg, struct rvu_quota *quota, int new_val)
+{
+	struct rvu_pfvf *pf = arg;
+	int type;
+
+	for (type = 0; type < BLKTYPE_MAX; type++) {
+		if (rvu_get_rsrc_mapcount(pf, type) > 0)
+			return 1;
+	}
+	return 0;
+}
+
+static struct rvu_quota_ops pf_limit_ops = {
+	.pre_store = check_mapped_rsrcs,
+};
+
+static void rvu_set_default_limits(struct rvu *rvu)
+{
+	int i, sso_rvus = 0, totalvfs;
+
+	/* First pass, count number of SSO/TIM PFs. */
+	for (i = 0; i < rvu->hw->total_pfs; i++) {
+		if (rvu->pf[i].pdev == NULL)
+			continue;
+		if (rvu->pf[i].pdev->device == PCI_DEVID_OCTEONTX2_SSO_RVU_PF)
+			sso_rvus++;
+	}
+	/* Second pass, set the default limit values. */
+	for (i = 0; i < rvu->hw->total_pfs; i++) {
+		if (rvu->pf[i].pdev == NULL)
+			continue;
+		totalvfs = pci_sriov_get_totalvfs(rvu->pf[i].pdev);
+		switch (rvu->pf[i].pdev->device) {
+		case PCI_DEVID_OCTEONTX2_RVU_AF:
+			rvu->pf_limits.nix->a[i].val = totalvfs;
+			rvu->pf_limits.npa->a[i].val = totalvfs;
+			break;
+		case PCI_DEVID_OCTEONTX2_RVU_PF:
+			rvu->pf_limits.nix->a[i].val = 1 + totalvfs;
+			rvu->pf_limits.npa->a[i].val = 1 + totalvfs;
+			break;
+		case PCI_DEVID_OCTEONTX2_SSO_RVU_PF:
+			rvu->pf_limits.npa->a[i].val = totalvfs;
+			rvu->pf_limits.sso->a[i].val =
+				rvu->hw->block[BLKADDR_SSO].lf.max / sso_rvus;
+			rvu->pf_limits.ssow->a[i].val =
+				rvu->hw->block[BLKADDR_SSOW].lf.max / sso_rvus;
+			rvu->pf_limits.tim->a[i].val =
+				rvu->hw->block[BLKADDR_TIM].lf.max / sso_rvus;
+			/* All users of CPT should not share CPUs so if there
+			 * are multiple SSO/TIM PFs, then divide CPTs equally.
+			 */
+			rvu->pf_limits.cpt->a[i].val =
+				num_online_cpus() / sso_rvus;
+			break;
+		case PCI_DEVID_OCTEONTX2_NPA_RVU_PF:
+			rvu->pf_limits.npa->a[i].val = totalvfs;
+			break;
+		case PCI_DEVID_OCTEONTX2_CPT_RVU_PF:
+			rvu->pf_limits.cpt->a[i].val = num_online_cpus();
+			rvu->pf_limits.npa->a[i].val = 1;
+			break;
+		}
+	}
+}
+
+static int rvu_create_limits_sysfs(struct rvu *rvu)
+{
+	struct pci_dev *pdev;
+	struct rvu_pfvf *pf;
+	int i, err = 0;
+
+	for (i = 0; i < rvu->hw->total_pfs; i++) {
+		pf = &rvu->pf[i];
+		pdev = pf->pdev;
+
+		pf->limits_kobj = kobject_create_and_add("limits",
+							 &pdev->dev.kobj);
+
+		if (quota_sysfs_create("sso", pf->limits_kobj, rvu->dev,
+				       &rvu->pf_limits.sso->a[i], pf)) {
+			dev_err(rvu->dev,
+				"Failed to allocate quota for sso on %s\n",
+				pci_name(pdev));
+			err = -EFAULT;
+			break;
+		}
+
+		if (quota_sysfs_create("ssow", pf->limits_kobj, rvu->dev,
+				       &rvu->pf_limits.ssow->a[i], pf)) {
+			dev_err(rvu->dev,
+				"Failed to allocate quota for ssow, on %s\n",
+				pci_name(pdev));
+			err = -EFAULT;
+			break;
+		}
+
+		if (quota_sysfs_create("tim", pf->limits_kobj, rvu->dev,
+				       &rvu->pf_limits.tim->a[i], pf)) {
+			dev_err(rvu->dev,
+				"Failed to allocate quota for tim, on %s\n",
+				pci_name(pdev));
+			err = -EFAULT;
+			break;
+		}
+
+		if (quota_sysfs_create("cpt", pf->limits_kobj, rvu->dev,
+				       &rvu->pf_limits.cpt->a[i], pf)) {
+			dev_err(rvu->dev,
+				"Failed to allocate quota for cpt, on %s\n",
+				pci_name(pdev));
+			err = -EFAULT;
+			break;
+		}
+
+		if (quota_sysfs_create("npa", pf->limits_kobj, rvu->dev,
+				       &rvu->pf_limits.npa->a[i], pf)) {
+			dev_err(rvu->dev,
+				"Failed to allocate quota for npa, on %s\n",
+				pci_name(pdev));
+			err = -EFAULT;
+			break;
+		}
+
+		if (quota_sysfs_create("nix", pf->limits_kobj, rvu->dev,
+				       &rvu->pf_limits.nix->a[i], pf)) {
+			dev_err(rvu->dev,
+				"Failed to allocate quota for nix, on %s\n",
+				pci_name(pdev));
+			err = -EFAULT;
+			break;
+		}
+	}
+
+	return err;
+}
+
+void rvu_policy_destroy(struct rvu *rvu)
+{
+	struct rvu_pfvf *pf = NULL;
+	int i;
+
+	quotas_free(rvu->pf_limits.sso);
+	quotas_free(rvu->pf_limits.ssow);
+	quotas_free(rvu->pf_limits.npa);
+	quotas_free(rvu->pf_limits.cpt);
+	quotas_free(rvu->pf_limits.tim);
+	quotas_free(rvu->pf_limits.nix);
+	rvu->pf_limits.sso = NULL;
+	rvu->pf_limits.ssow = NULL;
+	rvu->pf_limits.npa = NULL;
+	rvu->pf_limits.cpt = NULL;
+	rvu->pf_limits.tim = NULL;
+	rvu->pf_limits.nix = NULL;
+
+	for (i = 0; i < rvu->hw->total_pfs; i++) {
+		pf = &rvu->pf[i];
+		kobject_del(pf->limits_kobj);
+	}
+}
+
+int rvu_policy_init(struct rvu *rvu)
+{
+	struct pci_dev *pdev = rvu->pdev;
+	struct rvu_hwinfo *hw = rvu->hw;
+	int err, i = 0;
+	u32 max = 0;
+
+	max = hw->block[BLKADDR_SSO].lf.max;
+	rvu->pf_limits.sso = quotas_alloc(rvu->hw->total_pfs, max, max,
+					  0, &rvu->rsrc_lock, &pf_limit_ops);
+	if (!rvu->pf_limits.sso) {
+		dev_err(rvu->dev, "Failed to allocate sso limits\n");
+		err = -EFAULT;
+		goto error;
+	}
+
+	max = hw->block[BLKADDR_SSOW].lf.max;
+	rvu->pf_limits.ssow = quotas_alloc(rvu->hw->total_pfs, max, max,
+					   0, &rvu->rsrc_lock, &pf_limit_ops);
+	if (!rvu->pf_limits.ssow) {
+		dev_err(rvu->dev, "Failed to allocate ssow limits\n");
+		err = -EFAULT;
+		goto error;
+	}
+
+	max = hw->block[BLKADDR_TIM].lf.max;
+	rvu->pf_limits.tim = quotas_alloc(rvu->hw->total_pfs, max, max,
+					  0, &rvu->rsrc_lock, &pf_limit_ops);
+	if (!rvu->pf_limits.tim) {
+		dev_err(rvu->dev, "Failed to allocate tim limits\n");
+		err = -EFAULT;
+		goto error;
+	}
+
+	max = hw->block[BLKADDR_CPT0].lf.max;
+	rvu->pf_limits.cpt = quotas_alloc(rvu->hw->total_pfs, max, max,
+					  0, &rvu->rsrc_lock, &pf_limit_ops);
+	if (!rvu->pf_limits.cpt) {
+		dev_err(rvu->dev, "Failed to allocate cpt limits\n");
+		err = -EFAULT;
+		goto error;
+	}
+
+	/* Because limits track also VFs under PF, the maximum NPA LF limit for
+	 * a single PF has to be max, not 1. Same for NIX below.
+	 */
+	max = hw->block[BLKADDR_NPA].lf.max;
+	rvu->pf_limits.npa = quotas_alloc(rvu->hw->total_pfs, max, max,
+					  0, &rvu->rsrc_lock, &pf_limit_ops);
+	if (!rvu->pf_limits.npa) {
+		dev_err(rvu->dev, "Failed to allocate npa limits\n");
+		err = -EFAULT;
+		goto error;
+	}
+
+	max = hw->block[BLKADDR_NIX0].lf.max;
+	rvu->pf_limits.nix = quotas_alloc(rvu->hw->total_pfs, max, max,
+					  0, &rvu->rsrc_lock, &pf_limit_ops);
+	if (!rvu->pf_limits.nix) {
+		dev_err(rvu->dev, "Failed to allocate nix limits\n");
+		err = -EFAULT;
+		goto error;
+	}
+
+	for (i = 0; i < hw->total_pfs; i++)
+		rvu->pf[i].pdev =
+			pci_get_domain_bus_and_slot(pci_domain_nr(pdev->bus),
+						    i + 1, 0);
+
+	rvu_set_default_limits(rvu);
+
+	err = rvu_create_limits_sysfs(rvu);
+	if (err) {
+		dev_err(rvu->dev, "Failed to create limits sysfs\n");
+		goto error;
+	}
+
+	return 0;
+
+error:
+	rvu_policy_destroy(rvu);
+	return err;
+}
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.h b/drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.h
new file mode 100644
index 000000000000..3d3888fadadd
--- /dev/null
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_validation.h
@@ -0,0 +1,64 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Marvell OcteonTx2 RVU Admin Function driver
+ *
+ * Copyright (C) 2018 Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef RVU_VALIDATION_H
+#define RVU_VALIDATION_H
+
+struct rvu;
+struct rvu_quotas;
+
+struct rvu_quota {
+	struct kobj_attribute	sysfs;
+	/* Device to scope logs to */
+	struct device		*dev;
+	/* Kobject of the sysfs file */
+	struct kobject		*parent;
+	/* Pointer to base structure */
+	struct rvu_quotas	*base;
+	/* Argument passed to the quota_ops when this quota is modified */
+	void			*ops_arg;
+	/* Value of the quota */
+	int			val;
+};
+
+struct rvu_quota_ops {
+	/*
+	 * Called before sysfs store(). store() will proceed if returns 0.
+	 * It is called with struct rvu_quotas::lock taken.
+	 */
+	int (*pre_store)(void *arg, struct rvu_quota *quota, int new_val);
+	/** called after sysfs store(). */
+	void (*post_store)(void *arg, struct rvu_quota *quota, int old_val);
+};
+
+struct rvu_quotas {
+	struct rvu_quota_ops	ops;
+	struct mutex		*lock; /* lock taken for each sysfs operation */
+	u32			cnt; /* number of elements in arr */
+	u32			max; /* maximum value for a single quota */
+	u64			max_sum; /* maximum sum of all quotas */
+	struct rvu_quota	a[0]; /* array of quota assignments */
+};
+
+struct rvu_limits {
+	struct rvu_quotas	*sso;
+	struct rvu_quotas	*ssow;
+	struct rvu_quotas	*tim;
+	struct rvu_quotas	*cpt;
+	struct rvu_quotas	*npa;
+	struct rvu_quotas	*nix;
+};
+
+int rvu_policy_init(struct rvu *rvu);
+void rvu_policy_destroy(struct rvu *rvu);
+int rvu_check_rsrc_policy(struct rvu *rvu,
+			  struct rsrc_attach *req, u16 pcifunc);
+
+#endif /* RVU_VALIDATION_H */
-- 
2.17.1

