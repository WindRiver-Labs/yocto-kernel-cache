From ee56f8d69d22be8deb3446ade88b33b0eeb284ef Mon Sep 17 00:00:00 2001
From: Smadar Fuks <smadarf@marvell.com>
Date: Tue, 26 May 2020 16:33:01 -0400
Subject: [PATCH 2/4] octeontx2-af: Add new Mbox messages and handlers for new
 REE blocks

commit 49706eeef363a5d492919e91e01489c173dc520b from
git@git.assembla.com:cavium/WindRiver.linux.git

Mailbox support for REE0 and REE1 blocks in 98xx platform.
This patch adds new REE mailbox messages to mbox.h:
- Config LF
- REE BAR0 registers read and write
- REE get Rule database length
- REE get Rule database for incremental programming
- REE Rule database programming

This patch adds the corresponding new mbox handlers to rvu_ree.c

The rule database that is passed via mbox contains instruction of
address and data and is programmed to REE.
For programming REE allocates memory that is accessed by HW:
- 128-MB structure containing the complete compiled regular-expression
  graphs
- AF Queue that points to 16KB blocks that hold filtering instructions

REE also allocates memory for keeping the rule database that was
received via mbox. It is used for immediate programming and future
incremental programming

Change-Id: I3679b736bafac8ba944f47252dad52fb2fce6506
Signed-off-by: Smadar Fuks <smadarf@marvell.com>
Reviewed-on: https://sj1git1.cavium.com/c/IP/SW/kernel/linux/+/29294
Tested-by: sa_ip-sw-jenkins <sa_ip-sw-jenkins@marvell.com>
Reviewed-by: Sunil Kovvuri Goutham <Sunil.Goutham@cavium.com>
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 .../net/ethernet/marvell/octeontx2/af/mbox.h  | 106 +++
 .../ethernet/marvell/octeontx2/af/rvu_ree.c   | 844 ++++++++++++++++++
 .../ethernet/marvell/octeontx2/af/rvu_reg.h   |  20 +-
 .../marvell/octeontx2/af/rvu_struct.h         |  17 +
 4 files changed, 986 insertions(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/af/mbox.h b/drivers/net/ethernet/marvell/octeontx2/af/mbox.h
index 88ffd8cc6f84..64f73a73632d 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/mbox.h
+++ b/drivers/net/ethernet/marvell/octeontx2/af/mbox.h
@@ -212,6 +212,19 @@ M(CPT_SET_CRYPTO_GRP,	0xA03, cpt_set_crypto_grp,			\
 			       cpt_set_crypto_grp_req_msg)		\
 M(CPT_INLINE_IPSEC_CFG,	0xA04, cpt_inline_ipsec_cfg,			\
 			       cpt_inline_ipsec_cfg_msg, msg_rsp)	\
+/* REE mbox IDs (range 0xE00 - 0xFFF) */				\
+M(REE_CONFIG_LF,	0xE01, ree_config_lf, ree_lf_req_msg,		\
+				msg_rsp)				\
+M(REE_RD_WR_REGISTER,	0xE02, ree_rd_wr_register, ree_rd_wr_reg_msg,	\
+				ree_rd_wr_reg_msg)			\
+M(REE_RULE_DB_PROG,	0xE03, ree_rule_db_prog,			\
+				ree_rule_db_prog_req_msg,		\
+				msg_rsp)				\
+M(REE_RULE_DB_LEN_GET,	0xE04, ree_rule_db_len_get, ree_req_msg,	\
+				ree_rule_db_len_rsp_msg)		\
+M(REE_RULE_DB_GET,	0xE05, ree_rule_db_get,				\
+				ree_rule_db_get_req_msg,		\
+				ree_rule_db_get_rsp_msg)		\
 /* NPC mbox IDs (range 0x6000 - 0x7FFF) */				\
 M(NPC_MCAM_ALLOC_ENTRY,	0x6000, npc_mcam_alloc_entry, npc_mcam_alloc_entry_req,\
 				npc_mcam_alloc_entry_rsp)		\
@@ -1576,4 +1589,97 @@ struct cpt_inline_ipsec_cfg_msg {
 	u16 nix_pf_func; /* outbound path NIX_PF_FUNC */
 };
 
+/* REE mailbox error codes
+ * Range 1001 - 1100.
+ */
+enum ree_af_status {
+	REE_AF_ERR_RULE_UNKNOWN_VALUE		= -1001,
+	REE_AF_ERR_LF_NO_MORE_RESOURCES		= -1002,
+	REE_AF_ERR_LF_INVALID			= -1003,
+	REE_AF_ERR_ACCESS_DENIED		= -1004,
+	REE_AF_ERR_RULE_DB_PARTIAL		= -1005,
+	REE_AF_ERR_RULE_DB_EQ_BAD_VALUE		= -1006,
+	REE_AF_ERR_RULE_DB_BLOCK_ALLOC_FAILED	= -1007,
+	REE_AF_ERR_BLOCK_NOT_IMPLEMENTED	= -1008,
+	REE_AF_ERR_RULE_DB_INC_OFFSET_TOO_BIG	= -1009,
+	REE_AF_ERR_RULE_DB_OFFSET_TOO_BIG	= -1010,
+	REE_AF_ERR_Q_IS_GRACEFUL_DIS		= -1011,
+	REE_AF_ERR_Q_NOT_GRACEFUL_DIS		= -1012,
+	REE_AF_ERR_RULE_DB_ALLOC_FAILED		= -1013,
+	REE_AF_ERR_RULE_DB_TOO_BIG		= -1014,
+	REE_AF_ERR_RULE_DB_GEQ_BAD_VALUE	= -1015,
+	REE_AF_ERR_RULE_DB_LEQ_BAD_VALUE	= -1016,
+	REE_AF_ERR_RULE_DB_WRONG_LENGTH		= -1017,
+	REE_AF_ERR_RULE_DB_WRONG_OFFSET		= -1018,
+	REE_AF_ERR_RULE_DB_BLOCK_TOO_BIG	= -1019,
+	REE_AF_ERR_RULE_DB_SHOULD_FILL_REQUEST	= -1020,
+	REE_AF_ERR_RULE_DBI_ALLOC_FAILED	= -1021,
+	REE_AF_ERR_LF_WRONG_PRIORITY		= -1022,
+	REE_AF_ERR_LF_SIZE_TOO_BIG		= -1023,
+};
+
+/* REE mbox message formats */
+
+struct ree_req_msg {
+	struct mbox_msghdr hdr;
+	u32 blkaddr;
+};
+
+struct ree_lf_req_msg {
+	struct mbox_msghdr hdr;
+	u32 blkaddr;
+	u32 size;
+	u8 lf;
+	u8 pri;
+};
+
+struct ree_rule_db_prog_req_msg {
+	struct mbox_msghdr	hdr;
+/* Rule DB passed in MBOX and is copied to internal REE DB
+ * This size should be power of 2 to fit into rule DB internal blocks
+ */
+#define REE_RULE_DB_REQ_BLOCK_SIZE (MBOX_SIZE >> 1)
+	u8 rule_db[REE_RULE_DB_REQ_BLOCK_SIZE];
+	u32 blkaddr;		/* REE0 or REE1 */
+	u32 total_len;		/* Total len of rule db */
+	u32 offset;		/* Offset of current rule db block */
+	u16 len;		/* Length of rule db block */
+	u8 is_last;		/* Is this the last block */
+	u8 is_incremental;	/* Is incremental flow */
+	u8 is_dbi;		/* Is rule db incremental */
+};
+
+struct ree_rule_db_get_req_msg {
+	struct mbox_msghdr hdr;
+	u32 blkaddr;
+	u32 offset;	/* Retrieve db from this offset */
+	u8 is_dbi;	/* Is request for rule db incremental */
+};
+
+struct ree_rd_wr_reg_msg {
+	struct mbox_msghdr hdr;
+	u64 reg_offset;
+	u64 *ret_val;
+	u64 val;
+	u32 blkaddr;
+	u8 is_write;
+};
+
+struct ree_rule_db_len_rsp_msg {
+	struct mbox_msghdr hdr;
+	u32 blkaddr;
+	u32 len;
+	u32 inc_len;
+};
+
+struct ree_rule_db_get_rsp_msg {
+	struct mbox_msghdr hdr;
+#define REE_RULE_DB_RSP_BLOCK_SIZE (MBOX_DOWN_TX_SIZE - SZ_1K)
+	u8 rule_db[REE_RULE_DB_RSP_BLOCK_SIZE];
+	u32 total_len;		/* Total len of rule db */
+	u32 offset;		/* Offset of current rule db block */
+	u16 len;		/* Length of rule db block */
+	u8 is_last;		/* Is this the last block */
+};
+
 #endif /* MBOX_H */
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_ree.c b/drivers/net/ethernet/marvell/octeontx2/af/rvu_ree.c
index e201c1fe2d98..4383c16b3b59 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_ree.c
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_ree.c
@@ -14,9 +14,67 @@
 /* Maximum number of REE blocks */
 #define MAX_REE_BLKS		2
 
+/* Graph maximum number of entries, each of 8B */
+#define REE_GRAPH_CNT		(16 * 1024 * 1024)
+
+/* Prefix Block size 1K of 16B entries
+ * maximum number of blocks for a single ROF is 128
+ */
+#define REE_PREFIX_PTR_LEN	1024
+#define REE_PREFIX_CNT		(128 * 1024)
+
+/* Rule DB entries are held in memory */
+#define REE_RULE_DB_ALLOC_SIZE	(4 * 1024 * 1024)
+#define REE_RULE_DB_ALLOC_SHIFT	22
+#define REE_RULE_DB_BLOCK_CNT	64
+
+/* Rule DB incremental */
+#define REE_RULE_DBI_SIZE	(16 * 6)
+
 /* Administrative instruction queue size */
 #define REE_AQ_SIZE		128
 
+enum ree_cmp_ops {
+	REE_CMP_EQ,	/* Equal to data*/
+	REE_CMP_GEQ,	/* Equal or greater than data */
+	REE_CMP_LEQ,	/* Equal or less than data */
+	REE_CMP_KEY_FIELDS_MAX,
+};
+
+enum ree_rof_types {
+	REE_ROF_TYPE_0	= 0, /* Legacy */
+	REE_ROF_TYPE_1	= 1, /* Check CSR EQ */
+	REE_ROF_TYPE_2	= 2, /* Check CSR GEQ */
+	REE_ROF_TYPE_3	= 3, /* Check CSR LEQ */
+	REE_ROF_TYPE_4	= 4, /* Not relevant */
+	REE_ROF_TYPE_5	= 5, /* Check CSR checksum only for internal memory */
+	REE_ROF_TYPE_6	= 6, /* Internal memory */
+	REE_ROF_TYPE_7	= 7, /* External memory */
+};
+
+struct ree_rule_db_entry {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 addr		: 32;
+	u64 pad			: 24;
+	u64 type		:  8;
+#else
+	u64 type		:  8;
+	u64 pad			: 24;
+	u64 addr		: 32;
+#endif
+	u64 value;
+};
+
+static void ree_reex_enable(struct rvu *rvu, struct rvu_block *block)
+{
+	u64 reg;
+
+	/* Set GO bit */
+	reg = rvu_read64(rvu, block->addr, REE_AF_REEXM_CTRL);
+	reg |= REE_AF_REEXM_CTRL_GO;
+	rvu_write64(rvu, block->addr, REE_AF_REEXM_CTRL, reg);
+}
+
 static void ree_reex_force_clock(struct rvu *rvu, struct rvu_block *block,
 				 bool force_on)
 {
@@ -31,6 +89,792 @@ static void ree_reex_force_clock(struct rvu *rvu, struct rvu_block *block,
 	rvu_write64(rvu, block->addr, REE_AF_CMD_CTL, reg);
 }
 
+static int ree_graceful_disable_control(struct rvu *rvu,
+					struct rvu_block *block, bool apply)
+{
+	u64 val, mask;
+	int err;
+
+	/* Graceful Disable is available on all queues 0..35
+	 * 0 = Queue is not gracefully-disabled (apply is false)
+	 * 1 = Queue was gracefully-disabled (apply is true)
+	 */
+	mask = GENMASK(35, 0);
+
+	/* Check what is graceful disable status */
+	val = rvu_read64(rvu, block->addr, REE_AF_GRACEFUL_DIS_STATUS) & mask;
+	if (apply & val)
+		return REE_AF_ERR_Q_IS_GRACEFUL_DIS;
+	else if (!apply & !val)
+		return REE_AF_ERR_Q_NOT_GRACEFUL_DIS;
+
+	/* Apply Graceful Enable or Disable on all queues 0..35 */
+	if (apply)
+		val = GENMASK(35, 0);
+	else
+		val = 0;
+
+	rvu_write64(rvu, block->addr, REE_AF_GRACEFUL_DIS_CTL, val);
+
+	/* Poll For graceful disable if it is applied or not on all queues */
+	/* This might take time */
+	err = rvu_poll_reg(rvu, block->addr, REE_AF_GRACEFUL_DIS_STATUS, mask,
+			   !apply);
+	if (err) {
+		dev_err(rvu->dev, "REE graceful disable control failed");
+		return err;
+	}
+	return 0;
+}
+
+static int ree_reex_programming(struct rvu *rvu, struct rvu_block *block,
+				u8 incremental)
+{
+	int err;
+
+	if (!incremental) {
+		/* REEX Set & Clear MAIN_CSR init */
+		rvu_write64(rvu, block->addr, REE_AF_REEXM_CTRL,
+			    REE_AF_REEXM_CTRL_INIT);
+		rvu_write64(rvu, block->addr, REE_AF_REEXM_CTRL, 0x0);
+
+		/* REEX Poll MAIN_CSR INIT_DONE */
+		err = rvu_poll_reg(rvu, block->addr, REE_AF_REEXM_STATUS,
+				   REE_AF_REEXM_STATUS_INIT_DONE, false);
+		if (err) {
+			dev_err(rvu->dev, "REE poll reexm status failed");
+			return err;
+		}
+
+		/* REEX Set Mem Init Mode */
+		rvu_write64(rvu, block->addr, REE_AF_REEXR_CTRL,
+			    (REE_AF_REEXR_CTRL_INIT |
+			     REE_AF_REEXR_CTRL_MODE_IM_L1_L2));
+
+		/* REEX Set & Clear Mem Init */
+		rvu_write64(rvu, block->addr, REE_AF_REEXR_CTRL,
+			    REE_AF_REEXR_CTRL_MODE_IM_L1_L2);
+
+		/* REEX Poll all RTRU DONE 3 bits */
+		err = rvu_poll_reg(rvu, block->addr, REE_AF_REEXR_STATUS,
+				   (REE_AF_REEXR_STATUS_IM_INIT_DONE |
+				    REE_AF_REEXR_STATUS_L1_CACHE_INIT_DONE |
+				    REE_AF_REEXR_STATUS_L2_CACHE_INIT_DONE),
+				    false);
+		if (err) {
+			dev_err(rvu->dev, "REE for cache done failed");
+			return err;
+		}
+	} else {
+		/* REEX Set Mem Init Mode */
+		rvu_write64(rvu, block->addr, REE_AF_REEXR_CTRL,
+			    (REE_AF_REEXR_CTRL_INIT |
+			     REE_AF_REEXR_CTRL_MODE_L1_L2));
+
+		/* REEX Set & Clear Mem Init */
+		rvu_write64(rvu, block->addr, REE_AF_REEXR_CTRL,
+			    REE_AF_REEXR_CTRL_MODE_L1_L2);
+
+		/* REEX Poll all RTRU DONE 2 bits */
+		err = rvu_poll_reg(rvu, block->addr, REE_AF_REEXR_STATUS,
+				   (REE_AF_REEXR_STATUS_L1_CACHE_INIT_DONE |
+				    REE_AF_REEXR_STATUS_L2_CACHE_INIT_DONE),
+				    false);
+		if (err) {
+			dev_err(rvu->dev, "REE cache & init done failed");
+			return err;
+		}
+	}
+
+	/* Before 1st time en-queue, set REEX RTRU.GO bit to 1 */
+	rvu_write64(rvu, block->addr, REE_AF_REEXR_CTRL, REE_AF_REEXR_CTRL_GO);
+	return 0;
+}
+
+int ree_aq_verify_type6_completion(struct rvu *rvu, struct rvu_block *block)
+{
+	u64 val;
+	int err;
+
+	/* Poll on Done count until it is 1 to see that last instruction
+	 * is completed. Then write this value to DONE_ACK to decrement
+	 * the value of Done count
+	 * Note that no interrupts are used for this counters
+	 */
+	err = rvu_poll_reg(rvu, block->addr, REE_AF_AQ_DONE,
+			   0x1, false);
+	if (err) {
+		dev_err(rvu->dev, "REE AFAQ done failed");
+		return err;
+	}
+	val = rvu_read64(rvu, block->addr, REE_AF_AQ_DONE);
+	rvu_write64(rvu, block->addr, REE_AF_AQ_DONE_ACK, val);
+	return 0;
+}
+
+int ree_aq_inst_enq(struct rvu *rvu, struct rvu_block *block,
+		    struct ree_rsrc *ree, dma_addr_t head, u32 size,
+		    int doneint)
+{
+	struct admin_queue *aq = block->aq;
+	struct ree_af_aq_inst_s inst;
+
+	/* Fill instruction */
+	memset(&inst, 0, sizeof(struct ree_af_aq_inst_s));
+	inst.length = size;
+	inst.rof_ptr_addr = (u64)head;
+	inst.doneint = doneint;
+	/* Copy instruction to AF AQ head */
+	memcpy(aq->inst->base + (ree->aq_head * aq->inst->entry_sz),
+	       &inst, aq->inst->entry_sz);
+	/* Sync into memory */
+	wmb();
+	/* SW triggers HW AQ.DOORBELL */
+	rvu_write64(rvu, block->addr, REE_AF_AQ_DOORBELL, 1);
+	/* Move Head to next cell in AF AQ.
+	 * HW CSR gives only AF AQ tail address
+	 */
+	ree->aq_head++;
+	if (ree->aq_head >= aq->inst->qsize)
+		ree->aq_head = 0;
+	return 0;
+}
+
+static int ree_reex_memory_alloc(struct rvu *rvu, struct rvu_block *block,
+				 struct ree_rsrc *ree, int db_len,
+				 int is_incremental)
+{
+	int alloc_len, err, i;
+
+	/* Allocate Graph Memory 128MB. This is an IOVA base address
+	 * for the memory image of regular expressions graphs.
+	 * Software is filling this memory with graph instructions (type 7)
+	 * and HW uses this as external memory for graph search.
+	 */
+	if (!ree->graph_ctx) {
+		err = qmem_alloc(rvu->dev, &ree->graph_ctx, REE_GRAPH_CNT,
+				 sizeof(u64));
+		if (err)
+			return err;
+		/* Update Graph address in DRAM */
+		rvu_write64(rvu, block->addr, REE_AF_EM_BASE,
+			    (u64)ree->graph_ctx->iova);
+	}
+
+	/* If not incremental programming, clear Graph Memory
+	 * before programming
+	 */
+	if (!is_incremental)
+		memset(ree->graph_ctx->base, 0, REE_GRAPH_CNT * sizeof(u64));
+
+	/* Allocate buffers to hold ROF data. Each buffer holds maximum length
+	 * of 16384 Bytes, which is 1K instructions block. These blocks are
+	 * pointed to by REE_AF_AQ_INST_S:ROF_PTR_ADDR. Multiple blocks are
+	 * allocated for concurrent work with HW
+	 */
+	if (!ree->prefix_ctx) {
+		err = qmem_alloc(rvu->dev, &ree->prefix_ctx, REE_PREFIX_CNT,
+				 sizeof(struct ree_rof_s));
+		if (err) {
+			qmem_free(rvu->dev, ree->graph_ctx);
+			ree->graph_ctx = NULL;
+			return err;
+		}
+	}
+
+	/* Allocate memory to hold incremental programming checksum reference
+	 * data which later be retrieved via mbox by the application
+	 */
+	if (!ree->ruledbi) {
+		ree->ruledbi = kmalloc_array(REE_RULE_DBI_SIZE, sizeof(void *),
+					     GFP_KERNEL);
+		if (!ree->ruledbi) {
+			qmem_free(rvu->dev, ree->graph_ctx);
+			ree->graph_ctx = NULL;
+			qmem_free(rvu->dev, ree->prefix_ctx);
+			ree->prefix_ctx = NULL;
+			return REE_AF_ERR_RULE_DBI_ALLOC_FAILED;
+		}
+	}
+	/* Allocate memory to hold ROF instructions. ROF instructions are
+	 * passed from application by multiple mbox messages. Once the last
+	 * instruction is passed, they are programmed to REE.
+	 * ROF instructions are kept in memory for future retrieve by
+	 * application in order to make incremental programming
+	 */
+	if (!ree->ruledb) {
+		ree->ruledb = kmalloc_array(REE_RULE_DB_BLOCK_CNT,
+					    sizeof(void *), GFP_KERNEL);
+		if (!ree->ruledb) {
+			qmem_free(rvu->dev, ree->graph_ctx);
+			ree->graph_ctx = NULL;
+			qmem_free(rvu->dev, ree->prefix_ctx);
+			ree->prefix_ctx = NULL;
+			kfree(ree->ruledbi);
+			ree->ruledbi = NULL;
+			return REE_AF_ERR_RULE_DB_ALLOC_FAILED;
+		}
+		ree->ruledb_blocks = 0;
+	}
+	alloc_len = ree->ruledb_blocks * REE_RULE_DB_ALLOC_SIZE;
+	while (alloc_len < db_len) {
+		if (ree->ruledb_blocks >= REE_RULE_DB_BLOCK_CNT) {
+			/* No need to free memory here since it is just
+			 * indication of rule DB that is too big.
+			 * Unlike previous allocation that happens only once,
+			 * this allocation can happen along time if larger
+			 * ROF files are sent
+			 */
+			return REE_AF_ERR_RULE_DB_TOO_BIG;
+		}
+		ree->ruledb[ree->ruledb_blocks] =
+			kmalloc(REE_RULE_DB_ALLOC_SIZE, GFP_KERNEL);
+		if (!ree->ruledb[ree->ruledb_blocks]) {
+			for (i = 0; i < ree->ruledb_blocks; i++)
+				kfree(ree->ruledb[i]);
+			qmem_free(rvu->dev, ree->graph_ctx);
+			ree->graph_ctx = NULL;
+			qmem_free(rvu->dev, ree->prefix_ctx);
+			ree->prefix_ctx = NULL;
+			kfree(ree->ruledbi);
+			ree->ruledbi = NULL;
+			kfree(ree->ruledb);
+			ree->ruledb = NULL;
+			return REE_AF_ERR_RULE_DB_BLOCK_ALLOC_FAILED;
+		}
+		ree->ruledb_blocks += 1;
+		alloc_len += REE_RULE_DB_ALLOC_SIZE;
+	}
+
+	return 0;
+}
+
+static
+int ree_reex_cksum_compare(struct rvu *rvu, int blkaddr,
+			   struct ree_rule_db_entry **rule_db,
+			   int *rule_db_len, enum ree_cmp_ops cmp)
+{
+	u64 offset;
+	u64 reg;
+
+	/* ROF instructions have 3 fields: type, address and data.
+	 * Instructions of type 1,2,3 and 5 are compared against CSR values.
+	 * The address of the CSR is calculated from the instruction address.
+	 * The CSR value is compared against instruction data.
+	 * REE AF REEX comparison registers are in 2 sections: main and rtru.
+	 * Main CSR base address is 0x8000, rtru CSR base address is 0x8200
+	 * Instruction address bits 16 to 18 indicate the block from which one
+	 * can take the base address. Main is 0x0000, RTRU is 0x0001
+	 * Low 5 bits indicate the offset, one should multiply it by 8.
+	 * The address is calculated as follows:
+	 * - Base address is 0x8000
+	 * - bits 16 to 18 are multiplied by 0x200
+	 * - Low 5 bits are multiplied by 8
+	 */
+	offset = REE_AF_REEX_CSR_BLOCK_BASE_ADDR +
+		((((*rule_db)->addr & REE_AF_REEX_CSR_BLOCK_ID_MASK) >>
+		  REE_AF_REEX_CSR_BLOCK_ID_SHIFT) *
+		 REE_AF_REEX_CSR_BLOCK_ID) +
+		(((*rule_db)->addr & REE_AF_REEX_CSR_INDEX_MASK) *
+		 REE_AF_REEX_CSR_INDEX);
+	reg = rvu_read64(rvu, blkaddr, offset);
+	switch (cmp) {
+	case REE_CMP_EQ:
+		if (reg != (*rule_db)->value) {
+			dev_err(rvu->dev, "REE addr %llx data %llx neq %llx",
+				offset, reg, (*rule_db)->value);
+			return REE_AF_ERR_RULE_DB_EQ_BAD_VALUE;
+		}
+		break;
+	case REE_CMP_GEQ:
+		if (reg < (*rule_db)->value) {
+			dev_err(rvu->dev, "REE addr %llx data %llx ngeq %llx",
+				offset, reg, (*rule_db)->value);
+			return REE_AF_ERR_RULE_DB_GEQ_BAD_VALUE;
+		}
+		break;
+	case REE_CMP_LEQ:
+		if (reg > (*rule_db)->value) {
+			dev_err(rvu->dev, "REE addr %llx data %llx nleq %llx",
+				offset, reg, (*rule_db)->value);
+			return REE_AF_ERR_RULE_DB_LEQ_BAD_VALUE;
+		}
+		break;
+	default:
+		dev_err(rvu->dev, "REE addr %llx data %llx default %llx",
+			offset, reg, (*rule_db)->value);
+		return REE_AF_ERR_RULE_UNKNOWN_VALUE;
+	}
+
+	(*rule_db)++;
+	*rule_db_len -= sizeof(struct ree_rule_db_entry);
+	return 0;
+}
+
+static
+void ree_reex_prefix_write(void **prefix_ptr,
+			   struct ree_rule_db_entry **rule_db,
+			   int *rule_db_len, int *count)
+{
+	struct ree_rof_s rof_entry;
+
+	while ((*rule_db)->type == REE_ROF_TYPE_6) {
+		rof_entry.typ = (*rule_db)->type;
+		rof_entry.addr = (*rule_db)->addr;
+		rof_entry.data = (*rule_db)->value;
+		memcpy((*prefix_ptr), (void *)(&rof_entry),
+		       sizeof(struct ree_rof_s));
+		/* AF AQ prefix block to copy to */
+		(*prefix_ptr) += sizeof(struct ree_rof_s);
+		/* Location in ROF DB that was parsed by now */
+		(*rule_db)++;
+		/* Length of ROF DB left to handle*/
+		(*rule_db_len) -= sizeof(struct ree_rule_db_entry);
+		/* Number of type 6 rows that were parsed */
+		(*count)++;
+	}
+}
+
+static
+void ree_reex_graph_write(struct ree_rsrc *ree,
+			  struct ree_rule_db_entry **rule_db, int *rule_db_len)
+{
+	u32 offset;
+
+	while ((*rule_db)->type == REE_ROF_TYPE_7) {
+		offset = ((*rule_db)->addr & 0xFFFFFF) << 3;
+		memcpy(ree->graph_ctx->base + offset,
+		       &(*rule_db)->value, sizeof((*rule_db)->value));
+		(*rule_db)++;
+		*rule_db_len -= sizeof(struct ree_rule_db_entry);
+	}
+}
+
+static
+int ree_rof_data_validation(struct rvu *rvu, int blkaddr,
+			    struct ree_rsrc *ree, int *db_block,
+			    struct ree_rule_db_entry **rule_db_ptr,
+			    int *rule_db_len)
+{
+	int err;
+
+	/* Parse ROF data */
+	while (*rule_db_len > 0) {
+		switch ((*rule_db_ptr)->type) {
+		case REE_ROF_TYPE_1:
+			err = ree_reex_cksum_compare(rvu, blkaddr, rule_db_ptr,
+						     rule_db_len, REE_CMP_EQ);
+			if (err < 0)
+				return err;
+			break;
+		case REE_ROF_TYPE_2:
+			err = ree_reex_cksum_compare(rvu, blkaddr, rule_db_ptr,
+						     rule_db_len, REE_CMP_GEQ);
+			if (err < 0)
+				return err;
+			break;
+		case REE_ROF_TYPE_3:
+			err = ree_reex_cksum_compare(rvu, blkaddr, rule_db_ptr,
+						     rule_db_len, REE_CMP_LEQ);
+			if (err < 0)
+				return err;
+			break;
+		case REE_ROF_TYPE_4:
+			/* Type 4 handles internal memory */
+			(*rule_db_ptr)++;
+			(*rule_db_len) -= sizeof(struct ree_rof_s);
+			break;
+		case REE_ROF_TYPE_5:
+			err = ree_reex_cksum_compare(rvu, blkaddr, rule_db_ptr,
+						     rule_db_len, REE_CMP_EQ);
+			if (err < 0)
+				return err;
+			break;
+		case REE_ROF_TYPE_6:
+		case REE_ROF_TYPE_7:
+			return 0;
+		default:
+			/* Other types not supported */
+			(*rule_db_ptr)++;
+			*rule_db_len -= sizeof(struct ree_rof_s);
+			break;
+		}
+		/* If rule DB is larger than 4M there is a need
+		 * to move between db blocks of 4M
+		 */
+		if ((uint64_t)(*rule_db_ptr) -
+					  (uint64_t)ree->ruledb[(*db_block)] >=
+			 REE_RULE_DB_ALLOC_SIZE) {
+			(*db_block)++;
+			*rule_db_ptr = ree->ruledb[(*db_block)];
+		}
+	}
+	return 0;
+}
+
+static
+int ree_rof_data_enq(struct rvu *rvu, struct rvu_block *block,
+		     struct ree_rsrc *ree,
+		     struct ree_rule_db_entry **rule_db_ptr,
+		     int *rule_db_len, int *db_block)
+{
+	void *prefix_ptr = ree->prefix_ctx->base;
+	int err, size, num_of_entries = 0;
+	dma_addr_t head;
+
+	/* Parse ROF data */
+	while (*rule_db_len > 0) {
+		switch ((*rule_db_ptr)->type) {
+		case REE_ROF_TYPE_1:
+		case REE_ROF_TYPE_2:
+		case REE_ROF_TYPE_3:
+		case REE_ROF_TYPE_4:
+		case REE_ROF_TYPE_5:
+			break;
+		case REE_ROF_TYPE_6:
+			ree_reex_prefix_write(&prefix_ptr, rule_db_ptr,
+					      rule_db_len, &num_of_entries);
+			break;
+		case REE_ROF_TYPE_7:
+			ree_reex_graph_write(ree, rule_db_ptr, rule_db_len);
+			break;
+		default:
+			/* Other types not supported */
+			(*rule_db_ptr)++;
+			(*rule_db_len) -= sizeof(struct ree_rof_s);
+			break;
+		}
+		/* If rule DB is larger than 4M there is a need
+		 * to move between db blocks of 4M
+		 */
+		if ((uint64_t)(*rule_db_ptr) -
+					  (uint64_t)ree->ruledb[(*db_block)] >=
+			 REE_RULE_DB_ALLOC_SIZE) {
+			(*db_block)++;
+			*rule_db_ptr = ree->ruledb[(*db_block)];
+		}
+		/* If there are no more prefix and graph data
+		 * en-queue prefix data and continue with data validation
+		 */
+		if (((*rule_db_ptr)->type != REE_ROF_TYPE_6) &&
+		    ((*rule_db_ptr)->type != REE_ROF_TYPE_7))
+			break;
+	}
+
+	/* Block is filled with 1K instructions
+	 * En-queue to AF AQ all available blocks
+	 */
+	head = ree->prefix_ctx->iova;
+	while (num_of_entries > 0) {
+		if (num_of_entries > REE_PREFIX_PTR_LEN) {
+			size = REE_PREFIX_PTR_LEN * sizeof(struct ree_rof_s);
+			err = ree_aq_inst_enq(rvu, block, ree, head, size,
+					      false);
+			head += REE_PREFIX_PTR_LEN * sizeof(struct ree_rof_s);
+		} else {
+			size = num_of_entries * sizeof(struct ree_rof_s);
+			err = ree_aq_inst_enq(rvu, block, ree, head, size,
+					      true);
+		}
+		if (err)
+			return err;
+		num_of_entries -= REE_PREFIX_PTR_LEN;
+	}
+	/* Verify completion of type 6 */
+	ree_aq_verify_type6_completion(rvu, block);
+
+	return 0;
+}
+
+static
+int ree_rule_db_prog(struct rvu *rvu, struct rvu_block *block,
+		     struct ree_rsrc *ree, int inc)
+{
+	struct ree_rule_db_entry *rule_db_ptr;
+	int rule_db_len, err = 0, db_block = 0;
+	u64 reg;
+
+	/* If it is incremental programming, stop fetching new instructions */
+	if (inc) {
+		err = ree_graceful_disable_control(rvu, block, true);
+		if (err)
+			return err;
+	}
+
+	/* Force Clock ON
+	 * Force bits should be set throughout REEX programming, whether full
+	 * or incremental
+	 */
+	ree_reex_force_clock(rvu, block, true);
+
+	/* Reinitialize REEX block for programming */
+	err = ree_reex_programming(rvu, block, inc);
+	if (err)
+		return err;
+
+	/* Parse ROF data - validation part*/
+	rule_db_len = ree->ruledb_len;
+	rule_db_ptr = (struct ree_rule_db_entry *)ree->ruledb[0];
+	db_block = 0;
+	err = ree_rof_data_validation(rvu, block->addr, ree, &db_block,
+				      &rule_db_ptr, &rule_db_len);
+	if (err)
+		return err;
+
+	/* Parse ROF data - data part*/
+	err = ree_rof_data_enq(rvu, block, ree, &rule_db_ptr, &rule_db_len,
+			       &db_block);
+	if (err)
+		return err;
+
+	/* Parse ROF data - validation part*/
+	err = ree_rof_data_validation(rvu, block->addr, ree, &db_block,
+				      &rule_db_ptr, &rule_db_len);
+	if (err)
+		return err;
+
+	/* REEX Programming DONE: clear GO bit */
+	reg = rvu_read64(rvu, block->addr, REE_AF_REEXR_CTRL);
+	reg = reg & ~(REE_AF_REEXR_CTRL_GO);
+	rvu_write64(rvu, block->addr, REE_AF_REEXR_CTRL, reg);
+
+	ree_reex_enable(rvu, block);
+
+	/* Force Clock OFF */
+	ree_reex_force_clock(rvu, block, false);
+
+	/* If it is incremental programming, resume fetching instructions */
+	if (inc) {
+		err = ree_graceful_disable_control(rvu, block, false);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+int rvu_mbox_handler_ree_rule_db_prog(struct rvu *rvu,
+				      struct ree_rule_db_prog_req_msg *req,
+				      struct msg_rsp *rsp)
+{
+	int blkaddr, db_block = 0, blkid = 0, err;
+	struct rvu_block *block;
+	struct ree_rsrc *ree;
+
+	blkaddr = req->blkaddr;
+	if (!is_block_implemented(rvu->hw, blkaddr))
+		return REE_AF_ERR_BLOCK_NOT_IMPLEMENTED;
+	if (blkaddr == BLKADDR_REE1)
+		blkid = 1;
+
+	block = &rvu->hw->block[blkaddr];
+	ree = &rvu->hw->ree[blkid];
+
+	/* If this is the first block of ROF */
+	if (!req->offset) {
+		if (req->total_len >
+				REE_RULE_DB_ALLOC_SIZE * REE_RULE_DB_BLOCK_CNT)
+			return REE_AF_ERR_RULE_DB_TOO_BIG;
+
+		/* Initialize Programming memory */
+		err = ree_reex_memory_alloc(rvu, block, ree, req->total_len,
+					    req->is_incremental);
+		if (err)
+			return err;
+		/* Programming overwrites existing rule db
+		 * Incremental programming overwrites both rule db and rule dbi
+		 */
+		ree->ruledb_len = 0;
+		if (!req->is_incremental)
+			ree->ruledbi_len = 0;
+	}
+
+	/* Copy rof data from mbox to ruledb.
+	 * Rule db is later used for programming
+	 */
+	if (ree->ruledb_len + req->len >
+			ree->ruledb_blocks * REE_RULE_DB_ALLOC_SIZE)
+		return REE_AF_ERR_RULE_DB_WRONG_LENGTH;
+	if (ree->ruledb_len != req->offset)
+		return REE_AF_ERR_RULE_DB_WRONG_OFFSET;
+	/* All messages should be in block size, apart for last one */
+	if (req->len < REE_RULE_DB_REQ_BLOCK_SIZE && !req->is_last)
+		return REE_AF_ERR_RULE_DB_SHOULD_FILL_REQUEST;
+	/* Each mbox is 32KB each ruledb block is 4096KB
+	 * Single mbox shouldn't spread over blocks
+	 */
+	db_block = ree->ruledb_len >> REE_RULE_DB_ALLOC_SHIFT;
+	if (db_block >= ree->ruledb_blocks)
+		return REE_AF_ERR_RULE_DB_BLOCK_TOO_BIG;
+	memcpy((void *)((u64)ree->ruledb[db_block] + ree->ruledb_len),
+	       req->rule_db, req->len);
+	ree->ruledb_len += req->len;
+	/* ROF file is sent in chunks
+	 * wait for last chunk to start programming
+	 */
+	if (!req->is_last)
+		return 0;
+
+	if (req->total_len != ree->ruledb_len)
+		return REE_AF_ERR_RULE_DB_PARTIAL;
+
+	if (!req->is_incremental || req->is_dbi)
+		ree_rule_db_prog(rvu, block, ree, req->is_incremental);
+
+	if (req->is_dbi) {
+		memcpy(ree->ruledbi,
+		       ree->ruledb[db_block] +
+				req->total_len - REE_RULE_DBI_SIZE,
+		       REE_RULE_DBI_SIZE);
+		ree->ruledbi_len = REE_RULE_DBI_SIZE;
+	}
+
+	return 0;
+}
+
+int
+rvu_mbox_handler_ree_rule_db_get(struct rvu *rvu,
+				 struct ree_rule_db_get_req_msg *req,
+				 struct ree_rule_db_get_rsp_msg *rsp)
+{
+	int blkaddr, len, blkid = 0, db_block;
+	struct ree_rsrc *ree;
+
+	blkaddr = req->blkaddr;
+	if (!is_block_implemented(rvu->hw, blkaddr))
+		return REE_AF_ERR_BLOCK_NOT_IMPLEMENTED;
+	if (blkaddr == BLKADDR_REE1)
+		blkid = 1;
+	ree = &rvu->hw->ree[blkid];
+
+	/* In case no programming or incremental programming was done yet */
+	if ((req->is_dbi && ree->ruledbi_len == 0) ||
+	    (!req->is_dbi && ree->ruledb_len == 0)) {
+		rsp->len = 0;
+		return 0;
+	}
+
+	/* ROF file is sent in chunks
+	 * Verify that offset is inside db range
+	 */
+	if (req->is_dbi) {
+		if (ree->ruledbi_len < req->offset)
+			return REE_AF_ERR_RULE_DB_INC_OFFSET_TOO_BIG;
+		len = ree->ruledbi_len - req->offset;
+	} else {
+		if (ree->ruledb_len < req->offset)
+			return REE_AF_ERR_RULE_DB_OFFSET_TOO_BIG;
+		len = ree->ruledb_len - req->offset;
+	}
+
+	/* Check if this is the last chunk of db */
+	if (len < REE_RULE_DB_RSP_BLOCK_SIZE) {
+		rsp->is_last = true;
+		rsp->len = len;
+	} else {
+		rsp->is_last = false;
+		rsp->len = REE_RULE_DB_RSP_BLOCK_SIZE;
+	}
+
+	/* Copy DB chunk to response */
+	if (req->is_dbi) {
+		memcpy(rsp->rule_db, ree->ruledbi + req->offset, rsp->len);
+	} else {
+		db_block = req->offset >> 22;
+		memcpy(rsp->rule_db, ree->ruledb[db_block] + req->offset,
+		       rsp->len);
+	}
+
+	return 0;
+}
+
+int
+rvu_mbox_handler_ree_rule_db_len_get(struct rvu *rvu, struct ree_req_msg *req,
+				     struct ree_rule_db_len_rsp_msg *rsp)
+{
+	int blkaddr, blkid = 0;
+
+	blkaddr = req->blkaddr;
+	if (!is_block_implemented(rvu->hw, blkaddr))
+		return REE_AF_ERR_BLOCK_NOT_IMPLEMENTED;
+	if (blkaddr == BLKADDR_REE1)
+		blkid = 1;
+	rsp->len = rvu->hw->ree[blkid].ruledb_len;
+	rsp->inc_len = rvu->hw->ree[blkid].ruledbi_len;
+	return 0;
+}
+
+int rvu_mbox_handler_ree_config_lf(struct rvu *rvu,
+				   struct ree_lf_req_msg *req,
+				   struct msg_rsp *rsp)
+{
+	u16 pcifunc = req->hdr.pcifunc;
+	int lf, blkaddr, num_lfs;
+	struct rvu_block *block;
+	u64 val;
+
+	blkaddr = req->blkaddr;
+	if (!is_block_implemented(rvu->hw, blkaddr))
+		return REE_AF_ERR_BLOCK_NOT_IMPLEMENTED;
+	block = &rvu->hw->block[blkaddr];
+
+	/* Need to translate REE LF slot to global number
+	 * VFs use local numbering from 0 to number of LFs - 1
+	 */
+	lf = rvu_get_lf(rvu, block, pcifunc, req->lf);
+	if (lf < 0)
+		return REE_AF_ERR_LF_INVALID;
+
+	num_lfs = rvu_get_rsrc_mapcount(rvu_get_pfvf(rvu, req->hdr.pcifunc),
+					blkaddr);
+	if (lf >= num_lfs)
+		return REE_AF_ERR_LF_NO_MORE_RESOURCES;
+
+	/* LF instruction buffer size and priority are configured by AF.
+	 * Priority value can be 0 or 1
+	 */
+	if (req->pri > 1)
+		return REE_AF_ERR_LF_WRONG_PRIORITY;
+	if (req->size > REE_AF_QUE_SBUF_CTL_MAX_SIZE)
+		return REE_AF_ERR_LF_SIZE_TOO_BIG;
+	val =  req->size;
+	val =  val << REE_AF_QUE_SBUF_CTL_SIZE_SHIFT;
+	val +=  req->pri;
+	rvu_write64(rvu, blkaddr, REE_AF_QUE_SBUF_CTL(lf), val);
+
+	return 0;
+}
+
+int rvu_mbox_handler_ree_rd_wr_register(struct rvu *rvu,
+					struct ree_rd_wr_reg_msg *req,
+					struct ree_rd_wr_reg_msg *rsp)
+{
+	int blkaddr;
+
+	blkaddr = req->blkaddr;
+	if (!is_block_implemented(rvu->hw, blkaddr))
+		return REE_AF_ERR_BLOCK_NOT_IMPLEMENTED;
+	rsp->reg_offset = req->reg_offset;
+	rsp->ret_val = req->ret_val;
+	rsp->is_write = req->is_write;
+
+	switch (req->reg_offset) {
+	case REE_AF_REEXM_MAX_MATCH:
+	break;
+
+	default:
+		/* Access to register denied */
+		return REE_AF_ERR_ACCESS_DENIED;
+	}
+
+	if (req->is_write)
+		rvu_write64(rvu, blkaddr, req->reg_offset, req->val);
+	else
+		rsp->val = rvu_read64(rvu, blkaddr, req->reg_offset);
+
+	return 0;
+}
+
 static int ree_aq_inst_alloc(struct rvu *rvu, struct admin_queue **ad_queue,
 			     int qsize, int inst_size, int res_size)
 {
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_reg.h b/drivers/net/ethernet/marvell/octeontx2/af/rvu_reg.h
index 53eab4db6160..adde5988d9ff 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_reg.h
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_reg.h
@@ -875,10 +875,28 @@
 #define REE_AF_AQ_INT_IRDE		BIT_ULL(1)
 #define REE_AF_AQ_INT_PRDE		BIT_ULL(2)
 #define REE_AF_AQ_INT_PLLE		BIT_ULL(3)
+#define REE_AF_REEXM_CTRL_INIT		BIT_ULL(0)
+#define REE_AF_REEXM_CTRL_GO		BIT_ULL(3)
+#define REE_AF_REEXM_STATUS_INIT_DONE	BIT_ULL(0)
+#define REE_AF_REEXR_CTRL_INIT		BIT_ULL(0)
+#define REE_AF_REEXR_CTRL_GO		BIT_ULL(1)
+#define REE_AF_REEXR_CTRL_MODE_IM_L1_L2	BIT_ULL(4)
+#define REE_AF_REEXR_CTRL_MODE_L1_L2	BIT_ULL(5)
+
 #define REE_AF_AQ_SBUF_CTL_SIZE_SHIFT	32
 #define REE_AF_REEXM_MAX_MATCH_MAX	0xFEull
 #define REE_AF_REEXM_MAX_PRE_CNT_COUNT	0x3F0ull
 #define REE_AF_REEXM_MAX_PTHREAD_COUNT	0xFFFFull
 #define REE_AF_REEXM_MAX_LATENCY_COUNT	0xFFFFull
-
+#define REE_AF_QUE_SBUF_CTL_SIZE_SHIFT	32
+#define REE_AF_REEX_CSR_BLOCK_BASE_ADDR	(0x8000ull)
+#define REE_AF_REEX_CSR_BLOCK_ID	(0x200ull)
+#define REE_AF_REEX_CSR_BLOCK_ID_MASK	GENMASK_ULL(18, 16)
+#define REE_AF_REEX_CSR_BLOCK_ID_SHIFT	16
+#define REE_AF_REEX_CSR_INDEX		8
+#define REE_AF_REEX_CSR_INDEX_MASK	GENMASK_ULL(4, 0)
+#define REE_AF_QUE_SBUF_CTL_MAX_SIZE	GENMASK_ULL((50 - 32), 0)
+#define REE_AF_REEXR_STATUS_IM_INIT_DONE	BIT_ULL(4)
+#define REE_AF_REEXR_STATUS_L1_CACHE_INIT_DONE	BIT_ULL(5)
+#define REE_AF_REEXR_STATUS_L2_CACHE_INIT_DONE	BIT_ULL(6)
 #endif /* RVU_REG_H */
diff --git a/drivers/net/ethernet/marvell/octeontx2/af/rvu_struct.h b/drivers/net/ethernet/marvell/octeontx2/af/rvu_struct.h
index 5986e0a2c45a..3018610c70ee 100644
--- a/drivers/net/ethernet/marvell/octeontx2/af/rvu_struct.h
+++ b/drivers/net/ethernet/marvell/octeontx2/af/rvu_struct.h
@@ -1005,4 +1005,21 @@ struct ree_af_aq_inst_s {
 	u64 reserved_111_127	: 17;
 #endif
 };
+
+/* REE ROF file entry structure */
+struct ree_rof_s {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 reserved_40_63	: 24;
+	u64 typ			:  8;
+	u64 reserved_24_31	:  8;
+	u64 addr		: 24;
+#else
+	u64 addr		: 24;
+	u64 reserved_24_31	:  8;
+	u64 typ			:  8;
+	u64 reserved_40_63	: 24;
+#endif
+	u64 data;
+};
+
 #endif /* RVU_STRUCT_H */
-- 
2.17.1

