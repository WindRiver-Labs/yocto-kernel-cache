From e50dbd2463f1f84d5848f8773411cc60a7b14a3c Mon Sep 17 00:00:00 2001
From: Yuval Caduri <cyuval@marvell.com>
Date: Thu, 11 Feb 2016 18:52:31 +0200
Subject: [PATCH 0088/1345] net: eth: mvpp2x: add marvell mvpp2x related files

commit  88916ecb61d46e2c394cbee4a8a4dcb9c573bf9e from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

Added mvpp2x directory with all relevant files

Change-Id: Ieb4eb4cfbcc6e771c7ba491326a6f2b4502e7e7f
Signed-off-by: Alan Winkowski <walan@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/27462
Reviewed-by: Yehuda Yitschak <yehuday@marvell.com>
Tested-by: Yehuda Yitschak <yehuday@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2x/Makefile       |    5 +
 drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c | 2687 ++++++++
 drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h |  467 ++
 .../ethernet/marvell/mvpp2x/mv_gop110_hw_type.h    | 2179 +++++++
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h      |  789 +++
 .../net/ethernet/marvell/mvpp2x/mv_pp2x_debug.c    | 2017 ++++++
 .../net/ethernet/marvell/mvpp2x/mv_pp2x_debug.h    |  133 +
 .../net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c  |  482 ++
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c   | 6698 ++++++++++++++++++++
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h   |  763 +++
 .../net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h  | 2269 +++++++
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c | 5029 +++++++++++++++
 12 files changed, 23518 insertions(+)
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/Makefile
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw_type.h
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.c
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.h
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
 create mode 100644 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c

diff --git a/drivers/net/ethernet/marvell/mvpp2x/Makefile b/drivers/net/ethernet/marvell/mvpp2x/Makefile
new file mode 100644
index 0000000..f9a1e3e
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/Makefile
@@ -0,0 +1,5 @@
+
+obj-$(CONFIG_MVPP2X) += mvpp2x.o
+
+mvpp2x-objs := mv_pp2x_ethtool.o mv_pp2x_hw.o mv_pp2x_main.o mv_pp2x_debug.o
+mvpp2x-objs += mv_gop110_hw.o
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c
new file mode 100644
index 0000000..ea1dd21
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.c
@@ -0,0 +1,2687 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifdef ARMADA_390
+#include <linux/phy.h>
+#endif
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/inetdevice.h>
+#include <uapi/linux/ppp_defs.h>
+
+#include <net/ip.h>
+#include <net/ipv6.h>
+
+#include "mv_pp2x.h"
+#include "mv_gop110_hw.h"
+
+void mv_gop110_register_bases_dump(struct gop_hw *gop)
+{
+	pr_info("  %-32s: 0x%p\n", "GMAC", gop->gop_110.gmac.base);
+	pr_info("  %-32s: 0x%p\n", "XLG_MAC", gop->gop_110.xlg_mac.base);
+	pr_info("  %-32s: 0x%p\n", "SERDES", gop->gop_110.serdes.base);
+	pr_info("  %-32s: 0x%p\n", "XMIB", gop->gop_110.xmib.base);
+	pr_info("  %-32s: 0x%p\n", "SMI", gop->gop_110.smi_base);
+	pr_info("  %-32s: 0x%p\n", "XSMI", gop->gop_110.xsmi_base);
+	pr_info("  %-32s: 0x%p\n", "MSPG", gop->gop_110.mspg_base);
+	pr_info("  %-32s: 0x%p\n", "XPCS", gop->gop_110.xpcs_base);
+	pr_info("  %-32s: 0x%p\n", "PTP", gop->gop_110.ptp.base);
+	pr_info("  %-32s: 0x%p\n", "RFU1", gop->gop_110.rfu1_base);
+
+}
+EXPORT_SYMBOL(mv_gop110_register_bases_dump);
+
+/* print value of unit registers */
+void mv_gop110_gmac_regs_dump(struct gop_hw *gop, int port)
+{
+	int ind;
+	char reg_name[32];
+
+	mv_gop110_gmac_print(gop, "PORT_MAC_CTRL0", port,
+			     MV_GMAC_PORT_CTRL0_REG);
+	mv_gop110_gmac_print(gop, "PORT_MAC_CTRL1", port,
+			     MV_GMAC_PORT_CTRL1_REG);
+	mv_gop110_gmac_print(gop, "PORT_MAC_CTRL2", port,
+			     MV_GMAC_PORT_CTRL2_REG);
+	mv_gop110_gmac_print(gop, "PORT_AUTO_NEG_CFG", port,
+			     MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+	mv_gop110_gmac_print(gop, "PORT_STATUS0", port,
+			     MV_GMAC_PORT_STATUS0_REG);
+	mv_gop110_gmac_print(gop, "PORT_SERIAL_PARAM_CFG", port,
+			     MV_GMAC_PORT_SERIAL_PARAM_CFG_REG);
+	mv_gop110_gmac_print(gop, "PORT_FIFO_CFG_0", port,
+			     MV_GMAC_PORT_FIFO_CFG_0_REG);
+	mv_gop110_gmac_print(gop, "PORT_FIFO_CFG_1", port,
+			     MV_GMAC_PORT_FIFO_CFG_1_REG);
+	mv_gop110_gmac_print(gop, "PORT_SERDES_CFG0", port,
+			     MV_GMAC_PORT_SERDES_CFG0_REG);
+	mv_gop110_gmac_print(gop, "PORT_SERDES_CFG1", port,
+			     MV_GMAC_PORT_SERDES_CFG1_REG);
+	mv_gop110_gmac_print(gop, "PORT_SERDES_CFG2", port,
+			     MV_GMAC_PORT_SERDES_CFG2_REG);
+	mv_gop110_gmac_print(gop, "PORT_SERDES_CFG3", port,
+			     MV_GMAC_PORT_SERDES_CFG3_REG);
+	mv_gop110_gmac_print(gop, "PORT_PRBS_STATUS", port,
+			     MV_GMAC_PORT_PRBS_STATUS_REG);
+	mv_gop110_gmac_print(gop, "PORT_PRBS_ERR_CNTR", port,
+			     MV_GMAC_PORT_PRBS_ERR_CNTR_REG);
+	mv_gop110_gmac_print(gop, "PORT_STATUS1", port,
+			     MV_GMAC_PORT_STATUS1_REG);
+	mv_gop110_gmac_print(gop, "PORT_MIB_CNTRS_CTRL", port,
+			     MV_GMAC_PORT_MIB_CNTRS_CTRL_REG);
+	mv_gop110_gmac_print(gop, "PORT_MAC_CTRL3", port,
+			     MV_GMAC_PORT_CTRL3_REG);
+	mv_gop110_gmac_print(gop, "QSGMII", port,
+			     MV_GMAC_QSGMII_REG);
+	mv_gop110_gmac_print(gop, "QSGMII_STATUS", port,
+			     MV_GMAC_QSGMII_STATUS_REG);
+	mv_gop110_gmac_print(gop, "QSGMII_PRBS_CNTR", port,
+			     MV_GMAC_QSGMII_PRBS_CNTR_REG);
+	for (ind = 0; ind < 8; ind++) {
+		sprintf(reg_name, "CCFC_PORT_SPEED_TIMER%d", ind);
+		mv_gop110_gmac_print(gop, reg_name, port,
+				     MV_GMAC_CCFC_PORT_SPEED_TIMER_REG(ind));
+	}
+	for (ind = 0; ind < 4; ind++) {
+		sprintf(reg_name, "FC_DSA_TAG%d", ind);
+		mv_gop110_gmac_print(gop, reg_name, port,
+				     MV_GMAC_FC_DSA_TAG_REG(ind));
+	}
+	mv_gop110_gmac_print(gop, "LINK_LEVEL_FLOW_CTRL_WIN_REG_0", port,
+			     MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_0);
+	mv_gop110_gmac_print(gop, "LINK_LEVEL_FLOW_CTRL_WIN_REG_1", port,
+			     MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1);
+	mv_gop110_gmac_print(gop, "PORT_MAC_CTRL4", port,
+			     MV_GMAC_PORT_CTRL4_REG);
+	mv_gop110_gmac_print(gop, "PORT_SERIAL_PARAM_1_CFG", port,
+			     MV_GMAC_PORT_SERIAL_PARAM_1_CFG_REG);
+	mv_gop110_gmac_print(gop, "LPI_CTRL_0", port,
+			     MV_GMAC_LPI_CTRL_0_REG);
+	mv_gop110_gmac_print(gop, "LPI_CTRL_1", port,
+			     MV_GMAC_LPI_CTRL_1_REG);
+	mv_gop110_gmac_print(gop, "LPI_CTRL_2", port,
+			     MV_GMAC_LPI_CTRL_2_REG);
+	mv_gop110_gmac_print(gop, "LPI_STATUS", port,
+			     MV_GMAC_LPI_STATUS_REG);
+	mv_gop110_gmac_print(gop, "LPI_CNTR", port,
+			     MV_GMAC_LPI_CNTR_REG);
+	mv_gop110_gmac_print(gop, "PULSE_1_MS_LOW", port,
+			     MV_GMAC_PULSE_1_MS_LOW_REG);
+	mv_gop110_gmac_print(gop, "PULSE_1_MS_HIGH", port,
+			     MV_GMAC_PULSE_1_MS_HIGH_REG);
+	mv_gop110_gmac_print(gop, "PORT_INT_MASK", port,
+			     MV_GMAC_INTERRUPT_MASK_REG);
+	mv_gop110_gmac_print(gop, "INT_SUM_MASK", port,
+			     MV_GMAC_INTERRUPT_SUM_MASK_REG);
+}
+EXPORT_SYMBOL(mv_gop110_gmac_regs_dump);
+
+/* Set the MAC to reset or exit from reset */
+int mv_gop110_gmac_reset(struct gop_hw *gop, int mac_num, enum mv_reset reset)
+{
+	u32 reg_addr;
+	u32 val;
+
+	reg_addr = MV_GMAC_PORT_CTRL2_REG;
+
+	/* read - modify - write */
+	val = mv_gop110_gmac_read(gop, mac_num, reg_addr);
+	if (reset == RESET)
+		val |= MV_GMAC_PORT_CTRL2_PORTMACRESET_MASK;
+	else
+		val &= ~MV_GMAC_PORT_CTRL2_PORTMACRESET_MASK;
+	mv_gop110_gmac_write(gop, mac_num, reg_addr, val);
+
+	return 0;
+}
+
+static void mv_gop110_gmac_rgmii_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 val, thresh, an;
+
+	/* configure minimal level of the Tx FIFO before the lower
+	 * part starts to read a packet
+	 */
+	thresh = MV_RGMII_TX_FIFO_MIN_TH;
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK,
+		      (thresh << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG, val);
+
+	/* Disable bypass of sync module */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL4_REG);
+	val |= MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK;
+	/* configure DP clock select according to mode */
+	val &= ~MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK;
+	val |= MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
+	val |= MV_GMAC_PORT_CTRL4_EXT_PIN_GMII_SEL_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
+	val |= MV_GMAC_PORT_CTRL2_CLK_125_BYPS_EN_MASK;
+	val &= ~MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	/* configure GIG MAC to SGMII mode */
+	val &= ~MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* configure AN 0xb8e8 */
+	an = MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK   |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK      |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK     |
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_AUTO_NEG_CFG_REG, an);
+}
+
+static void mv_gop110_gmac_qsgmii_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 val, thresh, an;
+
+	/* configure minimal level of the Tx FIFO before the lower
+	 * part starts to read a packet
+	 */
+	thresh = MV_SGMII_TX_FIFO_MIN_TH;
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK,
+		      (thresh << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG, val);
+
+	/* Disable bypass of sync module */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL4_REG);
+	val |= MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK;
+	/* configure DP clock select according to mode */
+	val &= ~MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK;
+	val &= ~MV_GMAC_PORT_CTRL4_EXT_PIN_GMII_SEL_MASK;
+	/* configure QSGMII bypass according to mode */
+	val &= ~MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
+	val &= ~MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	/* configure GIG MAC to SGMII mode */
+	val &= ~MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* configure AN 0xB8EC */
+	an = MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK     |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK    |
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_AUTO_NEG_CFG_REG, an);
+}
+
+static void mv_gop110_gmac_sgmii_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 val, thresh, an;
+
+	/* configure minimal level of the Tx FIFO before the lower
+	 * part starts to read a packet
+	 */
+	thresh = MV_SGMII_TX_FIFO_MIN_TH;
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK,
+		      (thresh << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG, val);
+
+	/* Disable bypass of sync module */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL4_REG);
+	val |= MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK;
+	/* configure DP clock select according to mode */
+	val &= ~MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK;
+	/* configure QSGMII bypass according to mode */
+	val |= MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
+	val |= MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	/* configure GIG MAC to SGMII mode */
+	val &= ~MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* configure AN */
+	an = MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK     |
+		MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK    |
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_AUTO_NEG_CFG_REG, an);
+}
+
+static void mv_gop110_gmac_sgmii2_5_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 val, thresh, an;
+
+	/* configure minimal level of the Tx FIFO before the lower
+	 * part starts to read a packet
+	 */
+	thresh = MV_SGMII2_5_TX_FIFO_MIN_TH;
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK,
+		      (thresh << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_FIFO_CFG_1_REG, val);
+
+	/* Disable bypass of sync module */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL4_REG);
+	val |= MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK;
+	/* configure DP clock select according to mode */
+	val |= MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK;
+	/* configure QSGMII bypass according to mode */
+	val |= MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL4_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL2_REG);
+	val |= MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL2_REG, val);
+
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	/* configure GIG MAC to 1000Base-X mode connected to a
+	 * fiber transceiver
+	 */
+	val |= MV_GMAC_PORT_CTRL0_PORTTYPE_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* configure AN 0x9268 */
+	an = MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_MASK |
+		MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK     |
+		MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK    |
+		MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_MASK  |
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK;
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_AUTO_NEG_CFG_REG, an);
+}
+
+/* Set the internal mux's to the required MAC in the GOP */
+int mv_gop110_gmac_mode_cfg(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	u32 reg_addr;
+	u32 val;
+
+	int mac_num = mac->gop_index;
+
+	/* Set TX FIFO thresholds */
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_SGMII:
+		if (mac->speed == 2500)
+			mv_gop110_gmac_sgmii2_5_cfg(gop, mac_num);
+		else
+			mv_gop110_gmac_sgmii_cfg(gop, mac_num);
+	break;
+	case PHY_INTERFACE_MODE_RGMII:
+		mv_gop110_gmac_rgmii_cfg(gop, mac_num);
+	break;
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_qsgmii_cfg(gop, mac_num);
+	break;
+	default:
+		return -1;
+	}
+
+	/* Jumbo frame support - 0x1400*2= 0x2800 bytes */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	U32_SET_FIELD(val, MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_MASK,
+		      (0x1400 << MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_OFFS));
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, val);
+
+	/* PeriodicXonEn disable */
+	reg_addr = MV_GMAC_PORT_CTRL1_REG;
+	val = mv_gop110_gmac_read(gop, mac_num, reg_addr);
+	val &= ~MV_GMAC_PORT_CTRL1_EN_PERIODIC_FC_XON_MASK;
+	mv_gop110_gmac_write(gop, mac_num, reg_addr, val);
+
+	/* mask all ports interrupts */
+	mv_gop110_gmac_port_link_event_mask(gop, mac_num);
+
+	/* unmask link change interrupt */
+	val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_INTERRUPT_MASK_REG);
+	val |= MV_GMAC_INTERRUPT_CAUSE_LINK_CHANGE_MASK;
+	val |= 1; /* unmask summary bit */
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_INTERRUPT_MASK_REG, val);
+
+	return 0;
+}
+
+/* Configure MAC loopback */
+int mv_gop110_gmac_loopback_cfg(struct gop_hw *gop, int mac_num,
+				enum mv_lb_type type)
+{
+	u32 reg_addr;
+	u32 val;
+
+	reg_addr = MV_GMAC_PORT_CTRL1_REG;
+	val = mv_gop110_gmac_read(gop, mac_num, reg_addr);
+	switch (type) {
+	case MV_DISABLE_LB:
+		val &= ~MV_GMAC_PORT_CTRL1_GMII_LOOPBACK_MASK;
+		break;
+	case MV_TX_2_RX_LB:
+		val |= MV_GMAC_PORT_CTRL1_GMII_LOOPBACK_MASK;
+		break;
+	case MV_RX_2_TX_LB:
+	default:
+		return -1;
+	}
+	mv_gop110_gmac_write(gop, mac_num, reg_addr, val);
+
+	return 0;
+}
+
+/* Get MAC link status */
+bool mv_gop110_gmac_link_status_get(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_addr;
+	u32 val;
+
+	reg_addr = MV_GMAC_PORT_STATUS0_REG;
+
+	val = mv_gop110_gmac_read(gop, mac_num, reg_addr);
+	return (val & 1) ? true : false;
+}
+
+/* Enable port and MIB counters */
+void mv_gop110_gmac_port_enable(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	reg_val |= MV_GMAC_PORT_CTRL0_PORTEN_MASK;
+	reg_val |= MV_GMAC_PORT_CTRL0_COUNT_EN_MASK;
+
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, reg_val);
+}
+
+/* Disable port */
+void mv_gop110_gmac_port_disable(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	/* mask all ports interrupts */
+	mv_gop110_gmac_port_link_event_mask(gop, mac_num);
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	reg_val &= ~MV_GMAC_PORT_CTRL0_PORTEN_MASK;
+
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, reg_val);
+}
+
+void mv_gop110_gmac_port_periodic_xon_set(struct gop_hw *gop,
+					  int mac_num, int enable)
+{
+	u32 reg_val;
+
+	reg_val =  mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL1_REG);
+
+	if (enable)
+		reg_val |= MV_GMAC_PORT_CTRL1_EN_PERIODIC_FC_XON_MASK;
+	else
+		reg_val &= ~MV_GMAC_PORT_CTRL1_EN_PERIODIC_FC_XON_MASK;
+
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL1_REG, reg_val);
+}
+
+int mv_gop110_gmac_link_status(struct gop_hw *gop, int mac_num,
+			       struct mv_port_link_status *pstatus)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_STATUS0_REG);
+
+	if (reg_val & MV_GMAC_PORT_STATUS0_GMIISPEED_MASK)
+		pstatus->speed = MV_PORT_SPEED_1000;
+	else if (reg_val & MV_GMAC_PORT_STATUS0_MIISPEED_MASK)
+		pstatus->speed = MV_PORT_SPEED_100;
+	else
+		pstatus->speed = MV_PORT_SPEED_10;
+
+	if (reg_val & MV_GMAC_PORT_STATUS0_LINKUP_MASK)
+		pstatus->linkup = 1 /*TRUE*/;
+	else
+		pstatus->linkup = 0 /*FALSE*/;
+
+	if (reg_val & MV_GMAC_PORT_STATUS0_FULLDX_MASK)
+		pstatus->duplex = MV_PORT_DUPLEX_FULL;
+	else
+		pstatus->duplex = MV_PORT_DUPLEX_HALF;
+
+	if (reg_val & MV_GMAC_PORT_STATUS0_PORTTXPAUSE_MASK)
+		pstatus->tx_fc = MV_PORT_FC_ACTIVE;
+	else if (reg_val & MV_GMAC_PORT_STATUS0_TXFCEN_MASK)
+		pstatus->tx_fc = MV_PORT_FC_ENABLE;
+	else
+		pstatus->tx_fc = MV_PORT_FC_DISABLE;
+
+	if (reg_val & MV_GMAC_PORT_STATUS0_PORTRXPAUSE_MASK)
+		pstatus->rx_fc = MV_PORT_FC_ACTIVE;
+	else if (reg_val & MV_GMAC_PORT_STATUS0_RXFCEN_MASK)
+		pstatus->rx_fc = MV_PORT_FC_ENABLE;
+	else
+		pstatus->rx_fc = MV_PORT_FC_DISABLE;
+
+	return 0;
+}
+
+/* Change maximum receive size of the port */
+int mv_gop110_gmac_max_rx_size_set(struct gop_hw *gop,
+				   int mac_num, int max_rx_size)
+{
+	u32	reg_val;
+
+	reg_val =  mv_gop110_gmac_read(gop, mac_num, MV_GMAC_PORT_CTRL0_REG);
+	reg_val &= ~MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_MASK;
+	reg_val |= (((max_rx_size - MVPP2_MH_SIZE) / 2) <<
+			MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_OFFS);
+	mv_gop110_gmac_write(gop, mac_num, MV_GMAC_PORT_CTRL0_REG, reg_val);
+
+	return 0;
+}
+
+/* Sets "Force Link Pass" and "Do Not Force Link Fail" bits.
+*  This function should only be called when the port is disabled.
+* INPUT:
+*	int  port		- port number
+*	bool force_link_pass	- Force Link Pass
+*	bool force_link_fail - Force Link Failure
+*		0, 0 - normal state: detect link via PHY and connector
+*		1, 1 - prohibited state.
+*/
+int mv_gop110_gmac_force_link_mode_set(struct gop_hw *gop, int mac_num,
+				       bool force_link_up,
+				       bool force_link_down)
+{
+	u32 reg_val;
+
+	/* Can't force link pass and link fail at the same time */
+	if ((force_link_up) && (force_link_down))
+		return -EINVAL;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+
+	if (force_link_up)
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_UP_MASK;
+	else
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_UP_MASK;
+
+	if (force_link_down)
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_DOWN_MASK;
+	else
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_DOWN_MASK;
+
+	mv_gop110_gmac_write(gop, mac_num,
+			     MV_GMAC_PORT_AUTO_NEG_CFG_REG, reg_val);
+
+	return 0;
+}
+
+/* Get "Force Link Pass" and "Do Not Force Link Fail" bits.
+* INPUT:
+*	int  port		- port number
+* OUTPUT:
+*	bool *force_link_pass	- Force Link Pass
+*	bool *force_link_fail	- Force Link Failure
+*/
+int mv_gop110_gmac_force_link_mode_get(struct gop_hw *gop, int mac_num,
+				       bool *force_link_up,
+				       bool *force_link_down)
+{
+	u32 reg_val;
+
+	/* Can't force link pass and link fail at the same time */
+	if ((!force_link_up) || (!force_link_down))
+		return -EINVAL;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+
+	if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_UP_MASK)
+		*force_link_up = true;
+	else
+		*force_link_up = false;
+
+	if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_DOWN_MASK)
+		*force_link_down = true;
+	else
+		*force_link_down = false;
+
+	return 0;
+}
+
+/* Sets port speed to Auto Negotiation / 1000 / 100 / 10 Mbps.
+*  Sets port duplex to Auto Negotiation / Full / Half Duplex.
+*/
+int mv_gop110_gmac_speed_duplex_set(struct gop_hw *gop, int mac_num,
+				    enum mv_port_speed speed,
+				    enum mv_port_duplex duplex)
+{
+	u32 reg_val;
+
+	/* Check validity */
+	if ((speed == MV_PORT_SPEED_1000) && (duplex == MV_PORT_DUPLEX_HALF))
+		return -EINVAL;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+
+	switch (speed) {
+	case MV_PORT_SPEED_AN:
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK;
+		/* the other bits don't matter in this case */
+		break;
+	case MV_PORT_SPEED_1000:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK;
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK;
+		/* the 100/10 bit doesn't matter in this case */
+		break;
+	case MV_PORT_SPEED_100:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK;
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_MASK;
+		break;
+	case MV_PORT_SPEED_10:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_MASK;
+		break;
+	default:
+		pr_info("GMAC: Unexpected Speed value %d\n", speed);
+		return -EINVAL;
+	}
+
+	switch (duplex) {
+	case MV_PORT_DUPLEX_AN:
+		reg_val  |= MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK;
+		/* the other bits don't matter in this case */
+		break;
+	case MV_PORT_DUPLEX_HALF:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_MASK;
+		break;
+	case MV_PORT_DUPLEX_FULL:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK;
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_MASK;
+		break;
+	default:
+		pr_err("GMAC: Unexpected Duplex value %d\n", duplex);
+		return -EINVAL;
+	}
+
+	mv_gop110_gmac_write(gop, mac_num,
+			     MV_GMAC_PORT_AUTO_NEG_CFG_REG, reg_val);
+	return 0;
+}
+
+/* Gets port speed and duplex */
+int mv_gop110_gmac_speed_duplex_get(struct gop_hw *gop, int mac_num,
+				    enum mv_port_speed *speed,
+				    enum mv_port_duplex *duplex)
+{
+	u32 reg_val;
+
+	/* Check validity */
+	if (!speed || !duplex)
+		return -EINVAL;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+
+	if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK)
+		*speed = MV_PORT_SPEED_AN;
+	else if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK)
+		*speed = MV_PORT_SPEED_1000;
+	else if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_MASK)
+		*speed = MV_PORT_SPEED_100;
+	else
+		*speed = MV_PORT_SPEED_10;
+
+	if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK)
+		*duplex = MV_PORT_DUPLEX_AN;
+	else if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_MASK)
+		*duplex = MV_PORT_DUPLEX_FULL;
+	else
+		*duplex = MV_PORT_DUPLEX_HALF;
+
+	return 0;
+}
+
+/* Configure the port's Flow Control properties */
+int mv_gop110_gmac_fc_set(struct gop_hw *gop, int mac_num, enum mv_port_fc fc)
+{
+	u32 reg_val;
+	u32 fc_en;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+
+	switch (fc) {
+	case MV_PORT_FC_AN_NO:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_MASK;
+		break;
+
+	case MV_PORT_FC_AN_SYM:
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK;
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_MASK;
+		break;
+
+	case MV_PORT_FC_AN_ASYM:
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK;
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK;
+		reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_MASK;
+		break;
+
+	case MV_PORT_FC_DISABLE:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK;
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_MASK;
+		fc_en = mv_gop110_gmac_read(gop, mac_num,
+					    MV_GMAC_PORT_CTRL4_REG);
+		fc_en &= ~MV_GMAC_PORT_CTRL4_FC_EN_RX_MASK;
+		fc_en &= ~MV_GMAC_PORT_CTRL4_FC_EN_TX_MASK;
+		mv_gop110_gmac_write(gop, mac_num,
+				     MV_GMAC_PORT_CTRL4_REG, fc_en);
+		break;
+
+	case MV_PORT_FC_ENABLE:
+		reg_val &= ~MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK;
+		fc_en = mv_gop110_gmac_read(gop, mac_num,
+					    MV_GMAC_PORT_CTRL4_REG);
+		fc_en |= MV_GMAC_PORT_CTRL4_FC_EN_RX_MASK;
+		fc_en |= MV_GMAC_PORT_CTRL4_FC_EN_TX_MASK;
+		mv_gop110_gmac_write(gop, mac_num,
+				     MV_GMAC_PORT_CTRL4_REG, fc_en);
+		break;
+
+	default:
+		pr_err("GMAC: Unexpected FlowControl value %d\n", fc);
+		return -EINVAL;
+	}
+
+	mv_gop110_gmac_write(gop, mac_num,
+			     MV_GMAC_PORT_AUTO_NEG_CFG_REG, reg_val);
+	return 0;
+}
+
+/* Get Flow Control configuration of the port */
+void mv_gop110_gmac_fc_get(struct gop_hw *gop, int mac_num,
+			   enum mv_port_fc *fc)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+
+	if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK) {
+		/* Auto negotiation is enabled */
+		if (reg_val & MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK) {
+			if (reg_val &
+			    MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_MASK)
+				*fc = MV_PORT_FC_AN_ASYM;
+			else
+				*fc = MV_PORT_FC_AN_SYM;
+		} else {
+			*fc = MV_PORT_FC_AN_NO;
+		}
+	} else {
+		/* Auto negotiation is disabled */
+		reg_val = mv_gop110_gmac_read(gop, mac_num,
+					      MV_GMAC_PORT_CTRL4_REG);
+		if ((reg_val & MV_GMAC_PORT_CTRL4_FC_EN_RX_MASK) &&
+		    (reg_val & MV_GMAC_PORT_CTRL4_FC_EN_TX_MASK))
+			*fc = MV_PORT_FC_ENABLE;
+		else
+			*fc = MV_PORT_FC_DISABLE;
+	}
+}
+
+int mv_gop110_gmac_port_link_speed_fc(struct gop_hw *gop, int mac_num,
+				      enum mv_port_speed speed,
+				      int force_link_up)
+{
+	if (force_link_up) {
+		if (mv_gop110_gmac_speed_duplex_set(gop, mac_num, speed,
+						    MV_PORT_DUPLEX_FULL)) {
+			pr_err("mv_gop110_gmac_speed_duplex_set failed\n");
+			return -EPERM;
+		}
+		if (mv_gop110_gmac_fc_set(gop, mac_num, MV_PORT_FC_ENABLE)) {
+			pr_err("mv_gop110_gmac_fc_set failed\n");
+			return -EPERM;
+		}
+		if (mv_gop110_gmac_force_link_mode_set(gop, mac_num, 1, 0)) {
+			pr_err("mv_gop110_gmac_force_link_mode_set failed\n");
+			return -EPERM;
+		}
+	} else {
+		if (mv_gop110_gmac_force_link_mode_set(gop, mac_num, 0, 0)) {
+			pr_err("mv_gop110_gmac_force_link_mode_set failed\n");
+			return -EPERM;
+		}
+		if (mv_gop110_gmac_speed_duplex_set(gop, mac_num,
+						    MV_PORT_SPEED_AN,
+						    MV_PORT_DUPLEX_AN)) {
+			pr_err("mv_gop110_gmac_speed_duplex_set failed\n");
+			return -EPERM;
+		}
+		if (mv_gop110_gmac_fc_set(gop, mac_num, MV_PORT_FC_AN_SYM)) {
+			pr_err("mv_gop110_gmac_fc_set failed\n");
+			return -EPERM;
+		}
+	}
+
+	return 0;
+}
+
+void mv_gop110_gmac_port_link_event_mask(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_INTERRUPT_SUM_MASK_REG);
+	reg_val &= ~MV_GMAC_INTERRUPT_SUM_CAUSE_LINK_CHANGE_MASK;
+	mv_gop110_gmac_write(gop, mac_num,
+			     MV_GMAC_INTERRUPT_SUM_MASK_REG, reg_val);
+}
+
+void mv_gop110_gmac_port_link_event_unmask(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_INTERRUPT_SUM_MASK_REG);
+	reg_val |= MV_GMAC_INTERRUPT_SUM_CAUSE_LINK_CHANGE_MASK;
+	reg_val |= 1; /* unmask summary bit */
+	mv_gop110_gmac_write(gop, mac_num,
+			     MV_GMAC_INTERRUPT_SUM_MASK_REG, reg_val);
+}
+
+void mv_gop110_gmac_port_link_event_clear(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_INTERRUPT_CAUSE_REG);
+}
+
+int mv_gop110_gmac_port_autoneg_restart(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_gmac_read(gop, mac_num,
+				      MV_GMAC_PORT_AUTO_NEG_CFG_REG);
+	/* enable AN and restart it */
+	reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_MASK;
+	reg_val |= MV_GMAC_PORT_AUTO_NEG_CFG_INBAND_RESTARTAN_MASK;
+	mv_gop110_gmac_write(gop, mac_num,
+			     MV_GMAC_PORT_AUTO_NEG_CFG_REG, reg_val);
+	return 0;
+}
+
+/*************************************************************************
+* mv_port_init
+*
+* DESCRIPTION:
+*       Init physical port. Configures the port mode and all it's elements
+*       accordingly.
+*       Does not verify that the selected mode/port number is valid at the
+*       core level.
+*
+* INPUTS:
+*       port_num    - physical port number
+*       port_mode   - port standard metric
+*
+* OUTPUTS:
+*       None.
+*
+* RETURNS:
+*       0  - on success
+*       1  - on error
+*
+*************************************************************************/
+int mv_gop110_port_init(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int num_of_act_lanes;
+	int mac_num = mac->gop_index;
+
+	if (mac_num >= MVCPN110_GOP_MAC_NUM) {
+		pr_err("%s: illegal port number %d", __func__, mac_num);
+		return -1;
+	}
+	MVPP2_PRINT_VAR(mac_num);
+	MVPP2_PRINT_VAR(mac->phy_mode);
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+		mv_gop110_gmac_reset(gop, mac_num, RESET);
+		/* configure PCS */
+		mv_gop110_gpcs_mode_cfg(gop, mac_num, false);
+
+		/* configure MAC */
+		mv_gop110_gmac_mode_cfg(gop, mac);
+
+		/* select proper Mac mode */
+		mv_gop110_xlg_2_gig_mac_cfg(gop, mac_num);
+
+		/* pcs unreset */
+		mv_gop110_gpcs_reset(gop, mac_num, UNRESET);
+		/* mac unreset */
+		mv_gop110_gmac_reset(gop, mac_num, UNRESET);
+	break;
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		num_of_act_lanes = 1;
+		/* configure PCS */
+		mv_gop110_gpcs_mode_cfg(gop, mac_num, true);
+
+		/* configure MAC */
+		mv_gop110_gmac_mode_cfg(gop, mac);
+		/* select proper Mac mode */
+		mv_gop110_xlg_2_gig_mac_cfg(gop, mac_num);
+
+		/* pcs unreset */
+		mv_gop110_gpcs_reset(gop, mac_num, UNRESET);
+		/* mac unreset */
+		mv_gop110_gmac_reset(gop, mac_num, UNRESET);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+		num_of_act_lanes = 4;
+		mac_num = 0;
+		/* configure PCS */
+		mv_gop110_xpcs_mode(gop, num_of_act_lanes);
+		/* configure MAC */
+		mv_gop110_xlg_mac_mode_cfg(gop, mac_num, num_of_act_lanes);
+
+		/* pcs unreset */
+		mv_gop110_xpcs_reset(gop, UNRESET);
+		/* mac unreset */
+		mv_gop110_xlg_mac_reset(gop, mac_num, UNRESET);
+	break;
+	case PHY_INTERFACE_MODE_RXAUI:
+		num_of_act_lanes = 2;
+		/* mapped to serdes 6 */
+		mv_gop110_serdes_init(gop, 0, MV_RXAUI);
+		/* mapped to serdes 5 */
+		mv_gop110_serdes_init(gop, 1, MV_RXAUI);
+
+		mac_num = 0;
+		/* configure PCS */
+		mv_gop110_xpcs_mode(gop, num_of_act_lanes);
+		/* configure MAC */
+		mv_gop110_xlg_mac_mode_cfg(gop, mac_num, num_of_act_lanes);
+
+		/* pcs unreset */
+		mv_gop110_xpcs_reset(gop, UNRESET);
+
+		/* mac unreset */
+		mv_gop110_xlg_mac_reset(gop, mac_num, UNRESET);
+
+		/* run digital reset / unreset */
+		mv_gop110_serdes_reset(gop, 0, false, false, true);
+		mv_gop110_serdes_reset(gop, 1, false, false, true);
+		mv_gop110_serdes_reset(gop, 0, false, false, false);
+		mv_gop110_serdes_reset(gop, 1, false, false, false);
+	break;
+	/* Stefan: need to check KR case */
+	case PHY_INTERFACE_MODE_KR:
+	break;
+	default:
+		pr_err("%s: Requested port mode (%d) not supported",
+		       __func__, mac->phy_mode);
+		return -1;
+	}
+
+	GOP_DEBUG(gop->gop_port_debug.flags = (1 << CREATED));
+
+	return 0;
+}
+
+/**************************************************************************
+* mv_port_reset
+*
+* DESCRIPTION:
+*       Clears the port mode and release all its resources
+*       according to selected.
+*       Does not verify that the selected mode/port number is valid at the core
+*       level and actual terminated mode.
+*
+* INPUTS:
+*       port_num   - physical port number
+*       port_mode  - port standard metric
+*       action     - Power down or reset
+*
+* OUTPUTS:
+*       None.
+*
+* RETURNS:
+*       0  - on success
+*       1  - on error
+*
+**************************************************************************/
+int mv_gop110_port_reset(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int mac_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		/* pcs unreset */
+		mv_gop110_gpcs_reset(gop, mac_num, RESET);
+		/* mac unreset */
+		mv_gop110_gmac_reset(gop, mac_num, RESET);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+		/* pcs unreset */
+		mv_gop110_xpcs_reset(gop, RESET);
+		/* mac unreset */
+		mv_gop110_xlg_mac_reset(gop, mac_num, RESET);
+	break;
+	case PHY_INTERFACE_MODE_RXAUI:
+		/* pcs unreset */
+		mv_gop110_xpcs_reset(gop, RESET);
+		/* mac unreset */
+		mv_gop110_xlg_mac_reset(gop, mac_num, RESET);
+	break;
+	/* Stefan: need to check KR case */
+	case PHY_INTERFACE_MODE_KR:
+		/* pcs unreset */
+		mv_gop110_xpcs_reset(gop, RESET);
+		/* mac unreset */
+		mv_gop110_xlg_mac_reset(gop, mac_num, RESET);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+
+	/* TBD:serdes reset or power down if needed*/
+
+	return 0;
+}
+
+/*-------------------------------------------------------------------*/
+void mv_gop110_port_enable(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_enable(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_mac_port_enable(gop, port_num);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return;
+	}
+
+	GOP_DEBUG(gop->gop_port_debug[port_num].flags |= (1 << ENABLED));
+}
+
+void mv_gop110_port_disable(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_disable(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_mac_port_disable(gop, port_num);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return;
+	}
+
+	GOP_DEBUG(gop->gop_port_debug[port_num].flags &= ~(1 << ENABLED));
+}
+
+void mv_gop110_port_periodic_xon_set(struct gop_hw *gop,
+				     struct mv_mac_data *mac,
+				     int enable)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_periodic_xon_set(gop, port_num, enable);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_mac_port_periodic_xon_set(gop, port_num, enable);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return;
+	}
+}
+
+bool mv_gop110_port_is_link_up(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		return mv_gop110_gmac_link_status_get(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		udelay(1000);
+		return mv_gop110_xlg_mac_link_status_get(gop, port_num);
+	break;
+	default:
+		pr_err("%s: Wrong port mode gop_port(%d), phy_mode(%d)",
+		       __func__, port_num, mac->phy_mode);
+		return false;
+	}
+}
+
+int mv_gop110_port_link_status(struct gop_hw *gop, struct mv_mac_data *mac,
+			       struct mv_port_link_status *pstatus)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_link_status(gop, port_num, pstatus);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_mac_link_status(gop, port_num, pstatus);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_port_regs(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		pr_info("\n[gop GMAC #%d registers]\n", port_num);
+		mv_gop110_gmac_regs_dump(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		pr_info("\n[gop XLG MAC #%d registers]\n", port_num);
+		mv_gop110_xlg_mac_regs_dump(gop, port_num);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_port_events_mask(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_link_event_mask(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_port_link_event_mask(gop, port_num);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_port_events_unmask(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_link_event_unmask(gop, port_num);
+		/* gige interrupt cause connected to CPU via XLG
+		 * external interrupt cause
+		 */
+		mv_gop110_xlg_port_external_event_unmask(gop, 0, 2);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_port_external_event_unmask(gop, port_num, 1);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_port_events_clear(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_link_event_clear(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_port_link_event_clear(gop, port_num);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_status_show(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+	struct mv_port_link_status port_status;
+	bool port_en = false;
+
+	mv_gop110_port_link_status(gop, mac, &port_status);
+#ifdef MV_PP22_GOP_DEBUG
+	port_en = (gop->gop_port_debug[port_num].flags &
+		  (1 << ENABLED)) ? true : false;
+#endif
+	pr_info("-------------- Port %d configuration ----------------",
+		port_num);
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+		pr_info("Port mode               : RGMII");
+	break;
+	case PHY_INTERFACE_MODE_SGMII:
+		pr_info("Port mode               : SGMII");
+	break;
+	case PHY_INTERFACE_MODE_QSGMII:
+		pr_info("Port mode               : QSGMII");
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+		pr_info("Port mode               : XAUI");
+	break;
+	case PHY_INTERFACE_MODE_RXAUI:
+		pr_info("Port mode               : RXAUI");
+	break;
+	case PHY_INTERFACE_MODE_KR:
+		pr_info("Port mode               : KR");
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	pr_info("\nMAC status              : %s",
+		(port_en) ? "enable" : "disable");
+	pr_info("\nLink status             : %s",
+		(port_status.linkup) ? "link up" : "link down");
+	pr_info("\n");
+
+	if ((mac->phy_mode == PHY_INTERFACE_MODE_SGMII) &&
+	    (mac->speed == 2500) &&
+	    (port_status.speed == MV_PORT_SPEED_1000))
+		port_status.speed = MV_PORT_SPEED_2000;
+
+	switch (port_status.speed) {
+	case MV_PORT_SPEED_AN:
+		pr_info("Port speed              : AutoNeg");
+	break;
+	case MV_PORT_SPEED_10:
+		pr_info("Port speed              : 10M");
+	break;
+	case MV_PORT_SPEED_100:
+		pr_info("Port speed              : 100M");
+	break;
+	case MV_PORT_SPEED_1000:
+		pr_info("Port speed              : 1G");
+	break;
+	case MV_PORT_SPEED_2000:
+		pr_info("Port speed              : 2.5G");
+	break;
+	case MV_PORT_SPEED_10000:
+		pr_info("Port speed              : 10G");
+	break;
+	default:
+		pr_err("%s: Wrong port speed (%d)\n", __func__,
+		       port_status.speed);
+		return -1;
+	}
+	pr_info("\n");
+	switch (port_status.duplex) {
+	case MV_PORT_DUPLEX_AN:
+		pr_info("Port duplex             : AutoNeg");
+	break;
+	case MV_PORT_DUPLEX_HALF:
+		pr_info("Port duplex             : half");
+	break;
+	case MV_PORT_DUPLEX_FULL:
+		pr_info("Port duplex             : full");
+	break;
+	default:
+		pr_err("%s: Wrong port duplex (%d)", __func__,
+		       port_status.duplex);
+		return -1;
+	}
+	pr_info("\n");
+
+	return 0;
+}
+
+/* get port speed and duplex */
+int mv_gop110_speed_duplex_get(struct gop_hw *gop, struct mv_mac_data *mac,
+			       enum mv_port_speed *speed,
+			       enum mv_port_duplex *duplex)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_speed_duplex_get(gop, port_num, speed,
+						duplex);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_mac_speed_duplex_get(gop, port_num, speed,
+						   duplex);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+/* set port speed and duplex */
+int mv_gop110_speed_duplex_set(struct gop_hw *gop, struct mv_mac_data *mac,
+			       enum mv_port_speed speed,
+			       enum mv_port_duplex duplex)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_speed_duplex_set(gop, port_num, speed, duplex);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		mv_gop110_xlg_mac_speed_duplex_set(gop, port_num, speed,
+						   duplex);
+	break;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_autoneg_restart(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	break;
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		mv_gop110_gmac_port_autoneg_restart(gop, port_num);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		pr_err("%s: on supported for port mode (%d)", __func__,
+		       mac->phy_mode);
+		return -1;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+int mv_gop110_fl_cfg(struct gop_hw *gop, struct mv_mac_data *mac)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		/* disable AN */
+		if (mac->speed == 2500)
+			mv_gop110_speed_duplex_set(gop, mac,
+						   MV_PORT_SPEED_2000,
+						   MV_PORT_DUPLEX_FULL);
+		else
+			mv_gop110_speed_duplex_set(gop, mac,
+						   MV_PORT_SPEED_1000,
+						   MV_PORT_DUPLEX_FULL);
+		/* force link */
+		mv_gop110_gmac_force_link_mode_set(gop, port_num, true, false);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		return 0;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+/* set port ForceLinkUp and ForceLinkDown*/
+int mv_gop110_force_link_mode_set(struct gop_hw *gop, struct mv_mac_data *mac,
+				  bool force_link_up,
+				  bool force_link_down)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		/* force link */
+		mv_gop110_gmac_force_link_mode_set(gop, port_num,
+						   force_link_up,
+						   force_link_down);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		return 0;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+/* get port ForceLinkUp and ForceLinkDown*/
+int mv_gop110_force_link_mode_get(struct gop_hw *gop, struct mv_mac_data *mac,
+				  bool *force_link_up,
+				  bool *force_link_down)
+{
+	int port_num = mac->gop_index;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		return mv_gop110_gmac_force_link_mode_get(gop, port_num,
+							  force_link_up,
+							  force_link_down);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		return 0;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+/* set port internal loopback*/
+int mv_gop110_loopback_set(struct gop_hw *gop, struct mv_mac_data *mac,
+			   bool lb)
+{
+	int port_num = mac->gop_index;
+	enum mv_lb_type type;
+
+	switch (mac->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+		/* set loopback */
+		if (lb)
+			type = MV_TX_2_RX_LB;
+		else
+			type = MV_DISABLE_LB;
+
+		mv_gop110_gmac_loopback_cfg(gop, port_num, type);
+	break;
+	case PHY_INTERFACE_MODE_XAUI:
+	case PHY_INTERFACE_MODE_RXAUI:
+	case PHY_INTERFACE_MODE_KR:
+		return 0;
+	default:
+		pr_err("%s: Wrong port mode (%d)", __func__, mac->phy_mode);
+		return -1;
+	}
+	return 0;
+}
+
+/**************************************************************************
+* mv_gop110_gpcs_mode_cfg
+*
+* DESCRIPTION:
+	Configure port to working with Gig PCS or don't.
+*
+* INPUTS:
+*       pcs_num   - physical PCS number
+*       en        - true to enable PCS
+*
+* OUTPUTS:
+*       None.
+*
+* RETURNS:
+*       0  - on success
+*       1  - on error
+*
+**************************************************************************/
+int mv_gop110_gpcs_mode_cfg(struct gop_hw *gop, int pcs_num, bool en)
+{
+	u32 val;
+
+	val = mv_gop110_gmac_read(gop, pcs_num, MV_GMAC_PORT_CTRL2_REG);
+
+	if (en)
+		val |= MV_GMAC_PORT_CTRL2_PCS_EN_MASK;
+	else
+		val &= ~MV_GMAC_PORT_CTRL2_PCS_EN_MASK;
+
+	/* enable / disable PCS on this port */
+	mv_gop110_gmac_write(gop, pcs_num, MV_GMAC_PORT_CTRL2_REG, val);
+
+	return 0;
+}
+
+/**************************************************************************
+* mv_gop110_gpcs_reset
+*
+* DESCRIPTION:
+*       Set the selected PCS number to reset or exit from reset.
+*
+* INPUTS:
+*       pcs_num    - physical PCS number
+*       action    - reset / unreset
+*
+* OUTPUTS:
+*       None.
+*
+* RETURNS:
+*       0  - on success
+*       1  - on error
+*
+*************************************************************************/
+int  mv_gop110_gpcs_reset(struct gop_hw *gop, int pcs_num, enum mv_reset act)
+{
+	u32 reg_data;
+
+	reg_data = mv_gop110_gmac_read(gop, pcs_num, MV_GMAC_PORT_CTRL2_REG);
+	if (act == RESET)
+		U32_SET_FIELD(reg_data, MV_GMAC_PORT_CTRL2_SGMII_MODE_MASK, 0);
+	else
+		U32_SET_FIELD(reg_data, MV_GMAC_PORT_CTRL2_SGMII_MODE_MASK,
+			      1 << MV_GMAC_PORT_CTRL2_SGMII_MODE_OFFS);
+
+	mv_gop110_gmac_write(gop, pcs_num, MV_GMAC_PORT_CTRL2_REG, reg_data);
+	return 0;
+}
+
+/* print value of unit registers */
+void mv_gop110_serdes_lane_regs_dump(struct gop_hw *gop, int lane)
+{
+	pr_info("\nSerdes Lane #%d registers]\n", lane);
+	mv_gop110_serdes_print(gop, "MV_SERDES_CFG_0_REG", lane,
+			       MV_SERDES_CFG_0_REG);
+	mv_gop110_serdes_print(gop, "MV_SERDES_CFG_1_REG", lane,
+			       MV_SERDES_CFG_1_REG);
+	mv_gop110_serdes_print(gop, "MV_SERDES_CFG_2_REG", lane,
+			       MV_SERDES_CFG_2_REG);
+	mv_gop110_serdes_print(gop, "MV_SERDES_CFG_3_REG", lane,
+			       MV_SERDES_CFG_3_REG);
+	mv_gop110_serdes_print(gop, "MV_SERDES_MISC_REG", lane,
+			       MV_SERDES_MISC_REG);
+}
+EXPORT_SYMBOL(mv_gop110_serdes_lane_regs_dump);
+
+void mv_gop110_serdes_init(struct gop_hw *gop, int lane,
+			   enum sd_media_mode mode)
+{
+	u32 reg_val;
+
+	/* Media Interface Mode */
+	reg_val = mv_gop110_serdes_read(gop, lane, MV_SERDES_CFG_0_REG);
+	if (mode == MV_RXAUI)
+		reg_val |= MV_SERDES_CFG_0_MEDIA_MODE_MASK;
+	else
+		reg_val &= ~MV_SERDES_CFG_0_MEDIA_MODE_MASK;
+
+	/* Pull-Up PLL to StandAlone mode */
+	reg_val |= MV_SERDES_CFG_0_PU_PLL_MASK;
+	/* powers up the SD Rx/Tx PLL */
+	reg_val |= MV_SERDES_CFG_0_RX_PLL_MASK;
+	reg_val |= MV_SERDES_CFG_0_TX_PLL_MASK;
+	mv_gop110_serdes_write(gop, lane, MV_SERDES_CFG_0_REG, reg_val);
+
+	mv_gop110_serdes_reset(gop, lane, false, false, false);
+
+	reg_val = 0x17f;
+	mv_gop110_serdes_write(gop, lane, MV_SERDES_MISC_REG, reg_val);
+}
+
+void mv_gop110_serdes_reset(struct gop_hw *gop, int lane, bool analog_reset,
+			    bool core_reset, bool digital_reset)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_serdes_read(gop, lane, MV_SERDES_CFG_1_REG);
+	if (analog_reset)
+		reg_val &= ~MV_SERDES_CFG_1_ANALOG_RESET_MASK;
+	else
+		reg_val |= MV_SERDES_CFG_1_ANALOG_RESET_MASK;
+
+	if (core_reset)
+		reg_val &= ~MV_SERDES_CFG_1_CORE_RESET_MASK;
+	else
+		reg_val |= MV_SERDES_CFG_1_CORE_RESET_MASK;
+
+	if (digital_reset)
+		reg_val &= ~MV_SERDES_CFG_1_DIGITAL_RESET_MASK;
+	else
+		reg_val |= MV_SERDES_CFG_1_DIGITAL_RESET_MASK;
+
+	mv_gop110_serdes_write(gop, lane, MV_SERDES_CFG_1_REG, reg_val);
+}
+
+/**************************************************************************
+* mv_gop110_smi_init
+**************************************************************************/
+int mv_gop110_smi_init(struct gop_hw *gop)
+{
+	u32 val;
+
+	/* not invert MDC */
+	val = mv_gop110_smi_read(gop, MV_SMI_MISC_CFG_REG);
+	val &= ~MV_SMI_MISC_CFG_INVERT_MDC_MASK;
+	mv_gop110_smi_write(gop, MV_SMI_MISC_CFG_REG, val);
+
+	return 0;
+}
+
+/**************************************************************************
+* mv_gop_phy_addr_cfg
+**************************************************************************/
+int mv_gop110_smi_phy_addr_cfg(struct gop_hw *gop, int port, int addr)
+{
+	mv_gop110_smi_write(gop, MV_SMI_PHY_ADDRESS_REG(port), addr);
+
+	return 0;
+}
+
+/* print value of unit registers */
+void mv_gop110_xlg_mac_regs_dump(struct gop_hw *gop, int port)
+{
+	int timer;
+	char reg_name[16];
+
+	mv_gop110_xlg_mac_print(gop, "PORT_MAC_CTRL0", port,
+				MV_XLG_PORT_MAC_CTRL0_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_MAC_CTRL1", port,
+				MV_XLG_PORT_MAC_CTRL1_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_MAC_CTRL2", port,
+				MV_XLG_PORT_MAC_CTRL2_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_STATUS", port,
+				MV_XLG_MAC_PORT_STATUS_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_FIFOS_THRS_CFG", port,
+				MV_XLG_PORT_FIFOS_THRS_CFG_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_MAC_CTRL3", port,
+				MV_XLG_PORT_MAC_CTRL3_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_PER_PRIO_FLOW_CTRL_STATUS", port,
+				MV_XLG_PORT_PER_PRIO_FLOW_CTRL_STATUS_REG);
+	mv_gop110_xlg_mac_print(gop, "DEBUG_BUS_STATUS", port,
+				MV_XLG_DEBUG_BUS_STATUS_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_METAL_FIX", port,
+				MV_XLG_PORT_METAL_FIX_REG);
+	mv_gop110_xlg_mac_print(gop, "XG_MIB_CNTRS_CTRL", port,
+				MV_XLG_MIB_CNTRS_CTRL_REG);
+	for (timer = 0; timer < 8; timer++) {
+		sprintf(reg_name, "CNCCFC_TIMER%d", timer);
+		mv_gop110_xlg_mac_print(gop, reg_name, port,
+					MV_XLG_CNCCFC_TIMERI_REG(timer));
+	}
+	mv_gop110_xlg_mac_print(gop, "PPFC_CTRL", port,
+				MV_XLG_MAC_PPFC_CTRL_REG);
+	mv_gop110_xlg_mac_print(gop, "FC_DSA_TAG_0", port,
+				MV_XLG_MAC_FC_DSA_TAG_0_REG);
+	mv_gop110_xlg_mac_print(gop, "FC_DSA_TAG_1", port,
+				MV_XLG_MAC_FC_DSA_TAG_1_REG);
+	mv_gop110_xlg_mac_print(gop, "FC_DSA_TAG_2", port,
+				MV_XLG_MAC_FC_DSA_TAG_2_REG);
+	mv_gop110_xlg_mac_print(gop, "FC_DSA_TAG_3", port,
+				MV_XLG_MAC_FC_DSA_TAG_3_REG);
+	mv_gop110_xlg_mac_print(gop, "DIC_BUDGET_COMPENSATION", port,
+				MV_XLG_MAC_DIC_BUDGET_COMPENSATION_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_MAC_CTRL4", port,
+				MV_XLG_PORT_MAC_CTRL4_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_MAC_CTRL5", port,
+				MV_XLG_PORT_MAC_CTRL5_REG);
+	mv_gop110_xlg_mac_print(gop, "EXT_CTRL", port,
+				MV_XLG_MAC_EXT_CTRL_REG);
+	mv_gop110_xlg_mac_print(gop, "MACRO_CTRL", port,
+				MV_XLG_MAC_MACRO_CTRL_REG);
+	mv_gop110_xlg_mac_print(gop, "MACRO_CTRL", port,
+				MV_XLG_MAC_MACRO_CTRL_REG);
+	mv_gop110_xlg_mac_print(gop, "PORT_INT_MASK", port,
+				MV_XLG_INTERRUPT_MASK_REG);
+	mv_gop110_xlg_mac_print(gop, "EXTERNAL_INT_MASK", port,
+				MV_XLG_EXTERNAL_INTERRUPT_MASK_REG);
+}
+EXPORT_SYMBOL(mv_gop110_xlg_mac_regs_dump);
+
+/* Set the MAC to reset or exit from reset */
+int mv_gop110_xlg_mac_reset(struct gop_hw *gop, int mac_num,
+			    enum mv_reset reset)
+{
+	u32 reg_addr;
+	u32 val;
+
+	reg_addr = MV_XLG_PORT_MAC_CTRL0_REG;
+
+	/* read - modify - write */
+	val = mv_gop110_xlg_mac_read(gop, mac_num, reg_addr);
+	if (reset == RESET)
+		val &= ~MV_XLG_MAC_CTRL0_MACRESETN_MASK;
+	else
+		val |= MV_XLG_MAC_CTRL0_MACRESETN_MASK;
+	mv_gop110_xlg_mac_write(gop, mac_num, reg_addr, val);
+
+	return 0;
+}
+
+/* Set the internal mux's to the required MAC in the GOP */
+int mv_gop110_xlg_mac_mode_cfg(struct gop_hw *gop, int mac_num,
+			       int num_of_act_lanes)
+{
+	u32 reg_addr;
+	u32 val;
+
+	/* Set TX FIFO thresholds */
+	reg_addr = MV_XLG_PORT_FIFOS_THRS_CFG_REG;
+	val = mv_gop110_xlg_mac_read(gop, mac_num, reg_addr);
+	U32_SET_FIELD(val, MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXRDTHR_MASK,
+		      (6 << MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXRDTHR_OFFS));
+	mv_gop110_xlg_mac_write(gop, mac_num, reg_addr, val);
+
+	/* configure 10G MAC mode */
+	reg_addr = MV_XLG_PORT_MAC_CTRL3_REG;
+	val = mv_gop110_xlg_mac_read(gop, mac_num, reg_addr);
+	U32_SET_FIELD(val, MV_XLG_MAC_CTRL3_MACMODESELECT_MASK,
+		      (1 << MV_XLG_MAC_CTRL3_MACMODESELECT_OFFS));
+	mv_gop110_xlg_mac_write(gop, mac_num, reg_addr, val);
+
+	reg_addr = MV_XLG_PORT_MAC_CTRL4_REG;
+
+	/* read - modify - write */
+	val = mv_gop110_xlg_mac_read(gop, mac_num, reg_addr);
+	U32_SET_FIELD(val, 0x1F10, 0x310);
+	mv_gop110_xlg_mac_write(gop, mac_num, reg_addr, val);
+
+	/* Jumbo frame support - 0x1400*2= 0x2800 bytes */
+	val = mv_gop110_xlg_mac_read(gop, mac_num, MV_XLG_PORT_MAC_CTRL1_REG);
+	U32_SET_FIELD(val, 0x1FFF, 0x1400);
+	mv_gop110_xlg_mac_write(gop, mac_num, MV_XLG_PORT_MAC_CTRL1_REG, val);
+
+	/* mask all port interrupts */
+	mv_gop110_xlg_port_link_event_mask(gop, mac_num);
+
+	/* unmask link change interrupt */
+	val = mv_gop110_xlg_mac_read(gop, mac_num, MV_XLG_INTERRUPT_MASK_REG);
+	val |= MV_XLG_INTERRUPT_LINK_CHANGE_MASK;
+	val |= 1; /* unmask summary bit */
+	mv_gop110_xlg_mac_write(gop, mac_num, MV_XLG_INTERRUPT_MASK_REG, val);
+
+	return 0;
+}
+
+/* Configure MAC loopback */
+int mv_gop110_xlg_mac_loopback_cfg(struct gop_hw *gop, int mac_num,
+				   enum mv_lb_type type)
+{
+	u32 reg_addr;
+	u32 val;
+
+	reg_addr = MV_XLG_PORT_MAC_CTRL1_REG;
+	val = mv_gop110_xlg_mac_read(gop, mac_num, reg_addr);
+	switch (type) {
+	case MV_DISABLE_LB:
+		val &= ~MV_XLG_MAC_CTRL1_MACLOOPBACKEN_MASK;
+		val &= ~MV_XLG_MAC_CTRL1_XGMIILOOPBACKEN_MASK;
+		break;
+	case MV_RX_2_TX_LB:
+		val &= ~MV_XLG_MAC_CTRL1_MACLOOPBACKEN_MASK;
+		val |= MV_XLG_MAC_CTRL1_XGMIILOOPBACKEN_MASK;
+		break;
+	case MV_TX_2_RX_LB:
+		val |= MV_XLG_MAC_CTRL1_MACLOOPBACKEN_MASK;
+		val |= MV_XLG_MAC_CTRL1_XGMIILOOPBACKEN_MASK;
+		break;
+	default:
+		return -1;
+	}
+	mv_gop110_xlg_mac_write(gop, mac_num, reg_addr, val);
+	return 0;
+}
+
+/* Get MAC link status */
+bool mv_gop110_xlg_mac_link_status_get(struct gop_hw *gop, int mac_num)
+{
+	if (mv_gop110_xlg_mac_read(gop, mac_num,
+				   MV_XLG_MAC_PORT_STATUS_REG) & 1)
+		return true;
+
+	return false;
+}
+
+/* Enable port and MIB counters update */
+void mv_gop110_xlg_mac_port_enable(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL0_REG);
+	reg_val |= MV_XLG_MAC_CTRL0_PORTEN_MASK;
+	reg_val &= ~MV_XLG_MAC_CTRL0_MIBCNTDIS_MASK;
+
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL0_REG, reg_val);
+}
+
+/* Disable port */
+void mv_gop110_xlg_mac_port_disable(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	/* mask all port interrupts */
+	mv_gop110_xlg_port_link_event_mask(gop, mac_num);
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL0_REG);
+	reg_val &= ~MV_XLG_MAC_CTRL0_PORTEN_MASK;
+
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL0_REG, reg_val);
+}
+
+void mv_gop110_xlg_mac_port_periodic_xon_set(struct gop_hw *gop,
+					     int mac_num,
+					     int enable)
+{
+	u32 reg_val;
+
+	reg_val =  mv_gop110_xlg_mac_read(gop, mac_num,
+					  MV_XLG_PORT_MAC_CTRL0_REG);
+
+	if (enable)
+		reg_val |= MV_XLG_MAC_CTRL0_PERIODICXONEN_MASK;
+	else
+		reg_val &= ~MV_XLG_MAC_CTRL0_PERIODICXONEN_MASK;
+
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL0_REG, reg_val);
+}
+
+int mv_gop110_xlg_mac_link_status(struct gop_hw *gop,
+				  int mac_num,
+				  struct mv_port_link_status *pstatus)
+{
+	u32 reg_val;
+	u32 mac_mode;
+	u32 fc_en;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL3_REG);
+	mac_mode = (reg_val & MV_XLG_MAC_CTRL3_MACMODESELECT_MASK) >>
+		    MV_XLG_MAC_CTRL3_MACMODESELECT_OFFS;
+
+	/* speed  and duplex */
+	switch (mac_mode) {
+	case 0:
+		pstatus->speed = MV_PORT_SPEED_1000;
+		pstatus->duplex = MV_PORT_DUPLEX_AN;
+		break;
+	case 1:
+		pstatus->speed = MV_PORT_SPEED_10000;
+		pstatus->duplex = MV_PORT_DUPLEX_FULL;
+		break;
+	default:
+		return -1;
+	}
+
+	/* link status */
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_MAC_PORT_STATUS_REG);
+	if (reg_val & MV_XLG_MAC_PORT_STATUS_LINKSTATUS_MASK)
+		pstatus->linkup = 1 /*TRUE*/;
+	else
+		pstatus->linkup = 0 /*FALSE*/;
+
+	/* flow control status */
+	fc_en = mv_gop110_xlg_mac_read(gop, mac_num,
+				       MV_XLG_PORT_MAC_CTRL0_REG);
+	if (reg_val & MV_XLG_MAC_PORT_STATUS_PORTTXPAUSE_MASK)
+		pstatus->tx_fc = MV_PORT_FC_ACTIVE;
+	else if (fc_en & MV_XLG_MAC_CTRL0_TXFCEN_MASK)
+		pstatus->tx_fc = MV_PORT_FC_ENABLE;
+	else
+		pstatus->tx_fc = MV_PORT_FC_DISABLE;
+
+	if (reg_val & MV_XLG_MAC_PORT_STATUS_PORTRXPAUSE_MASK)
+		pstatus->rx_fc = MV_PORT_FC_ACTIVE;
+	else if (fc_en & MV_XLG_MAC_CTRL0_RXFCEN_MASK)
+		pstatus->rx_fc = MV_PORT_FC_ENABLE;
+	else
+		pstatus->rx_fc = MV_PORT_FC_DISABLE;
+
+	return 0;
+}
+
+/* Change maximum receive size of the port */
+int mv_gop110_xlg_mac_max_rx_size_set(struct gop_hw *gop, int mac_num,
+				      int max_rx_size)
+{
+	u32	reg_val;
+
+	reg_val =  mv_gop110_xlg_mac_read(gop, mac_num,
+					  MV_XLG_PORT_MAC_CTRL1_REG);
+	reg_val &= ~MV_XLG_MAC_CTRL1_FRAMESIZELIMIT_MASK;
+	reg_val |= (((max_rx_size - MVPP2_MH_SIZE) / 2) <<
+		    MV_XLG_MAC_CTRL1_FRAMESIZELIMIT_OFFS);
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL1_REG, reg_val);
+
+	return 0;
+}
+
+/* Sets "Force Link Pass" and "Do Not Force Link Fail" bits.
+*  This function should only be called when the port is disabled.
+* INPUT:
+*	int  port		- port number
+*	bool force_link_pass	- Force Link Pass
+*	bool force_link_fail - Force Link Failure
+*		0, 0 - normal state: detect link via PHY and connector
+*		1, 1 - prohibited state.
+*/
+int mv_gop110_xlg_mac_force_link_mode_set(struct gop_hw *gop, int mac_num,
+					  bool force_link_up,
+					  bool force_link_down)
+{
+	u32 reg_val;
+
+	/* Can't force link pass and link fail at the same time */
+	if ((force_link_up) && (force_link_down))
+		return -EINVAL;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL0_REG);
+
+	if (force_link_up)
+		reg_val |= MV_XLG_MAC_CTRL0_FORCELINKPASS_MASK;
+	else
+		reg_val &= ~MV_XLG_MAC_CTRL0_FORCELINKPASS_MASK;
+
+	if (force_link_down)
+		reg_val |= MV_XLG_MAC_CTRL0_FORCELINKDOWN_MASK;
+	else
+		reg_val &= ~MV_XLG_MAC_CTRL0_FORCELINKDOWN_MASK;
+
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL0_REG, reg_val);
+
+	return 0;
+}
+
+/* Sets port speed to Auto Negotiation / 1000 / 100 / 10 Mbps.
+*  Sets port duplex to Auto Negotiation / Full / Half Duplex.
+*/
+int mv_gop110_xlg_mac_speed_duplex_set(struct gop_hw *gop, int mac_num,
+				       enum mv_port_speed speed,
+				       enum mv_port_duplex duplex)
+{
+	/* not supported */
+	return -1;
+}
+
+/* Gets port speed and duplex */
+int mv_gop110_xlg_mac_speed_duplex_get(struct gop_hw *gop, int mac_num,
+				       enum mv_port_speed *speed,
+				       enum mv_port_duplex *duplex)
+{
+	/* not supported */
+	return -1;
+}
+
+/* Configure the port's Flow Control properties */
+int mv_gop110_xlg_mac_fc_set(struct gop_hw *gop, int mac_num,
+			     enum mv_port_fc fc)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL0_REG);
+
+	switch (fc) {
+	case MV_PORT_FC_DISABLE:
+		reg_val &= ~MV_XLG_MAC_CTRL0_RXFCEN_MASK;
+		reg_val &= ~MV_XLG_MAC_CTRL0_TXFCEN_MASK;
+		break;
+
+	case MV_PORT_FC_ENABLE:
+		reg_val |= MV_XLG_MAC_CTRL0_RXFCEN_MASK;
+		reg_val |= MV_XLG_MAC_CTRL0_TXFCEN_MASK;
+		break;
+
+	case MV_PORT_FC_AN_NO:
+	case MV_PORT_FC_AN_SYM:
+	case MV_PORT_FC_AN_ASYM:
+	default:
+		pr_err("XLG MAC: Unexpected FlowControl value %d\n", fc);
+		return -EINVAL;
+	}
+
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL0_REG, reg_val);
+	return 0;
+}
+
+/* Get Flow Control configuration of the port */
+void mv_gop110_xlg_mac_fc_get(struct gop_hw *gop, int mac_num,
+			      enum mv_port_fc *fc)
+{
+	u32 reg_val;
+
+	/* No auto negotiation for flow control */
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL0_REG);
+
+	if ((reg_val & MV_XLG_MAC_CTRL0_RXFCEN_MASK) &&
+	    (reg_val & MV_XLG_MAC_CTRL0_TXFCEN_MASK))
+		*fc = MV_PORT_FC_ENABLE;
+	else
+		*fc = MV_PORT_FC_DISABLE;
+}
+
+int mv_gop110_xlg_mac_port_link_speed_fc(struct gop_hw *gop, int mac_num,
+					 enum mv_port_speed speed,
+					 int force_link_up)
+{
+	if (force_link_up) {
+		if (mv_gop110_xlg_mac_fc_set(gop, mac_num,
+					     MV_PORT_FC_ENABLE)) {
+			pr_err("mv_gop110_xlg_mac_fc_set failed\n");
+			return -EPERM;
+		}
+		if (mv_gop110_xlg_mac_force_link_mode_set(gop, mac_num,
+							  1, 0)) {
+			pr_err(
+				"mv_gop110_xlg_mac_force_link_mode_set failed\n");
+			return -EPERM;
+		}
+	} else {
+		if (mv_gop110_xlg_mac_force_link_mode_set(gop, mac_num,
+							  0, 0)) {
+			pr_err(
+				"mv_gop110_xlg_mac_force_link_mode_set failed\n");
+			return -EPERM;
+		}
+		if (mv_gop110_xlg_mac_fc_set(gop, mac_num,
+					     MV_PORT_FC_AN_SYM)) {
+			pr_err("mv_gop110_xlg_mac_fc_set failed\n");
+			return -EPERM;
+		}
+	}
+
+	return 0;
+}
+
+void mv_gop110_xlg_port_link_event_mask(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_EXTERNAL_INTERRUPT_MASK_REG);
+	reg_val &= ~(1 << 1);
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_EXTERNAL_INTERRUPT_MASK_REG, reg_val);
+}
+
+void mv_gop110_xlg_port_external_event_unmask(struct gop_hw *gop, int mac_num,
+					      int bit_2_open)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_EXTERNAL_INTERRUPT_MASK_REG);
+	reg_val |= (1 << bit_2_open);
+	reg_val |= 1; /* unmask summary bit */
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_EXTERNAL_INTERRUPT_MASK_REG, reg_val);
+}
+
+void mv_gop110_xlg_port_link_event_clear(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_INTERRUPT_CAUSE_REG);
+}
+
+void mv_gop110_xlg_2_gig_mac_cfg(struct gop_hw *gop, int mac_num)
+{
+	u32 reg_val;
+
+	/* relevant only for MAC0 (XLG0 and GMAC0) */
+	if (mac_num > 0)
+		return;
+
+	/* configure 1Gig MAC mode */
+	reg_val = mv_gop110_xlg_mac_read(gop, mac_num,
+					 MV_XLG_PORT_MAC_CTRL3_REG);
+	U32_SET_FIELD(reg_val, MV_XLG_MAC_CTRL3_MACMODESELECT_MASK,
+		      (0 << MV_XLG_MAC_CTRL3_MACMODESELECT_OFFS));
+	mv_gop110_xlg_mac_write(gop, mac_num,
+				MV_XLG_PORT_MAC_CTRL3_REG, reg_val);
+}
+
+/* print value of unit registers */
+void mv_gop110_xpcs_gl_regs_dump(struct gop_hw *gop)
+{
+	pr_info("\nXPCS Global registers]\n");
+	mv_gop110_xpcs_global_print(gop, "GLOBAL_CFG_0",
+				    MV_XPCS_GLOBAL_CFG_0_REG);
+	mv_gop110_xpcs_global_print(gop, "GLOBAL_CFG_1",
+				    MV_XPCS_GLOBAL_CFG_1_REG);
+	mv_gop110_xpcs_global_print(gop, "GLOBAL_FIFO_THR_CFG",
+				    MV_XPCS_GLOBAL_FIFO_THR_CFG_REG);
+	mv_gop110_xpcs_global_print(gop, "GLOBAL_MAX_IDLE_CNTR",
+				    MV_XPCS_GLOBAL_MAX_IDLE_CNTR_REG);
+	mv_gop110_xpcs_global_print(gop, "GLOBAL_STATUS",
+				    MV_XPCS_GLOBAL_STATUS_REG);
+	mv_gop110_xpcs_global_print(gop, "GLOBAL_DESKEW_ERR_CNTR",
+				    MV_XPCS_GLOBAL_DESKEW_ERR_CNTR_REG);
+	mv_gop110_xpcs_global_print(gop, "TX_PCKTS_CNTR_LSB",
+				    MV_XPCS_TX_PCKTS_CNTR_LSB_REG);
+	mv_gop110_xpcs_global_print(gop, "TX_PCKTS_CNTR_MSB",
+				    MV_XPCS_TX_PCKTS_CNTR_MSB_REG);
+}
+EXPORT_SYMBOL(mv_gop110_xpcs_gl_regs_dump);
+
+/* print value of unit registers */
+void mv_gop110_xpcs_lane_regs_dump(struct gop_hw *gop, int lane)
+{
+	pr_info("\nXPCS Lane #%d registers]\n", lane);
+	mv_gop110_xpcs_lane_print(gop, "LANE_CFG_0", lane,
+				  MV_XPCS_LANE_CFG_0_REG);
+	mv_gop110_xpcs_lane_print(gop, "LANE_CFG_1", lane,
+				  MV_XPCS_LANE_CFG_1_REG);
+	mv_gop110_xpcs_lane_print(gop, "LANE_STATUS", lane,
+				  MV_XPCS_LANE_STATUS_REG);
+	mv_gop110_xpcs_lane_print(gop, "SYMBOL_ERR_CNTR", lane,
+				  MV_XPCS_SYMBOL_ERR_CNTR_REG);
+	mv_gop110_xpcs_lane_print(gop, "DISPARITY_ERR_CNTR", lane,
+				  MV_XPCS_DISPARITY_ERR_CNTR_REG);
+	mv_gop110_xpcs_lane_print(gop, "PRBS_ERR_CNTR", lane,
+				  MV_XPCS_PRBS_ERR_CNTR_REG);
+	mv_gop110_xpcs_lane_print(gop, "RX_PCKTS_CNTR_LSB", lane,
+				  MV_XPCS_RX_PCKTS_CNTR_LSB_REG);
+	mv_gop110_xpcs_lane_print(gop, "RX_PCKTS_CNTR_MSB", lane,
+				  MV_XPCS_RX_PCKTS_CNTR_MSB_REG);
+	mv_gop110_xpcs_lane_print(gop, "RX_BAD_PCKTS_CNTR_LSB", lane,
+				  MV_XPCS_RX_BAD_PCKTS_CNTR_LSB_REG);
+	mv_gop110_xpcs_lane_print(gop, "RX_BAD_PCKTS_CNTR_MSB", lane,
+				  MV_XPCS_RX_BAD_PCKTS_CNTR_MSB_REG);
+	mv_gop110_xpcs_lane_print(gop, "CYCLIC_DATA_0", lane,
+				  MV_XPCS_CYCLIC_DATA_0_REG);
+	mv_gop110_xpcs_lane_print(gop, "CYCLIC_DATA_1", lane,
+				  MV_XPCS_CYCLIC_DATA_1_REG);
+	mv_gop110_xpcs_lane_print(gop, "CYCLIC_DATA_2", lane,
+				  MV_XPCS_CYCLIC_DATA_2_REG);
+	mv_gop110_xpcs_lane_print(gop, "CYCLIC_DATA_3", lane,
+				  MV_XPCS_CYCLIC_DATA_3_REG);
+}
+EXPORT_SYMBOL(mv_gop110_xpcs_lane_regs_dump);
+
+/* Set PCS to reset or exit from reset */
+int mv_gop110_xpcs_reset(struct gop_hw *gop, enum mv_reset reset)
+{
+	u32 reg_addr;
+	u32 val;
+
+	reg_addr = MV_XPCS_GLOBAL_CFG_0_REG;
+
+	/* read - modify - write */
+	val = mv_gop110_xpcs_global_read(gop, reg_addr);
+	if (reset == RESET)
+		val &= ~MV_XPCS_GLOBAL_CFG_0_PCSRESET_MASK;
+	else
+		val |= MV_XPCS_GLOBAL_CFG_0_PCSRESET_MASK;
+	mv_gop110_xpcs_global_write(gop, reg_addr, val);
+
+	return 0;
+}
+
+/* Set the internal mux's to the required PCS in the PI */
+int mv_gop110_xpcs_mode(struct gop_hw *gop, int num_of_lanes)
+{
+	u32 reg_addr;
+	u32 val;
+	int lane;
+
+	switch (num_of_lanes) {
+	case 1:
+		lane = 0;
+	break;
+	case 2:
+		lane = 1;
+	break;
+	case 4:
+		lane = 2;
+	break;
+	default:
+		return -1;
+	}
+
+	/* configure XG MAC mode */
+	reg_addr = MV_XPCS_GLOBAL_CFG_0_REG;
+	val = mv_gop110_xpcs_global_read(gop, reg_addr);
+	val &= ~MV_XPCS_GLOBAL_CFG_0_PCSMODE_MASK;
+	U32_SET_FIELD(val, MV_XPCS_GLOBAL_CFG_0_PCSMODE_MASK, 0);
+	U32_SET_FIELD(val, MV_XPCS_GLOBAL_CFG_0_LANEACTIVE_MASK, (2 * lane) <<
+			MV_XPCS_GLOBAL_CFG_0_LANEACTIVE_OFFS);
+	mv_gop110_xpcs_global_write(gop, reg_addr, val);
+
+	return 0;
+}
+
+u64 mv_gop110_mib_read64(struct gop_hw *gop, int port, unsigned int offset)
+{
+	u64 val, val2;
+
+	val = mv_gop110_xmib_mac_read(gop, port, offset);
+	if (offset == MV_MIB_GOOD_OCTETS_RECEIVED_LOW ||
+	    offset == MV_MIB_GOOD_OCTETS_SENT_LOW) {
+		val2 = mv_gop110_xmib_mac_read(gop, port, offset + 4);
+		val += (val2 << 32);
+	}
+
+	return val;
+}
+
+static void mv_gop110_mib_print(struct gop_hw *gop, int port, u32 offset,
+				char *mib_name)
+{
+	u64 val;
+
+	val = mv_gop110_mib_read64(gop, port, offset);
+	pr_info("  %-32s: 0x%02x = 0x%08llx\n", mib_name, offset, val);
+}
+
+void mv_gop110_mib_counters_show(struct gop_hw *gop, int port)
+{
+	pr_info("\n[Rx]\n");
+	mv_gop110_mib_print(gop, port, MV_MIB_GOOD_OCTETS_RECEIVED_LOW,
+			    "GOOD_OCTETS_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_BAD_OCTETS_RECEIVED,
+			    "BAD_OCTETS_RECEIVED");
+
+	mv_gop110_mib_print(gop, port, MV_MIB_UNICAST_FRAMES_RECEIVED,
+			    "UNCAST_FRAMES_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_BROADCAST_FRAMES_RECEIVED,
+			    "BROADCAST_FRAMES_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_MULTICAST_FRAMES_RECEIVED,
+			    "MULTICAST_FRAMES_RECEIVED");
+
+	pr_info("\n[RMON]\n");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAMES_64_OCTETS,
+			    "FRAMES_64_OCTETS");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAMES_65_TO_127_OCTETS,
+			    "FRAMES_65_TO_127_OCTETS");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAMES_128_TO_255_OCTETS,
+			    "FRAMES_128_TO_255_OCTETS");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAMES_256_TO_511_OCTETS,
+			    "FRAMES_256_TO_511_OCTETS");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAMES_512_TO_1023_OCTETS,
+			    "FRAMES_512_TO_1023_OCTETS");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAMES_1024_TO_MAX_OCTETS,
+			    "FRAMES_1024_TO_MAX_OCTETS");
+
+	pr_info("\n[Tx]\n");
+	mv_gop110_mib_print(gop, port, MV_MIB_GOOD_OCTETS_SENT_LOW,
+			    "GOOD_OCTETS_SENT");
+	mv_gop110_mib_print(gop, port, MV_MIB_UNICAST_FRAMES_SENT,
+			    "UNICAST_FRAMES_SENT");
+	mv_gop110_mib_print(gop, port, MV_MIB_MULTICAST_FRAMES_SENT,
+			    "MULTICAST_FRAMES_SENT");
+	mv_gop110_mib_print(gop, port, MV_MIB_BROADCAST_FRAMES_SENT,
+			    "BROADCAST_FRAMES_SENT");
+	mv_gop110_mib_print(gop, port, MV_MIB_CRC_ERRORS_SENT,
+			    "CRC_ERRORS_SENT");
+
+	pr_info("\n[FC control]\n");
+	mv_gop110_mib_print(gop, port, MV_MIB_FC_RECEIVED,
+			    "FC_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_FC_SENT,
+			    "FC_SENT");
+
+	pr_info("\n[Errors]\n");
+	mv_gop110_mib_print(gop, port, MV_MIB_RX_FIFO_OVERRUN,
+			    "RX_FIFO_OVERRUN");
+	mv_gop110_mib_print(gop, port, MV_MIB_UNDERSIZE_RECEIVED,
+			    "UNDERSIZE_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_FRAGMENTS_RECEIVED,
+			    "FRAGMENTS_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_OVERSIZE_RECEIVED,
+			    "OVERSIZE_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_JABBER_RECEIVED,
+			    "JABBER_RECEIVED");
+	mv_gop110_mib_print(gop, port, MV_MIB_MAC_RECEIVE_ERROR,
+			    "MAC_RECEIVE_ERROR");
+	mv_gop110_mib_print(gop, port, MV_MIB_BAD_CRC_EVENT,
+			    "BAD_CRC_EVENT");
+	mv_gop110_mib_print(gop, port, MV_MIB_COLLISION,
+			    "COLLISION");
+	/* This counter must be read last. Read it clear all the counters */
+	mv_gop110_mib_print(gop, port, MV_MIB_LATE_COLLISION,
+			    "LATE_COLLISION");
+}
+EXPORT_SYMBOL(mv_gop110_mib_counters_show);
+
+void mv_gop110_ptp_enable(struct gop_hw *gop, int port, bool state)
+{
+	u32 reg_data;
+
+	if (state) {
+		/* PTP enable */
+		reg_data = mv_gop110_ptp_read(gop, port,
+					      MV_PTP_GENERAL_CTRL_REG);
+		reg_data |= MV_PTP_GENERAL_CTRL_PTP_UNIT_ENABLE_MASK;
+		/* enable PTP */
+		mv_gop110_ptp_write(gop, port, MV_PTP_GENERAL_CTRL_REG,
+				    reg_data);
+		/* unreset unit */
+		reg_data |= MV_PTP_GENERAL_CTRL_PTP_RESET_MASK;
+		mv_gop110_ptp_write(gop, port, MV_PTP_GENERAL_CTRL_REG,
+				    reg_data);
+	} else {
+		reg_data = mv_gop110_ptp_read(gop, port,
+					      MV_PTP_GENERAL_CTRL_REG);
+		reg_data &= ~MV_PTP_GENERAL_CTRL_PTP_UNIT_ENABLE_MASK;
+		/* disable PTP */
+		mv_gop110_ptp_write(gop, port, MV_PTP_GENERAL_CTRL_REG,
+				    reg_data);
+	}
+}
+
+void mv_gop110_netc_active_port(struct gop_hw *gop, u32 port, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_PORTS_CONTROL_1);
+	reg &= ~(NETC_PORTS_ACTIVE_MASK(port));
+
+	val <<= NETC_PORTS_ACTIVE_OFFSET(port);
+	val &= NETC_PORTS_ACTIVE_MASK(port);
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_PORTS_CONTROL_1, reg);
+}
+
+static void mv_gop110_netc_xaui_enable(struct gop_hw *gop, u32 port, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, SD1_CONTROL_1_REG);
+	reg &= ~SD1_CONTROL_XAUI_EN_MASK;
+
+	val <<= SD1_CONTROL_XAUI_EN_OFFSET;
+	val &= SD1_CONTROL_XAUI_EN_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, SD1_CONTROL_1_REG, reg);
+}
+
+static void mv_gop110_netc_rxaui0_enable(struct gop_hw *gop, u32 port, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, SD1_CONTROL_1_REG);
+	reg &= ~SD1_CONTROL_RXAUI0_L23_EN_MASK;
+
+	val <<= SD1_CONTROL_RXAUI0_L23_EN_OFFSET;
+	val &= SD1_CONTROL_RXAUI0_L23_EN_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, SD1_CONTROL_1_REG, reg);
+}
+
+static void mv_gop110_netc_rxaui1_enable(struct gop_hw *gop, u32 port, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, SD1_CONTROL_1_REG);
+	reg &= ~SD1_CONTROL_RXAUI1_L45_EN_MASK;
+
+	val <<= SD1_CONTROL_RXAUI1_L45_EN_OFFSET;
+	val &= SD1_CONTROL_RXAUI1_L45_EN_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, SD1_CONTROL_1_REG, reg);
+}
+
+
+
+static void mv_gop110_netc_mii_mode(struct gop_hw *gop, u32 port, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_CONTROL_0);
+	reg &= ~NETC_GBE_PORT1_MII_MODE_MASK;
+
+	val <<= NETC_GBE_PORT1_MII_MODE_OFFSET;
+	val &= NETC_GBE_PORT1_MII_MODE_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_CONTROL_0, reg);
+}
+
+
+
+static void mv_gop110_netc_gop_reset(struct gop_hw *gop, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_GOP_SOFT_RESET_1_REG);
+	reg &= ~NETC_GOP_SOFT_RESET_MASK;
+
+	val <<= NETC_GOP_SOFT_RESET_OFFSET;
+	val &= NETC_GOP_SOFT_RESET_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_GOP_SOFT_RESET_1_REG, reg);
+}
+
+static void mv_gop110_netc_gop_clock_logic_set(struct gop_hw *gop, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_PORTS_CONTROL_0);
+	reg &= ~NETC_CLK_DIV_PHASE_MASK;
+
+	val <<= NETC_CLK_DIV_PHASE_OFFSET;
+	val &= NETC_CLK_DIV_PHASE_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_PORTS_CONTROL_0, reg);
+}
+
+static void mv_gop110_netc_port_rf_reset(struct gop_hw *gop, u32 port, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_PORTS_CONTROL_1);
+	reg &= ~(NETC_PORT_GIG_RF_RESET_MASK(port));
+
+	val <<= NETC_PORT_GIG_RF_RESET_OFFSET(port);
+	val &= NETC_PORT_GIG_RF_RESET_MASK(port);
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_PORTS_CONTROL_1, reg);
+}
+
+static void mv_gop110_netc_gbe_sgmii_mode_select(struct gop_hw *gop, u32 port,
+						u32 val)
+{
+	u32 reg, mask, offset;
+
+	if (port == 2) {
+		mask = NETC_GBE_PORT0_SGMII_MODE_MASK;
+		offset = NETC_GBE_PORT0_SGMII_MODE_OFFSET;
+	} else {
+		mask = NETC_GBE_PORT1_SGMII_MODE_MASK;
+		offset = NETC_GBE_PORT1_SGMII_MODE_OFFSET;
+	}
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_CONTROL_0);
+	reg &= ~mask;
+
+	val <<= offset;
+	val &= mask;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_CONTROL_0, reg);
+}
+
+static void mv_gop110_netc_bus_width_select(struct gop_hw *gop, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_PORTS_CONTROL_0);
+	reg &= ~NETC_BUS_WIDTH_SELECT_MASK;
+
+	val <<= NETC_BUS_WIDTH_SELECT_OFFSET;
+	val &= NETC_BUS_WIDTH_SELECT_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_PORTS_CONTROL_0, reg);
+}
+
+static void mv_gop110_netc_sample_stages_timing(struct gop_hw *gop, u32 val)
+{
+	u32 reg;
+
+	reg = mv_gop110_rfu1_read(gop, MV_NETCOMP_PORTS_CONTROL_0);
+	reg &= ~NETC_GIG_RX_DATA_SAMPLE_MASK;
+
+	val <<= NETC_GIG_RX_DATA_SAMPLE_OFFSET;
+	val &= NETC_GIG_RX_DATA_SAMPLE_MASK;
+
+	reg |= val;
+
+	mv_gop110_rfu1_write(gop, MV_NETCOMP_PORTS_CONTROL_0, reg);
+}
+
+static void mv_gop110_netc_mac_to_xgmii(struct gop_hw *gop, u32 port,
+					enum mv_netc_phase phase)
+{
+	switch (phase) {
+	case MV_NETC_FIRST_PHASE:
+		/* Set Bus Width to HB mode = 1 */
+		mv_gop110_netc_bus_width_select(gop, 1);
+		/* Select RGMII mode */
+		mv_gop110_netc_gbe_sgmii_mode_select(gop, port,
+							MV_NETC_GBE_XMII);
+		break;
+	case MV_NETC_SECOND_PHASE:
+		/* De-assert the relevant port HB reset */
+		mv_gop110_netc_port_rf_reset(gop, port, 1);
+		break;
+	}
+}
+
+static void mv_gop110_netc_mac_to_sgmii(struct gop_hw *gop, u32 port,
+					enum mv_netc_phase phase)
+{
+	switch (phase) {
+	case MV_NETC_FIRST_PHASE:
+		/* Set Bus Width to HB mode = 1 */
+		mv_gop110_netc_bus_width_select(gop, 1);
+		/* Select SGMII mode */
+		if (port >= 1)
+			mv_gop110_netc_gbe_sgmii_mode_select(gop, port,
+			MV_NETC_GBE_SGMII);
+
+		/* Configure the sample stages */
+		mv_gop110_netc_sample_stages_timing(gop, 0);
+		/* Configure the ComPhy Selector */
+		/* mv_gop110_netc_com_phy_selector_config(netComplex); */
+		break;
+	case MV_NETC_SECOND_PHASE:
+		/* De-assert the relevant port HB reset */
+		mv_gop110_netc_port_rf_reset(gop, port, 1);
+		break;
+	}
+}
+
+static void mv_gop110_netc_mac_to_rxaui(struct gop_hw *gop, u32 port,
+					enum mv_netc_phase phase,
+					enum mv_netc_lanes lanes)
+{
+	/* Currently only RXAUI0 supported */
+	if (port != 0)
+		return;
+
+	switch (phase) {
+	case MV_NETC_FIRST_PHASE:
+		/* RXAUI Serdes/s Clock alignment */
+		if (lanes == MV_NETC_LANE_23)
+			mv_gop110_netc_rxaui0_enable(gop, port, 1);
+		else
+			mv_gop110_netc_rxaui1_enable(gop, port, 1);
+		break;
+	case MV_NETC_SECOND_PHASE:
+		/* De-assert the relevant port HB reset */
+		mv_gop110_netc_port_rf_reset(gop, port, 1);
+		break;
+	}
+}
+
+static void mv_gop110_netc_mac_to_xaui(struct gop_hw *gop, u32 port,
+					enum mv_netc_phase phase)
+{
+	switch (phase) {
+	case MV_NETC_FIRST_PHASE:
+		/* RXAUI Serdes/s Clock alignment */
+		mv_gop110_netc_xaui_enable(gop, port, 1);
+		break;
+	case MV_NETC_SECOND_PHASE:
+		/* De-assert the relevant port HB reset */
+		mv_gop110_netc_port_rf_reset(gop, port, 1);
+		break;
+	}
+}
+
+
+int mv_gop110_netc_init(struct gop_hw *gop,
+			u32 net_comp_config, enum mv_netc_phase phase)
+{
+	u32 c = net_comp_config;
+
+	MVPP2_PRINT_VAR(net_comp_config);
+
+	if (c & MV_NETC_GE_MAC0_RXAUI_L23)
+		mv_gop110_netc_mac_to_rxaui(gop, 0, phase, MV_NETC_LANE_23);
+
+	if (c & MV_NETC_GE_MAC0_RXAUI_L45)
+		mv_gop110_netc_mac_to_rxaui(gop, 0, phase, MV_NETC_LANE_45);
+
+	if (c & MV_NETC_GE_MAC0_XAUI)
+		mv_gop110_netc_mac_to_xaui(gop, 0, phase);
+
+	if (c & MV_NETC_GE_MAC2_SGMII)
+		mv_gop110_netc_mac_to_sgmii(gop, 2, phase);
+	else
+		mv_gop110_netc_mac_to_xgmii(gop, 2, phase);
+	if (c & MV_NETC_GE_MAC3_SGMII)
+		mv_gop110_netc_mac_to_sgmii(gop, 3, phase);
+	else {
+		mv_gop110_netc_mac_to_xgmii(gop, 3, phase);
+		if (c & MV_NETC_GE_MAC3_RGMII)
+			mv_gop110_netc_mii_mode(gop, 3, MV_NETC_GBE_RGMII);
+		else
+			mv_gop110_netc_mii_mode(gop, 3, MV_NETC_GBE_MII);
+	}
+
+	/* Activate gop ports 0, 2, 3 */
+	mv_gop110_netc_active_port(gop, 0, 1);
+	mv_gop110_netc_active_port(gop, 2, 1);
+	mv_gop110_netc_active_port(gop, 3, 1);
+
+	if (phase == MV_NETC_SECOND_PHASE) {
+		/* Enable the GOP internal clock logic */
+		mv_gop110_netc_gop_clock_logic_set(gop, 1);
+		/* De-assert GOP unit reset */
+		mv_gop110_netc_gop_reset(gop, 1);
+	}
+	return 0;
+}
+
+
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h
new file mode 100644
index 0000000..a344ef2
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw.h
@@ -0,0 +1,467 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifndef _MV_GOP_HW_H_
+#define _MV_GOP_HW_H_
+
+/* Sets the field located at the specified in data.     */
+#define U32_SET_FIELD(data, mask, val)	((data) = (((data) & ~(mask)) | (val)))
+
+/* port related */
+enum mv_reset {RESET, UNRESET};
+
+enum mv_port_speed {
+	MV_PORT_SPEED_AN,
+	MV_PORT_SPEED_10,
+	MV_PORT_SPEED_100,
+	MV_PORT_SPEED_1000,
+	MV_PORT_SPEED_2000,
+	MV_PORT_SPEED_10000
+};
+
+enum mv_port_duplex {
+	MV_PORT_DUPLEX_AN,
+	MV_PORT_DUPLEX_HALF,
+	MV_PORT_DUPLEX_FULL
+};
+
+enum mv_port_fc {
+	MV_PORT_FC_AN_NO,
+	MV_PORT_FC_AN_SYM,
+	MV_PORT_FC_AN_ASYM,
+	MV_PORT_FC_DISABLE,
+	MV_PORT_FC_ENABLE,
+	MV_PORT_FC_ACTIVE
+};
+
+struct mv_port_link_status {
+	int			linkup; /*flag*/
+	enum mv_port_speed	speed;
+	enum mv_port_duplex	duplex;
+	enum mv_port_fc		rx_fc;
+	enum mv_port_fc		tx_fc;
+};
+
+/* different loopback types can be configure on different levels:
+ * MAC, PCS, SERDES
+ */
+enum mv_lb_type {
+	MV_DISABLE_LB,
+	MV_RX_2_TX_LB,
+	MV_TX_2_RX_LB,         /* on SERDES level - analog loopback */
+	MV_TX_2_RX_DIGITAL_LB  /* on SERDES level - digital loopback */
+};
+
+enum sd_media_mode {MV_RXAUI, MV_XAUI};
+
+/* Net Complex */
+enum mv_netc_topology {
+	MV_NETC_GE_MAC0_RXAUI_L23	=	BIT(0),
+	MV_NETC_GE_MAC0_RXAUI_L45	=	BIT(1),
+	MV_NETC_GE_MAC0_XAUI		=	BIT(2),
+	MV_NETC_GE_MAC2_SGMII		=	BIT(3),
+	MV_NETC_GE_MAC3_SGMII		=	BIT(4),
+	MV_NETC_GE_MAC3_RGMII		=	BIT(5),
+};
+
+enum mv_netc_phase {
+	MV_NETC_FIRST_PHASE,
+	MV_NETC_SECOND_PHASE,
+};
+
+enum mv_netc_sgmii_xmi_mode {
+	MV_NETC_GBE_SGMII,
+	MV_NETC_GBE_XMII,
+};
+
+enum mv_netc_mii_mode {
+	MV_NETC_GBE_RGMII,
+	MV_NETC_GBE_MII,
+};
+
+enum mv_netc_lanes {
+	MV_NETC_LANE_23,
+	MV_NETC_LANE_45,
+};
+
+/* pp3_gop_ctrl flags */
+#define mv_gop_F_DEBUG_BIT		0
+#define mv_gop_F_ATTACH_BIT		1
+
+#define mv_gop_F_DEBUG		(1 << mv_gop_F_DEBUG_BIT)
+#define mv_gop_F_ATTACH		(1 << mv_gop_F_ATTACH_BIT)
+
+enum gop_port_flags {NOT_CREATED, CREATED, UNDER_RESET, ENABLED};
+
+struct gop_port_ctrl {
+	u32  flags;
+};
+
+#define MV_RGMII_TX_FIFO_MIN_TH		(0x41)
+#define MV_SGMII_TX_FIFO_MIN_TH		(0x5)
+#define MV_SGMII2_5_TX_FIFO_MIN_TH	(0xB)
+
+static inline u32 mv_gop_gen_read(void __iomem *base, u32 offset)
+{
+	void *reg_ptr = base + offset;
+	u32 val;
+
+	val = readl(reg_ptr);
+	return val;
+}
+
+static inline void mv_gop_gen_write(void __iomem *base, u32 offset, u32 data)
+{
+	void *reg_ptr = base + offset;
+
+	writel(data, reg_ptr);
+}
+
+/* GOP port configuration functions */
+int mv_gop110_port_init(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_port_reset(struct gop_hw *gop, struct mv_mac_data *mac);
+void mv_gop110_port_enable(struct gop_hw *gop, struct mv_mac_data *mac);
+void mv_gop110_port_disable(struct gop_hw *gop, struct mv_mac_data *mac);
+void mv_gop110_port_periodic_xon_set(struct gop_hw *gop,
+				     struct mv_mac_data *mac,
+				     int enable);
+bool mv_gop110_port_is_link_up(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_port_link_status(struct gop_hw *gop, struct mv_mac_data *mac,
+			       struct mv_port_link_status *pstatus);
+int mv_gop110_port_regs(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_port_events_mask(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_port_events_unmask(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_port_events_clear(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_status_show(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_speed_duplex_get(struct gop_hw *gop, struct mv_mac_data *mac,
+			       enum mv_port_speed *speed,
+			       enum mv_port_duplex *duplex);
+int mv_gop110_speed_duplex_set(struct gop_hw *gop, struct mv_mac_data *mac,
+			       enum mv_port_speed speed,
+			       enum mv_port_duplex duplex);
+int mv_gop110_autoneg_restart(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_fl_cfg(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_force_link_mode_set(struct gop_hw *gop, struct mv_mac_data *mac,
+				  bool force_link_up,
+				  bool force_link_down);
+int mv_gop110_force_link_mode_get(struct gop_hw *gop, struct mv_mac_data *mac,
+				  bool *force_link_up,
+				  bool *force_link_down);
+int mv_gop110_loopback_set(struct gop_hw *gop, struct mv_mac_data *mac,
+			   bool lb);
+void mv_gop_reg_print(char *reg_name, u32 reg);
+
+/* Gig PCS Functions */
+int mv_gop110_gpcs_mode_cfg(struct gop_hw *gop, int pcs_num, bool en);
+int mv_gop110_gpcs_reset(struct gop_hw *gop, int pcs_num, enum mv_reset act);
+
+/* Serdes Functions */
+static inline u32 mv_gop110_serdes_read(struct gop_hw *gop, int lane_num,
+					u32 offset)
+{
+	return(mv_gop_gen_read(gop->gop_110.serdes.base,
+		lane_num * gop->gop_110.serdes.obj_size + offset));
+}
+
+static inline void mv_gop110_serdes_write(struct gop_hw *gop, int lane_num,
+					  u32 offset, u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.serdes.base,
+			 lane_num * gop->gop_110.serdes.obj_size +
+			 offset, data);
+}
+
+static inline void mv_gop110_serdes_print(struct gop_hw *gop, char *reg_name,
+					  int lane_num, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_serdes_read(gop, lane_num, reg));
+}
+
+void mv_gop110_serdes_lane_regs_dump(struct gop_hw *gop, int lane);
+void mv_gop110_serdes_init(struct gop_hw *gop, int lane,
+			   enum sd_media_mode mode);
+void mv_gop110_serdes_reset(struct gop_hw *gop, int lane, bool analog_reset,
+			    bool core_reset, bool digital_reset);
+
+/* XPCS Functions */
+
+static inline u32 mv_gop110_xpcs_global_read(struct gop_hw *gop, u32 offset)
+{
+	return mv_gop_gen_read(gop->gop_110.xpcs_base, offset);
+}
+
+static inline void mv_gop110_xpcs_global_write(struct gop_hw *gop, u32 offset,
+					       u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.xpcs_base, offset, data);
+}
+
+static inline void mv_gop110_xpcs_global_print(struct gop_hw *gop,
+					       char *reg_name, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_xpcs_global_read(gop, reg));
+}
+
+static inline u32 mv_gop110_xpcs_lane_read(struct gop_hw *gop, int lane_num,
+					   u32 offset)
+{
+	return mv_gop_gen_read(gop->gop_110.xpcs_base, offset);
+}
+
+static inline void mv_gop110_xpcs_lane_write(struct gop_hw *gop, int lane_num,
+					     u32 offset, u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.xpcs_base, offset, data);
+}
+
+static inline void mv_gop110_xpcs_lane_print(struct gop_hw *gop,
+					     char *reg_name,
+					     int lane_num, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_xpcs_lane_read(gop, lane_num, reg));
+}
+
+void mv_gop110_xpcs_gl_regs_dump(struct gop_hw *gop);
+void mv_gop110_xpcs_lane_regs_dump(struct gop_hw *gop, int lane);
+int mv_gop110_xpcs_reset(struct gop_hw *gop, enum mv_reset reset);
+int mv_gop110_xpcs_mode(struct gop_hw *gop, int num_of_lanes);
+
+/* XLG MAC Functions */
+static inline u32 mv_gop110_xlg_mac_read(struct gop_hw *gop, int mac_num,
+					 u32 offset)
+{
+	return(mv_gop_gen_read(gop->gop_110.xlg_mac.base,
+	       mac_num * gop->gop_110.xlg_mac.obj_size + offset));
+}
+
+static inline void mv_gop110_xlg_mac_write(struct gop_hw *gop, int mac_num,
+					   u32 offset, u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.xlg_mac.base,
+		mac_num * gop->gop_110.xlg_mac.obj_size + offset, data);
+}
+
+static inline void mv_gop110_xlg_mac_print(struct gop_hw *gop, char *reg_name,
+					   int mac_num, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_xlg_mac_read(gop, mac_num, reg));
+}
+
+/* MIB MAC Functions */
+static inline u32 mv_gop110_xmib_mac_read(struct gop_hw *gop, int mac_num,
+					  u32 offset)
+{
+	return(mv_gop_gen_read(gop->gop_110.xmib.base,
+	       mac_num * gop->gop_110.xmib.obj_size + offset));
+}
+
+static inline void mv_gop110_xmib_mac_write(struct gop_hw *gop, int mac_num,
+					    u32 offset, u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.xmib.base,
+			 mac_num * gop->gop_110.xmib.obj_size + offset, data);
+}
+
+static inline void mv_gop110_xmib_mac_print(struct gop_hw *gop, char *reg_name,
+					    int mac_num, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_xmib_mac_read(gop, mac_num, reg));
+}
+
+void mv_gop110_xlg_mac_regs_dump(struct gop_hw *gop, int port);
+int mv_gop110_xlg_mac_reset(struct gop_hw *gop, int mac_num,
+			    enum mv_reset reset);
+int mv_gop110_xlg_mac_mode_cfg(struct gop_hw *gop, int mac_num,
+			       int num_of_act_lanes);
+int mv_gop110_xlg_mac_loopback_cfg(struct gop_hw *gop, int mac_num,
+				   enum mv_lb_type type);
+
+bool mv_gop110_xlg_mac_link_status_get(struct gop_hw *gop, int mac_num);
+void mv_gop110_xlg_mac_port_enable(struct gop_hw *gop, int mac_num);
+void mv_gop110_xlg_mac_port_disable(struct gop_hw *gop, int mac_num);
+void mv_gop110_xlg_mac_port_periodic_xon_set(struct gop_hw *gop,
+					     int mac_num,
+					     int enable);
+int mv_gop110_xlg_mac_link_status(struct gop_hw *gop, int mac_num,
+				  struct mv_port_link_status *pstatus);
+int mv_gop110_xlg_mac_max_rx_size_set(struct gop_hw *gop, int mac_num,
+				      int max_rx_size);
+int mv_gop110_xlg_mac_force_link_mode_set(struct gop_hw *gop, int mac_num,
+					  bool force_link_up,
+					  bool force_link_down);
+int mv_gop110_xlg_mac_speed_duplex_set(struct gop_hw *gop, int mac_num,
+				       enum mv_port_speed speed,
+				       enum mv_port_duplex duplex);
+int mv_gop110_xlg_mac_speed_duplex_get(struct gop_hw *gop, int mac_num,
+				       enum mv_port_speed *speed,
+				       enum mv_port_duplex *duplex);
+int mv_gop110_xlg_mac_fc_set(struct gop_hw *gop, int mac_num,
+			     enum mv_port_fc fc);
+void mv_gop110_xlg_mac_fc_get(struct gop_hw *gop, int mac_num,
+			      enum mv_port_fc *fc);
+int mv_gop110_xlg_mac_port_link_speed_fc(struct gop_hw *gop, int mac_num,
+					 enum mv_port_speed speed,
+					 int force_link_up);
+void mv_gop110_xlg_port_link_event_mask(struct gop_hw *gop, int mac_num);
+void mv_gop110_xlg_port_external_event_unmask(struct gop_hw *gop,
+					      int mac_num,
+					      int bit_2_open);
+void mv_gop110_xlg_port_link_event_clear(struct gop_hw *gop, int mac_num);
+void mv_gop110_xlg_2_gig_mac_cfg(struct gop_hw *gop, int mac_num);
+
+/* GMAC Functions  */
+static inline u32 mv_gop110_gmac_read(struct gop_hw *gop, int mac_num,
+				      u32 offset)
+{
+	return(mv_gop_gen_read(gop->gop_110.gmac.base,
+	       mac_num * gop->gop_110.gmac.obj_size + offset));
+}
+
+static inline void mv_gop110_gmac_write(struct gop_hw *gop, int mac_num,
+					u32 offset, u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.gmac.base,
+			 mac_num * gop->gop_110.gmac.obj_size + offset, data);
+}
+
+static inline void mv_gop110_gmac_print(struct gop_hw *gop, char *reg_name,
+					int mac_num, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_gmac_read(gop, mac_num, reg));
+}
+
+void mv_gop110_register_bases_dump(struct gop_hw *gop);
+void mv_gop110_gmac_regs_dump(struct gop_hw *gop, int port);
+int mv_gop110_gmac_reset(struct gop_hw *gop, int mac_num,
+			 enum mv_reset reset);
+int mv_gop110_gmac_mode_cfg(struct gop_hw *gop, struct mv_mac_data *mac);
+int mv_gop110_gmac_loopback_cfg(struct gop_hw *gop, int mac_num,
+				enum mv_lb_type type);
+bool mv_gop110_gmac_link_status_get(struct gop_hw *gop, int mac_num);
+void mv_gop110_gmac_port_enable(struct gop_hw *gop, int mac_num);
+void mv_gop110_gmac_port_disable(struct gop_hw *gop, int mac_num);
+void mv_gop110_gmac_port_periodic_xon_set(struct gop_hw *gop, int mac_num,
+					  int enable);
+int mv_gop110_gmac_link_status(struct gop_hw *gop, int mac_num,
+			       struct mv_port_link_status *pstatus);
+int mv_gop110_gmac_max_rx_size_set(struct gop_hw *gop, int mac_num,
+				   int max_rx_size);
+int mv_gop110_gmac_force_link_mode_set(struct gop_hw *gop, int mac_num,
+				       bool force_link_up,
+				       bool force_link_down);
+int mv_gop110_gmac_force_link_mode_get(struct gop_hw *gop, int mac_num,
+				       bool *force_link_up,
+				       bool *force_link_down);
+int mv_gop110_gmac_speed_duplex_set(struct gop_hw *gop, int mac_num,
+				    enum mv_port_speed speed,
+				    enum mv_port_duplex duplex);
+int mv_gop110_gmac_speed_duplex_get(struct gop_hw *gop, int mac_num,
+				    enum mv_port_speed *speed,
+				    enum mv_port_duplex *duplex);
+int mv_gop110_gmac_fc_set(struct gop_hw *gop, int mac_num,
+			  enum mv_port_fc fc);
+void mv_gop110_gmac_fc_get(struct gop_hw *gop, int mac_num,
+			   enum mv_port_fc *fc);
+int mv_gop110_gmac_port_link_speed_fc(struct gop_hw *gop, int mac_num,
+				      enum mv_port_speed speed,
+				      int force_link_up);
+void mv_gop110_gmac_port_link_event_mask(struct gop_hw *gop, int mac_num);
+void mv_gop110_gmac_port_link_event_unmask(struct gop_hw *gop, int mac_num);
+void mv_gop110_gmac_port_link_event_clear(struct gop_hw *gop, int mac_num);
+int mv_gop110_gmac_port_autoneg_restart(struct gop_hw *gop, int mac_num);
+
+/* SMI Functions  */
+static inline u32 mv_gop110_smi_read(struct gop_hw *gop, u32 offset)
+{
+	return mv_gop_gen_read(gop->gop_110.smi_base, offset);
+}
+
+static inline void mv_gop110_smi_write(struct gop_hw *gop, u32 offset,
+				       u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.smi_base, offset, data);
+}
+
+static inline void mv_gop110_smi_print(struct gop_hw *gop, char *reg_name,
+				       u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_smi_read(gop, reg));
+}
+
+/* RFU1 Functions  */
+static inline u32 mv_gop110_rfu1_read(struct gop_hw *gop, u32 offset)
+{
+	return mv_gop_gen_read(gop->gop_110.rfu1_base, offset);
+}
+static inline void mv_gop110_rfu1_write(struct gop_hw *gop, u32 offset,
+		u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.rfu1_base, offset, data);
+}
+static inline void mv_gop110_rfu1_print(struct gop_hw *gop, char *reg_name,
+		u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_rfu1_read(gop, reg));
+}
+
+/* PTP Functions  */
+static inline u32 mv_gop110_ptp_read(struct gop_hw *gop, int mac_num,
+				     u32 offset)
+{
+	return mv_gop_gen_read(gop->gop_110.ptp.base,
+			       mac_num * gop->gop_110.ptp.obj_size + offset);
+}
+
+static inline void mv_gop110_ptp_write(struct gop_hw *gop, int mac_num,
+				       u32 offset, u32 data)
+{
+	mv_gop_gen_write(gop->gop_110.ptp.base,
+			 mac_num * gop->gop_110.ptp.obj_size + offset, data);
+}
+
+static inline void mv_gop110_ptp_print(struct gop_hw *gop, char *reg_name,
+				       int mac_num, u32 reg)
+{
+	pr_info("  %-32s: 0x%x = 0x%08x\n", reg_name, reg,
+		mv_gop110_ptp_read(gop, mac_num, reg));
+}
+
+int mv_gop110_smi_init(struct gop_hw *gop);
+int mv_gop110_smi_phy_addr_cfg(struct gop_hw *gop, int port, int addr);
+
+/* MIB Functions  */
+u64 mv_gop110_mib_read64(struct gop_hw *gop, int port, unsigned int offset);
+void mv_gop110_mib_counters_show(struct gop_hw *gop, int port);
+
+/* PTP Functions */
+void mv_gop110_ptp_enable(struct gop_hw *gop, int port, bool state);
+
+/*RFU Functions */
+int mv_gop110_netc_init(struct gop_hw *gop,
+			u32 net_comp_config, enum mv_netc_phase phase);
+void mv_gop110_netc_active_port(struct gop_hw *gop, u32 port, u32 val);
+
+#endif /* _MV_GOP_HW_H_ */
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw_type.h b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw_type.h
new file mode 100644
index 0000000..68770ef
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_gop110_hw_type.h
@@ -0,0 +1,2179 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifndef _MV_GOP_HW_TYPE_H_
+#define _MV_GOP_HW_TYPE_H_
+
+#include <linux/kernel.h>
+#include <linux/skbuff.h>
+#include <linux/bitops.h>
+
+#define MVCPN110_GOP_MAC_NUM                4
+
+/***********/
+/*GMAC REGS */
+/***********/
+
+/* Port Mac Control0 */
+#define MV_GMAC_PORT_CTRL0_REG			(0x0000)
+#define MV_GMAC_PORT_CTRL0_PORTEN_OFFS		0
+#define MV_GMAC_PORT_CTRL0_PORTEN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL0_PORTEN_OFFS)
+
+#define MV_GMAC_PORT_CTRL0_PORTTYPE_OFFS		1
+#define MV_GMAC_PORT_CTRL0_PORTTYPE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL0_PORTTYPE_OFFS)
+
+#define MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_OFFS		2
+#define MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_MASK    \
+		(0x00001fff << MV_GMAC_PORT_CTRL0_FRAMESIZELIMIT_OFFS)
+
+#define MV_GMAC_PORT_CTRL0_COUNT_EN_OFFS		15
+#define MV_GMAC_PORT_CTRL0_COUNT_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL0_COUNT_EN_OFFS)
+
+/* Port Mac Control1 */
+#define MV_GMAC_PORT_CTRL1_REG			(0x0004)
+#define MV_GMAC_PORT_CTRL1_EN_RX_CRC_CHECK_OFFS	0
+#define MV_GMAC_PORT_CTRL1_EN_RX_CRC_CHECK_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_EN_RX_CRC_CHECK_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_EN_PERIODIC_FC_XON_OFFS		1
+#define MV_GMAC_PORT_CTRL1_EN_PERIODIC_FC_XON_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_EN_PERIODIC_FC_XON_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_MGMII_MODE_OFFS		2
+#define MV_GMAC_PORT_CTRL1_MGMII_MODE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_MGMII_MODE_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_PFC_CASCADE_PORT_ENABLE_OFFS		3
+#define MV_GMAC_PORT_CTRL1_PFC_CASCADE_PORT_ENABLE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_PFC_CASCADE_PORT_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_DIS_EXCESSIVE_COL_OFFS		4
+#define MV_GMAC_PORT_CTRL1_DIS_EXCESSIVE_COL_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_DIS_EXCESSIVE_COL_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_GMII_LOOPBACK_OFFS		5
+#define MV_GMAC_PORT_CTRL1_GMII_LOOPBACK_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_GMII_LOOPBACK_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_PCS_LOOPBACK_OFFS		6
+#define MV_GMAC_PORT_CTRL1_PCS_LOOPBACK_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_PCS_LOOPBACK_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_FC_SA_ADDR_LO_OFFS		7
+#define MV_GMAC_PORT_CTRL1_FC_SA_ADDR_LO_MASK    \
+		(0x000000ff << MV_GMAC_PORT_CTRL1_FC_SA_ADDR_LO_OFFS)
+
+#define MV_GMAC_PORT_CTRL1_EN_SHORT_PREAMBLE_OFFS		15
+#define MV_GMAC_PORT_CTRL1_EN_SHORT_PREAMBLE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL1_EN_SHORT_PREAMBLE_OFFS)
+
+/* Port Mac Control2 */
+#define MV_GMAC_PORT_CTRL2_REG			(0x0008)
+#define MV_GMAC_PORT_CTRL2_SGMII_MODE_OFFS		0
+#define MV_GMAC_PORT_CTRL2_SGMII_MODE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_SGMII_MODE_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_FC_MODE_OFFS		1
+#define MV_GMAC_PORT_CTRL2_FC_MODE_MASK    \
+		(0x00000003 << MV_GMAC_PORT_CTRL2_FC_MODE_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_PCS_EN_OFFS		3
+#define MV_GMAC_PORT_CTRL2_PCS_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_PCS_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_RGMII_MODE_OFFS		4
+#define MV_GMAC_PORT_CTRL2_RGMII_MODE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_RGMII_MODE_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS		5
+#define MV_GMAC_PORT_CTRL2_DIS_PADING_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_DIS_PADING_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_PORTMACRESET_OFFS		6
+#define MV_GMAC_PORT_CTRL2_PORTMACRESET_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_PORTMACRESET_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_TX_DRAIN_OFFS		7
+#define MV_GMAC_PORT_CTRL2_TX_DRAIN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_TX_DRAIN_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_EN_MII_ODD_PRE_OFFS		8
+#define MV_GMAC_PORT_CTRL2_EN_MII_ODD_PRE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_EN_MII_ODD_PRE_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_CLK_125_BYPS_EN_OFFS		9
+#define MV_GMAC_PORT_CTRL2_CLK_125_BYPS_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_CLK_125_BYPS_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_PRBS_CHECK_EN_OFFS		10
+#define MV_GMAC_PORT_CTRL2_PRBS_CHECK_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_PRBS_CHECK_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_PRBS_GEN_EN_OFFS		11
+#define MV_GMAC_PORT_CTRL2_PRBS_GEN_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_PRBS_GEN_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_SELECT_DATA_TO_TX_OFFS		12
+#define MV_GMAC_PORT_CTRL2_SELECT_DATA_TO_TX_MASK    \
+		(0x00000003 << MV_GMAC_PORT_CTRL2_SELECT_DATA_TO_TX_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_EN_COL_ON_BP_OFFS		14
+#define MV_GMAC_PORT_CTRL2_EN_COL_ON_BP_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_EN_COL_ON_BP_OFFS)
+
+#define MV_GMAC_PORT_CTRL2_EARLY_REJECT_MODE_OFFS		15
+#define MV_GMAC_PORT_CTRL2_EARLY_REJECT_MODE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL2_EARLY_REJECT_MODE_OFFS)
+
+/* Port Auto-negotiation Configuration */
+#define MV_GMAC_PORT_AUTO_NEG_CFG_REG			(0x000c)
+#define MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_DOWN_OFFS		0
+#define MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_DOWN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_DOWN_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_UP_OFFS		1
+#define MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_UP_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_FORCE_LINK_UP_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_OFFS		2
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_EN_PCS_AN_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_OFFS		3
+#define MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_AN_BYPASS_EN_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_INBAND_RESTARTAN_OFFS		4
+#define MV_GMAC_PORT_AUTO_NEG_CFG_INBAND_RESTARTAN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_INBAND_RESTARTAN_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_OFFS		5
+#define MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_SET_MII_SPEED_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_OFFS		6
+#define MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_SET_GMII_SPEED_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_OFFS		7
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_EN_AN_SPEED_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_OFFS		9
+#define MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_ADV_PAUSE_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_OFFS		10
+#define MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_ADV_ASM_PAUSE_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_OFFS			11
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_EN_FC_AN_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_OFFS		12
+#define MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_SET_FULL_DX_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_OFFS		13
+#define MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_EN_FDX_AN_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_PHY_MODE_OFFS		14
+#define MV_GMAC_PORT_AUTO_NEG_CFG_PHY_MODE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_AUTO_NEG_CFG_PHY_MODE_OFFS)
+
+#define MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_OFFS		15
+#define MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_AUTO_NEG_CFG_CHOOSE_SAMPLE_TX_CONFIG_OFFS)
+
+/* Port Status0 */
+#define MV_GMAC_PORT_STATUS0_REG				(0x0010)
+#define MV_GMAC_PORT_STATUS0_LINKUP_OFFS		0
+#define MV_GMAC_PORT_STATUS0_LINKUP_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_LINKUP_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_GMIISPEED_OFFS		1
+#define MV_GMAC_PORT_STATUS0_GMIISPEED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_GMIISPEED_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_MIISPEED_OFFS		2
+#define MV_GMAC_PORT_STATUS0_MIISPEED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_MIISPEED_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_FULLDX_OFFS		3
+#define MV_GMAC_PORT_STATUS0_FULLDX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_FULLDX_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_RXFCEN_OFFS		4
+#define MV_GMAC_PORT_STATUS0_RXFCEN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_RXFCEN_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_TXFCEN_OFFS		5
+#define MV_GMAC_PORT_STATUS0_TXFCEN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_TXFCEN_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_PORTRXPAUSE_OFFS		6
+#define MV_GMAC_PORT_STATUS0_PORTRXPAUSE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_PORTRXPAUSE_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_PORTTXPAUSE_OFFS		7
+#define MV_GMAC_PORT_STATUS0_PORTTXPAUSE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_PORTTXPAUSE_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_PORTIS_DOINGPRESSURE_OFFS		8
+#define MV_GMAC_PORT_STATUS0_PORTIS_DOINGPRESSURE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_PORTIS_DOINGPRESSURE_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_PORTBUFFULL_OFFS		9
+#define MV_GMAC_PORT_STATUS0_PORTBUFFULL_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_PORTBUFFULL_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_SYNCFAIL10MS_OFFS		10
+#define MV_GMAC_PORT_STATUS0_SYNCFAIL10MS_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_SYNCFAIL10MS_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_ANDONE_OFFS		11
+#define MV_GMAC_PORT_STATUS0_ANDONE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_ANDONE_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_INBAND_AUTONEG_BYPASSACT_OFFS		12
+#define MV_GMAC_PORT_STATUS0_INBAND_AUTONEG_BYPASSACT_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_STATUS0_INBAND_AUTONEG_BYPASSACT_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_SERDESPLL_LOCKED_OFFS		13
+#define MV_GMAC_PORT_STATUS0_SERDESPLL_LOCKED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_SERDESPLL_LOCKED_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_SYNCOK_OFFS		14
+#define MV_GMAC_PORT_STATUS0_SYNCOK_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_SYNCOK_OFFS)
+
+#define MV_GMAC_PORT_STATUS0_SQUELCHNOT_DETECTED_OFFS		15
+#define MV_GMAC_PORT_STATUS0_SQUELCHNOT_DETECTED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS0_SQUELCHNOT_DETECTED_OFFS)
+
+/* Port Serial Parameters Configuration */
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_REG			(0x0014)
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_UNIDIRECTIONAL_ENABLE_OFFS	0
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_UNIDIRECTIONAL_ENABLE_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_UNIDIRECTIONAL_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_RETRANSMIT_COLLISION_DOMAIN_OFFS	1
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_RETRANSMIT_COLLISION_DOMAIN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_RETRANSMIT_COLLISION_DOMAIN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_PUMA2_BTS1444_EN_OFFS		2
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_PUMA2_BTS1444_EN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_PUMA2_BTS1444_EN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_FORWARD_802_3X_FC_EN_OFFS		3
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_FORWARD_802_3X_FC_EN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_FORWARD_802_3X_FC_EN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_BP_EN_OFFS		4
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_BP_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERIAL_PARAM_CFG_BP_EN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_RX_NEGEDGE_SAMPLE_EN_OFFS		5
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_RX_NEGEDGE_SAMPLE_EN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_RX_NEGEDGE_SAMPLE_EN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_COL_DOMAIN_LIMIT_OFFS		6
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_COL_DOMAIN_LIMIT_MASK    \
+		(0x0000003f << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_COL_DOMAIN_LIMIT_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_PERIODIC_TYPE_SELECT_OFFS		12
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_PERIODIC_TYPE_SELECT_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_PERIODIC_TYPE_SELECT_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_PER_PRIORITY_FC_EN_OFFS		13
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_PER_PRIORITY_FC_EN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_PER_PRIORITY_FC_EN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_TX_STANDARD_PRBS7_OFFS		14
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_TX_STANDARD_PRBS7_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_TX_STANDARD_PRBS7_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_REVERSE_PRBS_RX_OFFS		15
+#define MV_GMAC_PORT_SERIAL_PARAM_CFG_REVERSE_PRBS_RX_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_CFG_REVERSE_PRBS_RX_OFFS)
+
+/* Port Fifo Configuration 0 */
+#define MV_GMAC_PORT_FIFO_CFG_0_REG				(0x0018)
+#define MV_GMAC_PORT_FIFO_CFG_0_TX_FIFO_HIGH_WM_OFFS		0
+#define MV_GMAC_PORT_FIFO_CFG_0_TX_FIFO_HIGH_WM_MASK    \
+		(0x000000ff << \
+		MV_GMAC_PORT_FIFO_CFG_0_TX_FIFO_HIGH_WM_OFFS)
+
+#define MV_GMAC_PORT_FIFO_CFG_0_TX_FIFO_LOW_WM_OFFS		8
+#define MV_GMAC_PORT_FIFO_CFG_0_TX_FIFO_LOW_WM_MASK    \
+		(0x000000ff << \
+		MV_GMAC_PORT_FIFO_CFG_0_TX_FIFO_LOW_WM_OFFS)
+
+/* Port Fifo Configuration 1 */
+#define MV_GMAC_PORT_FIFO_CFG_1_REG				(0x001c)
+#define MV_GMAC_PORT_FIFO_CFG_1_RX_FIFO_MAX_TH_OFFS		0
+#define MV_GMAC_PORT_FIFO_CFG_1_RX_FIFO_MAX_TH_MASK    \
+		(0x0000003f << MV_GMAC_PORT_FIFO_CFG_1_RX_FIFO_MAX_TH_OFFS)
+
+#define MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS		6
+#define MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_MASK    \
+		(0x000000ff << MV_GMAC_PORT_FIFO_CFG_1_TX_FIFO_MIN_TH_OFFS)
+
+#define MV_GMAC_PORT_FIFO_CFG_1_PORT_EN_FIX_EN_OFFS		15
+#define MV_GMAC_PORT_FIFO_CFG_1_PORT_EN_FIX_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_FIFO_CFG_1_PORT_EN_FIX_EN_OFFS)
+
+/* Port Serdes Configuration0 */
+#define MV_GMAC_PORT_SERDES_CFG0_REG				(0x0028)
+#define MV_GMAC_PORT_SERDES_CFG0_SERDESRESET_OFFS		0
+#define MV_GMAC_PORT_SERDES_CFG0_SERDESRESET_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_SERDESRESET_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_PU_TX_OFFS		1
+#define MV_GMAC_PORT_SERDES_CFG0_PU_TX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_PU_TX_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_PU_RX_OFFS		2
+#define MV_GMAC_PORT_SERDES_CFG0_PU_RX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_PU_RX_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_PU_PLL_OFFS		3
+#define MV_GMAC_PORT_SERDES_CFG0_PU_PLL_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_PU_PLL_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_PU_IVREF_OFFS		4
+#define MV_GMAC_PORT_SERDES_CFG0_PU_IVREF_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_PU_IVREF_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_TESTEN_OFFS		5
+#define MV_GMAC_PORT_SERDES_CFG0_TESTEN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_TESTEN_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_DPHER_EN_OFFS		6
+#define MV_GMAC_PORT_SERDES_CFG0_DPHER_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_DPHER_EN_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_RUDI_INVALID_ENABLE_OFFS		7
+#define MV_GMAC_PORT_SERDES_CFG0_RUDI_INVALID_ENABLE_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERDES_CFG0_RUDI_INVALID_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_ACK_OVERRIDE_ENABLE_OFFS		8
+#define MV_GMAC_PORT_SERDES_CFG0_ACK_OVERRIDE_ENABLE_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERDES_CFG0_ACK_OVERRIDE_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_CONFIG_WORD_ENABLE_OFFS		9
+#define MV_GMAC_PORT_SERDES_CFG0_CONFIG_WORD_ENABLE_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERDES_CFG0_CONFIG_WORD_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_SYNC_FAIL_INT_ENABLE_OFFS		10
+#define MV_GMAC_PORT_SERDES_CFG0_SYNC_FAIL_INT_ENABLE_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERDES_CFG0_SYNC_FAIL_INT_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_MASTER_MODE_ENABLE_OFFS		11
+#define MV_GMAC_PORT_SERDES_CFG0_MASTER_MODE_ENABLE_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERDES_CFG0_MASTER_MODE_ENABLE_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_TERM75_TX_OFFS		12
+#define MV_GMAC_PORT_SERDES_CFG0_TERM75_TX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_TERM75_TX_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_OUTAMP_OFFS		13
+#define MV_GMAC_PORT_SERDES_CFG0_OUTAMP_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_OUTAMP_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_BTS712_FIX_EN_OFFS		14
+#define MV_GMAC_PORT_SERDES_CFG0_BTS712_FIX_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_BTS712_FIX_EN_OFFS)
+
+#define MV_GMAC_PORT_SERDES_CFG0_BTS156_FIX_EN_OFFS		15
+#define MV_GMAC_PORT_SERDES_CFG0_BTS156_FIX_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_SERDES_CFG0_BTS156_FIX_EN_OFFS)
+
+/* Port Serdes Configuration1 */
+#define MV_GMAC_PORT_SERDES_CFG1_REG			(0x002c)
+#define MV_GMAC_PORT_SERDES_CFG1_SMII_RX_10MB_CLK_EDGE_SEL_OFFS	0
+#define MV_GMAC_PORT_SERDES_CFG1_SMII_RX_10MB_CLK_EDGE_SEL_MASK    \
+		(0x00000001 << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_SMII_RX_10MB_CLK_EDGE_SEL_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_SMII_TX_10MB_CLK_EDGE_SEL_OFFS	1
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_SMII_TX_10MB_CLK_EDGE_SEL_MASK    \
+		(0x00000001 << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_SMII_TX_10MB_CLK_EDGE_SEL_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_MEN_OFFS		2
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_MEN_MASK    \
+		(0x00000003 << MV_GMAC_GMAC_PORT_SERDES_CFG1_MEN_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_VCMS_OFFS		4
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_VCMS_MASK    \
+		(0x00000001 << MV_GMAC_GMAC_PORT_SERDES_CFG1_VCMS_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_USE_SIGDET_OFFS		5
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_USE_SIGDET_MASK    \
+		(0x00000001 << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_USE_SIGDET_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_EN_CRS_MASK_TX_OFFS		6
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_EN_CRS_MASK_TX_MASK    \
+		(0x00000001 << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_EN_CRS_MASK_TX_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_ENABLE_OFFS		7
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_ENABLE_MASK    \
+		(0x00000001 << MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_ENABLE_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_PHY_ADDRESS_OFFS	8
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_PHY_ADDRESS_MASK    \
+		(0x0000001f << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_PHY_ADDRESS_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_SIGDET_POLARITY_OFFS	13
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_SIGDET_POLARITY_MASK    \
+		(0x00000001 << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_SIGDET_POLARITY_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_INTERRUPT_POLARITY_OFFS	14
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_INTERRUPT_POLARITY_MASK    \
+		(0x00000001 << \
+	MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_INTERRUPT_POLARITY_OFFS)
+
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_SERDES_POLARITY_OFFS	15
+#define MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_SERDES_POLARITY_MASK    \
+		(0x00000001 << \
+		MV_GMAC_GMAC_PORT_SERDES_CFG1_100FX_PCS_SERDES_POLARITY_OFFS)
+
+/* Port Serdes Configuration2 */
+#define MV_GMAC_PORT_SERDES_CFG2_REG				(0x0030)
+#define MV_GMAC_PORT_SERDES_CFG2_AN_ADV_CONFIGURATION_OFFS	0
+#define MV_GMAC_PORT_SERDES_CFG2_AN_ADV_CONFIGURATION_MASK    \
+		(0x0000ffff << \
+		MV_GMAC_PORT_SERDES_CFG2_AN_ADV_CONFIGURATION_OFFS)
+
+/* Port Serdes Configuration3 */
+#define MV_GMAC_PORT_SERDES_CFG3_REG				(0x0034)
+#define MV_GMAC_PORT_SERDES_CFG3_ABILITY_MATCH_STATUS_OFFS		0
+#define MV_GMAC_PORT_SERDES_CFG3_ABILITY_MATCH_STATUS_MASK    \
+		(0x0000ffff << \
+		MV_GMAC_PORT_SERDES_CFG3_ABILITY_MATCH_STATUS_OFFS)
+
+/* Port Prbs Status */
+#define MV_GMAC_PORT_PRBS_STATUS_REG				(0x0038)
+#define MV_GMAC_PORT_PRBS_STATUS_PRBSCHECK_LOCKED_OFFS		0
+#define MV_GMAC_PORT_PRBS_STATUS_PRBSCHECK_LOCKED_MASK    \
+		(0x00000001 << MV_GMAC_PORT_PRBS_STATUS_PRBSCHECK_LOCKED_OFFS)
+
+#define MV_GMAC_PORT_PRBS_STATUS_PRBSCHECKRDY_OFFS		1
+#define MV_GMAC_PORT_PRBS_STATUS_PRBSCHECKRDY_MASK    \
+		(0x00000001 << MV_GMAC_PORT_PRBS_STATUS_PRBSCHECKRDY_OFFS)
+
+/* Port Prbs Error Counter */
+#define MV_GMAC_PORT_PRBS_ERR_CNTR_REG				(0x003c)
+#define MV_GMAC_PORT_PRBS_ERR_CNTR_PRBSBITERRCNT_OFFS		0
+#define MV_GMAC_PORT_PRBS_ERR_CNTR_PRBSBITERRCNT_MASK    \
+		(0x0000ffff << MV_GMAC_PORT_PRBS_ERR_CNTR_PRBSBITERRCNT_OFFS)
+
+/* Port Status1 */
+#define MV_GMAC_PORT_STATUS1_REG				(0x0040)
+#define MV_GMAC_PORT_STATUS1_MEDIAACTIVE_OFFS		0
+#define MV_GMAC_PORT_STATUS1_MEDIAACTIVE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_STATUS1_MEDIAACTIVE_OFFS)
+
+/* Port Mib Counters Control */
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_REG			(0x0044)
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_COPY_TRIGGER_OFFS	0
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_COPY_TRIGGER_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_COPY_TRIGGER_OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_CLEAR_ON_READ__OFFS		1
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_CLEAR_ON_READ__MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_CLEAR_ON_READ__OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_RX_HISTOGRAM_EN_OFFS		2
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_RX_HISTOGRAM_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_MIB_CNTRS_CTRL_RX_HISTOGRAM_EN_OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_TX_HISTOGRAM_EN_OFFS		3
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_TX_HISTOGRAM_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_MIB_CNTRS_CTRL_TX_HISTOGRAM_EN_OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MFA1_BTT940_FIX_ENABLE__OFFS	4
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MFA1_BTT940_FIX_ENABLE__MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_MIB_CNTRS_CTRL_MFA1_BTT940_FIX_ENABLE__OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_XCAT_BTS_340_EN__OFFS		5
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_XCAT_BTS_340_EN__MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_MIB_CNTRS_CTRL_XCAT_BTS_340_EN__OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_4_COUNT_HIST_OFFS		6
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_4_COUNT_HIST_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_4_COUNT_HIST_OFFS)
+
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_4_LIMIT_1518_1522_OFFS		7
+#define MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_4_LIMIT_1518_1522_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_MIB_CNTRS_CTRL_MIB_4_LIMIT_1518_1522_OFFS)
+
+/* Port Mac Control3 */
+#define MV_GMAC_PORT_CTRL3_REG				(0x0048)
+#define MV_GMAC_PORT_CTRL3_BUF_SIZE_OFFS		0
+#define MV_GMAC_PORT_CTRL3_BUF_SIZE_MASK    \
+		(0x0000003f << MV_GMAC_PORT_CTRL3_BUF_SIZE_OFFS)
+
+#define MV_GMAC_PORT_CTRL3_IPG_DATA_OFFS		6
+#define MV_GMAC_PORT_CTRL3_IPG_DATA_MASK    \
+		(0x000001ff << MV_GMAC_PORT_CTRL3_IPG_DATA_OFFS)
+
+#define MV_GMAC_PORT_CTRL3_LLFC_GLOBAL_FC_ENABLE_OFFS		15
+#define MV_GMAC_PORT_CTRL3_LLFC_GLOBAL_FC_ENABLE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL3_LLFC_GLOBAL_FC_ENABLE_OFFS)
+
+/* QSGMII */
+#define MV_GMAC_QSGMII_REG				(0x004c)
+#define MV_GMAC_QSGMII_QSGMII_REG_OFFS		0
+#define MV_GMAC_QSGMII_QSGMII_REG_MASK    \
+		(0x0000ffff << MV_GMAC_QSGMII_QSGMII_REG_OFFS)
+
+/* Qsgmii Status */
+#define MV_GMAC_QSGMII_STATUS_REG				(0x0050)
+#define MV_GMAC_QSGMII_STATUS_QSGMII_STATUS_OFFS		0
+#define MV_GMAC_QSGMII_STATUS_QSGMII_STATUS_MASK    \
+		(0x000000ff << MV_GMAC_QSGMII_STATUS_QSGMII_STATUS_OFFS)
+
+/* Qsgmii Prbs Counter */
+#define MV_GMAC_QSGMII_PRBS_CNTR_REG			(0x0054)
+#define MV_GMAC_QSGMII_PRBS_CNTR_QSGMII_PRBS_ERR_CNT_REG_OFFS		0
+#define MV_GMAC_QSGMII_PRBS_CNTR_QSGMII_PRBS_ERR_CNT_REG_MASK    \
+		(0x0000ffff << \
+		MV_GMAC_QSGMII_PRBS_CNTR_QSGMII_PRBS_ERR_CNT_REG_OFFS)
+
+/* Ccfc Port Speed Timer%p */
+#define MV_GMAC_CCFC_PORT_SPEED_TIMER_REG(t)		(0x0058 + t * 4)
+#define MV_GMAC_CCFC_PORT_SPEED_TIMER_PORTSPEEDTIMER_OFFS		0
+#define MV_GMAC_CCFC_PORT_SPEED_TIMER_PORTSPEEDTIMER_MASK    \
+		(0x0000ffff << \
+		MV_GMAC_CCFC_PORT_SPEED_TIMER_PORTSPEEDTIMER_OFFS)
+
+/* Fc Dsa Tag %n */
+#define MV_GMAC_FC_DSA_TAG_REG(n)			(0x0078 + 4 * n)
+#define MV_GMAC_FC_DSA_TAG_DSATAGREGN_OFFS		0
+#define MV_GMAC_FC_DSA_TAG_DSATAGREGN_MASK    \
+		(0x0000ffff << MV_GMAC_FC_DSA_TAG_DSATAGREGN_OFFS)
+
+/* Link Level Flow Control Window Reg 0 */
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_0		(0x0088)
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_0_LLFC_FC_WINDOW_REG0_OFFS 0
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_0_LLFC_FC_WINDOW_REG0_MASK    \
+		(0x0000ffff << \
+	MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_0_LLFC_FC_WINDOW_REG0_OFFS)
+
+/* Link Level Flow Control Window Reg 1 */
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1		(0x008c)
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1_LLFC_FC_WINDOW_REG1_OFFS 0
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1_LLFC_FC_WINDOW_REG1_MASK    \
+		(0x00007fff << \
+	MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1_LLFC_FC_WINDOW_REG1_OFFS)
+
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1_LLFC_RATE_LIMIT_EN_OFFS 15
+#define MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1_LLFC_RATE_LIMIT_EN_MASK    \
+		(0x00000001 << \
+	MV_GMAC_LINK_LEVEL_FLOW_CTRL_WINDOW_REG_1_LLFC_RATE_LIMIT_EN_OFFS)
+
+/* Port Mac Control4 */
+#define MV_GMAC_PORT_CTRL4_REG				(0x0090)
+#define MV_GMAC_PORT_CTRL4_EXT_PIN_GMII_SEL_OFFS		0
+#define MV_GMAC_PORT_CTRL4_EXT_PIN_GMII_SEL_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_EXT_PIN_GMII_SEL_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_PREAMBLE_FIX_OFFS		1
+#define MV_GMAC_PORT_CTRL4_PREAMBLE_FIX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_PREAMBLE_FIX_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_SQ_DETECT_FIX_EN_OFFS		2
+#define MV_GMAC_PORT_CTRL4_SQ_DETECT_FIX_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_SQ_DETECT_FIX_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_FC_EN_RX_OFFS		3
+#define MV_GMAC_PORT_CTRL4_FC_EN_RX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_FC_EN_RX_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_FC_EN_TX_OFFS		4
+#define MV_GMAC_PORT_CTRL4_FC_EN_TX_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_FC_EN_TX_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_DP_CLK_SEL_OFFS		5
+#define MV_GMAC_PORT_CTRL4_DP_CLK_SEL_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_DP_CLK_SEL_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_SYNC_BYPASS_OFFS		6
+#define MV_GMAC_PORT_CTRL4_SYNC_BYPASS_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_SYNC_BYPASS_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_OFFS		7
+#define MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_QSGMII_BYPASS_ACTIVE_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_COUNT_EXTERNAL_FC_EN_OFFS		8
+#define MV_GMAC_PORT_CTRL4_COUNT_EXTERNAL_FC_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_COUNT_EXTERNAL_FC_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_MARVELL_HEADER_EN_OFFS		9
+#define MV_GMAC_PORT_CTRL4_MARVELL_HEADER_EN_MASK    \
+		(0x00000001 << MV_GMAC_PORT_CTRL4_MARVELL_HEADER_EN_OFFS)
+
+#define MV_GMAC_PORT_CTRL4_LEDS_NUMBER_OFFS		10
+#define MV_GMAC_PORT_CTRL4_LEDS_NUMBER_MASK    \
+		(0x0000003f << MV_GMAC_PORT_CTRL4_LEDS_NUMBER_OFFS)
+
+/* Port Serial Parameters 1 Configuration */
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_REG			(0x0094)
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_RX_STANDARD_PRBS7_OFFS		0
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_RX_STANDARD_PRBS7_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_1_CFG_RX_STANDARD_PRBS7_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_FORWARD_PFC_EN_OFFS		1
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_FORWARD_PFC_EN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_1_CFG_FORWARD_PFC_EN_OFFS)
+
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_FORWARD_UNKNOWN_FC_EN_OFFS	2
+#define MV_GMAC_PORT_SERIAL_PARAM_1_CFG_FORWARD_UNKNOWN_FC_EN_MASK    \
+		(0x00000001 << \
+		MV_GMAC_PORT_SERIAL_PARAM_1_CFG_FORWARD_UNKNOWN_FC_EN_OFFS)
+
+/* Lpi Control 0 */
+#define MV_GMAC_LPI_CTRL_0_REG				(0x00c0)
+#define MV_GMAC_LPI_CTRL_0_LI_LIMIT_OFFS		0
+#define MV_GMAC_LPI_CTRL_0_LI_LIMIT_MASK    \
+		(0x000000ff << MV_GMAC_LPI_CTRL_0_LI_LIMIT_OFFS)
+
+#define MV_GMAC_LPI_CTRL_0_TS_LIMIT_OFFS		8
+#define MV_GMAC_LPI_CTRL_0_TS_LIMIT_MASK    \
+		(0x000000ff << MV_GMAC_LPI_CTRL_0_TS_LIMIT_OFFS)
+
+/* Lpi Control 1 */
+#define MV_GMAC_LPI_CTRL_1_REG				(0x00c4)
+#define MV_GMAC_LPI_CTRL_1_LPI_REQUEST_EN_OFFS		0
+#define MV_GMAC_LPI_CTRL_1_LPI_REQUEST_EN_MASK    \
+		(0x00000001 << MV_GMAC_LPI_CTRL_1_LPI_REQUEST_EN_OFFS)
+
+#define MV_GMAC_LPI_CTRL_1_LPI_REQUEST_FORCE_OFFS		1
+#define MV_GMAC_LPI_CTRL_1_LPI_REQUEST_FORCE_MASK    \
+		(0x00000001 << MV_GMAC_LPI_CTRL_1_LPI_REQUEST_FORCE_OFFS)
+
+#define MV_GMAC_LPI_CTRL_1_LPI_MANUAL_MODE_OFFS		2
+#define MV_GMAC_LPI_CTRL_1_LPI_MANUAL_MODE_MASK    \
+		(0x00000001 << MV_GMAC_LPI_CTRL_1_LPI_MANUAL_MODE_OFFS)
+
+#define MV_GMAC_LPI_CTRL_1_EN_GTX_CLK_HALT_OFFS		3
+#define MV_GMAC_LPI_CTRL_1_EN_GTX_CLK_HALT_MASK    \
+		(0x00000001 << MV_GMAC_LPI_CTRL_1_EN_GTX_CLK_HALT_OFFS)
+
+#define MV_GMAC_LPI_CTRL_1_TW_LIMIT_OFFS		4
+#define MV_GMAC_LPI_CTRL_1_TW_LIMIT_MASK    \
+		(0x00000fff << MV_GMAC_LPI_CTRL_1_TW_LIMIT_OFFS)
+
+/* Lpi Control 2 */
+#define MV_GMAC_LPI_CTRL_2_REG				(0x00c8)
+#define MV_GMAC_LPI_CTRL_2_LPI_CLK_DIV_OFFS		0
+#define MV_GMAC_LPI_CTRL_2_LPI_CLK_DIV_MASK    \
+		(0x0000007f << MV_GMAC_LPI_CTRL_2_LPI_CLK_DIV_OFFS)
+
+#define MV_GMAC_LPI_CTRL_2_PCS_RX_ER_MASK_DISABLE_OFFS		7
+#define MV_GMAC_LPI_CTRL_2_PCS_RX_ER_MASK_DISABLE_MASK    \
+		(0x00000001 << MV_GMAC_LPI_CTRL_2_PCS_RX_ER_MASK_DISABLE_OFFS)
+
+#define MV_GMAC_LPI_CTRL_2_EN_GMII2MII_LPI_FIX_OFFS		8
+#define MV_GMAC_LPI_CTRL_2_EN_GMII2MII_LPI_FIX_MASK    \
+		(0x00000001 << MV_GMAC_LPI_CTRL_2_EN_GMII2MII_LPI_FIX_OFFS)
+
+/* Lpi Status */
+#define MV_GMAC_LPI_STATUS_REG				(0x00cc)
+#define MV_GMAC_LPI_STATUS_PCS_RX_LPI_STATUS_OFFS		0
+#define MV_GMAC_LPI_STATUS_PCS_RX_LPI_STATUS_MASK    \
+		(0x00000001 << MV_GMAC_LPI_STATUS_PCS_RX_LPI_STATUS_OFFS)
+
+#define MV_GMAC_LPI_STATUS_PCS_TX_LPI_STATUS_OFFS		1
+#define MV_GMAC_LPI_STATUS_PCS_TX_LPI_STATUS_MASK    \
+		(0x00000001 << MV_GMAC_LPI_STATUS_PCS_TX_LPI_STATUS_OFFS)
+
+#define MV_GMAC_LPI_STATUS_MAC_RX_LP_IDLE_STATUS_OFFS		2
+#define MV_GMAC_LPI_STATUS_MAC_RX_LP_IDLE_STATUS_MASK    \
+		(0x00000001 << MV_GMAC_LPI_STATUS_MAC_RX_LP_IDLE_STATUS_OFFS)
+
+#define MV_GMAC_LPI_STATUS_MAC_TX_LP_WAIT_STATUS_OFFS		3
+#define MV_GMAC_LPI_STATUS_MAC_TX_LP_WAIT_STATUS_MASK    \
+		(0x00000001 << MV_GMAC_LPI_STATUS_MAC_TX_LP_WAIT_STATUS_OFFS)
+
+#define MV_GMAC_LPI_STATUS_MAC_TX_LP_IDLE_STATUS_OFFS		4
+#define MV_GMAC_LPI_STATUS_MAC_TX_LP_IDLE_STATUS_MASK    \
+		(0x00000001 << MV_GMAC_LPI_STATUS_MAC_TX_LP_IDLE_STATUS_OFFS)
+
+/* Lpi Counter */
+#define MV_GMAC_LPI_CNTR_REG				(0x00d0)
+#define MV_GMAC_LPI_CNTR_LPI_COUNTER_OFFS		0
+#define MV_GMAC_LPI_CNTR_LPI_COUNTER_MASK    \
+		(0x0000ffff << MV_GMAC_LPI_CNTR_LPI_COUNTER_OFFS)
+
+/* Pulse 1 Ms Low */
+#define MV_GMAC_PULSE_1_MS_LOW_REG			(0x00d4)
+#define MV_GMAC_PULSE_1_MS_LOW_PULSE_1MS_MAX_LOW_OFFS		0
+#define MV_GMAC_PULSE_1_MS_LOW_PULSE_1MS_MAX_LOW_MASK    \
+		(0x0000ffff << MV_GMAC_PULSE_1_MS_LOW_PULSE_1MS_MAX_LOW_OFFS)
+
+/* Pulse 1 Ms High */
+#define MV_GMAC_PULSE_1_MS_HIGH_REG			(0x00d8)
+#define MV_GMAC_PULSE_1_MS_HIGH_PULSE_1MS_MAX_HIGH_OFFS		0
+#define MV_GMAC_PULSE_1_MS_HIGH_PULSE_1MS_MAX_HIGH_MASK    \
+		(0x0000ffff << MV_GMAC_PULSE_1_MS_HIGH_PULSE_1MS_MAX_HIGH_OFFS)
+
+/* Port Interrupt Cause */
+#define MV_GMAC_INTERRUPT_CAUSE_REG			(0x0020)
+/* Port Interrupt Mask */
+#define MV_GMAC_INTERRUPT_MASK_REG			(0x0024)
+#define MV_GMAC_INTERRUPT_CAUSE_LINK_CHANGE_OFFS	1
+#define MV_GMAC_INTERRUPT_CAUSE_LINK_CHANGE_MASK	(0x1 << \
+		MV_GMAC_INTERRUPT_CAUSE_LINK_CHANGE_OFFS)
+
+/* Port Interrupt Summary Cause */
+#define MV_GMAC_INTERRUPT_SUM_CAUSE_REG			(0x00A0)
+/* Port Interrupt Summary Mask */
+#define MV_GMAC_INTERRUPT_SUM_MASK_REG			(0x00A4)
+#define MV_GMAC_INTERRUPT_SUM_CAUSE_LINK_CHANGE_OFFS	1
+#define MV_GMAC_INTERRUPT_SUM_CAUSE_LINK_CHANGE_MASK	(0x1 << \
+		MV_GMAC_INTERRUPT_SUM_CAUSE_LINK_CHANGE_OFFS)
+
+/**************/
+/* XLGMAC REGS  */
+/**************/
+
+/* Port Mac Control0 */
+#define MV_XLG_PORT_MAC_CTRL0_REG			(0x0000)
+#define MV_XLG_MAC_CTRL0_PORTEN_OFFS		0
+#define MV_XLG_MAC_CTRL0_PORTEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_PORTEN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_MACRESETN_OFFS		1
+#define MV_XLG_MAC_CTRL0_MACRESETN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_MACRESETN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_FORCELINKDOWN_OFFS		2
+#define MV_XLG_MAC_CTRL0_FORCELINKDOWN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_FORCELINKDOWN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_FORCELINKPASS_OFFS		3
+#define MV_XLG_MAC_CTRL0_FORCELINKPASS_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_FORCELINKPASS_OFFS)
+
+#define MV_XLG_MAC_CTRL0_TXIPGMODE_OFFS		5
+#define MV_XLG_MAC_CTRL0_TXIPGMODE_MASK    \
+		(0x00000003 << MV_XLG_MAC_CTRL0_TXIPGMODE_OFFS)
+
+#define MV_XLG_MAC_CTRL0_RXFCEN_OFFS		7
+#define MV_XLG_MAC_CTRL0_RXFCEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_RXFCEN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_TXFCEN_OFFS		8
+#define MV_XLG_MAC_CTRL0_TXFCEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_TXFCEN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_RXCRCCHECKEN_OFFS		9
+#define MV_XLG_MAC_CTRL0_RXCRCCHECKEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_RXCRCCHECKEN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_PERIODICXONEN_OFFS		10
+#define MV_XLG_MAC_CTRL0_PERIODICXONEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_PERIODICXONEN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_RXCRCSTRIPEN_OFFS		11
+#define MV_XLG_MAC_CTRL0_RXCRCSTRIPEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_RXCRCSTRIPEN_OFFS)
+
+#define MV_XLG_MAC_CTRL0_PADDINGDIS_OFFS		13
+#define MV_XLG_MAC_CTRL0_PADDINGDIS_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_PADDINGDIS_OFFS)
+
+#define MV_XLG_MAC_CTRL0_MIBCNTDIS_OFFS		14
+#define MV_XLG_MAC_CTRL0_MIBCNTDIS_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_MIBCNTDIS_OFFS)
+
+#define MV_XLG_MAC_CTRL0_PFC_CASCADE_PORT_ENABLE_OFFS		15
+#define MV_XLG_MAC_CTRL0_PFC_CASCADE_PORT_ENABLE_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL0_PFC_CASCADE_PORT_ENABLE_OFFS)
+
+/* Port Mac Control1 */
+#define MV_XLG_PORT_MAC_CTRL1_REG			(0x0004)
+#define MV_XLG_MAC_CTRL1_FRAMESIZELIMIT_OFFS		0
+#define MV_XLG_MAC_CTRL1_FRAMESIZELIMIT_MASK    \
+		(0x00001fff << MV_XLG_MAC_CTRL1_FRAMESIZELIMIT_OFFS)
+
+#define MV_XLG_MAC_CTRL1_MACLOOPBACKEN_OFFS		13
+#define MV_XLG_MAC_CTRL1_MACLOOPBACKEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL1_MACLOOPBACKEN_OFFS)
+
+#define MV_XLG_MAC_CTRL1_XGMIILOOPBACKEN_OFFS		14
+#define MV_XLG_MAC_CTRL1_XGMIILOOPBACKEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL1_XGMIILOOPBACKEN_OFFS)
+
+#define MV_XLG_MAC_CTRL1_LOOPBACKCLOCKSELECT_OFFS		15
+#define MV_XLG_MAC_CTRL1_LOOPBACKCLOCKSELECT_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL1_LOOPBACKCLOCKSELECT_OFFS)
+
+/* Port Mac Control2 */
+#define MV_XLG_PORT_MAC_CTRL2_REG		(0x0008)
+#define MV_XLG_MAC_CTRL2_SALOW_7_0_OFFS		0
+#define MV_XLG_MAC_CTRL2_SALOW_7_0_MASK    \
+		(0x000000ff << MV_XLG_MAC_CTRL2_SALOW_7_0_OFFS)
+
+#define MV_XLG_MAC_CTRL2_UNIDIRECTIONALEN_OFFS		8
+#define MV_XLG_MAC_CTRL2_UNIDIRECTIONALEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL2_UNIDIRECTIONALEN_OFFS)
+
+#define MV_XLG_MAC_CTRL2_FIXEDIPGBASE_OFFS		9
+#define MV_XLG_MAC_CTRL2_FIXEDIPGBASE_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL2_FIXEDIPGBASE_OFFS)
+
+#define MV_XLG_MAC_CTRL2_PERIODICXOFFEN_OFFS		10
+#define MV_XLG_MAC_CTRL2_PERIODICXOFFEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL2_PERIODICXOFFEN_OFFS)
+
+#define MV_XLG_MAC_CTRL2_SIMPLEXMODEEN_OFFS		13
+#define MV_XLG_MAC_CTRL2_SIMPLEXMODEEN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL2_SIMPLEXMODEEN_OFFS)
+
+#define MV_XLG_MAC_CTRL2_FC_MODE_OFFS		14
+#define MV_XLG_MAC_CTRL2_FC_MODE_MASK    \
+		(0x00000003 << MV_XLG_MAC_CTRL2_FC_MODE_OFFS)
+
+/* Port Status */
+#define MV_XLG_MAC_PORT_STATUS_REG		(0x000c)
+#define MV_XLG_MAC_PORT_STATUS_LINKSTATUS_OFFS		0
+#define MV_XLG_MAC_PORT_STATUS_LINKSTATUS_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_LINKSTATUS_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_REMOTEFAULT_OFFS		1
+#define MV_XLG_MAC_PORT_STATUS_REMOTEFAULT_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_REMOTEFAULT_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_LOCALFAULT_OFFS		2
+#define MV_XLG_MAC_PORT_STATUS_LOCALFAULT_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_LOCALFAULT_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_LINKSTATUSCLEAN_OFFS		3
+#define MV_XLG_MAC_PORT_STATUS_LINKSTATUSCLEAN_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_LINKSTATUSCLEAN_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_LOCALFAULTCLEAN_OFFS		4
+#define MV_XLG_MAC_PORT_STATUS_LOCALFAULTCLEAN_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_LOCALFAULTCLEAN_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_REMOTEFAULTCLEAN_OFFS		5
+#define MV_XLG_MAC_PORT_STATUS_REMOTEFAULTCLEAN_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_REMOTEFAULTCLEAN_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_PORTRXPAUSE_OFFS		6
+#define MV_XLG_MAC_PORT_STATUS_PORTRXPAUSE_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_PORTRXPAUSE_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_PORTTXPAUSE_OFFS		7
+#define MV_XLG_MAC_PORT_STATUS_PORTTXPAUSE_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_PORTTXPAUSE_OFFS)
+
+#define MV_XLG_MAC_PORT_STATUS_PFC_SYNC_FIFO_FULL_OFFS		8
+#define MV_XLG_MAC_PORT_STATUS_PFC_SYNC_FIFO_FULL_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_STATUS_PFC_SYNC_FIFO_FULL_OFFS)
+
+/* Port Fifos Thresholds Configuration */
+#define MV_XLG_PORT_FIFOS_THRS_CFG_REG		(0x001)
+#define MV_XLG_MAC_PORT_FIFOS_THRS_CFG_RXFULLTHR_OFFS		0
+#define MV_XLG_MAC_PORT_FIFOS_THRS_CFG_RXFULLTHR_MASK    \
+		(0x0000001f << MV_XLG_MAC_PORT_FIFOS_THRS_CFG_RXFULLTHR_OFFS)
+
+#define MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXFIFOSIZE_OFFS		5
+#define MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXFIFOSIZE_MASK    \
+		(0x0000003f << MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXFIFOSIZE_OFFS)
+
+#define MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXRDTHR_OFFS		11
+#define MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXRDTHR_MASK    \
+		(0x0000001f << MV_XLG_MAC_PORT_FIFOS_THRS_CFG_TXRDTHR_OFFS)
+
+/* Port Mac Control3 */
+#define MV_XLG_PORT_MAC_CTRL3_REG			(0x001c)
+#define MV_XLG_MAC_CTRL3_BUFSIZE_OFFS		0
+#define MV_XLG_MAC_CTRL3_BUFSIZE_MASK    \
+		(0x0000003f << MV_XLG_MAC_CTRL3_BUFSIZE_OFFS)
+
+#define MV_XLG_MAC_CTRL3_XTRAIPG_OFFS		6
+#define MV_XLG_MAC_CTRL3_XTRAIPG_MASK    \
+		(0x0000007f << MV_XLG_MAC_CTRL3_XTRAIPG_OFFS)
+
+#define MV_XLG_MAC_CTRL3_MACMODESELECT_OFFS		13
+#define MV_XLG_MAC_CTRL3_MACMODESELECT_MASK    \
+		(0x00000007 << MV_XLG_MAC_CTRL3_MACMODESELECT_OFFS)
+
+/* Port Per Prio Flow Control Status */
+#define MV_XLG_PORT_PER_PRIO_FLOW_CTRL_STATUS_REG      (0x0020)
+#define MV_XLG_MAC_PORT_PER_PRIO_FLOW_CTRL_STATUS_PRIONSTATUS_OFFS	0
+#define MV_XLG_MAC_PORT_PER_PRIO_FLOW_CTRL_STATUS_PRIONSTATUS_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_PORT_PER_PRIO_FLOW_CTRL_STATUS_PRIONSTATUS_OFFS)
+
+/* Debug Bus Status */
+#define MV_XLG_DEBUG_BUS_STATUS_REG		(0x0024)
+#define MV_XLG_MAC_DEBUG_BUS_STATUS_DEBUG_BUS_OFFS		0
+#define MV_XLG_MAC_DEBUG_BUS_STATUS_DEBUG_BUS_MASK    \
+		(0x0000ffff << MV_XLG_MAC_DEBUG_BUS_STATUS_DEBUG_BUS_OFFS)
+
+/* Port Metal Fix */
+#define MV_XLG_PORT_METAL_FIX_REG		(0x002c)
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_EOP_IN_FIFO__OFFS		0
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_EOP_IN_FIFO__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_EOP_IN_FIFO__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_LTF_FIX__OFFS		1
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_LTF_FIX__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_LTF_FIX__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_HOLD_FIX__OFFS		2
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_HOLD_FIX__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_HOLD_FIX__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_LED_FIX__OFFS		3
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_LED_FIX__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_LED_FIX__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_PAD_PROTECT__OFFS		4
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_PAD_PROTECT__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_PAD_PROTECT__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_NX_BTS44__OFFS		5
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_NX_BTS44__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_NX_BTS44__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_NX_BTS42__OFFS		6
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_NX_BTS42__MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_NX_BTS42__OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_FLUSH_FIX_OFFS		7
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_FLUSH_FIX_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_FLUSH_FIX_OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_PORT_EN_FIX_OFFS		8
+#define MV_XLG_MAC_PORT_METAL_FIX_EN_PORT_EN_FIX_MASK    \
+		(0x00000001 << MV_XLG_MAC_PORT_METAL_FIX_EN_PORT_EN_FIX_OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_SPARE_DEF0_BITS_OFFS		9
+#define MV_XLG_MAC_PORT_METAL_FIX_SPARE_DEF0_BITS_MASK    \
+		(0x0000000f << MV_XLG_MAC_PORT_METAL_FIX_SPARE_DEF0_BITS_OFFS)
+
+#define MV_XLG_MAC_PORT_METAL_FIX_SPARE_DEF1_BITS_OFFS		13
+#define MV_XLG_MAC_PORT_METAL_FIX_SPARE_DEF1_BITS_MASK    \
+		(0x00000007 << MV_XLG_MAC_PORT_METAL_FIX_SPARE_DEF1_BITS_OFFS)
+
+/* Xg Mib Counters Control */
+#define MV_XLG_MIB_CNTRS_CTRL_REG		(0x0030)
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGCAPTURETRIGGER_OFFS		0
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGCAPTURETRIGGER_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGCAPTURETRIGGER_OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGDONTCLEARAFTERREAD_OFFS		1
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGDONTCLEARAFTERREAD_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGDONTCLEARAFTERREAD_OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGRXHISTOGRAMEN_OFFS		2
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGRXHISTOGRAMEN_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGRXHISTOGRAMEN_OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGTXHISTOGRAMEN_OFFS		3
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGTXHISTOGRAMEN_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_XGTXHISTOGRAMEN_OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MFA1_BTT940_FIX_ENABLE__OFFS	4
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MFA1_BTT940_FIX_ENABLE__MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MFA1_BTT940_FIX_ENABLE__OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_LEDS_NUMBER_OFFS		5
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_LEDS_NUMBER_MASK    \
+		(0x0000003f << MV_XLG_MAC_XG_MIB_CNTRS_CTRL_LEDS_NUMBER_OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MIB_4_COUNT_HIST_OFFS		11
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MIB_4_COUNT_HIST_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MIB_4_COUNT_HIST_OFFS)
+
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MIB_4_LIMIT_1518_1522_OFFS		12
+#define MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MIB_4_LIMIT_1518_1522_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_XG_MIB_CNTRS_CTRL_MIB_4_LIMIT_1518_1522_OFFS)
+
+/* Cn/ccfc Timer%i */
+#define MV_XLG_CNCCFC_TIMERI_REG(t)		((0x0038 + t * 4))
+#define MV_XLG_MAC_CNCCFC_TIMERI_PORTSPEEDTIMER_OFFS	0
+#define MV_XLG_MAC_CNCCFC_TIMERI_PORTSPEEDTIMER_MASK    \
+		(0x0000ffff << MV_XLG_MAC_CNCCFC_TIMERI_PORTSPEEDTIMER_OFFS)
+
+/* Ppfc Control */
+#define MV_XLG_MAC_PPFC_CTRL_REG			(0x0060)
+#define MV_XLG_MAC_PPFC_CTRL_GLOBAL_PAUSE_ENI_OFFS		0
+#define MV_XLG_MAC_PPFC_CTRL_GLOBAL_PAUSE_ENI_MASK    \
+		(0x00000001 << MV_XLG_MAC_PPFC_CTRL_GLOBAL_PAUSE_ENI_OFFS)
+
+#define MV_XLG_MAC_PPFC_CTRL_DIP_BTS_677_EN_OFFS		9
+#define MV_XLG_MAC_PPFC_CTRL_DIP_BTS_677_EN_MASK    \
+		(0x00000001 << MV_XLG_MAC_PPFC_CTRL_DIP_BTS_677_EN_OFFS)
+
+/* Fc Dsa Tag 0 */
+#define MV_XLG_MAC_FC_DSA_TAG_0_REG		(0x0068)
+#define MV_XLG_MAC_FC_DSA_TAG_0_DSATAGREG0_OFFS		0
+#define MV_XLG_MAC_FC_DSA_TAG_0_DSATAGREG0_MASK    \
+		(0x0000ffff << MV_XLG_MAC_FC_DSA_TAG_0_DSATAGREG0_OFFS)
+
+/* Fc Dsa Tag 1 */
+#define MV_XLG_MAC_FC_DSA_TAG_1_REG		(0x006c)
+#define MV_XLG_MAC_FC_DSA_TAG_1_DSATAGREG1_OFFS		0
+#define MV_XLG_MAC_FC_DSA_TAG_1_DSATAGREG1_MASK    \
+		(0x0000ffff << MV_XLG_MAC_FC_DSA_TAG_1_DSATAGREG1_OFFS)
+
+/* Fc Dsa Tag 2 */
+#define MV_XLG_MAC_FC_DSA_TAG_2_REG		(0x0070)
+#define MV_XLG_MAC_FC_DSA_TAG_2_DSATAGREG2_OFFS		0
+#define MV_XLG_MAC_FC_DSA_TAG_2_DSATAGREG2_MASK    \
+		(0x0000ffff << MV_XLG_MAC_FC_DSA_TAG_2_DSATAGREG2_OFFS)
+
+/* Fc Dsa Tag 3 */
+#define MV_XLG_MAC_FC_DSA_TAG_3_REG		(0x0074)
+#define MV_XLG_MAC_FC_DSA_TAG_3_DSATAGREG3_OFFS		0
+#define MV_XLG_MAC_FC_DSA_TAG_3_DSATAGREG3_MASK    \
+		(0x0000ffff << MV_XLG_MAC_FC_DSA_TAG_3_DSATAGREG3_OFFS)
+
+/* Dic Budget Compensation */
+#define MV_XLG_MAC_DIC_BUDGET_COMPENSATION_REG	(0x0080)
+#define MV_XLG_MAC_DIC_BUDGET_COMPENSATION_DIC_COUNTER_TO_ADD_8BYTES_OFFS 0
+#define MV_XLG_MAC_DIC_BUDGET_COMPENSATION_DIC_COUNTER_TO_ADD_8BYTES_MASK    \
+		(0x0000ffff << \
+	MV_XLG_MAC_DIC_BUDGET_COMPENSATION_DIC_COUNTER_TO_ADD_8BYTES_OFFS)
+
+/* Port Mac Control4 */
+#define MV_XLG_PORT_MAC_CTRL4_REG			(0x0084)
+#define MV_XLG_MAC_CTRL4_LLFC_GLOBAL_FC_ENABLE_OFFS		0
+#define MV_XLG_MAC_CTRL4_LLFC_GLOBAL_FC_ENABLE_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_LLFC_GLOBAL_FC_ENABLE_OFFS)
+
+#define MV_XLG_MAC_CTRL4_LED_STREAM_SELECT_OFFS		1
+#define MV_XLG_MAC_CTRL4_LED_STREAM_SELECT_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_LED_STREAM_SELECT_OFFS)
+
+#define MV_XLG_MAC_CTRL4_DEBUG_BUS_SELECT_OFFS		2
+#define MV_XLG_MAC_CTRL4_DEBUG_BUS_SELECT_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_DEBUG_BUS_SELECT_OFFS)
+
+#define MV_XLG_MAC_CTRL4_MASK_PCS_RESET_OFFS		3
+#define MV_XLG_MAC_CTRL4_MASK_PCS_RESET_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_MASK_PCS_RESET_OFFS)
+
+#define MV_XLG_MAC_CTRL4_ENABLE_SHORT_PREAMBLE_FOR_XLG_OFFS		4
+#define MV_XLG_MAC_CTRL4_ENABLE_SHORT_PREAMBLE_FOR_XLG_MASK    \
+		(0x00000001 << \
+		MV_XLG_MAC_CTRL4_ENABLE_SHORT_PREAMBLE_FOR_XLG_OFFS)
+
+#define MV_XLG_MAC_CTRL4_FORWARD_802_3X_FC_EN_OFFS		5
+#define MV_XLG_MAC_CTRL4_FORWARD_802_3X_FC_EN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_FORWARD_802_3X_FC_EN_OFFS)
+
+#define MV_XLG_MAC_CTRL4_FORWARD_PFC_EN_OFFS		6
+#define MV_XLG_MAC_CTRL4_FORWARD_PFC_EN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_FORWARD_PFC_EN_OFFS)
+
+#define MV_XLG_MAC_CTRL4_FORWARD_UNKNOWN_FC_EN_OFFS		7
+#define MV_XLG_MAC_CTRL4_FORWARD_UNKNOWN_FC_EN_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_FORWARD_UNKNOWN_FC_EN_OFFS)
+
+#define MV_XLG_MAC_CTRL4_USE_XPCS_OFFS		8
+#define MV_XLG_MAC_CTRL4_USE_XPCS_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_USE_XPCS_OFFS)
+
+#define MV_XLG_MAC_CTRL4_DMA_INTERFACE_IS_64_BIT_OFFS		9
+#define MV_XLG_MAC_CTRL4_DMA_INTERFACE_IS_64_BIT_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_DMA_INTERFACE_IS_64_BIT_OFFS)
+
+#define MV_XLG_MAC_CTRL4_TX_DMA_INTERFACE_BITS_OFFS		10
+#define MV_XLG_MAC_CTRL4_TX_DMA_INTERFACE_BITS_MASK    \
+		(0x00000003 << MV_XLG_MAC_CTRL4_TX_DMA_INTERFACE_BITS_OFFS)
+
+#define MV_XLG_MAC_CTRL4_MAC_MODE_DMA_1G_OFFS		12
+#define MV_XLG_MAC_CTRL4_MAC_MODE_DMA_1G_MASK    \
+		(0x00000001 << MV_XLG_MAC_CTRL4_MAC_MODE_DMA_1G_OFFS)
+
+/* Port Mac Control5 */
+#define MV_XLG_PORT_MAC_CTRL5_REG			(0x0088)
+#define MV_XLG_MAC_CTRL5_TXIPGLENGTH_OFFS		0
+#define MV_XLG_MAC_CTRL5_TXIPGLENGTH_MASK    \
+		(0x0000000f << MV_XLG_MAC_CTRL5_TXIPGLENGTH_OFFS)
+
+#define MV_XLG_MAC_CTRL5_PREAMBLELENGTHTX_OFFS		4
+#define MV_XLG_MAC_CTRL5_PREAMBLELENGTHTX_MASK    \
+		(0x00000007 << MV_XLG_MAC_CTRL5_PREAMBLELENGTHTX_OFFS)
+
+#define MV_XLG_MAC_CTRL5_PREAMBLELENGTHRX_OFFS		7
+#define MV_XLG_MAC_CTRL5_PREAMBLELENGTHRX_MASK    \
+		(0x00000007 << MV_XLG_MAC_CTRL5_PREAMBLELENGTHRX_OFFS)
+
+#define MV_XLG_MAC_CTRL5_TXNUMCRCBYTES_OFFS		10
+#define MV_XLG_MAC_CTRL5_TXNUMCRCBYTES_MASK    \
+		(0x00000007 << MV_XLG_MAC_CTRL5_TXNUMCRCBYTES_OFFS)
+
+#define MV_XLG_MAC_CTRL5_RXNUMCRCBYTES_OFFS		13
+#define MV_XLG_MAC_CTRL5_RXNUMCRCBYTES_MASK    \
+		(0x00000007 << MV_XLG_MAC_CTRL5_RXNUMCRCBYTES_OFFS)
+
+/* External Control */
+#define MV_XLG_MAC_EXT_CTRL_REG			(0x0090)
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL0_OFFS		0
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL0_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL0_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL1_OFFS		1
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL1_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL1_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL2_OFFS		2
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL2_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL2_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL3_OFFS		3
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL3_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL3_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL4_OFFS		4
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL4_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL4_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL5_OFFS		5
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL5_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL5_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL6_OFFS		6
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL6_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL6_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL7_OFFS		7
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL7_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL7_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL8_OFFS		8
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL8_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL8_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL9_OFFS		9
+#define MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL9_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXTERNAL_CTRL9_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_10_OFFS		10
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_10_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXT_CTRL_10_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_11_OFFS		11
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_11_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXT_CTRL_11_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_12_OFFS		12
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_12_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXT_CTRL_12_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_13_OFFS		13
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_13_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXT_CTRL_13_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_14_OFFS		14
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_14_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXT_CTRL_14_OFFS)
+
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_15_OFFS		15
+#define MV_XLG_MAC_EXT_CTRL_EXT_CTRL_15_MASK    \
+		(0x00000001 << MV_XLG_MAC_EXT_CTRL_EXT_CTRL_15_OFFS)
+
+/* Macro Control */
+#define MV_XLG_MAC_MACRO_CTRL_REG			(0x0094)
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_0_OFFS		0
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_0_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_0_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_1_OFFS		1
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_1_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_1_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_2_OFFS		2
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_2_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_2_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_3_OFFS		3
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_3_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_3_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_4_OFFS		4
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_4_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_4_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_5_OFFS		5
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_5_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_5_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_6_OFFS		6
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_6_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_6_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_7_OFFS		7
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_7_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_7_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_8_OFFS		8
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_8_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_8_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_9_OFFS		9
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_9_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_9_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_10_OFFS		10
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_10_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_10_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_11_OFFS		11
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_11_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_11_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_12_OFFS		12
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_12_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_12_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_13_OFFS		13
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_13_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_13_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_14_OFFS		14
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_14_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_14_OFFS)
+
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_15_OFFS		15
+#define MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_15_MASK    \
+		(0x00000001 << MV_XLG_MAC_MACRO_CTRL_MACRO_CTRL_15_OFFS)
+
+#define MV_XLG_MAC_DIC_PPM_IPG_REDUCE_REG		(0x0094)
+
+/* Port Interrupt Cause */
+#define MV_XLG_INTERRUPT_CAUSE_REG		(0x0014)
+/* Port Interrupt Mask */
+#define MV_XLG_INTERRUPT_MASK_REG		(0x0018)
+#define MV_XLG_INTERRUPT_LINK_CHANGE_OFFS	1
+#define MV_XLG_INTERRUPT_LINK_CHANGE_MASK	(0x1 << \
+		MV_XLG_INTERRUPT_LINK_CHANGE_OFFS)
+
+/* Port Interrupt Summary Cause */
+#define MV_XLG_EXTERNAL_INTERRUPT_CAUSE_REG	(0x0058)
+/* Port Interrupt Summary Mask */
+#define MV_XLG_EXTERNAL_INTERRUPT_MASK_REG	(0x005C)
+#define MV_XLG_EXTERNAL_INTERRUPT_LINK_CHANGE_OFFS	1
+#define MV_XLG_EXTERNAL_INTERRUPT_LINK_CHANGE_MASK	(0x1 << \
+		MV_XLG_EXTERNAL_INTERRUPT_LINK_CHANGE_OFFS)
+
+/***********/
+/*XPCS REGS */
+/***********/
+
+/* Global Configuration 0 */
+#define MV_XPCS_GLOBAL_CFG_0_REG				(0x0)
+#define MV_XPCS_GLOBAL_CFG_0_PCSRESET_OFFS		0
+#define MV_XPCS_GLOBAL_CFG_0_PCSRESET_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_0_PCSRESET_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_DESKEWRESET_OFFS		1
+#define MV_XPCS_GLOBAL_CFG_0_DESKEWRESET_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_0_DESKEWRESET_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_TXRESET_OFFS		2
+#define MV_XPCS_GLOBAL_CFG_0_TXRESET_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_0_TXRESET_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_PCSMODE_OFFS		3
+#define MV_XPCS_GLOBAL_CFG_0_PCSMODE_MASK    \
+		(0x00000003 << MV_XPCS_GLOBAL_CFG_0_PCSMODE_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_LANEACTIVE_OFFS		5
+#define MV_XPCS_GLOBAL_CFG_0_LANEACTIVE_MASK    \
+		(0x00000003 << MV_XPCS_GLOBAL_CFG_0_LANEACTIVE_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_INDIVIDUALMODE_OFFS		7
+#define MV_XPCS_GLOBAL_CFG_0_INDIVIDUALMODE_MASK    \
+		(0x0000003f << MV_XPCS_GLOBAL_CFG_0_INDIVIDUALMODE_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_TXSMMODE_OFFS		13
+#define MV_XPCS_GLOBAL_CFG_0_TXSMMODE_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_0_TXSMMODE_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_TXSMIDLECNTDISABLE_OFFS		14
+#define MV_XPCS_GLOBAL_CFG_0_TXSMIDLECNTDISABLE_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_0_TXSMIDLECNTDISABLE_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_0_COMMADETCT2NDSYNCSMEN_OFFS		15
+#define MV_XPCS_GLOBAL_CFG_0_COMMADETCT2NDSYNCSMEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_0_COMMADETCT2NDSYNCSMEN_OFFS)
+
+/* Global Configuration 1 */
+#define MV_XPCS_GLOBAL_CFG_1_REG				(0x4)
+#define MV_XPCS_GLOBAL_CFG_1_MACLOOPBACKEN_OFFS		0
+#define MV_XPCS_GLOBAL_CFG_1_MACLOOPBACKEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_MACLOOPBACKEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_PCSLOOPBACKEN_OFFS		1
+#define MV_XPCS_GLOBAL_CFG_1_PCSLOOPBACKEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_PCSLOOPBACKEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_REPEATERMODEEN_OFFS		2
+#define MV_XPCS_GLOBAL_CFG_1_REPEATERMODEEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_REPEATERMODEEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_LOOPBACKCLKSEL_OFFS		3
+#define MV_XPCS_GLOBAL_CFG_1_LOOPBACKCLKSEL_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_LOOPBACKCLKSEL_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_DESKEWCLKSEL_OFFS		4
+#define MV_XPCS_GLOBAL_CFG_1_DESKEWCLKSEL_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_DESKEWCLKSEL_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_TXSMREPEATERMODE_OFFS		5
+#define MV_XPCS_GLOBAL_CFG_1_TXSMREPEATERMODE_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_TXSMREPEATERMODE_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_RXLOCKBYPASSEN_OFFS		6
+#define MV_XPCS_GLOBAL_CFG_1_RXLOCKBYPASSEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_RXLOCKBYPASSEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_TXLOCKBYPASSEN_OFFS		7
+#define MV_XPCS_GLOBAL_CFG_1_TXLOCKBYPASSEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_TXLOCKBYPASSEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_REMOTEFAULTDIS_OFFS		8
+#define MV_XPCS_GLOBAL_CFG_1_REMOTEFAULTDIS_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_REMOTEFAULTDIS_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_SIGNALDETDOWNLOCALFAULTGENDIS_OFFS		9
+#define MV_XPCS_GLOBAL_CFG_1_SIGNALDETDOWNLOCALFAULTGENDIS_MASK    \
+		(0x00000001 << \
+		MV_XPCS_GLOBAL_CFG_1_SIGNALDETDOWNLOCALFAULTGENDIS_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_CJPATGENEN_OFFS		10
+#define MV_XPCS_GLOBAL_CFG_1_CJPATGENEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_CJPATGENEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_CRPATGENEN_OFFS		11
+#define MV_XPCS_GLOBAL_CFG_1_CRPATGENEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_CRPATGENEN_OFFS)
+
+#define MV_XPCS_GLOBAL_CFG_1_CJRFORCEDISPEN_OFFS		12
+#define MV_XPCS_GLOBAL_CFG_1_CJRFORCEDISPEN_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_CFG_1_CJRFORCEDISPEN_OFFS)
+
+/* Global Fifo Threshold Configuration */
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_REG				(0x8)
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWTIMEOUTLIMIT_OFFS		0
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWTIMEOUTLIMIT_MASK    \
+		(0x0000000f << \
+		MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWTIMEOUTLIMIT_OFFS)
+
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWFIFOWRADDRFIX_OFFS		4
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWFIFOWRADDRFIX_MASK    \
+		(0x0000001f << \
+		MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWFIFOWRADDRFIX_OFFS)
+
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWFIFORDTH_OFFS		9
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWFIFORDTH_MASK    \
+		(0x0000000f << MV_XPCS_GLOBAL_FIFO_THR_CFG_DESKEWFIFORDTH_OFFS)
+
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_PPMFIFORDTH_OFFS		13
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_PPMFIFORDTH_MASK    \
+		(0x00000007 << MV_XPCS_GLOBAL_FIFO_THR_CFG_PPMFIFORDTH_OFFS)
+
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_PPMFIFOEXTRAIDLECHKDIS_OFFS		16
+#define MV_XPCS_GLOBAL_FIFO_THR_CFG_PPMFIFOEXTRAIDLECHKDIS_MASK    \
+		(0x00000001 << \
+		MV_XPCS_GLOBAL_FIFO_THR_CFG_PPMFIFOEXTRAIDLECHKDIS_OFFS)
+
+/* Global Max Idle Counter */
+#define MV_XPCS_GLOBAL_MAX_IDLE_CNTR_REG			(0xc)
+#define MV_XPCS_GLOBAL_MAX_IDLE_CNTR_MAXIDLECNT_OFFS		0
+#define MV_XPCS_GLOBAL_MAX_IDLE_CNTR_MAXIDLECNT_MASK    \
+		(0x0000ffff << MV_XPCS_GLOBAL_MAX_IDLE_CNTR_MAXIDLECNT_OFFS)
+
+/* Global Status */
+#define MV_XPCS_GLOBAL_STATUS_REG				(0x10)
+#define MV_XPCS_GLOBAL_STATUS_LINKUP_OFFS		0
+#define MV_XPCS_GLOBAL_STATUS_LINKUP_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_STATUS_LINKUP_OFFS)
+
+#define MV_XPCS_GLOBAL_STATUS_DESKEWACQUIRED_OFFS		1
+#define MV_XPCS_GLOBAL_STATUS_DESKEWACQUIRED_MASK    \
+		(0x00000001 << MV_XPCS_GLOBAL_STATUS_DESKEWACQUIRED_OFFS)
+
+/* Global Deskew Error Counter */
+#define MV_XPCS_GLOBAL_DESKEW_ERR_CNTR_REG			(0x20)
+#define MV_XPCS_GLOBAL_DESKEW_ERR_CNTR_DESKEWERRCNT_OFFS		0
+#define MV_XPCS_GLOBAL_DESKEW_ERR_CNTR_DESKEWERRCNT_MASK    \
+		(0x0000ffff << MV_XPCS_GLOBAL_DESKEW_ERR_CNTR_DESKEWERRCNT_OFFS)
+
+/* Tx Packets Counter LSB */
+#define MV_XPCS_TX_PCKTS_CNTR_LSB_REG				(0x30)
+#define MV_XPCS_TX_PCKTS_CNTR_LSB_TXPCKTCNTRLSB_OFFS		0
+#define MV_XPCS_TX_PCKTS_CNTR_LSB_TXPCKTCNTRLSB_MASK    \
+		(0x0000ffff << MV_XPCS_TX_PCKTS_CNTR_LSB_TXPCKTCNTRLSB_OFFS)
+
+/* Tx Packets Counter MSB */
+#define MV_XPCS_TX_PCKTS_CNTR_MSB_REG				(0x34)
+#define MV_XPCS_TX_PCKTS_CNTR_MSB_TXPCKTCNTRMSB_OFFS		0
+#define MV_XPCS_TX_PCKTS_CNTR_MSB_TXPCKTCNTRMSB_MASK    \
+		(0x0000ffff << MV_XPCS_TX_PCKTS_CNTR_MSB_TXPCKTCNTRMSB_OFFS)
+
+/* XPCS per Lane registers */
+
+/* Lane Configuration 0 */
+#define MV_XPCS_LANE_CFG_0_REG				(0x50)
+#define MV_XPCS_LANE_CFG_0_TXRESETIND_OFFS		0
+#define MV_XPCS_LANE_CFG_0_TXRESETIND_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_TXRESETIND_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_RXRESETIND_OFFS		1
+#define MV_XPCS_LANE_CFG_0_RXRESETIND_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_RXRESETIND_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_INDIVIDUALLOOPBACK_OFFS		2
+#define MV_XPCS_LANE_CFG_0_INDIVIDUALLOOPBACK_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_INDIVIDUALLOOPBACK_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_INDIVIDUALLINELOOPBACK_OFFS		3
+#define MV_XPCS_LANE_CFG_0_INDIVIDUALLINELOOPBACK_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_INDIVIDUALLINELOOPBACK_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_TXSMBYPASSEN_OFFS		4
+#define MV_XPCS_LANE_CFG_0_TXSMBYPASSEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_TXSMBYPASSEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_RXIDLEGENBYPASSEN_OFFS		5
+#define MV_XPCS_LANE_CFG_0_RXIDLEGENBYPASSEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_RXIDLEGENBYPASSEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_SIGNALDETECTBYPASSEN_OFFS		6
+#define MV_XPCS_LANE_CFG_0_SIGNALDETECTBYPASSEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_SIGNALDETECTBYPASSEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_CJPATCHKEN_OFFS		7
+#define MV_XPCS_LANE_CFG_0_CJPATCHKEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_CJPATCHKEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_CRPATCHKEN_OFFS		8
+#define MV_XPCS_LANE_CFG_0_CRPATCHKEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_CRPATCHKEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_PRBSCHECKEN_OFFS		11
+#define MV_XPCS_LANE_CFG_0_PRBSCHECKEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_PRBSCHECKEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_TESTGENEN_OFFS		12
+#define MV_XPCS_LANE_CFG_0_TESTGENEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_TESTGENEN_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_TESTMODE_OFFS		13
+#define MV_XPCS_LANE_CFG_0_TESTMODE_MASK    \
+		(0x00000003 << MV_XPCS_LANE_CFG_0_TESTMODE_OFFS)
+
+#define MV_XPCS_LANE_CFG_0_TESTMODEEN_OFFS		15
+#define MV_XPCS_LANE_CFG_0_TESTMODEEN_MASK    \
+		(0x00000001 << MV_XPCS_LANE_CFG_0_TESTMODEEN_OFFS)
+
+/* Lane Configuration 1 */
+#define MV_XPCS_LANE_CFG_1_REG				(0x54)
+#define MV_XPCS_LANE_CFG_1_LED0CTRL_OFFS		0
+#define MV_XPCS_LANE_CFG_1_LED0CTRL_MASK    \
+		(0x0000000f << MV_XPCS_LANE_CFG_1_LED0CTRL_OFFS)
+
+#define MV_XPCS_LANE_CFG_1_LED1CTRL_OFFS		4
+#define MV_XPCS_LANE_CFG_1_LED1CTRL_MASK    \
+		(0x0000000f << MV_XPCS_LANE_CFG_1_LED1CTRL_OFFS)
+
+#define MV_XPCS_LANE_CFG_1_TXSWPSEL_OFFS		8
+#define MV_XPCS_LANE_CFG_1_TXSWPSEL_MASK    \
+		(0x00000007 << MV_XPCS_LANE_CFG_1_TXSWPSEL_OFFS)
+
+#define MV_XPCS_LANE_CFG_1_RXSWPSEL_OFFS		11
+#define MV_XPCS_LANE_CFG_1_RXSWPSEL_MASK    \
+		(0x00000007 << MV_XPCS_LANE_CFG_1_RXSWPSEL_OFFS)
+
+/* Lane Status */
+#define MV_XPCS_LANE_STATUS_REG				(0x5c)
+#define MV_XPCS_LANE_STATUS_PRBSCHECKLOCKED_OFFS		0
+#define MV_XPCS_LANE_STATUS_PRBSCHECKLOCKED_MASK    \
+		(0x00000001 << MV_XPCS_LANE_STATUS_PRBSCHECKLOCKED_OFFS)
+
+#define MV_XPCS_LANE_STATUS_PLLLOCKED_OFFS		1
+#define MV_XPCS_LANE_STATUS_PLLLOCKED_MASK    \
+		(0x00000001 << MV_XPCS_LANE_STATUS_PLLLOCKED_OFFS)
+
+#define MV_XPCS_LANE_STATUS_SIGNALDETECTED_OFFS		2
+#define MV_XPCS_LANE_STATUS_SIGNALDETECTED_MASK    \
+		(0x00000001 << MV_XPCS_LANE_STATUS_SIGNALDETECTED_OFFS)
+
+#define MV_XPCS_LANE_STATUS_COMMADETECTED_OFFS		3
+#define MV_XPCS_LANE_STATUS_COMMADETECTED_MASK    \
+		(0x00000001 << MV_XPCS_LANE_STATUS_COMMADETECTED_OFFS)
+
+#define MV_XPCS_LANE_STATUS_SYNCOK_OFFS		4
+#define MV_XPCS_LANE_STATUS_SYNCOK_MASK    \
+		(0x00000001 << MV_XPCS_LANE_STATUS_SYNCOK_OFFS)
+
+/* Symbol Error Counter */
+#define MV_XPCS_SYMBOL_ERR_CNTR_REG			(0x68)
+#define MV_XPCS_SYMBOL_ERR_CNTR_SYMBOLERRCNT_OFFS	0
+#define MV_XPCS_SYMBOL_ERR_CNTR_SYMBOLERRCNT_MASK    \
+		(0x0000ffff << MV_XPCS_SYMBOL_ERR_CNTR_SYMBOLERRCNT_OFFS)
+
+/* Disparity Error Counter */
+#define MV_XPCS_DISPARITY_ERR_CNTR_REG			(0x6c)
+#define MV_XPCS_DISPARITY_ERR_CNTR_DISPARITYERRCNT_OFFS		0
+#define MV_XPCS_DISPARITY_ERR_CNTR_DISPARITYERRCNT_MASK    \
+		(0x0000ffff << MV_XPCS_DISPARITY_ERR_CNTR_DISPARITYERRCNT_OFFS)
+
+/* Prbs Error Counter */
+#define MV_XPCS_PRBS_ERR_CNTR_REG			(0x70)
+#define MV_XPCS_PRBS_ERR_CNTR_PRBSERRCNT_OFFS		0
+#define MV_XPCS_PRBS_ERR_CNTR_PRBSERRCNT_MASK    \
+		(0x0000ffff << MV_XPCS_PRBS_ERR_CNTR_PRBSERRCNT_OFFS)
+
+/* Rx Packets Counter LSB */
+#define MV_XPCS_RX_PCKTS_CNTR_LSB_REG			(0x74)
+#define MV_XPCS_RX_PCKTS_CNTR_LSB_RXPCKTCNTRLSB_OFFS		0
+#define MV_XPCS_RX_PCKTS_CNTR_LSB_RXPCKTCNTRLSB_MASK    \
+		(0x0000ffff << MV_XPCS_RX_PCKTS_CNTR_LSB_RXPCKTCNTRLSB_OFFS)
+
+/* Rx Packets Counter MSB */
+#define MV_XPCS_RX_PCKTS_CNTR_MSB_REG			(0x78)
+#define MV_XPCS_RX_PCKTS_CNTR_MSB_RXPCKTCNTRMSB_OFFS	0
+#define MV_XPCS_RX_PCKTS_CNTR_MSB_RXPCKTCNTRMSB_MASK    \
+		(0x0000ffff << MV_XPCS_RX_PCKTS_CNTR_MSB_RXPCKTCNTRMSB_OFFS)
+
+/* Rx Bad Packets Counter LSB */
+#define MV_XPCS_RX_BAD_PCKTS_CNTR_LSB_REG			(0x7c)
+#define MV_XPCS_RX_BAD_PCKTS_CNTR_LSB_RXBADPCKTCNTRLSB_OFFS		0
+#define MV_XPCS_RX_BAD_PCKTS_CNTR_LSB_RXBADPCKTCNTRLSB_MASK    \
+		(0x0000ffff << \
+		MV_XPCS_RX_BAD_PCKTS_CNTR_LSB_RXBADPCKTCNTRLSB_OFFS)
+
+/* Rx Bad Packets Counter MSB */
+#define MV_XPCS_RX_BAD_PCKTS_CNTR_MSB_REG			(0x80)
+#define MV_XPCS_RX_BAD_PCKTS_CNTR_MSB_RXBADPCKTCNTRMSB_OFFS		0
+#define MV_XPCS_RX_BAD_PCKTS_CNTR_MSB_RXBADPCKTCNTRMSB_MASK    \
+		(0x0000ffff << \
+		MV_XPCS_RX_BAD_PCKTS_CNTR_MSB_RXBADPCKTCNTRMSB_OFFS)
+
+/* Cyclic Data 0 */
+#define MV_XPCS_CYCLIC_DATA_0_REG				(0x84)
+#define MV_XPCS_CYCLIC_DATA_0_CYCLICDATA0_OFFS		0
+#define MV_XPCS_CYCLIC_DATA_0_CYCLICDATA0_MASK    \
+		(0x000003ff << MV_XPCS_CYCLIC_DATA_0_CYCLICDATA0_OFFS)
+
+/* Cyclic Data 1 */
+#define MV_XPCS_CYCLIC_DATA_1_REG				(0x88)
+#define MV_XPCS_CYCLIC_DATA_1_CYCLICDATA1_OFFS		0
+#define MV_XPCS_CYCLIC_DATA_1_CYCLICDATA1_MASK    \
+		(0x000003ff << MV_XPCS_CYCLIC_DATA_1_CYCLICDATA1_OFFS)
+
+/* Cyclic Data 2 */
+#define MV_XPCS_CYCLIC_DATA_2_REG				(0x8c)
+#define MV_XPCS_CYCLIC_DATA_2_CYCLICDATA2_OFFS		0
+#define MV_XPCS_CYCLIC_DATA_2_CYCLICDATA2_MASK    \
+		(0x000003ff << MV_XPCS_CYCLIC_DATA_2_CYCLICDATA2_OFFS)
+
+/* Cyclic Data 3 */
+#define MV_XPCS_CYCLIC_DATA_3_REG				(0x90)
+#define MV_XPCS_CYCLIC_DATA_3_CYCLICDATA3_OFFS		0
+#define MV_XPCS_CYCLIC_DATA_3_CYCLICDATA3_MASK    \
+		(0x000003ff << MV_XPCS_CYCLIC_DATA_3_CYCLICDATA3_OFFS)
+
+/*************/
+/*SERDES REGS  */
+/*************/
+
+#define MV_SERDES_CFG_0_REG			(0x00)
+
+#define MV_SERDES_CFG_0_PU_PLL_OFFS		1
+#define MV_SERDES_CFG_0_PU_PLL_MASK		(0x00000001 << \
+			MV_SERDES_CFG_0_PU_PLL_OFFS)
+#define MV_SERDES_CFG_0_RX_PLL_OFFS		11
+#define MV_SERDES_CFG_0_RX_PLL_MASK		(0x00000001 << \
+			MV_SERDES_CFG_0_RX_PLL_OFFS)
+#define MV_SERDES_CFG_0_TX_PLL_OFFS		12
+#define MV_SERDES_CFG_0_TX_PLL_MASK		(0x00000001 << \
+			MV_SERDES_CFG_0_TX_PLL_OFFS)
+#define MV_SERDES_CFG_0_MEDIA_MODE_OFFS		15
+#define MV_SERDES_CFG_0_MEDIA_MODE_MASK		(0x00000001 << \
+			MV_SERDES_CFG_0_MEDIA_MODE_OFFS)
+
+#define MV_SERDES_CFG_1_REG			(0x04)
+#define MV_SERDES_CFG_1_ANALOG_RESET_OFFS	3
+#define MV_SERDES_CFG_1_ANALOG_RESET_MASK    \
+		(0x00000001 << MV_SERDES_CFG_1_ANALOG_RESET_OFFS)
+
+#define MV_SERDES_CFG_1_CORE_RESET_OFFS		5
+#define MV_SERDES_CFG_1_CORE_RESET_MASK    \
+		(0x00000001 << MV_SERDES_CFG_1_CORE_RESET_OFFS)
+
+#define MV_SERDES_CFG_1_DIGITAL_RESET_OFFS	6
+#define MV_SERDES_CFG_1_DIGITAL_RESET_MASK    \
+		(0x00000001 << MV_SERDES_CFG_1_DIGITAL_RESET_OFFS)
+
+#define MV_SERDES_CFG_1_TX_SYNC_EN_OFFS		7
+#define MV_SERDES_CFG_1_TX_SYNC_EN_MASK    \
+		(0x00000001 << MV_SERDES_CFG_1_TX_SYNC_EN_OFFS)
+
+#define MV_SERDES_CFG_2_REG			(0x08)
+#define MV_SERDES_CFG_3_REG			(0x0c)
+#define MV_SERDES_MISC_REG			(0x14)
+
+/*************/
+/*SMI  REGS      */
+/*************/
+
+#define MV_SMI_MANAGEMENT_BUSY_OFFS					28
+#define MV_SMI_MANAGEMENT_BUSY_MASK					\
+	(0x1 << MV_SMI_MANAGEMENT_BUSY_OFFS)
+#define MV_SMI_MANAGEMENT_READ_VALID_OFFS				27
+#define MV_SMI_MANAGEMENT_READ_VALID_MASK				\
+	(0x1 << MV_SMI_MANAGEMENT_READ_VALID_OFFS)
+#define MV_SMI_MANAGEMENT_OPCODE_OFFS					26
+#define MV_SMI_MANAGEMENT_OPCODE_MASK					\
+	(0x1 << MV_SMI_MANAGEMENT_OPCODE_OFFS)
+#define MV_SMI_MANAGEMENT_REGAD_OFFS					21
+#define MV_SMI_MANAGEMENT_REGAD_MASK					\
+	(0x1F << MV_SMI_MANAGEMENT_REGAD_OFFS)
+#define MV_SMI_MANAGEMENT_PHYAD_OFFS					16
+#define MV_SMI_MANAGEMENT_PHYAD_MASK					\
+	(0x1F << MV_SMI_MANAGEMENT_PHYAD_OFFS)
+#define MV_SMI_MANAGEMENT_DATA_OFFS					0
+#define MV_SMI_MANAGEMENT_DATA_MASK					\
+	(0xFFFF << MV_SMI_MANAGEMENT_DATA_OFFS)
+
+/* SMI_MISC_CFG Register */
+#define MV_SMI_MISC_CFG_REG						(0x4)
+
+#define MV_SMI_MISC_CFG_SMI_ACCELERATE_OFFS				0
+#define MV_SMI_MISC_CFG_SMI_ACCELERATE_MASK				\
+	(0x1 << MV_SMI_MISC_CFG_SMI_ACCELERATE_OFFS)
+#define MV_SMI_MISC_CFG_SMI_FASTMDC_OFFS				1
+#define MV_SMI_MISC_CFG_SMI_FASTMDC_MASK				\
+	(0x1 << MV_SMI_MISC_CFG_SMI_FASTMDC_OFFS)
+#define MV_SMI_MISC_CFG_FAST_MDC_DIVISION_SELECTOR_OFFS			2
+#define MV_SMI_MISC_CFG_FAST_MDC_DIVISION_SELECTOR_MASK			\
+	(0x3 << MV_SMI_MISC_CFG_FAST_MDC_DIVISION_SELECTOR_OFFS)
+#define MV_SMI_MISC_CFG_ENABLE_MDIO_OUT_LATENCY_OFFS			4
+#define MV_SMI_MISC_CFG_ENABLE_MDIO_OUT_LATENCY_MASK			\
+	(0x1 << MV_SMI_MISC_CFG_ENABLE_MDIO_OUT_LATENCY_OFFS)
+#define MV_SMI_MISC_CFG_AUTOPOLLNUMOFPORTS_OFFS				5
+#define MV_SMI_MISC_CFG_AUTOPOLLNUMOFPORTS_MASK				\
+	(0x1F << MV_SMI_MISC_CFG_AUTOPOLLNUMOFPORTS_OFFS)
+#define MV_SMI_MISC_CFG_ENABLE_POLLING_OFFS				10
+#define MV_SMI_MISC_CFG_ENABLE_POLLING_MASK				\
+	(0x1 << MV_SMI_MISC_CFG_ENABLE_POLLING_OFFS)
+#define MV_SMI_MISC_CFG_INVERT_MDC_OFFS					11
+#define MV_SMI_MISC_CFG_INVERT_MDC_MASK					\
+	(0x1 << MV_SMI_MISC_CFG_INVERT_MDC_OFFS)
+
+/* PHY_AN_CFG Register */
+#define MV_SMI_PHY_AN_CFG_REG						(0x8)
+
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT0_OFFS			0
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT0_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT0_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT1_OFFS			1
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT1_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT1_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT2_OFFS			2
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT2_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT2_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT3_OFFS			3
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT3_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT3_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT4_OFFS			4
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT4_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT4_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT5_OFFS			5
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT5_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT5_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT6_OFFS			6
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT6_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT6_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT7_OFFS			7
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT7_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT7_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT8_OFFS			8
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT8_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT8_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT9_OFFS			9
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT9_MASK			\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT9_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT10_OFFS		10
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT10_MASK		\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT10_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT11_OFFS		11
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT11_MASK		\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT11_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT12_OFFS		12
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT12_MASK		\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT12_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT13_OFFS		13
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT13_MASK		\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT13_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT14_OFFS		14
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT14_MASK		\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT14_OFFS)
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT15_OFFS		15
+#define MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT15_MASK		\
+	(0x1 << MV_SMI_PHY_AN_CFG_AUTOMEDIA_SELECTEN_PORT15_OFFS)
+#define MV_SMI_PHY_AN_CFG_SKIPSWRESET_SMI_OFFS				16
+#define MV_SMI_PHY_AN_CFG_SKIPSWRESET_SMI_MASK				\
+	(0x1 << MV_SMI_PHY_AN_CFG_SKIPSWRESET_SMI_OFFS)
+#define MV_SMI_PHY_AN_CFG_STOP_AUTONEGSMI_OFFS				17
+#define MV_SMI_PHY_AN_CFG_STOP_AUTONEGSMI_MASK				\
+	(0x1 << MV_SMI_PHY_AN_CFG_STOP_AUTONEGSMI_OFFS)
+#define MV_SMI_PHY_AN_CFG_MASTERSMI_OFFS				18
+#define MV_SMI_PHY_AN_CFG_MASTERSMI_MASK				\
+	(0x1 << MV_SMI_PHY_AN_CFG_MASTERSMI_OFFS)
+#define MV_SMI_PHY_AN_CFG_SGMIIINBANDFCEN_OFFS				19
+#define MV_SMI_PHY_AN_CFG_SGMIIINBANDFCEN_MASK				\
+	(0x1 << MV_SMI_PHY_AN_CFG_SGMIIINBANDFCEN_OFFS)
+#define MV_SMI_PHY_AN_CFG_FCADVSETFIBER_OFFS				20
+#define MV_SMI_PHY_AN_CFG_FCADVSETFIBER_MASK				\
+	(0x1 << MV_SMI_PHY_AN_CFG_FCADVSETFIBER_OFFS)
+
+/* PHY_ADDRESS_REGISTER0 Register */
+#define MV_SMI_PHY_ADDRESS_REG(n)				(0xC + 0x4 * n)
+#define MV_SMI_PHY_ADDRESS_PHYAD_OFFS					0
+#define MV_SMI_PHY_ADDRESS_PHYAD_MASK					\
+	(0x1F << MV_SMI_PHY_ADDRESS_PHYAD_OFFS)
+
+/*************/
+/*   MIB  REGS    */
+/*************/
+
+/* GMAC_MIB Counters register definitions */
+#define MV_MIB_GOOD_OCTETS_RECEIVED_LOW		0x0
+#define MV_MIB_GOOD_OCTETS_RECEIVED_HIGH	0x4
+#define MV_MIB_BAD_OCTETS_RECEIVED		0x8
+#define MV_MIB_CRC_ERRORS_SENT			0xc
+#define MV_MIB_UNICAST_FRAMES_RECEIVED		0x10
+/* Reserved					0x14 */
+#define MV_MIB_BROADCAST_FRAMES_RECEIVED	0x18
+#define MV_MIB_MULTICAST_FRAMES_RECEIVED	0x1c
+#define MV_MIB_FRAMES_64_OCTETS			0x20
+#define MV_MIB_FRAMES_65_TO_127_OCTETS		0x24
+#define MV_MIB_FRAMES_128_TO_255_OCTETS		0x28
+#define MV_MIB_FRAMES_256_TO_511_OCTETS		0x2c
+#define MV_MIB_FRAMES_512_TO_1023_OCTETS	0x30
+#define MV_MIB_FRAMES_1024_TO_MAX_OCTETS	0x34
+#define MV_MIB_GOOD_OCTETS_SENT_LOW		0x38
+#define MV_MIB_GOOD_OCTETS_SENT_HIGH		0x3c
+#define MV_MIB_UNICAST_FRAMES_SENT		0x40
+/* Reserved					0x44 */
+#define MV_MIB_MULTICAST_FRAMES_SENT		0x48
+#define MV_MIB_BROADCAST_FRAMES_SENT		0x4c
+/* Reserved					0x50 */
+#define MV_MIB_FC_SENT				0x54
+#define MV_MIB_FC_RECEIVED			0x58
+#define MV_MIB_RX_FIFO_OVERRUN			0x5c
+#define MV_MIB_UNDERSIZE_RECEIVED		0x60
+#define MV_MIB_FRAGMENTS_RECEIVED		0x64
+#define MV_MIB_OVERSIZE_RECEIVED		0x68
+#define MV_MIB_JABBER_RECEIVED			0x6c
+#define MV_MIB_MAC_RECEIVE_ERROR		0x70
+#define MV_MIB_BAD_CRC_EVENT			0x74
+#define MV_MIB_COLLISION			0x78
+#define MV_MIB_LATE_COLLISION			0x7c
+
+/*************/
+/*   PTP  REGS    */
+/*************/
+
+/* Ptp General Control */
+#define MV_PTP_GENERAL_CTRL_REG					(0x0808)
+#define MV_PTP_GENERAL_CTRL_PTP_UNIT_ENABLE_OFFS		0
+#define MV_PTP_GENERAL_CTRL_PTP_UNIT_ENABLE_MASK    \
+		(0x00000001 << MV_PTP_GENERAL_CTRL_PTP_UNIT_ENABLE_OFFS)
+
+#define MV_PTP_GENERAL_CTRL_PTP_RESET_OFFS		1
+#define MV_PTP_GENERAL_CTRL_PTP_RESET_MASK    \
+		(0x00000001 << MV_PTP_GENERAL_CTRL_PTP_RESET_OFFS)
+
+#define MV_PTP_GENERAL_CTRL_INTERFACE_WIDTH_SELECT_OFFS		2
+#define MV_PTP_GENERAL_CTRL_INTERFACE_WIDTH_SELECT_MASK    \
+		(0x00000003 << MV_PTP_GENERAL_CTRL_INTERFACE_WIDTH_SELECT_OFFS)
+
+#define MV_PTP_GENERAL_CTRL_CLEAR_COUNTERS_OFFS		4
+#define MV_PTP_GENERAL_CTRL_CLEAR_COUNTERS_MASK    \
+		(0x00000001 << MV_PTP_GENERAL_CTRL_CLEAR_COUNTERS_OFFS)
+
+#define MV_PTP_GENERAL_CTRL_TAI_SELECT_OFFS		5
+#define MV_PTP_GENERAL_CTRL_TAI_SELECT_MASK    \
+		(0x00000001 << MV_PTP_GENERAL_CTRL_TAI_SELECT_OFFS)
+
+#define MV_PTP_GENERAL_CTRL_TS_QUEUE_OVER_WRITE_ENABLE_OFFS		6
+#define MV_PTP_GENERAL_CTRL_TS_QUEUE_OVER_WRITE_ENABLE_MASK    \
+	(0x00000001 << MV_PTP_GENERAL_CTRL_TS_QUEUE_OVER_WRITE_ENABLE_OFFS)
+
+#define MV_PTP_GENERAL_CTRL_TAI_ACK_DELAY_OFFS		7
+#define MV_PTP_GENERAL_CTRL_TAI_ACK_DELAY_MASK    \
+		(0x0000001f << MV_PTP_GENERAL_CTRL_TAI_ACK_DELAY_OFFS)
+
+/* Ptp Tx Timestamp Queue0 Reg0 */
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_REG				(0x080c)
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_PTP_TX_TIMESTAMP_QUEUE0_VALID_OFFS 0
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_PTP_TX_TIMESTAMP_QUEUE0_VALID_MASK    \
+	(0x00000001 << \
+	MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_PTP_TX_TIMESTAMP_QUEUE0_VALID_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_QUEUE_ID_OFFS		1
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_QUEUE_ID_MASK    \
+		(0x000003ff << MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_QUEUE_ID_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TAI_SELECT_OFFS		11
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TAI_SELECT_MASK    \
+		(0x00000001 << MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TAI_SELECT_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TOD_UPDATE_FLAG_OFFS		12
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TOD_UPDATE_FLAG_MASK    \
+	(0x00000001 << MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TOD_UPDATE_FLAG_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TIMESTAMP_BITS_0_2_OFFS		13
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TIMESTAMP_BITS_0_2_MASK    \
+	(0x00000007 << MV_PTP_TX_TIMESTAMP_QUEUE0_REG0_TIMESTAMP_BITS_0_2_OFFS)
+
+/* Ptp Tx Timestamp Queue0 Reg1 */
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG1_REG				(0x0810)
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG1_TIMESTAMP_BITS_3_18_OFFS	0
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG1_TIMESTAMP_BITS_3_18_MASK    \
+	(0x0000ffff << \
+	MV_PTP_TX_TIMESTAMP_QUEUE0_REG1_TIMESTAMP_BITS_3_18_OFFS)
+
+/* Ptp Tx Timestamp Queue0 Reg2 */
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG2_REG				(0x0814)
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG2_TIMESTAMP_BITS_19_31_OFFS	0
+#define MV_PTP_TX_TIMESTAMP_QUEUE0_REG2_TIMESTAMP_BITS_19_31_MASK    \
+	(0x00001fff << \
+	MV_PTP_TX_TIMESTAMP_QUEUE0_REG2_TIMESTAMP_BITS_19_31_OFFS)
+
+/* Ptp Tx Timestamp Queue1 Reg0 */
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_REG				(0x0818)
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_PTP_TX_TIMESTAMP_QUEUE1_VALID_OFFS 0
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_PTP_TX_TIMESTAMP_QUEUE1_VALID_MASK    \
+	(0x00000001 << \
+	MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_PTP_TX_TIMESTAMP_QUEUE1_VALID_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_QUEUE_ID_OFFS		1
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_QUEUE_ID_MASK    \
+		(0x000003ff << MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_QUEUE_ID_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TAI_SELECT_OFFS		11
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TAI_SELECT_MASK    \
+		(0x00000001 << MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TAI_SELECT_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TOD_UPDATE_FLAG_OFFS		12
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TOD_UPDATE_FLAG_MASK    \
+	(0x00000001 << MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TOD_UPDATE_FLAG_OFFS)
+
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TIMESTAMP_BITS_0_2_OFFS		13
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TIMESTAMP_BITS_0_2_MASK    \
+	(0x00000007 << MV_PTP_TX_TIMESTAMP_QUEUE1_REG0_TIMESTAMP_BITS_0_2_OFFS)
+
+/* Ptp Tx Timestamp Queue1 Reg1 */
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG1_REG				(0x081c)
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG1_TIMESTAMP_BITS_3_18_OFFS	0
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG1_TIMESTAMP_BITS_3_18_MASK    \
+	(0x0000ffff << MV_PTP_TX_TIMESTAMP_QUEUE1_REG1_TIMESTAMP_BITS_3_18_OFFS)
+
+/* Ptp Tx Timestamp Queue1 Reg2 */
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG2_REG				(0x0820)
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG2_TIMESTAMP_BITS_19_31_OFFS	0
+#define MV_PTP_TX_TIMESTAMP_QUEUE1_REG2_TIMESTAMP_BITS_19_31_MASK    \
+	(0x00001fff << \
+	MV_PTP_TX_TIMESTAMP_QUEUE1_REG2_TIMESTAMP_BITS_19_31_OFFS)
+
+/* Total Ptp Packets Counter */
+#define MV_PTP_TOTAL_PTP_PCKTS_CNTR_REG				(0x0824)
+#define MV_PTP_TOTAL_PTP_PCKTS_CNTR_TOTAL_PTP_PACKETS_COUNTER_OFFS	0
+#define MV_PTP_TOTAL_PTP_PCKTS_CNTR_TOTAL_PTP_PACKETS_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_TOTAL_PTP_PCKTS_CNTR_TOTAL_PTP_PACKETS_COUNTER_OFFS)
+
+/* Ptpv1 Packet Counter */
+#define MV_PTP_PTPV1_PCKT_CNTR_REG				(0x0828)
+#define MV_PTP_PTPV1_PCKT_CNTR_PTPV1_PACKET_COUNTER_OFFS		0
+#define MV_PTP_PTPV1_PCKT_CNTR_PTPV1_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_PTPV1_PCKT_CNTR_PTPV1_PACKET_COUNTER_OFFS)
+
+/* Ptpv2 Packet Counter */
+#define MV_PTP_PTPV2_PCKT_CNTR_REG				(0x082c)
+#define MV_PTP_PTPV2_PCKT_CNTR_PTPV2_PACKET_COUNTER_OFFS		0
+#define MV_PTP_PTPV2_PCKT_CNTR_PTPV2_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_PTPV2_PCKT_CNTR_PTPV2_PACKET_COUNTER_OFFS)
+
+/* Y1731 Packet Counter */
+#define MV_PTP_Y1731_PCKT_CNTR_REG				(0x0830)
+#define MV_PTP_Y1731_PCKT_CNTR_Y1731_PACKET_COUNTER_OFFS		0
+#define MV_PTP_Y1731_PCKT_CNTR_Y1731_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_Y1731_PCKT_CNTR_Y1731_PACKET_COUNTER_OFFS)
+
+/* Ntpts Packet Counter */
+#define MV_PTP_NTPTS_PCKT_CNTR_REG				(0x0834)
+#define MV_PTP_NTPTS_PCKT_CNTR_NTPTS_PACKET_COUNTER_OFFS		0
+#define MV_PTP_NTPTS_PCKT_CNTR_NTPTS_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_NTPTS_PCKT_CNTR_NTPTS_PACKET_COUNTER_OFFS)
+
+/* Ntpreceive Packet Counter */
+#define MV_PTP_NTPRECEIVE_PCKT_CNTR_REG				(0x0838)
+#define MV_PTP_NTPRECEIVE_PCKT_CNTR_NTPRX_PACKET_COUNTER_OFFS		0
+#define MV_PTP_NTPRECEIVE_PCKT_CNTR_NTPRX_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_NTPRECEIVE_PCKT_CNTR_NTPRX_PACKET_COUNTER_OFFS)
+
+/* Ntptransmit Packet Counter */
+#define MV_PTP_NTPTRANSMIT_PCKT_CNTR_REG			(0x083c)
+#define MV_PTP_NTPTRANSMIT_PCKT_CNTR_NTPTX_PACKET_COUNTER_OFFS		0
+#define MV_PTP_NTPTRANSMIT_PCKT_CNTR_NTPTX_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_NTPTRANSMIT_PCKT_CNTR_NTPTX_PACKET_COUNTER_OFFS)
+
+/* Wamp Packet Counter */
+#define MV_PTP_WAMP_PCKT_CNTR_REG				(0x0840)
+#define MV_PTP_WAMP_PCKT_CNTR_WAMP_PACKET_COUNTER_OFFS		0
+#define MV_PTP_WAMP_PCKT_CNTR_WAMP_PACKET_COUNTER_MASK    \
+		(0x000000ff << MV_PTP_WAMP_PCKT_CNTR_WAMP_PACKET_COUNTER_OFFS)
+
+/* None Action Packet Counter */
+#define MV_PTP_NONE_ACTION_PCKT_CNTR_REG			(0x0844)
+#define MV_PTP_NONE_ACTION_PCKT_CNTR_NONE_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_NONE_ACTION_PCKT_CNTR_NONE_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_NONE_ACTION_PCKT_CNTR_NONE_ACTION_PACKET_COUNTER_OFFS)
+
+/* Forward Action Packet Counter */
+#define MV_PTP_FORWARD_ACTION_PCKT_CNTR_REG			(0x0848)
+#define MV_PTP_FORWARD_ACTION_PCKT_CNTR_FORWARD_ACTION_PACKET_COUNTER_OFFS 0
+#define MV_PTP_FORWARD_ACTION_PCKT_CNTR_FORWARD_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_FORWARD_ACTION_PCKT_CNTR_FORWARD_ACTION_PACKET_COUNTER_OFFS)
+
+/* Drop Action Packet Counter */
+#define MV_PTP_DROP_ACTION_PCKT_CNTR_REG			(0x084c)
+#define MV_PTP_DROP_ACTION_PCKT_CNTR_DROP_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_DROP_ACTION_PCKT_CNTR_DROP_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_DROP_ACTION_PCKT_CNTR_DROP_ACTION_PACKET_COUNTER_OFFS)
+
+/* Capture Action Packet Counter */
+#define MV_PTP_CAPTURE_ACTION_PCKT_CNTR_REG			(0x0850)
+#define MV_PTP_CAPTURE_ACTION_PCKT_CNTR_CAPTURE_ACTION_PACKET_COUNTER_OFFS 0
+#define MV_PTP_CAPTURE_ACTION_PCKT_CNTR_CAPTURE_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_CAPTURE_ACTION_PCKT_CNTR_CAPTURE_ACTION_PACKET_COUNTER_OFFS)
+
+/* Addtime Action Packet Counter */
+#define MV_PTP_ADDTIME_ACTION_PCKT_CNTR_REG			(0x0854)
+#define MV_PTP_ADDTIME_ACTION_PCKT_CNTR_ADDTIME_ACTION_PACKET_COUNTER_OFFS 0
+#define MV_PTP_ADDTIME_ACTION_PCKT_CNTR_ADDTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_ADDTIME_ACTION_PCKT_CNTR_ADDTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Addcorrectedtime Action Packet Counter */
+#define MV_PTP_ADDCORRECTEDTIME_ACTION_PCKT_CNTR_REG		(0x0858)
+#define MV_PTP_ADDCORRECTEDTIME_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_ADDCORRECTEDTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_ADDCORRECTEDTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Captureaddtime Action Packet Counter */
+#define MV_PTP_CAPTUREADDTIME_ACTION_PCKT_CNTR_REG		(0x085c)
+#define MV_PTP_CAPTUREADDTIME_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_CAPTUREADDTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_CAPTUREADDTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Captureaddcorrectedtime Action Packet Counter */
+#define MV_PTP_CAPTUREADDCORRECTEDTIME_ACTION_PCKT_CNTR_REG	(0x0860)
+#define MV_PTP_CAPTUREADDCORRECTEDTIME_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_CAPTUREADDCORRECTEDTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << \
+	MV_PTP_CAPTUREADDCORRECTEDTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Addingresstime Action Packet Counter */
+#define MV_PTP_ADDINGRESSTIME_ACTION_PCKT_CNTR_REG		(0x0864)
+#define MV_PTP_ADDINGRESSTIME_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_ADDINGRESSTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_ADDINGRESSTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Captureaddingresstime Action Packet Counter */
+#define MV_PTP_CAPTUREADDINGRESSTIME_ACTION_PCKT_CNTR_REG	(0x0868)
+#define MV_PTP_CAPTUREADDINGRESSTIME_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_CAPTUREADDINGRESSTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_CAPTUREADDINGRESSTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Captureingresstime Action Packet Counter */
+#define MV_PTP_CAPTUREINGRESSTIME_ACTION_PCKT_CNTR_REG		(0x086c)
+#define MV_PTP_CAPTUREINGRESSTIME_ACTION_PACKET_COUNTER_OFFS	0
+#define MV_PTP_CAPTUREINGRESSTIME_ACTION_PACKET_COUNTER_MASK    \
+	(0x000000ff << MV_PTP_CAPTUREINGRESSTIME_ACTION_PACKET_COUNTER_OFFS)
+
+/* Ntp Ptp Offset High */
+#define MV_PTP_NTP_PTP_OFFSET_HIGH_REG				(0x0870)
+#define MV_PTP_NTP_PTP_OFFSET_HIGH_PTP_NTP_OFFSET_HIGH_OFFS	0
+#define MV_PTP_NTP_PTP_OFFSET_HIGH_PTP_NTP_OFFSET_HIGH_MASK    \
+	(0x0000ffff << MV_PTP_NTP_PTP_OFFSET_HIGH_PTP_NTP_OFFSET_HIGH_OFFS)
+
+/* Ntp Ptp Offset Low */
+#define MV_PTP_NTP_PTP_OFFSET_LOW_REG				(0x0874)
+#define MV_PTP_NTP_PTP_OFFSET_LOW_PTP_NTP_OFFSET_LOW_OFFS	0
+#define MV_PTP_NTP_PTP_OFFSET_LOW_PTP_NTP_OFFSET_LOW_MASK    \
+	(0x0000ffff << MV_PTP_NTP_PTP_OFFSET_LOW_PTP_NTP_OFFSET_LOW_OFFS)
+
+/******************************************************************************/
+/* System Soft Reset 1 */
+#define MV_GOP_SOFT_RESET_1_REG		0x108
+
+#define NETC_GOP_SOFT_RESET_OFFSET		6
+#define NETC_GOP_SOFT_RESET_MASK	(0x1 << NETC_GOP_SOFT_RESET_OFFSET)
+
+/* Ports Control 0 */
+#define MV_NETCOMP_PORTS_CONTROL_0	(0x110)
+
+#define NETC_CLK_DIV_PHASE_OFFSET		31
+#define NETC_CLK_DIV_PHASE_MASK		(0x1 << NETC_CLK_DIV_PHASE_OFFSET)
+
+#define NETC_GIG_RX_DATA_SAMPLE_OFFSET		29
+#define NETC_GIG_RX_DATA_SAMPLE_MASK	(0x1 << NETC_GIG_RX_DATA_SAMPLE_OFFSET)
+
+#define NETC_BUS_WIDTH_SELECT_OFFSET		1
+#define NETC_BUS_WIDTH_SELECT_MASK	(0x1 << NETC_BUS_WIDTH_SELECT_OFFSET)
+
+#define NETC_GOP_ENABLE_OFFSET			0
+#define NETC_GOP_ENABLE_MASK		(0x1 << NETC_GOP_ENABLE_OFFSET)
+
+/* Ports Control 1 */
+#define MV_NETCOMP_PORTS_CONTROL_1	(0x114)
+
+#define NETC_PORT_GIG_RF_RESET_OFFSET(port)	(28 + port)
+#define NETC_PORT_GIG_RF_RESET_MASK(port)	\
+	(0x1 << NETC_PORT_GIG_RF_RESET_OFFSET(port))
+
+#define NETC_PORTS_ACTIVE_OFFSET(port)		(0 + port)
+#define NETC_PORTS_ACTIVE_MASK(port)	(0x1 << NETC_PORTS_ACTIVE_OFFSET(port))
+
+/* Ports Status */
+#define MV_NETCOMP_PORTS_STATUS		(0x11C)
+#define NETC_PORTS_STATUS_OFFSET(port)		(0 + port)
+#define NETC_PORTS_STATUS_MASK(port)	(0x1 << NETC_PORTS_STATUS_OFFSET(port))
+
+/* Networking Complex Control 0 */
+#define MV_NETCOMP_CONTROL_0		(0x120)
+
+#define NETC_GBE_PORT1_MII_MODE_OFFSET		2
+#define NETC_GBE_PORT1_MII_MODE_MASK	\
+	(0x1 << NETC_GBE_PORT1_MII_MODE_OFFSET)
+
+#define NETC_GBE_PORT1_SGMII_MODE_OFFSET	1
+#define NETC_GBE_PORT1_SGMII_MODE_MASK	\
+	(0x1 << NETC_GBE_PORT1_SGMII_MODE_OFFSET)
+
+#define NETC_GBE_PORT0_SGMII_MODE_OFFSET	0
+#define NETC_GBE_PORT0_SGMII_MODE_MASK	\
+	(0x1 << NETC_GBE_PORT0_SGMII_MODE_OFFSET)
+
+/* ComPhy Selector */
+#define COMMON_PHYS_SELECT_REG		(0x40)
+
+#define COMMON_PHYS_SELECT_LANE_OFFSET(lane)	(4 * lane)
+#define COMMON_PHYS_SELECT_LANE_MASK(lane)    \
+	(0xF << COMMON_PHYS_SELECT_LANE_OFFSET(lane))
+#define COMMON_PHYS_SELECT_LANE_UNCONNECTED	(0x0)
+
+#define COMMON_PHYS_SELECT_LANE_0_ETH2		(0x1)
+#define COMMON_PHYS_SELECT_LANE_1_ETH3    \
+	(0x1 << COMMON_PHYS_SELECT_LANE_OFFSET(1))
+#define COMMON_PHYS_SELECT_LANE_2_ETH0    \
+	(0x1 << COMMON_PHYS_SELECT_LANE_OFFSET(2))
+#define COMMON_PHYS_SELECT_LANE_3_ETH1    \
+	(0x1 << COMMON_PHYS_SELECT_LANE_OFFSET(3))
+#define COMMON_PHYS_SELECT_LANE_3_ETH2    \
+	(0x2 << COMMON_PHYS_SELECT_LANE_OFFSET(3))
+#define COMMON_PHYS_SELECT_LANE_4_ETH2    \
+	(0x1 << COMMON_PHYS_SELECT_LANE_OFFSET(4))
+#define COMMON_PHYS_SELECT_LANE_4_ETH0    \
+	(0x2 << COMMON_PHYS_SELECT_LANE_OFFSET(4))
+#define COMMON_PHYS_SELECT_LANE_5_ETH3    \
+	(0x1 << COMMON_PHYS_SELECT_LANE_OFFSET(5))
+#define COMMON_PHYS_SELECT_LANE_5_ETH1    \
+	(0x2 << COMMON_PHYS_SELECT_LANE_OFFSET(5))
+
+/* SD1 Control1 */
+#define SD1_CONTROL_1_REG		(0x148)
+
+#define SD1_CONTROL_XAUI_EN_OFFSET		28
+#define SD1_CONTROL_XAUI_EN_MASK	(0x1 << SD1_CONTROL_XAUI_EN_OFFSET)
+
+#define SD1_CONTROL_RXAUI0_L23_EN_OFFSET	27
+#define SD1_CONTROL_RXAUI0_L23_EN_MASK	(0x1 << \
+					SD1_CONTROL_RXAUI0_L23_EN_OFFSET)
+
+#define SD1_CONTROL_RXAUI1_L45_EN_OFFSET	26
+#define SD1_CONTROL_RXAUI1_L45_EN_MASK	(0x1 << \
+					SD1_CONTROL_RXAUI1_L45_EN_OFFSET)
+
+#endif /*_MV_GOP_HW_TYPE_H_*/
+
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
new file mode 100644
index 0000000..b27a3a2
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
@@ -0,0 +1,789 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifndef _MVPP2_H_
+#define _MVPP2_H_
+#ifdef ARMADA_390
+#include <linux/interrupt.h>
+#endif
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/string.h>
+#include <linux/log2.h>
+
+#include "mv_pp2x_hw_type.h"
+#include "mv_gop110_hw_type.h"
+
+#define MVPP2_DRIVER_NAME "mvpp2"
+#define MVPP2_DRIVER_VERSION "1.0"
+
+#define PFX			MVPP2_DRIVER_NAME ": "
+
+#if defined(CONFIG_MV_PP2_FPGA) || defined(CONFIG_MV_PP2_PALLADIUM)
+#define CONFIG_MV_PP2_POLLING
+#endif
+
+#ifdef CONFIG_MV_PP2_PALLADIUM
+#define PALAD(x)	x
+#else
+#define PALAD(x)
+#endif
+
+#ifdef CONFIG_MV_PP2_FPGA
+#define FPGA	1
+#else
+#define FPGA	0
+#endif
+
+#if defined(CONFIG_MV_PP2_PALLADIUM)
+/*These are the indexes of
+ * MVPP2_PRS_FL_IP4_UNTAG_NO_OPV4_OPTIONS/MVPP2_PRS_FL_NON_IP_UNTAG
+ * in mv_pp2x_prs_flow_id_array[]
+ */
+#define MVPP2_PRS_FL_IP4_UNTAG_NO_OPV4_OPTIONS	40
+#define MVPP2_PRS_FL_NON_IP_UNTAG_INDEX		50
+#endif
+
+#ifndef REDEFINE_DEBUG_ONCE
+
+#define REDEFINE_DEBUG_ONCE
+#if defined(DEBUG) || defined(CONFIG_MVPP2_DEBUG)
+#undef DEBUG
+#define DEBUG	1
+#define VERBOSE	1
+#else
+#define DEBUG	0
+#define VERBOSE	0
+#endif /*DEBUG||CONFIG_MVPP2_DEBUG*/
+#endif /*REDEFINE_DEBUG_ONCE*/
+
+#if VERBOSE
+#define DBG_MSG(fmt, args...)	pr_crit(PFX fmt, ## args)
+#else
+#ifdef MVPP2_DEBUG
+#define DBG_MSG(fmt, args...)	printk(fmt, ## args)
+#else
+#define DBG_MSG(fmt, args...)	pr_debug(PFX fmt, ## args)
+#endif /*MVPP2_DEBUG*/
+#endif /*VERBOSE*/
+
+#ifdef MVPP2_DEBUG
+#define STAT_DBG(c) c
+#else
+#define STAT_DBG(c)
+#endif
+
+#ifdef MV_PP22_GOP_DEBUG
+#define GOP_DEBUG(x)	x
+#else
+#define GOP_DEBUG(x)
+#endif
+
+#define MV_ETH_SKB_SHINFO_SIZE	SKB_DATA_ALIGN(sizeof(struct skb_shared_info))
+
+#define CONFIG_MV_PP2_POLLING
+
+/* START - Taken from mvPp2Commn.h, need to order TODO */
+/*--------------------------------------------------------------------*/
+/*			PP2 COMMON DEFINETIONS			      */
+/*--------------------------------------------------------------------*/
+
+#define MV_ERROR		(-1)
+#define MV_OK			(0)
+
+#define WAY_MAX			1
+
+/*--------------------------------------------------------------------*/
+/*			PP2 COMMON DEFINETIONS			      */
+/*--------------------------------------------------------------------*/
+#define NOT_IN_USE					(-1)
+#define IN_USE						(1)
+#define DWORD_BITS_LEN					32
+#define DWORD_BYTES_LEN                                 4
+#define RETRIES_EXCEEDED				15000
+#define ONE_BIT_MAX					1
+#define UNI_MAX						7
+#define ETH_PORTS_NUM					7
+
+/*--------------------------------------------------------------------*/
+/*			PNC COMMON DEFINETIONS			      */
+/*--------------------------------------------------------------------*/
+
+/* HW_BYTE_OFFS
+ * return HW byte offset in 4 bytes register
+ * _offs_: native offset (LE)
+ * LE example: HW_BYTE_OFFS(1) = 1
+ * BE example: HW_BYTE_OFFS(1) = 2
+ */
+#define SRAM_BIT_TO_BYTE(_bit_) HW_BYTE_OFFS((_bit_) / 8)
+
+#if defined(__LITTLE_ENDIAN)
+#define HW_BYTE_OFFS(_offs_) (_offs_)
+#else
+#define HW_BYTE_OFFS(_offs_) ((3 - ((_offs_) % 4)) + (((_offs_) / 4) * 4))
+#endif
+
+#define TCAM_DATA_BYTE_OFFS_LE(_offs_)		(((_offs_) - \
+	((_offs_) % 2)) * 2 + ((_offs_) % 2))
+#define TCAM_DATA_MASK_OFFS_LE(_offs_) (((_offs_) * 2) - ((_offs_) % 2)  + 2)
+
+/* TCAM_DATA_BYTE/MASK
+ * tcam data devide into 4 bytes registers
+ * each register include 2 bytes of data and 2 bytes of mask
+ * the next macros calc data/mask offset in 4 bytes register
+ * _offs_: native offset (LE) in data bytes array
+ * relevant only for TCAM data bytes
+ * used by PRS and CLS2
+ */
+#define TCAM_DATA_BYTE(_offs_) (HW_BYTE_OFFS(TCAM_DATA_BYTE_OFFS_LE(_offs_)))
+#define TCAM_DATA_MASK(_offs_) (HW_BYTE_OFFS(TCAM_DATA_MASK_OFFS_LE(_offs_)))
+
+/*END - Taken from mvPp2Commn.h, need to order TODO */
+/*--------------------------------------------------------------------*/
+
+#define __FILENAME__ (strrchr(__FILE__, '/') ? \
+	strrchr(__FILE__, '/') + 1 : __FILE__)
+
+
+#ifdef MVPP2_VERBOSE
+#define MVPP2_PRINT_2LINE() \
+	pr_crit("Passed: %s(%d)\n", __FILENAME__, __LINE__)
+#define MVPP2_PRINT_LINE() \
+	pr_crit("Passed: %s(%d)\n", __FILENAME__, __LINE__)
+
+#define MVPP2_PRINT_VAR(var) \
+	pr_crit("%s(%d): "#var"=0x%lx\n", __FILENAME__, __LINE__,\
+		(unsigned long)var)
+#define MVPP2_PRINT_VAR_NAME(var, name) \
+	pr_crit("%s(%d): %s=0x%lx\n", __FILENAME__, __LINE__, name, var)
+#else
+#define MVPP2_PRINT_LINE()
+#define MVPP2_PRINT_2LINE()
+#define MVPP2_PRINT_VAR(var)
+#define MVPP2_PRINT_VAR_NAME(var, name)
+#endif
+
+/* Descriptor ring Macros */
+#define MVPP2_QUEUE_NEXT_DESC(q, index) \
+	(((index) < (q)->last_desc) ? ((index) + 1) : 0)
+
+#define MVPP2_QUEUE_DESC_PTR(q, index)                 \
+	((q)->first_desc + index)
+
+/* Various constants */
+#define MVPP2_MAX_SW_THREADS	4
+#define MVPP2_MAX_CPUS		4
+#define MVPP2_MAX_SHARED	1
+
+/* Coalescing */
+#define MVPP2_TXDONE_COAL_PKTS		32
+#define MVPP2_TXDONE_HRTIMER_PERIOD_NS	1000000UL
+#define MVPP2_TXDONE_COAL_USEC		500
+
+#define MVPP2_RX_COAL_PKTS		32
+#define MVPP2_RX_COAL_USEC		100
+
+/* BM constants */
+#define MVPP2_BM_POOLS_NUM		16
+#define MVPP2_BM_POOL_SIZE_MAX		(16 * 1024 - \
+					MVPP2_BM_POOL_PTR_ALIGN / 4)
+#define MVPP2_BM_POOL_PTR_ALIGN		128
+
+#ifdef CONFIG_MV_PP2_PALLADIUM
+#define MVPP2_BM_SHORT_BUF_NUM		256
+#define MVPP2_BM_LONG_BUF_NUM		256
+#define MVPP2_BM_JUMBO_BUF_NUM		256
+#else
+#define MVPP2_BM_SHORT_BUF_NUM		2048
+#define MVPP2_BM_LONG_BUF_NUM		1024
+#define MVPP2_BM_JUMBO_BUF_NUM		512
+#endif
+
+#define MVPP2_ALL_BUFS			0
+
+#define RX_TOTAL_SIZE(buf_size)		((buf_size) + MV_ETH_SKB_SHINFO_SIZE)
+#define RX_TRUE_SIZE(total_size)	roundup_pow_of_two(total_size)
+extern  u32 debug_param;
+
+enum mvppv2_version {
+	PPV21 = 21,
+	PPV22
+};
+
+enum mv_pp2x_queue_vector_type {
+	MVPP2_SHARED,
+	MVPP2_PRIVATE
+};
+
+enum mv_pp2x_queue_distribution_mode {
+	/* All queues are shared.
+	 * PPv2.1: this is the only supported mode.
+	 * PPv2.2: Requires (N+1) interrupts. All rx_queues are
+	 * configured on the additional interrupt.
+	 */
+	MVPP2_QDIST_SINGLE_MODE,
+	MVPP2_QDIST_MULTI_MODE	/* PPv2.2 only requires N interrupts */
+};
+
+enum mv_pp2x_cos_classifier {
+	MVPP2_COS_CLS_VLAN, /* CoS based on VLAN pri */
+	MVPP2_COS_CLS_DSCP,
+	MVPP2_COS_CLS_VLAN_DSCP, /* CoS based on VLAN pri, */
+				/*if untagged and IP, then based on DSCP */
+	MVPP2_COS_CLS_DSCP_VLAN
+};
+
+enum mv_pp2x_rss_nf_udp_mode {
+	MVPP2_RSS_NF_UDP_2T,	/* non-frag UDP packet hash value
+				* is calculated based on 2T
+				*/
+	MVPP2_RSS_NF_UDP_5T	/* non-frag UDP packet hash value
+				*is calculated based on 5T
+				*/
+};
+
+struct mv_mac_data {
+	u8			gop_index;
+	unsigned long		flags;
+	/* Whether a PHY is present, and if yes, at which address. */
+	int			phy_addr;
+	phy_interface_t		phy_mode; /* RXAUI, SGMII, etc. */
+	struct phy_device	*phy_dev;
+	struct device_node	*phy_node;
+	int			link_irq;
+	bool			force_link;
+	unsigned int		autoneg;
+	unsigned int		link;
+	unsigned int		duplex;
+	unsigned int		speed;
+};
+
+/* Masks used for pp3_emac flags */
+#define MV_EMAC_F_LINK_UP_BIT	0
+#define MV_EMAC_F_INIT_BIT	1
+#define MV_EMAC_F_SGMII2_5_BIT	2
+
+#define MV_EMAC_F_LINK_UP	(1 << MV_EMAC_F_LINK_UP_BIT)
+#define MV_EMAC_F_INIT		(1 << MV_EMAC_F_INIT_BIT)
+#define MV_EMAC_F_SGMII2_5	(1 << MV_EMAC_F_SGMII2_5_BIT)
+
+#define MVPP2_NO_LINK_IRQ	0
+
+/* Per-CPU Tx queue control */
+struct mv_pp2x_txq_pcpu {
+	int cpu;
+
+	/* Number of Tx DMA descriptors in the descriptor ring */
+	int size;
+
+	/* Number of currently used Tx DMA descriptor in the
+	 * descriptor ring
+	 */
+	int count;
+
+	/* Number of Tx DMA descriptors reserved for each CPU */
+	int reserved_num;
+
+	/* Array of transmitted skb */
+	struct sk_buff **tx_skb;
+
+	/* Array of transmitted buffers' physical addresses */
+	dma_addr_t *tx_buffs;
+
+	/* Index of last TX DMA descriptor that was inserted */
+	int txq_put_index;
+
+	/* Index of the TX DMA descriptor to be cleaned up */
+	int txq_get_index;
+};
+
+struct mv_pp2x_tx_queue {
+	/* Physical number of this Tx queue */
+	u8 id;
+
+	/* Logical number of this Tx queue */
+	u8 log_id;
+
+	/* Number of Tx DMA descriptors in the descriptor ring */
+	int size;
+
+	/* Per-CPU control of physical Tx queues */
+	struct mv_pp2x_txq_pcpu __percpu *pcpu;
+
+	u32 pkts_coal;
+
+	/* Virtual pointer to address of the Tx DMA descriptors
+	* memory_allocation
+	*/
+	void *desc_mem;
+
+	/* Virtual address of thex Tx DMA descriptors array */
+	struct mv_pp2x_tx_desc *first_desc;
+
+	/* DMA address of the Tx DMA descriptors array */
+	dma_addr_t descs_phys;
+
+	/* Index of the last Tx DMA descriptor */
+	int last_desc;
+
+	/* Index of the next Tx DMA descriptor to process */
+	int next_desc_to_proc;
+};
+
+struct mv_pp2x_aggr_tx_queue {
+	/* Physical number of this Tx queue */
+	u8 id;
+
+	/* Number of Tx DMA descriptors in the descriptor ring */
+	int size;
+
+	/* Number of currently used Tx DMA descriptor in the descriptor ring */
+	int count;
+
+	/* Virtual pointer to address of the Aggr_Tx DMA descriptors
+	* memory_allocation
+	*/
+	void *desc_mem;
+
+	/* Virtual pointer to address of the Aggr_Tx DMA descriptors array */
+	struct mv_pp2x_tx_desc *first_desc;
+
+	/* DMA address of the Tx DMA descriptors array */
+	dma_addr_t descs_phys;
+
+	/* Index of the last Tx DMA descriptor */
+	int last_desc;
+
+	/* Index of the next Tx DMA descriptor to process */
+	int next_desc_to_proc;
+};
+
+struct mv_pp2x_rx_queue {
+	/* RX queue number, in the range 0-31 for physical RXQs */
+	u8 id;
+
+	/* Port's logic RXQ number to which physical RXQ is mapped */
+	int log_id;
+
+	/* Num of rx descriptors in the rx descriptor ring */
+	int size;
+
+	u32 pkts_coal;
+	u32 time_coal;
+
+	/* Virtual pointer to address of the Rx DMA descriptors
+	* memory_allocation
+	*/
+	void *desc_mem;
+
+	/* Virtual address of the RX DMA descriptors array */
+	struct mv_pp2x_rx_desc *first_desc;
+
+	/* DMA address of the RX DMA descriptors array */
+	dma_addr_t descs_phys;
+
+	/* Index of the last RX DMA descriptor */
+	int last_desc;
+
+	/* Index of the next RX DMA descriptor to process */
+	int next_desc_to_proc;
+
+	/* ID of port to which physical RXQ is mapped */
+	int port;
+
+};
+
+struct avanta_lp_gop_hw {
+	void __iomem *lms_base;
+};
+
+struct mv_mac_unit_desc {
+	void __iomem *base;
+	u32  obj_size;
+};
+
+struct cpn110_gop_hw {
+	struct mv_mac_unit_desc gmac;
+	struct mv_mac_unit_desc xlg_mac;
+	struct mv_mac_unit_desc serdes;
+	struct mv_mac_unit_desc xmib;
+	struct mv_mac_unit_desc ptp;
+	void __iomem *smi_base;
+	void __iomem *xsmi_base;
+	void __iomem *mspg_base;
+	void __iomem *xpcs_base;
+	void __iomem *rfu1_base;
+
+#ifdef MV_PP22_GOP_DEBUG
+	static struct gop_port_ctrl gop_port_debug[MVCPN110_GOP_MAC_NUM];
+#endif
+};
+
+struct gop_hw {
+	union {
+		struct avanta_lp_gop_hw gop_alp;
+		struct cpn110_gop_hw gop_110;
+	};
+};
+
+struct mv_pp2x_hw {
+	/* Shared registers' base addresses */
+	void __iomem *base;	/* PPV22 base_address as received in
+				 *devm_ioremap_resource().
+				 */
+	void __iomem *lms_base;
+	void __iomem *cpu_base[MVPP2_MAX_CPUS];
+
+	struct gop_hw gop;
+	/* ppv22_base_address for each CPU.
+	 * PPv2.2 - cpu_base[x] = base +
+	 * cpu_index[smp_processor_id]*MV_PP2_SPACE_64K,
+	 * for non-participating CPU it is NULL.
+	 * PPv2.1 cpu_base[x] = base
+	 */
+	/* Common clocks */
+	struct clk *pp_clk;
+	struct clk *gop_clk;
+	struct clk *gop_core_clk;
+	struct clk *mg_clk;
+	struct clk *mg_core_clk;
+
+	u32 tclk;
+
+	/* PRS shadow table */
+	struct mv_pp2x_prs_shadow *prs_shadow;
+	/* PRS auxiliary table for double vlan entries control */
+	bool *prs_double_vlans;
+	/* CLS shadow info for update in running time */
+	struct mv_pp2x_cls_shadow *cls_shadow;
+	/* C2 shadow info */
+	struct mv_pp2x_c2_shadow *c2_shadow;
+};
+
+struct mv_pp2x_cos {
+	u8 cos_classifier;	/* CoS based on VLAN or DSCP */
+	u8 num_cos_queues;	/* number of queue to do CoS */
+	u8 default_cos;		/* Default CoS value for non-IP or non-VLAN */
+	u8 reserved;
+	u32 pri_map;		/* 32 bits, each nibble maps a cos_value(0~7)
+				* to a queue.
+				*/
+};
+
+struct mv_pp2x_rss {
+	u8 rss_mode; /*UDP packet */
+	u8 dflt_cpu; /*non-IP packet */
+	u8 rss_en;
+};
+
+struct mv_pp2x_param_config {
+	struct mv_pp2x_cos cos_cfg;
+	struct mv_pp2x_rss rss_cfg;
+	u8 first_bm_pool;
+	bool jumbo_pool; /* pp2 always supports 2 pools :
+			 * short=MV_DEF_256, long=MV_DEF_2K.
+			 * Param defines option to have additional pool,
+			 * jumbo=MV_DEF_10K.
+			 */
+	u8 first_sw_thread; /* The index of the first PPv2.2
+			* sub-address space for this NET_INSTANCE.
+			*/
+	u8 first_log_rxq; /* The first cos rx queue used in the port */
+	u8 cell_index; /* The cell_index of the PPv22
+			* (could be 0,1, set according to dtsi)
+			*/
+	enum mv_pp2x_queue_distribution_mode queue_mode;
+	u32 rx_cpu_map; /* The CPU that port bind, each port has a nibble
+			* indexed by port_id, nibble value is CPU id
+			*/
+};
+
+/* Shared Packet Processor resources */
+struct mv_pp2x {
+	enum mvppv2_version pp2_version; /* Redundant, consider to delete.
+					* (prevents extra pointer lookup from
+					* mv_pp2x_platform_data)
+					*/
+	struct	mv_pp2x_hw hw;
+	struct mv_pp2x_platform_data *pp2xdata;
+
+	u16 cpu_map; /* Bitmap of the participating cpu's */
+
+	struct mv_pp2x_param_config pp2_cfg;
+
+	/* List of pointers to port structures */
+	u16 num_ports;
+	struct mv_pp2x_port **port_list;
+
+	/* Aggregated TXQs */
+	u16 num_aggr_qs;
+	struct mv_pp2x_aggr_tx_queue *aggr_txqs;
+
+	/* BM pools */
+	u16 num_pools;
+	struct mv_pp2x_bm_pool *bm_pools;
+
+	/* RX flow hash indir'n table, in pp22, the table contains the
+	* CPU idx according to weight
+	*/
+	u32 rx_indir_table[MVPP22_RSS_TBL_LINE_NUM];
+};
+
+struct mv_pp2x_pcpu_stats {
+	struct	u64_stats_sync syncp;
+	u64	rx_packets;
+	u64	rx_bytes;
+	u64	tx_packets;
+	u64	tx_bytes;
+};
+
+/* Per-CPU port control */
+struct mv_pp2x_port_pcpu {
+	struct hrtimer tx_done_timer;
+#ifdef CONFIG_MV_PP2_PALLADIUM
+	struct timer_list slow_tx_done_timer;
+#endif
+	bool timer_scheduled;
+	/* Tasklet for egress finalization */
+	struct tasklet_struct tx_done_tasklet;
+};
+
+struct queue_vector {
+	unsigned int irq;
+	struct napi_struct napi;
+	enum mv_pp2x_queue_vector_type qv_type;
+	u16 sw_thread_id; /* address_space index used to
+			* retrieve interrupt_cause
+			*/
+	u16 sw_thread_mask; /* Mask for Interrupt PORT_ENABLE Register */
+	u8 first_rx_queue; /* Relative to port */
+	u8 num_rx_queues;
+	u32 pending_cause_rx; /* mask in absolute port_queues, not relative as
+			* in Ethernet Occupied Interrupt Cause (EthOccIC))
+			*/
+	struct mv_pp2x_port *parent;
+};
+
+struct mv_pp2x_port {
+	u8 id;
+
+	u8 num_irqs;
+	u32 *of_irqs;
+
+	struct mv_pp2x *priv;
+
+	struct mv_mac_data mac_data;
+	struct tasklet_struct	link_change_tasklet;
+
+	/* Per-port registers' base address */
+	void __iomem *base;
+
+	/* Index of port's first physical RXQ */
+	u8 first_rxq;
+
+	/* port's  number of rx_queues */
+	u8 num_rx_queues;
+	/* port's  number of tx_queues */
+	u8 num_tx_queues;
+
+	struct mv_pp2x_rx_queue **rxqs; /*Each Port has up tp 32 rxq_queues.*/
+	struct mv_pp2x_tx_queue **txqs;
+	struct net_device *dev;
+
+	int pkt_size; /* pkt_size determines which is pool_long:
+			* jumbo_pool or regular long_pool.
+			*/
+
+	/* Per-CPU port control */
+	struct mv_pp2x_port_pcpu __percpu *pcpu;
+	/* Flags */
+	unsigned long flags;
+
+	u16 tx_ring_size;
+	u16 rx_ring_size;
+
+	u32 tx_time_coal;
+	struct mv_pp2x_pcpu_stats __percpu *stats;
+
+	struct mv_pp2x_bm_pool *pool_long; /* Pointer to the pool_id
+					* (long or jumbo)
+					*/
+	struct mv_pp2x_bm_pool *pool_short; /* Pointer to the short pool_id */
+
+	u32 num_qvector;
+	/* q_vector is the parameter that will be passed to
+	 * mv_pp2_isr(int irq, void *dev_id=q_vector)
+	 */
+	struct queue_vector q_vector[MVPP2_MAX_CPUS + MVPP2_MAX_SHARED];
+};
+
+struct pp2x_hw_params {
+	u8 desc_queue_addr_shift;
+};
+
+struct mv_pp2x_platform_data {
+	enum mvppv2_version pp2x_ver;
+	u8 pp2x_max_port_rxqs;
+	u8 num_port_irq;
+	bool multi_addr_space;
+	bool interrupt_tx_done;
+	bool multi_hw_instance;
+	void (*mv_pp2x_rxq_short_pool_set)(struct mv_pp2x_hw *, int, int);
+	void (*mv_pp2x_rxq_long_pool_set)(struct mv_pp2x_hw *, int, int);
+	void (*mv_pp2x_port_queue_vectors_init)(struct mv_pp2x_port *);
+	void (*mv_pp2x_port_isr_rx_group_cfg)(struct mv_pp2x_port *);
+	struct pp2x_hw_params hw;
+#ifdef CONFIG_64BIT
+	uintptr_t skb_base_addr;
+	uintptr_t skb_base_mask;
+#endif
+};
+
+static inline int mv_pp2x_max_check(int value, int limit, char *name)
+{
+	if ((value < 0) || (value >= limit)) {
+		DBG_MSG("%s %d is out of range [0..%d]\n",
+			name ? name : "value", value, (limit - 1));
+		return 1;
+	}
+	return 0;
+}
+
+static inline struct mv_pp2x_port *mv_pp2x_port_struct_get(struct mv_pp2x *priv,
+							   int port)
+{
+	int i;
+
+	for (i = 0; i < priv->num_ports; i++) {
+		if (priv->port_list[i]->id == port)
+			return priv->port_list[i];
+	}
+	return NULL;
+}
+
+static inline u8 mv_pp2x_cosval_queue_map(struct mv_pp2x_port *port,
+					  u8 cos_value)
+{
+	int cos_width, cos_mask;
+
+	cos_width = ilog2(roundup_pow_of_two(
+			  port->priv->pp2_cfg.cos_cfg.num_cos_queues));
+	cos_mask  = (1 << cos_width) - 1;
+
+	return((port->priv->pp2_cfg.cos_cfg.pri_map >>
+	       (cos_value * 4)) & cos_mask);
+}
+
+static inline u8 mv_pp2x_bound_cpu_first_rxq_calc(struct mv_pp2x_port *port)
+{
+	u8 cos_width, bind_cpu;
+
+	cos_width = ilog2(roundup_pow_of_two(
+			  port->priv->pp2_cfg.cos_cfg.num_cos_queues));
+	bind_cpu = (port->priv->pp2_cfg.rx_cpu_map >> (4 * port->id)) & 0xF;
+
+	return(port->first_rxq + (bind_cpu << cos_width));
+}
+
+/* Swap RX descriptor to be BE */
+static inline void mv_pp21_rx_desc_swap(struct mv_pp2x_rx_desc *rx_desc)
+{
+	cpu_to_le32s(&rx_desc->status);
+	cpu_to_le16s(&rx_desc->rsrvd_parser);
+	cpu_to_le16s(&rx_desc->data_size);
+	cpu_to_le32s(&rx_desc->u.pp21.buf_phys_addr);
+	cpu_to_le32s(&rx_desc->u.pp21.buf_cookie);
+	cpu_to_le16s(&rx_desc->u.pp21.rsrvd_gem);
+	cpu_to_le16s(&rx_desc->u.pp21.rsrvd_l4csum);
+	cpu_to_le16s(&rx_desc->u.pp21.rsrvd_cls_info);
+	cpu_to_le32s(&rx_desc->u.pp21.rsrvd_flow_id);
+	cpu_to_le32s(&rx_desc->u.pp21.rsrvd_abs);
+}
+
+static inline void mv_pp22_rx_desc_swap(struct mv_pp2x_rx_desc *rx_desc)
+{
+	cpu_to_le32s(&rx_desc->status);
+	cpu_to_le16s(&rx_desc->rsrvd_parser);
+	cpu_to_le16s(&rx_desc->data_size);
+	cpu_to_le16s(&rx_desc->u.pp22.rsrvd_gem);
+	cpu_to_le16s(&rx_desc->u.pp22.rsrvd_l4csum);
+	cpu_to_le32s(&rx_desc->u.pp22.rsrvd_timestamp);
+	cpu_to_le64s(&rx_desc->u.pp22.buf_phys_addr_key_hash);
+	cpu_to_le64s(&rx_desc->u.pp22.buf_cookie_bm_qset_cls_info);
+}
+
+/* Swap TX descriptor to be BE */
+static inline void mv_pp21_tx_desc_swap(struct mv_pp2x_tx_desc *tx_desc)
+{
+	cpu_to_le32s(&tx_desc->command);
+	cpu_to_le16s(&tx_desc->data_size);
+	cpu_to_le32s(&tx_desc->u.pp21.buf_phys_addr);
+	cpu_to_le32s(&tx_desc->u.pp21.buf_cookie);
+	cpu_to_le32s(&tx_desc->u.pp21.rsrvd_hw_cmd[0]);
+	cpu_to_le32s(&tx_desc->u.pp21.rsrvd_hw_cmd[1]);
+	cpu_to_le32s(&tx_desc->u.pp21.rsrvd_hw_cmd[2]);
+	cpu_to_le32s(&tx_desc->u.pp21.rsrvd1);
+}
+
+static inline void mv_pp22_tx_desc_swap(struct mv_pp2x_tx_desc *tx_desc)
+{
+	cpu_to_le32s(&tx_desc->command);
+	cpu_to_le16s(&tx_desc->data_size);
+	cpu_to_le64s(&tx_desc->u.pp22.rsrvd_hw_cmd1);
+	cpu_to_le64s(&tx_desc->u.pp22.buf_phys_addr_hw_cmd2);
+	cpu_to_le64s(&tx_desc->u.pp22.buf_cookie_bm_qset_hw_cmd3);
+}
+
+struct mv_pp2x_pool_attributes {
+	char description[32];
+	int pkt_size;
+	int buf_num;
+};
+
+extern struct mv_pp2x_pool_attributes mv_pp2x_pools[];
+
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+void *mv_pp2x_vfpga_address_get(void);
+#endif
+
+void mv_pp2x_bm_bufs_free(struct mv_pp2x *priv,
+			  struct mv_pp2x_bm_pool *bm_pool, int buf_num);
+int mv_pp2x_bm_bufs_add(struct mv_pp2x_port *port,
+			struct mv_pp2x_bm_pool *bm_pool, int buf_num);
+int mv_pp2x_open(struct net_device *dev);
+int mv_pp2x_check_ringparam_valid(struct net_device *dev,
+				  struct ethtool_ringparam *ring);
+void mv_pp2x_start_dev(struct mv_pp2x_port *port);
+void mv_pp2x_stop_dev(struct mv_pp2x_port *port);
+void mv_pp2x_cleanup_rxqs(struct mv_pp2x_port *port);
+int mv_pp2x_setup_rxqs(struct mv_pp2x_port *port);
+int mv_pp2x_setup_txqs(struct mv_pp2x_port *port);
+void mv_pp2x_cleanup_txqs(struct mv_pp2x_port *port);
+void mv_pp2x_set_ethtool_ops(struct net_device *netdev);
+int mv_pp22_rss_rxfh_indir_set(struct mv_pp2x_port *port);
+int mv_pp2x_cos_classifier_set(struct mv_pp2x_port *port,
+			       enum mv_pp2x_cos_classifier cos_mode);
+int mv_pp2x_cos_classifier_get(struct mv_pp2x_port *port);
+int mv_pp2x_cos_pri_map_set(struct mv_pp2x_port *port, int cos_pri_map);
+int mv_pp2x_cos_pri_map_get(struct mv_pp2x_port *port);
+int mv_pp2x_cos_default_value_set(struct mv_pp2x_port *port, int cos_value);
+int mv_pp2x_cos_default_value_get(struct mv_pp2x_port *port);
+int mv_pp22_rss_mode_set(struct mv_pp2x_port *port, int rss_mode);
+int mv_pp22_rss_default_cpu_set(struct mv_pp2x_port *port, int default_cpu);
+#endif /*_MVPP2_H_*/
+
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.c
new file mode 100644
index 0000000..9f1a59a
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.c
@@ -0,0 +1,2017 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+#include <linux/inetdevice.h>
+#include <linux/mbus.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/cpumask.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+
+#include <linux/phy.h>
+#include <linux/clk.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include <uapi/linux/ppp_defs.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+
+#include "mv_pp2x.h"
+#include "mv_pp2x_hw.h"
+#include "mv_pp2x_debug.h"
+
+void mv_pp2x_print_reg(struct mv_pp2x_hw *hw, unsigned int reg_addr,
+		       char *reg_name)
+{
+	DBG_MSG("  %-32s: 0x%x = 0x%08x\n", reg_name, reg_addr,
+		mv_pp2x_read(hw, reg_addr));
+}
+
+void mv_pp2x_print_reg2(struct mv_pp2x_hw *hw, unsigned int reg_addr,
+			char *reg_name, unsigned int index)
+{
+	char buf[64];
+
+	sprintf(buf, "%s[%d]", reg_name, index);
+	DBG_MSG("  %-32s: 0x%x = 0x%08x\n", reg_name, reg_addr,
+		mv_pp2x_read(hw, reg_addr));
+}
+
+void mv_pp2x_bm_pool_regs(struct mv_pp2x_hw *hw, int pool)
+{
+	if (mv_pp2x_max_check(pool, MVPP2_BM_POOLS_NUM, "bm_pool"))
+		return;
+
+	DBG_MSG("\n[BM pool registers: pool=%d]\n", pool);
+	mv_pp2x_print_reg(hw, MVPP2_BM_POOL_BASE_ADDR_REG(pool),
+			  "MV_BM_POOL_BASE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_POOL_SIZE_REG(pool),
+			  "MVPP2_BM_POOL_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_POOL_READ_PTR_REG(pool),
+			  "MVPP2_BM_POOL_READ_PTR_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_POOL_PTRS_NUM_REG(pool),
+			  "MVPP2_BM_POOL_PTRS_NUM_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_BPPI_READ_PTR_REG(pool),
+			  "MVPP2_BM_BPPI_READ_PTR_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_BPPI_PTRS_NUM_REG(pool),
+			  "MVPP2_BM_BPPI_PTRS_NUM_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_POOL_CTRL_REG(pool),
+			  "MVPP2_BM_POOL_CTRL_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_INTR_CAUSE_REG(pool),
+			  "MVPP2_BM_INTR_CAUSE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_BM_INTR_MASK_REG(pool),
+			  "MVPP2_BM_INTR_MASK_REG");
+}
+EXPORT_SYMBOL(mv_pp2x_bm_pool_regs);
+
+void mv_pp2x_bm_pool_drop_count(struct mv_pp2x_hw *hw, int pool)
+{
+	if (mv_pp2x_max_check(pool, MVPP2_BM_POOLS_NUM, "bm_pool"))
+		return;
+
+	mv_pp2x_print_reg2(hw, MVPP2_BM_DROP_CNTR_REG(pool),
+			   "MVPP2_BM_DROP_CNTR_REG", pool);
+	mv_pp2x_print_reg2(hw, MVPP2_BM_MC_DROP_CNTR_REG(pool),
+			   "MVPP2_BM_MC_DROP_CNTR_REG", pool);
+}
+EXPORT_SYMBOL(mv_pp2x_bm_pool_drop_count);
+
+void mv_pp2x_pool_status(struct mv_pp2x *priv, int log_pool_num)
+{
+	struct mv_pp2x_bm_pool *bm_pool = NULL;
+	int /*buf_size,*/ total_size, i, pool;
+
+	if (mv_pp2x_max_check(log_pool_num, MVPP2_BM_SWF_POOL_OUT_OF_RANGE,
+			      "log_pool"))
+		return;
+
+	for (i = 0; i < priv->num_pools; i++) {
+		if (priv->bm_pools[i].log_id == log_pool_num) {
+			bm_pool = &priv->bm_pools[i];
+			pool = bm_pool->id;
+		}
+	}
+	if (!bm_pool) {
+		pr_err("%s: Logical BM pool %d is not initialized\n",
+		       __func__, log_pool_num);
+		return;
+	}
+
+	total_size = RX_TOTAL_SIZE(bm_pool->buf_size);
+
+	DBG_MSG(
+		"\n%12s log_pool=%d, phy_pool=%d: pkt_size=%d, buf_size=%d total_size=%d\n",
+		mv_pp2x_pools[log_pool_num].description, log_pool_num, pool,
+		bm_pool->pkt_size, bm_pool->buf_size, total_size);
+	DBG_MSG("\tcapacity=%d, buf_num=%d, in_use=%u, in_use_thresh=%u\n",
+		bm_pool->size, bm_pool->buf_num, atomic_read(&bm_pool->in_use),
+		bm_pool->in_use_thresh);
+}
+EXPORT_SYMBOL(mv_pp2x_pool_status);
+
+void mv_pp2_pool_stats_print(struct mv_pp2x *priv, int log_pool_num)
+{
+	int i, pool;
+	struct mv_pp2x_bm_pool *bm_pool = NULL;
+
+	if (mv_pp2x_max_check(log_pool_num, MVPP2_BM_SWF_POOL_OUT_OF_RANGE,
+			      "log_pool"))
+		return;
+
+	for (i = 0; i < priv->num_pools; i++) {
+		if (priv->bm_pools[i].log_id == log_pool_num) {
+			bm_pool = &priv->bm_pools[i];
+			pool = bm_pool->id;
+		}
+	}
+	if (!bm_pool) {
+		pr_err("%s: Logical BM pool %d is not initialized\n",
+		       __func__, log_pool_num);
+		return;
+	}
+
+#ifdef MVPP2_DEBUG
+	DBG_MSG("skb_alloc_oom    = %u\n", bm_pool->stats.skb_alloc_oom);
+	DBG_MSG("skb_alloc_ok     = %u\n", bm_pool->stats.skb_alloc_ok);
+	DBG_MSG("bm_put           = %u\n", bm_pool->stats.bm_put);
+	memset(&bm_pool->stats, 0, sizeof(bm_pool->stats));
+#endif /* MVPP2_DEBUG */
+}
+EXPORT_SYMBOL(mv_pp2_pool_stats_print);
+
+void mvPp2RxDmaRegsPrint(struct mv_pp2x *priv, bool print_all,
+			 int start, int stop)
+{
+	int i, num_rx_queues, result;
+	bool enabled;
+
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	num_rx_queues = (MVPP2_MAX_PORTS * priv->pp2xdata->pp2x_max_port_rxqs);
+	if (stop >= num_rx_queues || start > stop || start < 0) {
+		DBG_MSG("\nERROR: wrong inputs\n");
+		return;
+	}
+
+	DBG_MSG("\n[RX DMA regs]\n");
+	DBG_MSG("\nRXQs [0..%d], registers\n", num_rx_queues - 1);
+
+	for (i = start; i <= stop; i++) {
+		if (!print_all) {
+			result = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(i));
+			enabled = !(result & MVPP2_RXQ_DISABLE_MASK);
+		}
+		if (print_all || enabled) {
+			DBG_MSG("RXQ[%d]:\n", i);
+			mv_pp2x_print_reg(hw, MVPP2_RXQ_STATUS_REG(i),
+					  "MVPP2_RX_STATUS");
+			mv_pp2x_print_reg2(hw, MVPP2_RXQ_CONFIG_REG(i),
+					   "MVPP2_RXQ_CONFIG_REG", i);
+		}
+	}
+	DBG_MSG("\nBM pools [0..%d] registers\n", MVPP2_BM_POOLS_NUM - 1);
+	for (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {
+		if (!print_all) {
+			enabled = mv_pp2x_read(hw, MVPP2_BM_POOL_CTRL_REG(i)) &
+				MVPP2_BM_STATE_MASK;
+		}
+		if (print_all || enabled) {
+			DBG_MSG("POOL[%d]:\n", i);
+			mv_pp2x_print_reg2(hw, MVPP2_POOL_BUF_SIZE_REG(i),
+					   "MVPP2_POOL_BUF_SIZE_REG", i);
+		}
+	}
+	DBG_MSG("\nIngress ports [0..%d] registers\n", MVPP2_MAX_PORTS - 1);
+	for (i = 0; i < MVPP2_MAX_PORTS; i++) {
+		mv_pp2x_print_reg2(hw, MVPP2_RX_CTRL_REG(i),
+				   "MVPP2_RX_CTRL_REG", i);
+	}
+	DBG_MSG("\n");
+}
+EXPORT_SYMBOL(mvPp2RxDmaRegsPrint);
+
+static void mvPp2RxQueueDetailedShow(struct mv_pp2x *priv,
+				     struct mv_pp2x_rx_queue *pp_rxq)
+{
+	int i;
+	struct mv_pp2x_rx_desc *rx_desc = pp_rxq->first_desc;
+
+	for (i = 0; i < pp_rxq->size; i++) {
+		DBG_MSG("%3d. desc=%p, status=%08x, data_size=%4d",
+			i, rx_desc+i, rx_desc[i].status,
+			rx_desc[i].data_size);
+		if (priv->pp2_version == PPV21) {
+			DBG_MSG("buf_addr=%lx, buf_cookie=%p",
+				(unsigned long)
+				mv_pp21_rxdesc_phys_addr_get(rx_desc),
+				mv_pp21_rxdesc_cookie_get(rx_desc));
+		} else {
+			DBG_MSG("buf_addr=%lx, buf_cookie=%p",
+				(unsigned long)
+				mv_pp22_rxdesc_phys_addr_get(rx_desc),
+				mv_pp22_rxdesc_cookie_get(rx_desc));
+		}
+		DBG_MSG("parser_info=%03x\n", rx_desc->rsrvd_parser);
+	}
+}
+
+/* Show Port/Rxq descriptors ring */
+void mvPp2RxqShow(struct mv_pp2x *priv, int port, int rxq, int mode)
+{
+	struct mv_pp2x_port *pp_port;
+	struct mv_pp2x_rx_queue *pp_rxq;
+
+	pp_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (pp_port == NULL) {
+		DBG_MSG("port #%d is not initialized\n", port);
+		return;
+	}
+
+	if (mv_pp2x_max_check(rxq, pp_port->num_rx_queues, "logical rxq"))
+		return;
+
+	pp_rxq = pp_port->rxqs[rxq];
+
+	if (pp_rxq->first_desc == NULL) {
+		DBG_MSG("rxq #%d of port #%d is not initialized\n", rxq, port);
+		return;
+	}
+
+	DBG_MSG("\n[PPv2 RxQ show: port=%d, logical rxq=%d -> phys rxq=%d]\n",
+		port, pp_rxq->log_id, pp_rxq->id);
+
+	DBG_MSG("size=%d, pkts_coal=%d, time_coal=%d\n",
+		pp_rxq->size, pp_rxq->pkts_coal, pp_rxq->time_coal);
+
+	DBG_MSG(
+		"first_virt_addr=%p, first_dma_addr=%lx, next_rx_desc=%d, rxq_cccupied=%d, rxq_nonoccupied=%d\n",
+		pp_rxq->first_desc,
+		(unsigned long)MVPP2_DESCQ_MEM_ALIGN(pp_rxq->descs_phys),
+		pp_rxq->next_desc_to_proc,
+		mv_pp2x_rxq_received(pp_port, pp_rxq->id),
+		mv_pp2x_rxq_free(pp_port, pp_rxq->id));
+	DBG_MSG("virt_mem_area_addr=%p, dma_mem_area_addr=%lx\n",
+		pp_rxq->desc_mem, (unsigned long)pp_rxq->descs_phys);
+
+	if (mode)
+		mvPp2RxQueueDetailedShow(priv, pp_rxq);
+}
+EXPORT_SYMBOL(mvPp2RxqShow);
+
+void mvPp2PhysRxqRegs(struct mv_pp2x *pp2, int rxq)
+{
+	struct mv_pp2x_hw *hw  = &pp2->hw;
+
+	DBG_MSG("\n[PPv2 RxQ registers: global rxq=%d]\n", rxq);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_NUM_REG, rxq);
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_NUM_REG,
+			  "MVPP2_RXQ_NUM_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_DESC_ADDR_REG,
+			  "MVPP2_RXQ_DESC_ADDR_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_DESC_SIZE_REG,
+			  "MVPP2_RXQ_DESC_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_STATUS_REG(rxq),
+			  "MVPP2_RXQ_STATUS_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_THRESH_REG,
+			  "MVPP2_RXQ_THRESH_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_INDEX_REG,
+			  "MVPP2_RXQ_INDEX_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RXQ_CONFIG_REG(rxq),
+			  "MVPP2_RXQ_CONFIG_REG");
+}
+EXPORT_SYMBOL(mvPp2PhysRxqRegs);
+
+void mvPp2PortRxqRegs(struct mv_pp2x *pp2, int port, int rxq)
+{
+	int phy_rxq;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(pp2, port);
+
+	DBG_MSG("\n[PPv2 RxQ registers: port=%d, local rxq=%d]\n", port, rxq);
+
+	if (rxq >= MVPP2_MAX_RXQ)
+		return;
+
+	if (!pp2_port)
+		return;
+
+	phy_rxq = pp2_port->first_rxq + rxq;
+	mvPp2PhysRxqRegs(pp2, phy_rxq);
+}
+EXPORT_SYMBOL(mvPp2PortRxqRegs);
+
+
+void mv_pp22_isr_rx_group_regs(struct mv_pp2x *priv, int port, bool print_all)
+{
+	int val, i, num_threads, cpu_offset, cpu;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp_port;
+
+
+	pp_port = mv_pp2x_port_struct_get(priv, port);
+	if (!pp_port) {
+		DBG_MSG("Input Error\n %s", __func__);
+		return;
+	}
+
+	if (print_all)
+		num_threads = MVPP2_MAX_SW_THREADS;
+	else
+		num_threads = pp_port->num_qvector;
+
+	for (i = 0; i < num_threads; i++) {
+		DBG_MSG("\n[PPv2 RxQ GroupConfig registers: port=%d cpu=%d]",
+				port, i);
+
+		val = (port << MVPP22_ISR_RXQ_GROUP_INDEX_GROUP_OFFSET) | i;
+		mv_pp2x_write(hw, MVPP22_ISR_RXQ_GROUP_INDEX_REG, val);
+
+		mv_pp2x_print_reg(hw, MVPP22_ISR_RXQ_GROUP_INDEX_REG,
+				  "MVPP22_ISR_RXQ_GROUP_INDEX_REG");
+		mv_pp2x_print_reg(hw, MVPP22_ISR_RXQ_SUB_GROUP_CONFIG_REG,
+				  "MVPP22_ISR_RXQ_SUB_GROUP_CONFIG_REG");
+		/*reg_val = mv_pp2x_read(hw,
+		 *	MVPP22_ISR_RXQ_SUB_GROUP_CONFIG_REG);
+		 */
+		/*start_queue  = reg_val &
+		 *	MVPP22_ISR_RXQ_SUB_GROUP_STARTQ_MASK;
+		 */
+		/*sub_group_size = reg_val &
+		 *	MVPP22_ISR_RXQ_SUB_GROUP_SIZE_MASK;
+		 */
+	}
+	DBG_MSG("\n[PPv2 Port Interrupt Enable register : port=%d]\n", port);
+	mv_pp2x_print_reg(hw, MVPP2_ISR_ENABLE_REG(port),
+			  "MVPP2_ISR_ENABLE_REG");
+
+	DBG_MSG("\n[PPv2 Eth Occupied Interrupt registers: port=%d]\n", port);
+	for (i = 0; i < num_threads; i++) {
+		if (print_all)
+			cpu = i;
+		else
+			cpu = pp_port->q_vector[i].sw_thread_id;
+		cpu_offset = cpu*MVPP2_ADDR_SPACE_SIZE;
+		DBG_MSG("cpu=%d]\n", cpu);
+		mv_pp2x_print_reg(hw, cpu_offset +
+				  MVPP2_ISR_RX_TX_CAUSE_REG(port),
+				  "MVPP2_ISR_RX_TX_CAUSE_REG");
+		mv_pp2x_print_reg(hw, cpu_offset +
+				  MVPP2_ISR_RX_TX_MASK_REG(port),
+				  "MVPP2_ISR_RX_TX_MASK_REG");
+	}
+
+}
+EXPORT_SYMBOL(mv_pp22_isr_rx_group_regs);
+
+static void mvPp2TxQueueDetailedShow(struct mv_pp2x *priv,
+				     void *pp_txq, bool aggr_queue)
+{
+	int i, j, size;
+	struct mv_pp2x_tx_desc *tx_desc;
+
+	if (aggr_queue) {
+		size = ((struct mv_pp2x_aggr_tx_queue *)pp_txq)->size;
+		tx_desc = ((struct mv_pp2x_aggr_tx_queue *)pp_txq)->first_desc;
+	} else {
+		size = ((struct mv_pp2x_tx_queue *)pp_txq)->size;
+		tx_desc = ((struct mv_pp2x_tx_queue *)pp_txq)->first_desc;
+	}
+
+	for (i = 0; i < 16/*size*/; i++) {
+		DBG_MSG(
+			"%3d. desc=%p, cmd=%x, data_size=%-4d pkt_offset=%-3d, phy_txq=%d\n",
+		   i, tx_desc+i, tx_desc[i].command, tx_desc[i].data_size,
+		   tx_desc[i].packet_offset, tx_desc[i].phys_txq);
+		if (priv->pp2_version == PPV21) {
+			DBG_MSG("buf_phys_addr=%08x, buf_cookie=%08x\n",
+				tx_desc[i].u.pp21.buf_phys_addr,
+				tx_desc[i].u.pp21.buf_cookie);
+			DBG_MSG(
+				"hw_cmd[0]=%x, hw_cmd[1]=%x, hw_cmd[2]=%x, rsrvd1=%x\n",
+				tx_desc[i].u.pp21.rsrvd_hw_cmd[0],
+				tx_desc[i].u.pp21.rsrvd_hw_cmd[1],
+				tx_desc[i].u.pp21.rsrvd_hw_cmd[2],
+				tx_desc[i].u.pp21.rsrvd1);
+		} else {
+			DBG_MSG(
+				"     rsrvd_hw_cmd1=%llx, buf_phys_addr_cmd2=%llx, buf_cookie_bm_cmd3=%llx\n",
+				tx_desc[i].u.pp22.rsrvd_hw_cmd1,
+				tx_desc[i].u.pp22.buf_phys_addr_hw_cmd2,
+				tx_desc[i].u.pp22.buf_cookie_bm_qset_hw_cmd3);
+
+			for (j = 0; j < 8; j++)
+				DBG_MSG("%d:%x\n", j, *((u32 *)(tx_desc+i)+j));
+		}
+	}
+}
+
+/* Show Port/TXQ descriptors ring */
+void mvPp2TxqShow(struct mv_pp2x *priv, int port, int txq, int mode)
+{
+	struct mv_pp2x_port *pp_port;
+	struct mv_pp2x_tx_queue *pp_txq;
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	int cpu;
+
+	pp_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (pp_port == NULL) {
+		DBG_MSG("port #%d is not initialized\n", port);
+		return;
+	}
+
+	if (mv_pp2x_max_check(txq, pp_port->num_tx_queues, "logical txq"))
+		return;
+
+	pp_txq = pp_port->txqs[txq];
+
+	if (pp_txq->first_desc == NULL) {
+		DBG_MSG("txq #%d of port #%d is not initialized\n", txq, port);
+		return;
+	}
+
+	DBG_MSG("\n[PPv2 TxQ show: port=%d, logical_txq=%d]\n",
+		port, pp_txq->log_id);
+
+	DBG_MSG("physical_txq=%d, size=%d, pkts_coal=%d\n",
+		pp_txq->id, pp_txq->size, pp_txq->pkts_coal);
+
+	DBG_MSG("first_virt_addr=%p, first_dma_addr=%lx, next_tx_desc=%d\n",
+		pp_txq->first_desc,
+		(unsigned long)MVPP2_DESCQ_MEM_ALIGN(pp_txq->descs_phys),
+		pp_txq->next_desc_to_proc);
+	DBG_MSG("virt_mem_area_addr=%p, dma_mem_area_addr=%lx\n",
+		pp_txq->desc_mem, (unsigned long)pp_txq->descs_phys);
+
+	for_each_online_cpu(cpu) {
+		txq_pcpu = per_cpu_ptr(pp_txq->pcpu, cpu);
+		DBG_MSG("\n[PPv2 TxQ %d cpu=%d show:\n", txq, cpu);
+
+		DBG_MSG("cpu=%d, size=%d, count=%d reserved_num=%d\n",
+			txq_pcpu->cpu, txq_pcpu->size, txq_pcpu->count,
+			txq_pcpu->reserved_num);
+		DBG_MSG("txq_put_index=%d, txq_get_index=%d\n",
+			txq_pcpu->txq_put_index, txq_pcpu->txq_get_index);
+		DBG_MSG("tx_skb=%p, tx_buffs=%p\n",
+			txq_pcpu->tx_skb, txq_pcpu->tx_buffs);
+	}
+
+	if (mode)
+		mvPp2TxQueueDetailedShow(priv, pp_txq, 0);
+}
+EXPORT_SYMBOL(mvPp2TxqShow);
+
+/* Show CPU aggregation TXQ descriptors ring */
+void mvPp2AggrTxqShow(struct mv_pp2x *priv, int cpu, int mode)
+{
+
+	struct mv_pp2x_aggr_tx_queue *aggr_queue = NULL;
+	int i;
+
+	DBG_MSG("\n[PPv2 AggrTxQ: cpu=%d]\n", cpu);
+
+	for (i = 0; i < priv->num_aggr_qs; i++) {
+		if (priv->aggr_txqs[i].id == cpu) {
+			aggr_queue = &priv->aggr_txqs[i];
+			break;
+		}
+	}
+	if (!aggr_queue) {
+		DBG_MSG("aggr_txq for cpu #%d is not initialized\n", cpu);
+		return;
+	}
+
+	DBG_MSG("id=%d, size=%d, count=%d, next_desc=%d, pending_cntr=%d\n",
+		aggr_queue->id,
+		aggr_queue->size, aggr_queue->count,
+		aggr_queue->next_desc_to_proc,
+		mv_pp2x_aggr_desc_num_read(priv, cpu));
+
+	if (mode)
+		mvPp2TxQueueDetailedShow(priv, aggr_queue, 1);
+
+}
+EXPORT_SYMBOL(mvPp2AggrTxqShow);
+
+void mvPp2PhysTxqRegs(struct mv_pp2x *priv, int txq)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	DBG_MSG("\n[PPv2 TxQ registers: global txq=%d]\n", txq);
+
+	if (mv_pp2x_max_check(txq, MVPP2_TXQ_TOTAL_NUM, "global txq"))
+		return;
+
+	mv_pp2x_write(hw, MVPP2_TXQ_NUM_REG, txq);
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_NUM_REG,
+			  "MVPP2_TXQ_NUM_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_DESC_ADDR_LOW_REG,
+			  "MVPP2_TXQ_DESC_ADDR_LOW_REG");
+	if (priv->pp2_version == PPV22)
+		mv_pp2x_print_reg(hw, MVPP22_TXQ_DESC_ADDR_HIGH_REG,
+				  "MVPP22_TXQ_DESC_ADDR_HIGH_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_DESC_SIZE_REG,
+			  "MVPP2_TXQ_DESC_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_DESC_HWF_SIZE_REG,
+			  "MVPP2_TXQ_DESC_HWF_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_INDEX_REG,
+			  "MVPP2_TXQ_INDEX_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_PREF_BUF_REG,
+			  "MVPP2_TXQ_PREF_BUF_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_PENDING_REG,
+			  "MVPP2_TXQ_PENDING_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXQ_INT_STATUS_REG,
+			  "MVPP2_TXQ_INT_STATUS_REG");
+	if (priv->pp2_version == PPV21)
+		mv_pp2x_print_reg(hw, MVPP21_TXQ_SENT_REG(txq),
+				  "MVPP21_TXQ_SENT_REG");
+	else
+		mv_pp2x_print_reg(hw, MVPP22_TXQ_SENT_REG(txq),
+				  "MVPP22_TXQ_SENT_REG");
+}
+EXPORT_SYMBOL(mvPp2PhysTxqRegs);
+
+void mvPp2PortTxqRegs(struct mv_pp2x *priv, int port, int txq)
+{
+	struct mv_pp2x_port *pp2_port;
+
+	pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (mv_pp2x_max_check(txq, pp2_port->num_tx_queues, "port txq"))
+		return;
+
+	DBG_MSG("\n[PPv2 TxQ registers: port=%d, local txq=%d]\n", port, txq);
+
+	mvPp2PhysTxqRegs(priv, pp2_port->txqs[txq]->id);
+}
+EXPORT_SYMBOL(mvPp2PortTxqRegs);
+
+void mvPp2AggrTxqRegs(struct mv_pp2x *priv, int cpu)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	DBG_MSG("\n[PP2 Aggr TXQ registers: cpu=%d]\n", cpu);
+
+	mv_pp2x_print_reg(hw, MVPP2_AGGR_TXQ_DESC_ADDR_REG(cpu),
+			  "MVPP2_AGGR_TXQ_DESC_ADDR_REG");
+	mv_pp2x_print_reg(hw, MVPP2_AGGR_TXQ_DESC_SIZE_REG(cpu),
+			  "MVPP2_AGGR_TXQ_DESC_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_AGGR_TXQ_STATUS_REG(cpu),
+			  "MVPP2_AGGR_TXQ_STATUS_REG");
+	mv_pp2x_print_reg(hw, MVPP2_AGGR_TXQ_INDEX_REG(cpu),
+			  "MVPP2_AGGR_TXQ_INDEX_REG");
+}
+EXPORT_SYMBOL(mvPp2AggrTxqRegs);
+
+void mvPp2V1TxqDbgCntrs(struct mv_pp2x *priv, int port, int txq)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	DBG_MSG("\n------ [Port #%d txq #%d counters] -----\n", port, txq);
+	mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, MVPP2_CNT_IDX_TX(port, txq));
+	mv_pp2x_print_reg(hw, MVPP2_CNT_IDX_REG,
+			  "MVPP2_CNT_IDX_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_DESC_ENQ_REG,
+			  "MVPP2_TX_DESC_ENQ_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_DESC_ENQ_TO_DRAM_REG,
+			  "MVPP2_TX_DESC_ENQ_TO_DRAM_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_BUF_ENQ_TO_DRAM_REG,
+			  "MVPP2_TX_BUF_ENQ_TO_DRAM_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_DESC_HWF_ENQ_REG,
+			  "MVPP2_TX_DESC_HWF_ENQ_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_PKT_DQ_REG,
+			  "MVPP2_TX_PKT_DQ_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_PKT_FULLQ_DROP_REG,
+			  "MVPP2_TX_PKT_FULLQ_DROP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_PKT_EARLY_DROP_REG,
+			  "MVPP2_TX_PKT_EARLY_DROP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_PKT_BM_DROP_REG,
+			  "MVPP2_TX_PKT_BM_DROP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TX_PKT_BM_MC_DROP_REG,
+			  "MVPP2_TX_PKT_BM_MC_DROP_REG");
+}
+EXPORT_SYMBOL(mvPp2V1TxqDbgCntrs);
+
+void mvPp2V1DropCntrs(struct mv_pp2x *priv, int port)
+{
+	int q;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	DBG_MSG("\n[global drop counters]\n");
+	mv_pp2x_print_reg(hw, MVPP2_V1_OVERFLOW_MC_DROP_REG,
+			  "MV_PP2_OVERRUN_DROP_REG");
+
+	DBG_MSG("\n[Port #%d Drop counters]\n", port);
+	mv_pp2x_print_reg(hw, MV_PP2_OVERRUN_DROP_REG(port),
+			  "MV_PP2_OVERRUN_DROP_REG");
+	mv_pp2x_print_reg(hw, MV_PP2_CLS_DROP_REG(port),
+			  "MV_PP2_CLS_DROP_REG");
+
+	for (q = 0; q < pp2_port->num_tx_queues; q++) {
+		DBG_MSG("\n------ [Port #%d txp #%d txq #%d counters] -----\n",
+				port, port, q);
+		mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, MVPP2_CNT_IDX_TX(port, q));
+		mv_pp2x_print_reg(hw, MVPP2_TX_PKT_FULLQ_DROP_REG,
+				  "MV_PP2_TX_PKT_FULLQ_DROP_REG");
+		mv_pp2x_print_reg(hw, MVPP2_TX_PKT_EARLY_DROP_REG,
+				  "MV_PP2_TX_PKT_EARLY_DROP_REG");
+		mv_pp2x_print_reg(hw, MVPP2_TX_PKT_BM_DROP_REG,
+				  "MV_PP2_TX_PKT_BM_DROP_REG");
+		mv_pp2x_print_reg(hw, MVPP2_TX_PKT_BM_MC_DROP_REG,
+				  "MV_PP2_TX_PKT_BM_MC_DROP_REG");
+	}
+
+	for (q = pp2_port->first_rxq; q < (pp2_port->first_rxq +
+			pp2_port->num_rx_queues); q++) {
+		DBG_MSG("\n------ [Port #%d, rxq #%d counters] -----\n",
+			port, q);
+		mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, q);
+		mv_pp2x_print_reg(hw, MVPP2_RX_PKT_FULLQ_DROP_REG,
+				  "MV_PP2_RX_PKT_FULLQ_DROP_REG");
+		mv_pp2x_print_reg(hw, MVPP2_RX_PKT_EARLY_DROP_REG,
+				  "MV_PP2_RX_PKT_EARLY_DROP_REG");
+		mv_pp2x_print_reg(hw, MVPP2_RX_PKT_BM_DROP_REG,
+				  "MV_PP2_RX_PKT_BM_DROP_REG");
+	}
+}
+EXPORT_SYMBOL(mvPp2V1DropCntrs);
+
+void mvPp2TxRegs(struct mv_pp2x *priv)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+	int i;
+
+	DBG_MSG("\n[TX general registers]\n");
+
+	mv_pp2x_print_reg(hw, MVPP2_TX_SNOOP_REG, "MVPP2_TX_SNOOP_REG");
+	if (priv->pp2_version == PPV21) {
+		mv_pp2x_print_reg(hw, MVPP21_TX_FIFO_THRESH_REG,
+				  "MVPP21_TX_FIFO_THRESH_REG");
+	} else {
+		for (i = 0 ; i < MVPP2_MAX_PORTS; i++) {
+			mv_pp2x_print_reg(hw, MVPP22_TX_FIFO_THRESH_REG(i),
+					  "MVPP22_TX_FIFO_THRESH_REG");
+		}
+	}
+	mv_pp2x_print_reg(hw, MVPP2_TX_PORT_FLUSH_REG,
+			  "MVPP2_TX_PORT_FLUSH_REG");
+}
+EXPORT_SYMBOL(mvPp2TxRegs);
+
+void mvPp2TxSchedRegs(struct mv_pp2x *priv, int port)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+	int physTxp, txq;
+
+	physTxp = mv_pp2x_egress_port(pp2_port);
+
+	DBG_MSG("\n[TXP Scheduler registers: port=%d, physPort=%d]\n",
+			port, physTxp);
+
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, physTxp);
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG,
+			  "MV_PP2_TXP_SCHED_PORT_INDEX_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_Q_CMD_REG,
+			  "MV_PP2_TXP_SCHED_Q_CMD_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_CMD_1_REG,
+			  "MV_PP2_TXP_SCHED_CMD_1_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_FIXED_PRIO_REG,
+			  "MV_PP2_TXP_SCHED_FIXED_PRIO_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_PERIOD_REG,
+			  "MV_PP2_TXP_SCHED_PERIOD_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_MTU_REG,
+			  "MV_PP2_TXP_SCHED_MTU_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_REFILL_REG,
+			  "MV_PP2_TXP_SCHED_REFILL_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_TOKEN_SIZE_REG,
+			  "MV_PP2_TXP_SCHED_TOKEN_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_TXP_SCHED_TOKEN_CNTR_REG,
+			  "MV_PP2_TXP_SCHED_TOKEN_CNTR_REG");
+
+	for (txq = 0; txq < MVPP2_MAX_TXQ; txq++) {
+		DBG_MSG("\n[TxQ Scheduler registers: port=%d, txq=%d]\n",
+			port, txq);
+		mv_pp2x_print_reg(hw, MVPP2_TXQ_SCHED_REFILL_REG(txq),
+				  "MV_PP2_TXQ_SCHED_REFILL_REG");
+		mv_pp2x_print_reg(hw, MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq),
+				  "MV_PP2_TXQ_SCHED_TOKEN_SIZE_REG");
+		mv_pp2x_print_reg(hw, MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(txq),
+				  "MV_PP2_TXQ_SCHED_TOKEN_CNTR_REG");
+	}
+}
+EXPORT_SYMBOL(mvPp2TxSchedRegs);
+
+/* Calculate period and tokens accordingly with required rate and accuracy */
+int mvPp2RateCalc(int rate, unsigned int accuracy, unsigned int *pPeriod,
+		  unsigned int *pTokens)
+{
+	/* Calculate refill tokens and period - rate [Kbps] =
+	 * tokens [bits] * 1000 / period [usec]
+	 */
+	/* Assume:  Tclock [MHz] / BasicRefillNoOfClocks = 1
+	*/
+	unsigned int period, tokens, calc;
+
+	if (rate == 0) {
+		/* Disable traffic from the port: tokens = 0 */
+		if (pPeriod != NULL)
+			*pPeriod = 1000;
+
+		if (pTokens != NULL)
+			*pTokens = 0;
+
+		return 0;
+	}
+
+	/* Find values of "period" and "tokens" match "rate" and
+	 * "accuracy" when period is minimal
+	 */
+	for (period = 1; period <= 1000; period++) {
+		tokens = 1;
+		while (1)	{
+			calc = (tokens * 1000) / period;
+			if (((MV_ABS(calc - rate) * 100) / rate) <= accuracy) {
+				if (pPeriod != NULL)
+					*pPeriod = period;
+
+				if (pTokens != NULL)
+					*pTokens = tokens;
+
+				return 0;
+			}
+			if (calc > rate)
+				break;
+
+			tokens++;
+		}
+	}
+	return -1;
+}
+
+/* Set bandwidth limitation for TX port
+ *   rate [Kbps]    - steady state TX bandwidth limitation
+ */
+int mvPp2TxpRateSet(struct mv_pp2x *priv, int port, int rate)
+{
+	u32 regVal;
+	unsigned int tokens, period, txPortNum, accuracy = 0;
+	int status;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (port >= MVPP2_MAX_PORTS)
+		return -1;
+
+	txPortNum = mv_pp2x_egress_port(pp2_port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, txPortNum);
+
+	regVal = mv_pp2x_read(hw, MVPP2_TXP_SCHED_PERIOD_REG);
+
+	status = mvPp2RateCalc(rate, accuracy, &period, &tokens);
+	if (status != MV_OK) {
+		DBG_MSG(
+			"%s: Can't provide rate of %d [Kbps] with accuracy of %d [%%]\n",
+			__func__, rate, accuracy);
+		return status;
+	}
+	if (tokens > MVPP2_TXP_REFILL_TOKENS_MAX)
+		tokens = MVPP2_TXP_REFILL_TOKENS_MAX;
+
+	if (period > MVPP2_TXP_REFILL_PERIOD_MAX)
+		period = MVPP2_TXP_REFILL_PERIOD_MAX;
+
+	regVal = mv_pp2x_read(hw, MVPP2_TXP_SCHED_REFILL_REG);
+
+	regVal &= ~MVPP2_TXP_REFILL_TOKENS_ALL_MASK;
+	regVal |= MVPP2_TXP_REFILL_TOKENS_MASK(tokens);
+
+	regVal &= ~MVPP2_TXP_REFILL_PERIOD_ALL_MASK;
+	regVal |= MVPP2_TXP_REFILL_PERIOD_MASK(period);
+
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_REFILL_REG, regVal);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvPp2TxpRateSet);
+
+/* Set maximum burst size for TX port
+ *   burst [bytes] - number of bytes to be sent with maximum possible TX rate,
+ *                    before TX rate limitation will take place.
+ */
+int mvPp2TxpBurstSet(struct mv_pp2x *priv, int port, int burst)
+{
+	u32 size, mtu;
+	int txPortNum;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (port >= MVPP2_MAX_PORTS)
+		return -1;
+
+	txPortNum = mv_pp2x_egress_port(pp2_port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, txPortNum);
+
+	/* Calculate Token Bucket Size */
+	size = 8 * burst;
+
+	if (size > MVPP2_TXP_TOKEN_SIZE_MAX)
+		size = MVPP2_TXP_TOKEN_SIZE_MAX;
+
+	/* Token bucket size must be larger then MTU */
+	mtu = mv_pp2x_read(hw, MVPP2_TXP_SCHED_MTU_REG);
+	if (mtu > size) {
+		DBG_MSG("%s Error: Bucket size (%d bytes) < MTU (%d bytes)\n",
+			__func__, (size / 8), (mtu / 8));
+		return -1;
+	}
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, size);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvPp2TxpBurstSet);
+
+/* Set bandwidth limitation for TXQ
+ *   rate  [Kbps]  - steady state TX rate limitation
+ */
+int mvPp2TxqRateSet(struct mv_pp2x *priv, int port, int txq, int rate)
+{
+	u32		regVal;
+	unsigned int	txPortNum, period, tokens, accuracy = 0;
+	int	status;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (port >= MVPP2_MAX_PORTS)
+		return -1;
+
+	if (txq >= MVPP2_MAX_TXQ)
+		return -1;
+
+	status = mvPp2RateCalc(rate, accuracy, &period, &tokens);
+	if (status != MV_OK) {
+		DBG_MSG(
+			"%s: Can't provide rate of %d [Kbps] with accuracy of %d [%%]\n",
+			__func__, rate, accuracy);
+		return status;
+	}
+
+	txPortNum = mv_pp2x_egress_port(pp2_port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, txPortNum);
+
+	if (tokens > MVPP2_TXQ_REFILL_TOKENS_MAX)
+		tokens = MVPP2_TXQ_REFILL_TOKENS_MAX;
+
+	if (period > MVPP2_TXQ_REFILL_PERIOD_MAX)
+		period = MVPP2_TXQ_REFILL_PERIOD_MAX;
+
+	regVal = mv_pp2x_read(hw, MVPP2_TXQ_SCHED_REFILL_REG(txq));
+
+	regVal &= ~MVPP2_TXQ_REFILL_TOKENS_ALL_MASK;
+	regVal |= MVPP2_TXQ_REFILL_TOKENS_MASK(tokens);
+
+	regVal &= ~MVPP2_TXQ_REFILL_PERIOD_ALL_MASK;
+	regVal |= MVPP2_TXQ_REFILL_PERIOD_MASK(period);
+
+	mv_pp2x_write(hw, MVPP2_TXQ_SCHED_REFILL_REG(txq), regVal);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvPp2TxqRateSet);
+
+/* Set maximum burst size for TX port
+ *   burst [bytes] - number of bytes to be sent with maximum possible TX rate,
+ *                    before TX bandwidth limitation will take place.
+ */
+int mvPp2TxqBurstSet(struct mv_pp2x *priv, int port, int txq, int burst)
+{
+	u32  size, mtu;
+	int txPortNum;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (port >= MVPP2_MAX_PORTS)
+		return -1;
+
+	if (txq >= MVPP2_MAX_TXQ)
+		return -1;
+
+	txPortNum = mv_pp2x_egress_port(pp2_port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, txPortNum);
+
+	/* Calculate Tocket Bucket Size */
+	size = 8 * burst;
+
+	if (size > MVPP2_TXQ_TOKEN_SIZE_MAX)
+		size = MVPP2_TXQ_TOKEN_SIZE_MAX;
+
+	/* Tocken bucket size must be larger then MTU */
+	mtu = mv_pp2x_read(hw, MVPP2_TXP_SCHED_MTU_REG);
+	if (mtu > size) {
+		DBG_MSG(
+			"%s Error: Bucket size (%d bytes) < MTU (%d bytes)\n",
+			__func__, (size / 8), (mtu / 8));
+		return -1;
+	}
+
+	mv_pp2x_write(hw, MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq), size);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvPp2TxqBurstSet);
+
+/* Set TXQ to work in FIX priority mode */
+int mvPp2TxqFixPrioSet(struct mv_pp2x *priv, int port, int txq)
+{
+	u32 regVal;
+	int txPortNum;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (port >= MVPP2_MAX_PORTS)
+		return -1;
+
+	if (txq >= MVPP2_MAX_TXQ)
+		return -1;
+
+	txPortNum = mv_pp2x_egress_port(pp2_port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, txPortNum);
+
+	regVal = mv_pp2x_read(hw, MVPP2_TXP_SCHED_FIXED_PRIO_REG);
+	regVal |= (1 << txq);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_FIXED_PRIO_REG, regVal);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mvPp2TxqFixPrioSet);
+
+/* Set TXQ to work in WRR mode and set relative weight. */
+/*   Weight range [1..N] */
+int mvPp2TxqWrrPrioSet(struct mv_pp2x *priv, int port, int txq, int weight)
+{
+	u32 regVal, mtu, mtu_aligned, weight_min;
+	int txPortNum;
+	struct mv_pp2x_hw *hw = &priv->hw;
+	struct mv_pp2x_port *pp2_port = mv_pp2x_port_struct_get(priv, port);
+
+	if (port >= MVPP2_MAX_PORTS)
+		return -1;
+
+	if (txq >= MVPP2_MAX_TXQ)
+		return -1;
+
+	txPortNum = mv_pp2x_egress_port(pp2_port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, txPortNum);
+
+	/* Weight * 256 bytes * 8 bits must be larger then MTU [bits] */
+	mtu = mv_pp2x_read(hw, MVPP2_TXP_SCHED_MTU_REG);
+
+	/* WA for wrong Token bucket update: Set MTU value =
+	 * 3*real MTU value, now get read MTU
+	 */
+	mtu /= MV_AMPLIFY_FACTOR_MTU;
+	mtu /= MV_BIT_NUM_OF_BYTE; /* move to bytes */
+	mtu_aligned = MV_ALIGN_UP(mtu, MV_WRR_WEIGHT_UNIT);
+	weight_min = mtu_aligned / MV_WRR_WEIGHT_UNIT;
+
+	if ((weight < weight_min) || (weight > MVPP2_TXQ_WRR_WEIGHT_MAX)) {
+		DBG_MSG("%s Error: weight=%d is out of range %d...%d\n",
+			__func__, weight, weight_min,
+			MVPP2_TXQ_WRR_WEIGHT_MAX);
+		return -1;
+	}
+
+	regVal = mv_pp2x_read(hw, MVPP2_TXQ_SCHED_WRR_REG(txq));
+
+	regVal &= ~MVPP2_TXQ_WRR_WEIGHT_ALL_MASK;
+	regVal |= MVPP2_TXQ_WRR_WEIGHT_MASK(weight);
+	mv_pp2x_write(hw, MVPP2_TXQ_SCHED_WRR_REG(txq), regVal);
+
+	regVal = mv_pp2x_read(hw, MVPP2_TXP_SCHED_FIXED_PRIO_REG);
+	regVal &= ~(1 << txq);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_FIXED_PRIO_REG, regVal);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvPp2TxqWrrPrioSet);
+
+void mvPp2V1RxqDbgCntrs(struct mv_pp2x *priv, int port, int rxq)
+{
+	struct mv_pp2x_port *pp_port;
+	int phy_rxq;
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	pp_port = mv_pp2x_port_struct_get(priv, port);
+	if (pp_port)
+		phy_rxq = pp_port->first_rxq + rxq;
+	else
+		return;
+
+	DBG_MSG("\n------ [Port #%d, rxq #%d counters] -----\n", port, rxq);
+	mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, phy_rxq);
+	mv_pp2x_print_reg(hw, MVPP2_RX_PKT_FULLQ_DROP_REG,
+			  "MV_PP2_RX_PKT_FULLQ_DROP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RX_PKT_EARLY_DROP_REG,
+			  "MVPP2_V1_RX_PKT_EARLY_DROP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RX_PKT_BM_DROP_REG,
+			  "MVPP2_V1_RX_PKT_BM_DROP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RX_DESC_ENQ_REG,
+			  "MVPP2_V1_RX_DESC_ENQ_REG");
+}
+EXPORT_SYMBOL(mvPp2V1RxqDbgCntrs);
+
+void mvPp2RxFifoRegs(struct mv_pp2x_hw *hw, int port)
+{
+	DBG_MSG("\n[Port #%d RX Fifo]\n", port);
+	mv_pp2x_print_reg(hw, MVPP2_RX_DATA_FIFO_SIZE_REG(port),
+			  "MVPP2_RX_DATA_FIFO_SIZE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_RX_ATTR_FIFO_SIZE_REG(port),
+			  "MVPP2_RX_ATTR_FIFO_SIZE_REG");
+	DBG_MSG("\n[Global RX Fifo regs]\n");
+	mv_pp2x_print_reg(hw, MVPP2_RX_MIN_PKT_SIZE_REG,
+			  "MVPP2_RX_MIN_PKT_SIZE_REG");
+}
+EXPORT_SYMBOL(mvPp2RxFifoRegs);
+
+static char *mv_pp2x_prs_l2_info_str(unsigned int l2_info)
+{
+	switch (l2_info << MVPP2_PRS_RI_L2_CAST_OFFS) {
+	case MVPP2_PRS_RI_L2_UCAST:
+		return "Ucast";
+	case MVPP2_PRS_RI_L2_MCAST:
+		return "Mcast";
+	case MVPP2_PRS_RI_L2_BCAST:
+		return "Bcast";
+	default:
+		return "Unknown";
+	}
+	return NULL;
+}
+
+static char *mv_pp2x_prs_vlan_info_str(unsigned int vlan_info)
+{
+	switch (vlan_info << MVPP2_PRS_RI_VLAN_OFFS) {
+	case MVPP2_PRS_RI_VLAN_NONE:
+		return "None";
+	case MVPP2_PRS_RI_VLAN_SINGLE:
+		return "Single";
+	case MVPP2_PRS_RI_VLAN_DOUBLE:
+		return "Double";
+	case MVPP2_PRS_RI_VLAN_TRIPLE:
+		return "Triple";
+	default:
+		return "Unknown";
+	}
+	return NULL;
+}
+
+void mv_pp2x_rx_desc_print(struct mv_pp2x *priv, struct mv_pp2x_rx_desc *desc)
+{
+	int i;
+	u32 *words = (u32 *) desc;
+
+	DBG_MSG("RX desc - %p: ", desc);
+	for (i = 0; i < 8; i++)
+		DBG_MSG("%8.8x ", *words++);
+	DBG_MSG("\n");
+
+	DBG_MSG("pkt_size=%d, L3_offs=%d, IP_hlen=%d, ",
+	       desc->data_size,
+	       (desc->status & MVPP2_RXD_L3_OFFSET_MASK) >>
+			MVPP2_RXD_L3_OFFSET_OFFS,
+	       (desc->status & MVPP2_RXD_IP_HLEN_MASK) >>
+			MVPP2_RXD_IP_HLEN_OFFS);
+
+	DBG_MSG("L2=%s, ",
+		mv_pp2x_prs_l2_info_str((desc->rsrvd_parser &
+			MVPP2_RXD_L2_CAST_MASK) >> MVPP2_RXD_L2_CAST_OFFS));
+
+	DBG_MSG("VLAN=");
+	DBG_MSG("%s, ",
+		mv_pp2x_prs_vlan_info_str((desc->rsrvd_parser &
+			MVPP2_RXD_VLAN_INFO_MASK) >> MVPP2_RXD_VLAN_INFO_OFFS));
+
+	DBG_MSG("L3=");
+	if (MVPP2_RXD_L3_IS_IP4(desc->status))
+		DBG_MSG("IPv4 (hdr=%s), ",
+			MVPP2_RXD_IP4_HDR_ERR(desc->status) ? "bad" : "ok");
+	else if (MVPP2_RXD_L3_IS_IP4_OPT(desc->status))
+		DBG_MSG("IPv4 Options (hdr=%s), ",
+			MVPP2_RXD_IP4_HDR_ERR(desc->status) ? "bad" : "ok");
+	else if (MVPP2_RXD_L3_IS_IP4_OTHER(desc->status))
+		DBG_MSG("IPv4 Other (hdr=%s), ",
+			MVPP2_RXD_IP4_HDR_ERR(desc->status) ? "bad" : "ok");
+	else if (MVPP2_RXD_L3_IS_IP6(desc->status))
+		DBG_MSG("IPv6, ");
+	else if (MVPP2_RXD_L3_IS_IP6_EXT(desc->status))
+		DBG_MSG("IPv6 Ext, ");
+	else
+		DBG_MSG("Unknown, ");
+
+	if (desc->status & MVPP2_RXD_IP_FRAG_MASK)
+		DBG_MSG("Frag, ");
+
+	DBG_MSG("L4=");
+	if (MVPP2_RXD_L4_IS_TCP(desc->status))
+		DBG_MSG("TCP (csum=%s)", (desc->status &
+			MVPP2_RXD_L4_CHK_OK_MASK) ? "Ok" : "Bad");
+	else if (MVPP2_RXD_L4_IS_UDP(desc->status))
+		DBG_MSG("UDP (csum=%s)", (desc->status &
+			MVPP2_RXD_L4_CHK_OK_MASK) ? "Ok" : "Bad");
+	else
+		DBG_MSG("Unknown");
+
+	DBG_MSG("\n");
+
+	DBG_MSG("Lookup_ID=0x%x, cpu_code=0x%x\n",
+		(desc->rsrvd_parser &
+			MVPP2_RXD_LKP_ID_MASK) >> MVPP2_RXD_LKP_ID_OFFS,
+		(desc->rsrvd_parser &
+			MVPP2_RXD_CPU_CODE_MASK) >> MVPP2_RXD_CPU_CODE_OFFS);
+
+	if (priv->pp2_version == PPV22) {
+		DBG_MSG("buf_phys_addr = 0x%llx\n",
+			desc->u.pp22.buf_phys_addr_key_hash &
+			DMA_BIT_MASK(40));
+		DBG_MSG("buf_virt_addr = 0x%llx\n",
+			desc->u.pp22.buf_cookie_bm_qset_cls_info &
+			DMA_BIT_MASK(40));
+	}
+}
+
+/* Dump memory in specific format:
+ * address: X1X1X1X1 X2X2X2X2 ... X8X8X8X8
+ */
+void mv_pp2x_skb_dump(struct sk_buff *skb, int size, int access)
+{
+	int i, j;
+	void *addr = skb->head + NET_SKB_PAD;
+	uintptr_t memAddr = (uintptr_t)addr;
+
+	DBG_MSG("skb=%p, buf=%p, ksize=%d\n", skb, skb->head,
+			(int)ksize(skb->head));
+
+	if (access == 0)
+		access = 1;
+
+	if ((access != 4) && (access != 2) && (access != 1)) {
+		DBG_MSG("%d wrong access size. Access must be 1 or 2 or 4\n",
+				access);
+		return;
+	}
+	memAddr = MV_ALIGN_DOWN((uintptr_t)addr, 4);
+	size = MV_ALIGN_UP(size, 4);
+	addr = (void *)MV_ALIGN_DOWN((uintptr_t)addr, access);
+	while (size > 0) {
+		DBG_MSG("%08lx: ", memAddr);
+		i = 0;
+		/* 32 bytes in the line */
+		while (i < 32) {
+			if (memAddr >= (uintptr_t)addr) {
+				switch (access) {
+				case 1:
+					DBG_MSG("%02x ",
+						MV_MEMIO8_READ(memAddr));
+					break;
+
+				case 2:
+					DBG_MSG("%04x ",
+						MV_MEMIO16_READ(memAddr));
+					break;
+
+				case 4:
+					DBG_MSG("%08x ",
+						MV_MEMIO32_READ(memAddr));
+					break;
+				}
+			} else {
+				for (j = 0; j < (access * 2 + 1); j++)
+					DBG_MSG(" ");
+			}
+			i += access;
+			memAddr += access;
+			size -= access;
+			if (size <= 0)
+				break;
+		}
+		DBG_MSG("\n");
+	}
+}
+
+/* Wrap the API just for debug */
+int mv_pp2x_wrap_cos_mode_set(struct mv_pp2x_port *port,
+			      enum mv_pp2x_cos_classifier cos_mode)
+{
+	return mv_pp2x_cos_classifier_set(port, cos_mode);
+}
+EXPORT_SYMBOL(mv_pp2x_wrap_cos_mode_set);
+
+int mv_pp2x_wrap_cos_mode_get(struct mv_pp2x_port *port)
+{
+	return mv_pp2x_cos_classifier_get(port);
+}
+EXPORT_SYMBOL(mv_pp2x_wrap_cos_mode_get);
+
+int mv_pp2x_wrap_cos_pri_map_set(struct mv_pp2x_port *port, int cos_pri_map)
+{
+	return mv_pp2x_cos_pri_map_set(port, cos_pri_map);
+}
+EXPORT_SYMBOL(mv_pp2x_wrap_cos_pri_map_set);
+
+int mv_pp2x_wrap_cos_pri_map_get(struct mv_pp2x_port *port)
+{
+	return mv_pp2x_cos_pri_map_get(port);
+}
+EXPORT_SYMBOL(mv_pp2x_wrap_cos_pri_map_get);
+
+int mv_pp2x_wrap_cos_dflt_value_set(struct mv_pp2x_port *port, int cos_value)
+{
+	return mv_pp2x_cos_default_value_set(port, cos_value);
+}
+EXPORT_SYMBOL(mv_pp2x_wrap_cos_dflt_value_set);
+
+int mv_pp2x_wrap_cos_dflt_value_get(struct mv_pp2x_port *port)
+{
+	return mv_pp2x_cos_default_value_get(port);
+}
+EXPORT_SYMBOL(mv_pp2x_wrap_cos_dflt_value_get);
+
+int mv_pp22_wrap_rss_mode_set(struct mv_pp2x_port *port, int rss_mode)
+{
+	return mv_pp22_rss_mode_set(port, rss_mode);
+}
+EXPORT_SYMBOL(mv_pp22_wrap_rss_mode_set);
+
+int mv_pp22_wrap_rss_dflt_cpu_set(struct mv_pp2x_port *port, int default_cpu)
+{
+	return mv_pp22_rss_default_cpu_set(port, default_cpu);
+}
+EXPORT_SYMBOL(mv_pp22_wrap_rss_dflt_cpu_set);
+
+/* mv_pp2x_port_bind_cpu_set
+*  -- Bind the port to cpu when rss disabled.
+*/
+int mv_pp2x_port_bind_cpu_set(struct mv_pp2x_port *port, u8 bind_cpu)
+{
+	int ret = 0;
+	u8 bound_cpu_first_rxq;
+
+	if (port->priv->pp2_cfg.rss_cfg.rss_en) {
+		netdev_err(port->dev,
+			"cannot bind cpu to port when rss is enabled\n");
+		return -EINVAL;
+	}
+
+	if (!(port->priv->cpu_map & (1 << bind_cpu))) {
+		netdev_err(port->dev, "invalid cpu(%d)\n", bind_cpu);
+		return -EINVAL;
+	}
+
+	/* Check original cpu and new cpu is same or not */
+	if (bind_cpu != ((port->priv->pp2_cfg.rx_cpu_map >> (port->id * 4)) &
+	    0xF)) {
+		port->priv->pp2_cfg.rx_cpu_map &= (~(0xF << (port->id * 4)));
+		port->priv->pp2_cfg.rx_cpu_map |= ((bind_cpu & 0xF) <<
+						   (port->id * 4));
+		bound_cpu_first_rxq = mv_pp2x_bound_cpu_first_rxq_calc(port);
+		ret = mv_pp2x_cls_c2_rule_set(port, bound_cpu_first_rxq);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(mv_pp2x_port_bind_cpu_set);
+
+static void mv_pp2x_bm_queue_map_dump(struct mv_pp2x_hw *hw, int queue)
+{
+	unsigned int regVal, shortQset, longQset;
+
+	DBG_MSG("-------- queue #%d --------\n", queue);
+
+	mv_pp2x_write(hw, MVPP2_BM_PRIO_IDX_REG, queue);
+	regVal = mv_pp2x_read(hw, MVPP2_BM_CPU_QSET_REG);
+
+	shortQset = ((regVal & (MVPP2_BM_CPU_SHORT_QSET_MASK)) >>
+		    MVPP2_BM_CPU_SHORT_QSET_OFFS);
+	longQset = ((regVal & (MVPP2_BM_CPU_LONG_QSET_MASK)) >>
+		    MVPP2_BM_CPU_LONG_QSET_OFFS);
+	DBG_MSG("CPU SHORT QSET = 0x%02x\n", shortQset);
+	DBG_MSG("CPU LONG QSET  = 0x%02x\n", longQset);
+
+	regVal = mv_pp2x_read(hw, MVPP2_BM_HWF_QSET_REG);
+	shortQset = ((regVal & (MVPP2_BM_HWF_SHORT_QSET_MASK)) >>
+		    MVPP2_BM_HWF_SHORT_QSET_OFFS);
+	longQset = ((regVal & (MVPP2_BM_HWF_LONG_QSET_MASK)) >>
+		    MVPP2_BM_HWF_LONG_QSET_OFFS);
+	DBG_MSG("HWF SHORT QSET = 0x%02x\n", shortQset);
+	DBG_MSG("HWF LONG QSET  = 0x%02x\n", longQset);
+}
+
+static bool mv_pp2x_bm_priority_en(struct mv_pp2x_hw *hw)
+{
+	return ((mv_pp2x_read(hw, MVPP2_BM_PRIO_CTRL_REG) == 0) ? false : true);
+}
+
+void mv_pp2x_bm_queue_map_dump_all(struct mv_pp2x_hw *hw)
+{
+	int queue;
+
+	if (!mv_pp2x_bm_priority_en(hw))
+		DBG_MSG("Note: The buffers priority algorithms is disabled.\n");
+
+	for (queue = 0; queue <= MVPP2_BM_PRIO_IDX_MAX; queue++)
+		mv_pp2x_bm_queue_map_dump(hw, queue);
+}
+EXPORT_SYMBOL(mv_pp2x_bm_queue_map_dump_all);
+
+int mv_pp2x_cls_c2_qos_prio_set(struct mv_pp2x_cls_c2_qos_entry *qos, u8 pri)
+{
+	if (!qos)
+		return -EINVAL;
+
+	qos->data &= ~MVPP2_CLS2_QOS_TBL_PRI_MASK;
+	qos->data |= (((u32)pri) << MVPP2_CLS2_QOS_TBL_PRI_OFF);
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_prio_set);
+
+int mv_pp2x_cls_c2_qos_dscp_set(struct mv_pp2x_cls_c2_qos_entry *qos, u8 dscp)
+{
+	if (!qos)
+		return -EINVAL;
+
+	qos->data &= ~MVPP2_CLS2_QOS_TBL_DSCP_MASK;
+	qos->data |= (((u32)dscp) << MVPP2_CLS2_QOS_TBL_DSCP_OFF);
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_dscp_set);
+
+int mv_pp2x_cls_c2_qos_color_set(struct mv_pp2x_cls_c2_qos_entry *qos, u8 color)
+{
+	if (!qos)
+		return -EINVAL;
+
+	qos->data &= ~MVPP2_CLS2_QOS_TBL_COLOR_MASK;
+	qos->data |= (((u32)color) << MVPP2_CLS2_QOS_TBL_COLOR_OFF);
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_color_set);
+
+int mv_pp2x_cls_c2_queue_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+		int queue, int from)
+{
+	int status = 0;
+	int qHigh, qLow;
+
+	/* cmd validation in set functions */
+
+	qHigh = (queue & MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK) >>
+		MVPP2_CLS2_ACT_QOS_ATTR_QH_OFF;
+	qLow = (queue & MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK) >>
+		MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF;
+
+	status |= mv_pp2x_cls_c2_queue_low_set(c2, cmd, qLow, from);
+	status |= mv_pp2x_cls_c2_queue_high_set(c2, cmd, qHigh, from);
+
+	return status;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_queue_set);
+
+int mv_pp2x_cls_c2_mtu_set(struct mv_pp2x_cls_c2_entry *c2, int mtu_inx)
+{
+	if (mv_pp2x_ptr_validate(c2) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(mtu_inx, 0,
+	    (1 << MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_BITS) - 1) == MV_ERROR)
+		return MV_ERROR;
+
+	c2->sram.regs.hwf_attr &= ~MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_MASK;
+	c2->sram.regs.hwf_attr |= (mtu_inx <<
+				  MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_OFF);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_mtu_set);
+
+int mv_pp2x_debug_param_set(u32 param)
+{
+	debug_param = param;
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_debug_param_set);
+
+
+static int mv_pp2x_prs_hw_tcam_cnt_dump(struct mv_pp2x_hw *hw,
+					int tid, unsigned int *cnt)
+{
+	unsigned int regVal;
+
+	if (mv_pp2x_range_validate(tid, 0,
+	    MVPP2_PRS_TCAM_SRAM_SIZE - 1) == MV_ERROR)
+		return MV_ERROR;
+
+	/* write index */
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_HIT_IDX_REG, tid);
+
+	regVal = mv_pp2x_read(hw, MVPP2_PRS_TCAM_HIT_CNT_REG);
+	regVal &= MVPP2_PRS_TCAM_HIT_CNT_MASK;
+
+	if (cnt)
+		*cnt = regVal;
+	else
+		DBG_MSG("HIT COUNTER: %d\n", regVal);
+
+	return MV_OK;
+}
+
+static int mv_pp2x_prs_sw_sram_ri_dump(struct mv_pp2x_prs_entry *pe)
+{
+	unsigned int data, mask;
+	int i, bitsOffs = 0;
+	char bits[100];
+
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	mv_pp2x_prs_sw_sram_ri_get(pe, &data, &mask);
+	if (mask == 0)
+		return 0;
+
+	DBG_MSG("\n       ");
+
+	DBG_MSG("S_RI=");
+	for (i = (MVPP2_PRS_SRAM_RI_CTRL_BITS-1); i > -1 ; i--)
+		if (mask & (1 << i)) {
+			DBG_MSG("%d", ((data & (1 << i)) != 0));
+			bitsOffs += sprintf(bits + bitsOffs, "%d:", i);
+		} else
+			DBG_MSG("x");
+
+	bits[bitsOffs] = '\0';
+	DBG_MSG(" %s", bits);
+
+	return 0;
+}
+
+static int mv_pp2x_prs_sw_sram_ai_dump(struct mv_pp2x_prs_entry *pe)
+{
+	int i, bitsOffs = 0;
+	unsigned int data, mask;
+	char bits[30];
+
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	mv_pp2x_prs_sw_sram_ai_get(pe, &data, &mask);
+
+	if (mask == 0)
+		return 0;
+
+	DBG_MSG("\n       ");
+
+	DBG_MSG("S_AI=");
+	for (i = (MVPP2_PRS_SRAM_AI_CTRL_BITS-1); i > -1 ; i--)
+		if (mask & (1 << i)) {
+			DBG_MSG("%d", ((data & (1 << i)) != 0));
+			bitsOffs += sprintf(bits + bitsOffs, "%d:", i);
+		} else
+			DBG_MSG("x");
+	bits[bitsOffs] = '\0';
+	DBG_MSG(" %s", bits);
+	return 0;
+}
+
+int mv_pp2x_prs_sw_dump(struct mv_pp2x_prs_entry *pe)
+{
+	u32 op, type, lu, done, flowid;
+	int	shift, offset, i;
+
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	/* hw entry id */
+	DBG_MSG("[%4d] ", pe->index);
+
+	i = MVPP2_PRS_TCAM_WORDS - 1;
+	DBG_MSG("%1.1x ", pe->tcam.word[i--] & 0xF);
+
+	while (i >= 0)
+		DBG_MSG("%4.4x ", (pe->tcam.word[i--]) & 0xFFFF);
+
+	DBG_MSG("| ");
+
+	/*DBG_MSG(PRS_SRAM_FMT, PRS_SRAM_VAL(pe->sram.word)); */
+	DBG_MSG("%4.4x %8.8x %8.8x %8.8x", pe->sram.word[3] & 0xFFFF,
+		 pe->sram.word[2],  pe->sram.word[1],  pe->sram.word[0]);
+
+	DBG_MSG("\n       ");
+
+	i = MVPP2_PRS_TCAM_WORDS - 1;
+	DBG_MSG("%1.1x ", (pe->tcam.word[i--] >> 16) & 0xF);
+
+	while (i >= 0)
+		DBG_MSG("%4.4x ", ((pe->tcam.word[i--]) >> 16)  & 0xFFFF);
+
+	DBG_MSG("| ");
+
+	mv_pp2x_prs_sw_sram_shift_get(pe, &shift);
+	DBG_MSG("SH=%d ", shift);
+
+	mv_pp2x_prs_sw_sram_offset_get(pe, &type, &offset, &op);
+	if (offset != 0 || ((op >> MVPP2_PRS_SRAM_OP_SEL_SHIFT_BITS) != 0))
+		DBG_MSG("UDFT=%u UDFO=%d ", type, offset);
+
+	DBG_MSG("op=%u ", op);
+
+	mv_pp2x_prs_sw_sram_next_lu_get(pe, &lu);
+	DBG_MSG("LU=%u ", lu);
+
+	mv_pp2x_prs_sw_sram_lu_done_get(pe, &done);
+	DBG_MSG("%s ", done ? "DONE" : "N_DONE");
+
+	/*flow id generation bit*/
+	mv_pp2x_prs_sw_sram_flowid_gen_get(pe, &flowid);
+	DBG_MSG("%s ", flowid ? "FIDG" : "N_FIDG");
+
+	if ((pe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] & MVPP2_PRS_TCAM_INV_MASK))
+		DBG_MSG(" [inv]");
+
+	if (mv_pp2x_prs_sw_sram_ri_dump(pe))
+		return MV_ERROR;
+
+	if (mv_pp2x_prs_sw_sram_ai_dump(pe))
+		return MV_ERROR;
+
+	DBG_MSG("\n");
+
+	return 0;
+
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_dump);
+
+int mv_pp2x_prs_hw_dump(struct mv_pp2x_hw *hw)
+{
+	int index;
+	struct mv_pp2x_prs_entry pe;
+
+
+	DBG_MSG("%s\n", __func__);
+
+	for (index = 0; index < MVPP2_PRS_TCAM_SRAM_SIZE; index++) {
+		pe.index = index;
+		mv_pp2x_prs_hw_read(hw, &pe);
+		if ((pe.tcam.word[MVPP2_PRS_TCAM_INV_WORD] &
+			MVPP2_PRS_TCAM_INV_MASK) ==
+			MVPP2_PRS_TCAM_ENTRY_VALID) {
+			mv_pp2x_prs_sw_dump(&pe);
+			mv_pp2x_prs_hw_tcam_cnt_dump(hw, index, NULL);
+			DBG_MSG("-----------------------------------------\n");
+		}
+	}
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_dump);
+
+int mv_pp2x_prs_hw_regs_dump(struct mv_pp2x_hw *hw)
+{
+	int i;
+	char reg_name[100];
+
+	mv_pp2x_print_reg(hw, MVPP2_PRS_INIT_LOOKUP_REG,
+			  "MVPP2_PRS_INIT_LOOKUP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_PRS_INIT_OFFS_REG(0),
+			  "MVPP2_PRS_INIT_OFFS_0_3_REG");
+	mv_pp2x_print_reg(hw, MVPP2_PRS_INIT_OFFS_REG(4),
+			  "MVPP2_PRS_INIT_OFFS_4_7_REG");
+	mv_pp2x_print_reg(hw, MVPP2_PRS_MAX_LOOP_REG(0),
+			  "MVPP2_PRS_MAX_LOOP_0_3_REG");
+	mv_pp2x_print_reg(hw, MVPP2_PRS_MAX_LOOP_REG(4),
+			  "MVPP2_PRS_MAX_LOOP_4_7_REG");
+
+	/*mv_pp2x_print_reg(hw, MVPP2_PRS_INTR_CAUSE_REG,
+	 *		     "MVPP2_PRS_INTR_CAUSE_REG");
+	 */
+	/*mv_pp2x_print_reg(hw, MVPP2_PRS_INTR_MASK_REG,
+	 *		     "MVPP2_PRS_INTR_MASK_REG");
+	 */
+	mv_pp2x_print_reg(hw, MVPP2_PRS_TCAM_IDX_REG,
+			  "MVPP2_PRS_TCAM_IDX_REG");
+
+	for (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++) {
+		sprintf(reg_name, "MVPP2_PRS_TCAM_DATA_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_PRS_TCAM_DATA_REG(i),
+			reg_name);
+	}
+	mv_pp2x_print_reg(hw, MVPP2_PRS_SRAM_IDX_REG,
+			  "MVPP2_PRS_SRAM_IDX_REG");
+
+	for (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++) {
+		sprintf(reg_name, "MVPP2_PRS_SRAM_DATA_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_PRS_SRAM_DATA_REG(i),
+			reg_name);
+	}
+
+	mv_pp2x_print_reg(hw, MVPP2_PRS_EXP_REG,
+			  "MVPP2_PRS_EXP_REG");
+	mv_pp2x_print_reg(hw, MVPP2_PRS_TCAM_CTRL_REG,
+			  "MVPP2_PRS_TCAM_CTRL_REG");
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_regs_dump);
+
+
+int mv_pp2x_prs_hw_hits_dump(struct mv_pp2x_hw *hw)
+{
+	int index;
+	unsigned int cnt;
+	struct mv_pp2x_prs_entry pe;
+
+	for (index = 0; index < MVPP2_PRS_TCAM_SRAM_SIZE; index++) {
+		pe.index = index;
+		mv_pp2x_prs_hw_read(hw, &pe);
+		if ((pe.tcam.word[MVPP2_PRS_TCAM_INV_WORD] &
+			MVPP2_PRS_TCAM_INV_MASK) ==
+			MVPP2_PRS_TCAM_ENTRY_VALID) {
+			mv_pp2x_prs_hw_tcam_cnt_dump(hw, index, &cnt);
+			if (cnt == 0)
+				continue;
+			mv_pp2x_prs_sw_dump(&pe);
+			DBG_MSG("INDEX: %d       HITS: %d\n", index, cnt);
+			DBG_MSG("-----------------------------------------\n");
+		}
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_hits_dump);
+
+int mvPp2ClsC2QosSwDump(struct mv_pp2x_cls_c2_qos_entry *qos)
+{
+	int int32bit;
+	int status = 0;
+
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	DBG_MSG(
+	"TABLE	SEL	LINE	PRI	DSCP	COLOR	GEM_ID	QUEUE\n");
+
+	/* table id */
+	DBG_MSG("0x%2.2x\t", qos->tbl_id);
+
+	/* table sel */
+	DBG_MSG("0x%1.1x\t", qos->tbl_sel);
+
+	/* table line */
+	DBG_MSG("0x%2.2x\t", qos->tbl_line);
+
+	/* priority */
+	status |= mvPp2ClsC2QosPrioGet(qos, &int32bit);
+	DBG_MSG("0x%1.1x\t", int32bit);
+
+	/* dscp */
+	status |= mvPp2ClsC2QosDscpGet(qos, &int32bit);
+	DBG_MSG("0x%2.2x\t", int32bit);
+
+	/* color */
+	status |= mvPp2ClsC2QosColorGet(qos, &int32bit);
+	DBG_MSG("0x%1.1x\t", int32bit);
+
+	/* gem port id */
+	status |= mvPp2ClsC2QosGpidGet(qos, &int32bit);
+	DBG_MSG("0x%3.3x\t", int32bit);
+
+	/* queue */
+	status |= mvPp2ClsC2QosQueueGet(qos, &int32bit);
+	DBG_MSG("0x%2.2x", int32bit);
+
+	DBG_MSG("\n");
+
+	return status;
+}
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_qos_dscp_hw_dump(struct mv_pp2x_hw *hw)
+{
+	int tbl_id, tbl_line, int32bit;
+	struct mv_pp2x_cls_c2_qos_entry qos;
+
+	for (tbl_id = 0; tbl_id < MVPP2_CLS_C2_QOS_DSCP_TBL_NUM; tbl_id++) {
+
+		DBG_MSG("\n------------ DSCP TABLE %d ------------\n", tbl_id);
+		DBG_MSG("LINE	DSCP	COLOR	GEM_ID	QUEUE\n");
+		for (tbl_line = 0; tbl_line < MVPP2_CLS_C2_QOS_DSCP_TBL_SIZE;
+				tbl_line++) {
+			mv_pp2x_cls_c2_qos_hw_read(hw, tbl_id,
+				1/*DSCP*/, tbl_line, &qos);
+			DBG_MSG("0x%2.2x\t", qos.tbl_line);
+			mvPp2ClsC2QosDscpGet(&qos, &int32bit);
+			DBG_MSG("0x%2.2x\t", int32bit);
+			mvPp2ClsC2QosColorGet(&qos, &int32bit);
+			DBG_MSG("0x%1.1x\t", int32bit);
+			mvPp2ClsC2QosGpidGet(&qos, &int32bit);
+			DBG_MSG("0x%3.3x\t", int32bit);
+			mvPp2ClsC2QosQueueGet(&qos, &int32bit);
+			DBG_MSG("0x%2.2x", int32bit);
+			DBG_MSG("\n");
+		}
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_dscp_hw_dump);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_qos_prio_hw_dump(struct mv_pp2x_hw *hw)
+{
+	int tbl_id, tbl_line, int32bit;
+
+	struct mv_pp2x_cls_c2_qos_entry qos;
+
+	for (tbl_id = 0; tbl_id < MVPP2_CLS_C2_QOS_PRIO_TBL_NUM; tbl_id++) {
+
+		DBG_MSG("\n-------- PRIORITY TABLE %d -----------\n", tbl_id);
+		DBG_MSG("LINE	PRIO	COLOR	GEM_ID	QUEUE\n");
+
+		for (tbl_line = 0; tbl_line < MVPP2_CLS_C2_QOS_PRIO_TBL_SIZE;
+				tbl_line++) {
+			mv_pp2x_cls_c2_qos_hw_read(hw, tbl_id,
+				0/*PRIO*/, tbl_line, &qos);
+			DBG_MSG("0x%2.2x\t", qos.tbl_line);
+			mvPp2ClsC2QosPrioGet(&qos, &int32bit);
+			DBG_MSG("0x%1.1x\t", int32bit);
+			mvPp2ClsC2QosColorGet(&qos, &int32bit);
+			DBG_MSG("0x%1.1x\t", int32bit);
+			mvPp2ClsC2QosGpidGet(&qos, &int32bit);
+			DBG_MSG("0x%3.3x\t", int32bit);
+			mvPp2ClsC2QosQueueGet(&qos, &int32bit);
+			DBG_MSG("0x%2.2x", int32bit);
+			DBG_MSG("\n");
+		}
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_prio_hw_dump);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_sw_dump(struct mv_pp2x_cls_c2_entry *c2)
+{
+	int id, sel, type, gemid, low_q, high_q, color, int32bit;
+
+	if (mv_pp2x_ptr_validate(c2) == MV_ERROR)
+		return MV_ERROR;
+
+	mv_pp2x_cls_c2_sw_words_dump(c2);
+	DBG_MSG("\n");
+
+	/*------------------------------*/
+	/*	action_tbl 0x1B30	*/
+	/*------------------------------*/
+
+	id = ((c2->sram.regs.action_tbl &
+	      (MVPP2_CLS2_ACT_DATA_TBL_ID_MASK)) >>
+	       MVPP2_CLS2_ACT_DATA_TBL_ID_OFF);
+	sel = ((c2->sram.regs.action_tbl &
+	       (MVPP2_CLS2_ACT_DATA_TBL_SEL_MASK)) >>
+		MVPP2_CLS2_ACT_DATA_TBL_SEL_OFF);
+	type = ((c2->sram.regs.action_tbl &
+	       (MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_MASK)) >>
+		MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_OFF);
+	gemid = ((c2->sram.regs.action_tbl &
+		 (MVPP2_CLS2_ACT_DATA_TBL_GEM_ID_MASK)) >>
+		  MVPP2_CLS2_ACT_DATA_TBL_GEM_ID_OFF);
+	low_q = ((c2->sram.regs.action_tbl &
+		 (MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_MASK)) >>
+		  MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_OFF);
+	high_q = ((c2->sram.regs.action_tbl &
+		  (MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_MASK)) >>
+		   MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_OFF);
+	color = ((c2->sram.regs.action_tbl &
+		 (MVPP2_CLS2_ACT_DATA_TBL_COLOR_MASK)) >>
+		  MVPP2_CLS2_ACT_DATA_TBL_COLOR_OFF);
+
+	DBG_MSG("FROM_QOS_%s_TBL[%2.2d]:  ", sel ? "DSCP" : "PRI", id);
+	if (type)
+		DBG_MSG("%s	", sel ? "DSCP" : "PRIO");
+	if (color)
+		DBG_MSG("COLOR	");
+	if (gemid)
+		DBG_MSG("GEMID	");
+	if (low_q)
+		DBG_MSG("LOW_Q	");
+	if (high_q)
+		DBG_MSG("HIGH_Q	");
+	DBG_MSG("\n");
+
+	DBG_MSG("FROM_ACT_TBL:		");
+	if (type == 0)
+		DBG_MSG("%s	", sel ? "DSCP" : "PRI");
+	if (gemid == 0)
+		DBG_MSG("GEMID	");
+	if (low_q == 0)
+		DBG_MSG("LOW_Q	");
+	if (high_q == 0)
+		DBG_MSG("HIGH_Q	");
+	if (color == 0)
+		DBG_MSG("COLOR	");
+	DBG_MSG("\n\n");
+
+	/*------------------------------*/
+	/*	actions 0x1B60		*/
+	/*------------------------------*/
+
+	DBG_MSG(
+		"ACT_CMD:	COLOR	PRIO	DSCP	GEMID	LOW_Q	HIGH_Q	FWD	POLICER	FID	RSS\n");
+	DBG_MSG("			");
+
+	DBG_MSG(
+		"%1.1d\t%1.1d\t%1.1d\t%1.1d\t%1.1d\t%1.1d\t%1.1d\t%1.1d\t%1.1d\t%1.1d\t",
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_DATA_TBL_COLOR_MASK) >>
+			MVPP2_CLS2_ACT_DATA_TBL_COLOR_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_PRI_MASK) >>
+			MVPP2_CLS2_ACT_PRI_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_DSCP_MASK) >>
+			MVPP2_CLS2_ACT_DSCP_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_DATA_TBL_GEM_ID_MASK) >>
+			MVPP2_CLS2_ACT_DATA_TBL_GEM_ID_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_MASK) >>
+			MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_MASK) >>
+			MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_FRWD_MASK) >>
+			MVPP2_CLS2_ACT_FRWD_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_PLCR_MASK) >>
+			MVPP2_CLS2_ACT_PLCR_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_FLD_EN_MASK) >>
+			MVPP2_CLS2_ACT_FLD_EN_OFF),
+		((c2->sram.regs.actions &
+			MVPP2_CLS2_ACT_RSS_MASK) >>
+			MVPP2_CLS2_ACT_RSS_OFF));
+	DBG_MSG("\n\n");
+
+	/*------------------------------*/
+	/*	qos_attr 0x1B64		*/
+	/*------------------------------*/
+	DBG_MSG(
+		"ACT_ATTR:		PRIO	DSCP	GEMID	LOW_Q	HIGH_Q	QUEUE\n");
+	DBG_MSG("		");
+	/* modify priority */
+	int32bit = ((c2->sram.regs.qos_attr &
+		    MVPP2_CLS2_ACT_QOS_ATTR_PRI_MASK) >>
+		    MVPP2_CLS2_ACT_QOS_ATTR_PRI_OFF);
+	DBG_MSG("	%1.1d\t", int32bit);
+
+	/* modify dscp */
+	int32bit = ((c2->sram.regs.qos_attr &
+		    MVPP2_CLS2_ACT_QOS_ATTR_DSCP_MASK) >>
+		    MVPP2_CLS2_ACT_QOS_ATTR_DSCP_OFF);
+	DBG_MSG("%2.2d\t", int32bit);
+
+	/* modify gemportid */
+	int32bit = ((c2->sram.regs.qos_attr &
+		    MVPP2_CLS2_ACT_QOS_ATTR_GEM_MASK) >>
+		    MVPP2_CLS2_ACT_QOS_ATTR_GEM_OFF);
+	DBG_MSG("0x%4.4x\t", int32bit);
+
+	/* modify low Q */
+	int32bit = ((c2->sram.regs.qos_attr &
+		    MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK) >>
+		    MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF);
+	DBG_MSG("%1.1d\t", int32bit);
+
+	/* modify high Q */
+	int32bit = ((c2->sram.regs.qos_attr &
+		    MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK) >>
+		    MVPP2_CLS2_ACT_QOS_ATTR_QH_OFF);
+	DBG_MSG("0x%2.2x\t", int32bit);
+
+	/*modify queue*/
+	int32bit = ((c2->sram.regs.qos_attr &
+		    (MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK |
+		    MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK)));
+	int32bit >>= MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF;
+
+	DBG_MSG("0x%2.2x\t", int32bit);
+	DBG_MSG("\n\n");
+
+	/*------------------------------*/
+	/*	hwf_attr 0x1B68		*/
+	/*------------------------------*/
+	DBG_MSG("HWF_ATTR:		IPTR	DPTR	CHKSM   MTU_IDX\n");
+	DBG_MSG("			");
+
+	/* HWF modification instraction pointer */
+	int32bit = ((c2->sram.regs.hwf_attr &
+		    MVPP2_CLS2_ACT_HWF_ATTR_IPTR_MASK) >>
+		    MVPP2_CLS2_ACT_HWF_ATTR_IPTR_OFF);
+	DBG_MSG("0x%1.1x\t", int32bit);
+
+	/* HWF modification data pointer */
+	int32bit = ((c2->sram.regs.hwf_attr &
+		    MVPP2_CLS2_ACT_HWF_ATTR_DPTR_MASK) >>
+		    MVPP2_CLS2_ACT_HWF_ATTR_DPTR_OFF);
+	DBG_MSG("0x%4.4x\t", int32bit);
+
+	/* HWF modification instraction pointer */
+	int32bit = ((c2->sram.regs.hwf_attr &
+		    MVPP2_CLS2_ACT_HWF_ATTR_L4CHK_MASK) >>
+		    MVPP2_CLS2_ACT_HWF_ATTR_L4CHK_OFF);
+	DBG_MSG("%s\t", int32bit ? "ENABLE " : "DISABLE");
+
+	/* mtu index */
+	int32bit = ((c2->sram.regs.hwf_attr &
+		    MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_MASK) >>
+		    MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_OFF);
+	DBG_MSG("0x%1.1x\t", int32bit);
+	DBG_MSG("\n\n");
+
+	/*------------------------------*/
+	/*	CLSC2_ATTR2 0x1B6C	*/
+	/*------------------------------*/
+	DBG_MSG("RSS_ATTR:		RSS_EN\n");
+	DBG_MSG("			%d\n",
+		((c2->sram.regs.rss_attr &
+			MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_MASK) >>
+			MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_OFF));
+	DBG_MSG("\n");
+
+	/*------------------------------*/
+	/*	seq_attr 0x1B70		*/
+	/*------------------------------*/
+	/*PPv2.1 new feature MAS 3.14*/
+	DBG_MSG("SEQ_ATTR:		ID	MISS\n");
+	DBG_MSG("			0x%2.2x    0x%2.2x\n",
+		((c2->sram.regs.seq_attr &
+			MVPP21_CLS2_ACT_SEQ_ATTR_ID_MASK) >>
+			MVPP21_CLS2_ACT_SEQ_ATTR_ID),
+		((c2->sram.regs.seq_attr &
+			MVPP21_CLS2_ACT_SEQ_ATTR_MISS_MASK) >>
+			MVPP21_CLS2_ACT_SEQ_ATTR_MISS_OFF));
+	DBG_MSG("\n\n");
+
+	return MV_OK;
+}
+
+/*----------------------------------------------------------------------*/
+int	mv_pp2x_cls_c2_hw_dump(struct mv_pp2x_hw *hw)
+{
+	int index;
+	unsigned cnt;
+
+	struct mv_pp2x_cls_c2_entry c2;
+
+	memset(&c2, 0, sizeof(c2));
+
+	for (index = 0; index < MVPP2_CLS_C2_TCAM_SIZE; index++) {
+		mv_pp2x_cls_c2_hw_read(hw, index, &c2);
+		if (c2.inv == 0) {
+			mv_pp2x_cls_c2_sw_dump(&c2);
+			mv_pp2x_cls_c2_hit_cntr_read(hw, index, &cnt);
+			DBG_MSG("HITS: %d\n", cnt);
+			DBG_MSG("-----------------------------------------\n");
+		}
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hw_dump);
+
+/*----------------------------------------------------------------------*/
+
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.h
new file mode 100644
index 0000000..08d4db5
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_debug.h
@@ -0,0 +1,133 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifndef _MVPP2_DEBUG_H_
+#define _MVPP2_DEBUG_H_
+
+#include <linux/kernel.h>
+#include <linux/skbuff.h>
+#include <linux/platform_device.h>
+
+#define MV_AMPLIFY_FACTOR_MTU				(3)
+#define MV_BIT_NUM_OF_BYTE				(8)
+#define MV_WRR_WEIGHT_UNIT				(256)
+
+/* Macro for alignment up. For example, MV_ALIGN_UP(0x0330, 0x20) = 0x0340   */
+#define MV_ALIGN_UP(number, align) (((number) & ((align) - 1)) ? \
+				    (((number) + (align)) & ~((align) - 1)) : \
+				    (number))
+
+/* Macro for alignment down. For example, MV_ALIGN_UP(0x0330, 0x20) = 0x0320 */
+#define MV_ALIGN_DOWN(number, align) ((number) & ~((align) - 1))
+/* CPU architecture dependent 32, 16, 8 bit read/write IO addresses */
+#define MV_MEMIO32_WRITE(addr, data)    \
+	((*((unsigned int *)(addr))) = ((unsigned int)(data)))
+
+#define MV_MEMIO32_READ(addr)           \
+	((*((unsigned int *)(addr))))
+
+#define MV_MEMIO16_WRITE(addr, data)    \
+	((*((unsigned short *)(addr))) = ((unsigned short)(data)))
+
+#define MV_MEMIO16_READ(addr)           \
+	((*((unsigned short *)(addr))))
+
+#define MV_MEMIO8_WRITE(addr, data)     \
+	((*((unsigned char *)(addr))) = ((unsigned char)(data)))
+
+#define MV_MEMIO8_READ(addr)            \
+	((*((unsigned char *)(addr))))
+
+/* This macro returns absolute value                                        */
+#define MV_ABS(number)  (((int)(number) < 0) ? -(int)(number) : (int)(number))
+
+
+void mv_pp2x_print_reg(struct mv_pp2x_hw *hw, unsigned int reg_addr,
+			   char *reg_name);
+void mv_pp2x_print_reg2(struct mv_pp2x_hw *hw, unsigned int reg_addr,
+			     char *reg_name, unsigned int index);
+
+void mv_pp2x_bm_pool_regs(struct mv_pp2x_hw *hw, int pool);
+void mv_pp2x_bm_pool_drop_count(struct mv_pp2x_hw *hw, int pool);
+void mv_pp2x_pool_status(struct mv_pp2x *priv, int log_pool_num);
+void mv_pp2_pool_stats_print(struct mv_pp2x *priv, int log_pool_num);
+
+void mvPp2RxDmaRegsPrint(struct mv_pp2x *priv, bool print_all,
+			 int start, int stop);
+void mvPp2RxqShow(struct mv_pp2x *priv, int port, int rxq, int mode);
+void mvPp2PhysRxqRegs(struct mv_pp2x *pp2, int rxq);
+void mvPp2PortRxqRegs(struct mv_pp2x *pp2, int port, int rxq);
+void mv_pp22_isr_rx_group_regs(struct mv_pp2x *priv, int port, bool print_all);
+
+void mvPp2V1RxqDbgCntrs(struct mv_pp2x *priv, int port, int rxq);
+void mvPp2RxFifoRegs(struct mv_pp2x_hw *hw, int port);
+
+void mv_pp2x_rx_desc_print(struct mv_pp2x *priv, struct mv_pp2x_rx_desc *desc);
+
+void mv_pp2x_skb_dump(struct sk_buff *skb, int size, int access);
+void mvPp2TxqShow(struct mv_pp2x *priv, int port, int txq, int mode);
+void mvPp2AggrTxqShow(struct mv_pp2x *priv, int cpu, int mode);
+void mvPp2PhysTxqRegs(struct mv_pp2x *priv, int txq);
+void mvPp2PortTxqRegs(struct mv_pp2x *priv, int port, int txq);
+void mvPp2AggrTxqRegs(struct mv_pp2x *priv, int cpu);
+void mvPp2V1TxqDbgCntrs(struct mv_pp2x *priv, int port, int txq);
+void mvPp2V1DropCntrs(struct mv_pp2x *priv, int port);
+void mvPp2TxRegs(struct mv_pp2x *priv);
+void mvPp2TxSchedRegs(struct mv_pp2x *priv, int port);
+int mvPp2TxpRateSet(struct mv_pp2x *priv, int port, int rate);
+int mvPp2TxpBurstSet(struct mv_pp2x *priv, int port, int burst);
+int mvPp2TxqRateSet(struct mv_pp2x *priv, int port, int txq, int rate);
+int mvPp2TxqBurstSet(struct mv_pp2x *priv, int port, int txq, int burst);
+int mvPp2TxqFixPrioSet(struct mv_pp2x *priv, int port, int txq);
+int mvPp2TxqWrrPrioSet(struct mv_pp2x *priv, int port, int txq, int weight);
+
+int mv_pp2x_wrap_cos_mode_set(struct mv_pp2x_port *port,
+			      enum mv_pp2x_cos_classifier cos_mode);
+int mv_pp2x_wrap_cos_mode_get(struct mv_pp2x_port *port);
+int mv_pp2x_wrap_cos_pri_map_set(struct mv_pp2x_port *port, int cos_pri_map);
+int mv_pp2x_wrap_cos_pri_map_get(struct mv_pp2x_port *port);
+int mv_pp2x_wrap_cos_dflt_value_set(struct mv_pp2x_port *port, int cos_value);
+int mv_pp2x_wrap_cos_dflt_value_get(struct mv_pp2x_port *port);
+int mv_pp22_wrap_rss_mode_set(struct mv_pp2x_port *port, int rss_mode);
+int mv_pp22_wrap_rss_dflt_cpu_set(struct mv_pp2x_port *port, int default_cpu);
+int mv_pp2x_port_bind_cpu_set(struct mv_pp2x_port *port, u8 bind_cpu);
+int mv_pp2x_debug_param_set(u32 param);
+
+void mv_pp2x_bm_queue_map_dump_all(struct mv_pp2x_hw *hw);
+
+int mv_pp2x_cls_c2_qos_prio_set(struct mv_pp2x_cls_c2_qos_entry *qos, u8 pri);
+int mv_pp2x_cls_c2_qos_dscp_set(struct mv_pp2x_cls_c2_qos_entry *qos, u8 dscp);
+int mv_pp2x_cls_c2_qos_color_set(struct mv_pp2x_cls_c2_qos_entry *qos,
+				 u8 color);
+int mv_pp2x_cls_c2_queue_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+			     int queue, int from);
+int mv_pp2x_cls_c2_mtu_set(struct mv_pp2x_cls_c2_entry *c2, int mtu_inx);
+
+int mv_pp2x_prs_sw_dump(struct mv_pp2x_prs_entry *pe);
+int mv_pp2x_prs_hw_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_prs_hw_regs_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_prs_hw_hits_dump(struct mv_pp2x_hw *hw);
+
+int mv_pp2x_cls_c2_sw_dump(struct mv_pp2x_cls_c2_entry *c2);
+int mv_pp2x_cls_c2_hw_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_c2_qos_dscp_hw_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_c2_qos_prio_hw_dump(struct mv_pp2x_hw *hw);
+
+
+
+#endif /* _MVPP2_DEBUG_H_ */
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c
new file mode 100644
index 0000000..71652a9
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_ethtool.c
@@ -0,0 +1,482 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+#include <linux/inetdevice.h>
+#include <linux/mbus.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/cpumask.h>
+#include <linux/version.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/phy.h>
+#include <linux/clk.h>
+#include <uapi/linux/ppp_defs.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+
+#include "mv_pp2x.h"
+#include "mv_pp2x_hw.h"
+#include "mv_gop110_hw.h"
+
+/* Ethtool methods */
+
+#ifdef CONFIG_MV_PP2_FPGA
+
+#define ETH_MIB_GOOD_OCTETS_RECEIVED_LOW    0x0
+#define ETH_MIB_GOOD_OCTETS_RECEIVED_HIGH   0x4
+#define ETH_MIB_BAD_OCTETS_RECEIVED         0x8
+#define ETH_MIB_INTERNAL_MAC_TRANSMIT_ERR   0xc
+#define ETH_MIB_GOOD_FRAMES_RECEIVED        0x10
+#define ETH_MIB_BAD_FRAMES_RECEIVED         0x14
+#define ETH_MIB_BROADCAST_FRAMES_RECEIVED   0x18
+#define ETH_MIB_MULTICAST_FRAMES_RECEIVED   0x1c
+#define ETH_MIB_FRAMES_64_OCTETS            0x20
+#define ETH_MIB_FRAMES_65_TO_127_OCTETS     0x24
+#define ETH_MIB_FRAMES_128_TO_255_OCTETS    0x28
+#define ETH_MIB_FRAMES_256_TO_511_OCTETS    0x2c
+#define ETH_MIB_FRAMES_512_TO_1023_OCTETS   0x30
+#define ETH_MIB_FRAMES_1024_TO_MAX_OCTETS   0x34
+#define ETH_MIB_GOOD_OCTETS_SENT_LOW        0x38
+#define ETH_MIB_GOOD_OCTETS_SENT_HIGH       0x3c
+#define ETH_MIB_GOOD_FRAMES_SENT            0x40
+#define ETH_MIB_EXCESSIVE_COLLISION         0x44
+#define ETH_MIB_MULTICAST_FRAMES_SENT       0x48
+#define ETH_MIB_BROADCAST_FRAMES_SENT       0x4c
+#define ETH_MIB_UNREC_MAC_CONTROL_RECEIVED  0x50
+#define ETH_MIB_FC_SENT                     0x54
+#define ETH_MIB_GOOD_FC_RECEIVED            0x58
+#define ETH_MIB_BAD_FC_RECEIVED             0x5c
+#define ETH_MIB_UNDERSIZE_RECEIVED          0x60
+#define ETH_MIB_FRAGMENTS_RECEIVED          0x64
+#define ETH_MIB_OVERSIZE_RECEIVED           0x68
+#define ETH_MIB_JABBER_RECEIVED             0x6c
+#define ETH_MIB_MAC_RECEIVE_ERROR           0x70
+#define ETH_MIB_BAD_CRC_EVENT               0x74
+#define ETH_MIB_COLLISION                   0x78
+#define ETH_MIB_LATE_COLLISION              0x7c
+
+#endif
+
+/* Get settings (phy address, speed) for ethtools */
+static int mv_pp2x_ethtool_get_settings(struct net_device *dev,
+					struct ethtool_cmd *cmd)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+#ifdef CONFIG_MV_PP2_FPGA
+	int val;
+	void *addr;
+	int port_id = port->id;
+	void *mv_pp2_vfpga_address;
+
+	pr_emerg("\n\n\n\nmv_pp2x_ethtool_get_drvinfo(%d):dev->name=%s port->id=%d\n",
+		 __LINE__, dev->name, port->id);
+
+	mv_pp2_vfpga_address = mv_pp2_vfpga_address_get();
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_GOOD_FRAMES_SENT;
+	val   = readl(addr);
+	pr_emerg("ETH_MIB_GOOD_FRAMES_SENT          =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_GOOD_FRAMES_RECEIVED;
+	val   = readl(addr);
+	pr_emerg("ETH_MIB_GOOD_FRAMES_RECEIVED      =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_GOOD_OCTETS_RECEIVED_LOW;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_GOOD_OCTETS_RECEIVED_LOW  =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_GOOD_OCTETS_RECEIVED_HIGH;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_GOOD_OCTETS_RECEIVED_HIGH =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_BAD_OCTETS_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_BAD_OCTETS_RECEIVED       =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_INTERNAL_MAC_TRANSMIT_ERR;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_INTERNAL_MAC_TRANSMIT_ERR =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_BAD_FRAMES_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_BAD_FRAMES_RECEIVED       =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_BROADCAST_FRAMES_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_BROADCAST_FRAMES_RECEIVED =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_MULTICAST_FRAMES_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_MULTICAST_FRAMES_RECEIVED =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_FRAMES_64_OCTETS;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_FRAMES_64_OCTETS          =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_GOOD_OCTETS_SENT_LOW;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_GOOD_OCTETS_SENT_LOW      =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_EXCESSIVE_COLLISION;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_EXCESSIVE_COLLISION       =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_BROADCAST_FRAMES_SENT;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_BROADCAST_FRAMES_SENT     =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_UNREC_MAC_CONTROL_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_UNREC_MAC_CONTROL_RECEIVED=%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_FC_SENT;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_FC_SENT                   =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_GOOD_FC_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_GOOD_FC_RECEIVED          =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_BAD_FC_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_BAD_FC_RECEIVED           =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_UNDERSIZE_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_UNDERSIZE_RECEIVED        =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_OVERSIZE_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_OVERSIZE_RECEIVED         =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_JABBER_RECEIVED;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_JABBER_RECEIVED           =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_MAC_RECEIVE_ERROR;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_MAC_RECEIVE_ERROR         =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_BAD_CRC_EVENT;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_BAD_CRC_EVENT             =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_COLLISION;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_COLLISION                 =%d  : 0x%p\n", val, addr);
+
+	addr = mv_pp2_vfpga_address + 0x100000 + 0x1000 + (port_id * 0x400) +
+			ETH_MIB_LATE_COLLISION;
+	val  = readl(addr);
+	pr_emerg("ETH_MIB_LATE_COLLISION            =%d  : 0x%p\n", val, addr);
+
+	val = readl(port->base + 0x10);
+	pr_emerg("print_reg(%d):port_id=%d: [0x%p] = 0x%x\n", __LINE__,
+		 port_id, port->base + 0x10, val);
+	return 0;
+#else
+	struct mv_port_link_status	status;
+	phy_interface_t			phy_mode;
+
+	mv_gop110_mib_counters_show(&port->priv->hw.gop,
+		port->mac_data.gop_index);
+
+	/* No Phy device mngmt */
+	if (!port->mac_data.phy_dev) {
+		/*for force link port, RXAUI port and link-down ports,
+		 * follow old strategy
+		 */
+
+		mv_gop110_port_link_status(&port->priv->hw.gop,
+					   &port->mac_data, &status);
+
+		if (status.linkup == true) {
+			switch (status.speed) {
+			case MV_PORT_SPEED_10000:
+				cmd->speed = SPEED_10000;
+				break;
+			case MV_PORT_SPEED_1000:
+				cmd->speed = SPEED_1000;
+				break;
+			case MV_PORT_SPEED_100:
+				cmd->speed = SPEED_100;
+				break;
+			case MV_PORT_SPEED_10:
+				cmd->speed = SPEED_10;
+				break;
+			default:
+				return -EINVAL;
+			}
+			if (status.duplex == MV_PORT_DUPLEX_FULL)
+				cmd->duplex = 1;
+			else
+				cmd->duplex = 0;
+		} else {
+			cmd->speed  = SPEED_UNKNOWN;
+			cmd->duplex = SPEED_UNKNOWN;
+		}
+
+		phy_mode = port->mac_data.phy_mode;
+		if ((phy_mode == PHY_INTERFACE_MODE_XAUI) ||
+		    (phy_mode == PHY_INTERFACE_MODE_RXAUI)) {
+			cmd->autoneg = AUTONEG_DISABLE;
+			cmd->supported = (SUPPORTED_10000baseT_Full |
+				SUPPORTED_FIBRE);
+			cmd->advertising = (ADVERTISED_10000baseT_Full |
+				ADVERTISED_FIBRE);
+			cmd->port = PORT_FIBRE;
+			cmd->transceiver = XCVR_EXTERNAL;
+		} else {
+			cmd->supported = (SUPPORTED_10baseT_Half |
+				SUPPORTED_10baseT_Full |
+				SUPPORTED_100baseT_Half	|
+				SUPPORTED_100baseT_Full |
+				SUPPORTED_Autoneg | SUPPORTED_TP |
+				SUPPORTED_MII |	SUPPORTED_1000baseT_Full);
+			cmd->transceiver = XCVR_INTERNAL;
+			cmd->port = PORT_MII;
+
+			/* check if speed and duplex are AN */
+			if (status.speed == MV_PORT_SPEED_AN &&
+			    status.duplex == MV_PORT_DUPLEX_AN) {
+				cmd->lp_advertising = cmd->advertising = 0;
+				cmd->autoneg = AUTONEG_ENABLE;
+			} else {
+				cmd->autoneg = AUTONEG_DISABLE;
+			}
+		}
+
+		return 0;
+	}
+
+	return phy_ethtool_gset(port->mac_data.phy_dev, cmd);
+#endif
+}
+
+/* Set settings (phy address, speed) for ethtools */
+static int mv_pp2x_ethtool_set_settings(struct net_device *dev,
+					struct ethtool_cmd *cmd)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+	if (!port->mac_data.phy_dev)
+		return -ENODEV;
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	return phy_ethtool_sset(port->mac_data.phy_dev, cmd);
+#else
+	return 0;
+#endif
+}
+
+/* Set interrupt coalescing for ethtools */
+static int mv_pp2x_ethtool_set_coalesce(struct net_device *dev,
+					struct ethtool_coalesce *c)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	int queue;
+
+	for (queue = 0; queue < port->num_rx_queues; queue++) {
+		struct mv_pp2x_rx_queue *rxq = port->rxqs[queue];
+
+		rxq->time_coal = c->rx_coalesce_usecs;
+		rxq->pkts_coal = c->rx_max_coalesced_frames;
+		mv_pp2x_rx_pkts_coal_set(port, rxq, rxq->pkts_coal);
+		mv_pp2x_rx_time_coal_set(port, rxq, rxq->time_coal);
+	}
+	port->tx_time_coal = c->tx_coalesce_usecs;
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		struct mv_pp2x_tx_queue *txq = port->txqs[queue];
+
+		txq->pkts_coal = c->tx_max_coalesced_frames;
+	}
+	if (port->priv->pp2xdata->interrupt_tx_done == true) {
+		mv_pp2x_tx_done_time_coal_set(port, port->tx_time_coal);
+		on_each_cpu(mv_pp2x_tx_done_pkts_coal_set, port, 1);
+	}
+
+	return 0;
+}
+
+/* get coalescing for ethtools */
+static int mv_pp2x_ethtool_get_coalesce(struct net_device *dev,
+					struct ethtool_coalesce *c)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+	c->rx_coalesce_usecs        = port->rxqs[0]->time_coal;
+	c->rx_max_coalesced_frames  = port->rxqs[0]->pkts_coal;
+	c->tx_max_coalesced_frames  = port->txqs[0]->pkts_coal;
+	c->tx_coalesce_usecs        = port->tx_time_coal;
+
+	return 0;
+}
+
+static void mv_pp2x_ethtool_get_drvinfo(struct net_device *dev,
+					struct ethtool_drvinfo *drvinfo)
+{
+	strlcpy(drvinfo->driver, MVPP2_DRIVER_NAME,
+		sizeof(drvinfo->driver));
+	strlcpy(drvinfo->version, MVPP2_DRIVER_VERSION,
+		sizeof(drvinfo->version));
+	strlcpy(drvinfo->bus_info, dev_name(&dev->dev),
+		sizeof(drvinfo->bus_info));
+}
+
+static void mv_pp2x_ethtool_get_ringparam(struct net_device *dev,
+					  struct ethtool_ringparam *ring)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+	ring->rx_max_pending = MVPP2_MAX_RXD;
+	ring->tx_max_pending = MVPP2_MAX_TXD;
+	ring->rx_pending = port->rx_ring_size;
+	ring->tx_pending = port->tx_ring_size;
+}
+
+static int mv_pp2x_ethtool_set_ringparam(struct net_device *dev,
+					 struct ethtool_ringparam *ring)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	u16 prev_rx_ring_size = port->rx_ring_size;
+	u16 prev_tx_ring_size = port->tx_ring_size;
+	int err;
+
+	err = mv_pp2x_check_ringparam_valid(dev, ring);
+	if (err)
+		return err;
+
+	if (!netif_running(dev)) {
+		port->rx_ring_size = ring->rx_pending;
+		port->tx_ring_size = ring->tx_pending;
+		return 0;
+	}
+
+	/* The interface is running, so we have to force a
+	 * reallocation of the queues
+	 */
+	mv_pp2x_stop_dev(port);
+	mv_pp2x_cleanup_rxqs(port);
+	mv_pp2x_cleanup_txqs(port);
+
+	port->rx_ring_size = ring->rx_pending;
+	port->tx_ring_size = ring->tx_pending;
+
+	err = mv_pp2x_setup_rxqs(port);
+	if (err) {
+		/* Reallocate Rx queues with the original ring size */
+		port->rx_ring_size = prev_rx_ring_size;
+		ring->rx_pending = prev_rx_ring_size;
+		err = mv_pp2x_setup_rxqs(port);
+		if (err)
+			goto err_out;
+	}
+	err = mv_pp2x_setup_txqs(port);
+	if (err) {
+		/* Reallocate Tx queues with the original ring size */
+		port->tx_ring_size = prev_tx_ring_size;
+		ring->tx_pending = prev_tx_ring_size;
+		err = mv_pp2x_setup_txqs(port);
+		if (err)
+			goto err_clean_rxqs;
+	}
+
+	mv_pp2x_start_dev(port);
+
+	return 0;
+
+err_clean_rxqs:
+	mv_pp2x_cleanup_rxqs(port);
+err_out:
+	netdev_err(dev, "fail to change ring parameters");
+	return err;
+}
+
+static u32 mv_pp2x_ethtool_get_rxfh_indir_size(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+	return ARRAY_SIZE(port->priv->rx_indir_table);
+}
+
+static int mv_pp2x_ethtool_get_rxnfc(struct net_device *dev,
+				     struct ethtool_rxnfc *info,
+				     u32 *rules)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+	if (port->priv->pp2_cfg.queue_mode == MVPP2_QDIST_SINGLE_MODE)
+		return -EOPNOTSUPP;
+
+	if (info->cmd == ETHTOOL_GRXRINGS) {
+		if (port)
+			info->data = ARRAY_SIZE(port->priv->rx_indir_table);
+	}
+	return 0;
+}
+
+static const struct ethtool_ops mv_pp2x_eth_tool_ops = {
+	.get_link		= ethtool_op_get_link,
+	.get_settings		= mv_pp2x_ethtool_get_settings,
+	.set_settings		= mv_pp2x_ethtool_set_settings,
+	.set_coalesce		= mv_pp2x_ethtool_set_coalesce,
+	.get_coalesce		= mv_pp2x_ethtool_get_coalesce,
+	.get_drvinfo		= mv_pp2x_ethtool_get_drvinfo,
+	.get_ringparam		= mv_pp2x_ethtool_get_ringparam,
+	.set_ringparam		= mv_pp2x_ethtool_set_ringparam,
+	/* For rxfh relevant command, only support LK-3.18 */
+	.get_rxfh_indir_size	= mv_pp2x_ethtool_get_rxfh_indir_size,
+	.get_rxnfc		= mv_pp2x_ethtool_get_rxnfc,
+};
+
+void mv_pp2x_set_ethtool_ops(struct net_device *netdev)
+{
+	netdev->ethtool_ops = &mv_pp2x_eth_tool_ops;
+}
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
new file mode 100644
index 0000000..9470a9b
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.c
@@ -0,0 +1,6698 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifdef ARMADA_390
+#include <linux/phy.h>
+#endif
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/inetdevice.h>
+#include <uapi/linux/ppp_defs.h>
+
+#include <net/ip.h>
+#include <net/ipv6.h>
+
+#include "mv_pp2x.h"
+#include "mv_pp2x_hw.h"
+#include "mv_pp2x_debug.h"
+
+/* Utility/helper methods */
+
+/*#define MVPP2_REG_BUF_SIZE (sizeof(last_used)/sizeof(last_used[0]))*/
+#define MVPP2_REG_BUF_SIZE ARRAY_SIZE(last_used)
+
+int mv_pp2x_ptr_validate(const void *ptr)
+{
+	if (ptr == NULL) {
+		DBG_MSG("%s: null pointer.\n", __func__);
+		return MV_ERROR;
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_ptr_validate);
+
+int mv_pp2x_range_validate(int value, int min, int max)
+{
+	if (((value) > (max)) || ((value) < (min))) {
+		DBG_MSG("%s: value 0x%X (%d) is out of range [0x%X , 0x%X].\n",
+			__func__, (value), (value), (min), (max));
+		return MV_ERROR;
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_range_validate);
+
+void mv_pp2x_write(struct mv_pp2x_hw *hw, u32 offset, u32 data)
+{
+#if defined(MVPP2_DEBUG) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	static void *last_used[20] = {0};
+	static int next_write;
+	int i;
+	void *reg_ptr = hw->cpu_base[smp_processor_id()] + offset;
+
+	for (i = 0; i < MVPP2_REG_BUF_SIZE; i++) {
+		if (last_used[i] == reg_ptr)
+			break;
+	}
+	if (i == MVPP2_REG_BUF_SIZE) {
+		pr_debug("NEW REG: mv_pp2x_write(%p)\n", reg_ptr);
+		last_used[next_write] = reg_ptr;
+		next_write++;
+		next_write = next_write%MVPP2_REG_BUF_SIZE;
+	} else {
+		/*pr_info("mv_pp2x_write(%d)=%d , caller %pS\n",
+		*	offset, data, __builtin_return_address(0));
+		*/
+	}
+#endif
+
+	writel(data, hw->cpu_base[smp_processor_id()] + offset);
+}
+EXPORT_SYMBOL(mv_pp2x_write);
+
+u32 mv_pp2x_read(struct mv_pp2x_hw *hw, u32 offset)
+{
+#if defined(MVPP2_DEBUG) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	static void *last_used[20] = {0};
+	static int next_write;
+	int i;
+#endif
+	void *reg_ptr = hw->cpu_base[smp_processor_id()] + offset;
+	u32 val;
+
+	val = readl(reg_ptr);
+#if defined(MVPP2_DEBUG) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	for (i = 0; i < MVPP2_REG_BUF_SIZE; i++) {
+		if (last_used[i] == reg_ptr)
+			break;
+	}
+	if (i == MVPP2_REG_BUF_SIZE) {
+		pr_debug("NEW REG: mv_pp2x_read(%p)\n", reg_ptr);
+		last_used[next_write] = reg_ptr;
+		next_write++;
+		next_write = next_write%MVPP2_REG_BUF_SIZE;
+	} else {
+		/*pr_info("mv_pp2x_read(%d)=%d , caller %pS\n",
+		 *	offset, val, __builtin_return_address(0));
+		*/
+	}
+#endif
+
+	return val;
+}
+EXPORT_SYMBOL(mv_pp2x_read);
+
+/* Parser configuration routines */
+
+/* Flow ID definetion array */
+static struct mv_pp2x_prs_flow_id
+	mv_pp2x_prs_flow_id_array[MVPP2_PRS_FL_TCAM_NUM] = {
+	/***********#Flow ID#**************#Result Info#************/
+	{MVPP2_PRS_FL_IP4_TCP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP4 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP4_OPT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP4_OTHER |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_UDP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP4 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP4_OPT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP4_OTHER |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_TCP_NF_TAG,	{MVPP2_PRS_RI_L3_IP4 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_NF_TAG,	{MVPP2_PRS_RI_L3_IP4_OPT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_NF_TAG,	{MVPP2_PRS_RI_L3_IP4_OTHER |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_UDP_NF_TAG,	{MVPP2_PRS_RI_L3_IP4 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_NF_TAG,	{MVPP2_PRS_RI_L3_IP4_OPT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_NF_TAG,	{MVPP2_PRS_RI_L3_IP4_OTHER |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_TCP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP6 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_TCP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP6_EXT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_UDP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP6 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_UDP_NF_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					 MVPP2_PRS_RI_L3_IP6_EXT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_VLAN_MASK |
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_TCP_NF_TAG,	{MVPP2_PRS_RI_L3_IP6 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_TCP_NF_TAG,	{MVPP2_PRS_RI_L3_IP6_EXT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_UDP_NF_TAG,	{MVPP2_PRS_RI_L3_IP6 |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_UDP_NF_TAG,	{MVPP2_PRS_RI_L3_IP6_EXT |
+					 MVPP2_PRS_RI_IP_FRAG_FALSE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_TCP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP4 |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_TCP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP4_OPT |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_TCP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP4_OTHER |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_TCP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_UDP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP4 |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_UDP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP4_OPT |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_UDP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP4_OTHER |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_UDP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_TCP_FRAG_TAG, {MVPP2_PRS_RI_L3_IP4 |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_FRAG_TAG, {MVPP2_PRS_RI_L3_IP4_OPT |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TCP_FRAG_TAG, {MVPP2_PRS_RI_L3_IP4_OTHER |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_UDP_FRAG_TAG, {MVPP2_PRS_RI_L3_IP4 |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_FRAG_TAG,	{MVPP2_PRS_RI_L3_IP4_OPT |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UDP_FRAG_TAG,	{MVPP2_PRS_RI_L3_IP4_OTHER |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_TCP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP6 |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_TCP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_TCP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP6_EXT |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_TCP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_UDP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP6 |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_UDP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_UDP_FRAG_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+					   MVPP2_PRS_RI_L3_IP6_EXT |
+					   MVPP2_PRS_RI_IP_FRAG_TRUE |
+					   MVPP2_PRS_RI_L4_UDP,
+					   MVPP2_PRS_RI_VLAN_MASK |
+					   MVPP2_PRS_RI_L3_PROTO_MASK |
+					   MVPP2_PRS_RI_IP_FRAG_MASK |
+					   MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_TCP_FRAG_TAG,	{MVPP2_PRS_RI_L3_IP6 |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_TCP_FRAG_TAG,	{MVPP2_PRS_RI_L3_IP6_EXT |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_TCP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_UDP_FRAG_TAG,	{MVPP2_PRS_RI_L3_IP6 |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_UDP_FRAG_TAG, {MVPP2_PRS_RI_L3_IP6_EXT |
+					 MVPP2_PRS_RI_IP_FRAG_TRUE |
+					 MVPP2_PRS_RI_L4_UDP,
+					 MVPP2_PRS_RI_L3_PROTO_MASK |
+					 MVPP2_PRS_RI_IP_FRAG_MASK |
+					 MVPP2_PRS_RI_L4_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+				  MVPP2_PRS_RI_L3_IP4,
+				  MVPP2_PRS_RI_VLAN_MASK |
+				  MVPP2_PRS_RI_L3_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+				  MVPP2_PRS_RI_L3_IP4_OPT,
+				  MVPP2_PRS_RI_VLAN_MASK |
+				  MVPP2_PRS_RI_L3_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+				  MVPP2_PRS_RI_L3_IP4_OTHER,
+				  MVPP2_PRS_RI_VLAN_MASK |
+				  MVPP2_PRS_RI_L3_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP4_TAG, {MVPP2_PRS_RI_L3_IP4,
+				MVPP2_PRS_RI_L3_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TAG, {MVPP2_PRS_RI_L3_IP4_OPT,
+				MVPP2_PRS_RI_L3_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP4_TAG, {MVPP2_PRS_RI_L3_IP4_OTHER,
+				MVPP2_PRS_RI_L3_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+				  MVPP2_PRS_RI_L3_IP6,
+				  MVPP2_PRS_RI_VLAN_MASK |
+				  MVPP2_PRS_RI_L3_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_UNTAG, {MVPP2_PRS_RI_VLAN_NONE |
+				  MVPP2_PRS_RI_L3_IP6_EXT,
+				  MVPP2_PRS_RI_VLAN_MASK |
+				  MVPP2_PRS_RI_L3_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_IP6_TAG, {MVPP2_PRS_RI_L3_IP6,
+				MVPP2_PRS_RI_L3_PROTO_MASK} },
+	{MVPP2_PRS_FL_IP6_TAG, {MVPP2_PRS_RI_L3_IP6_EXT,
+				MVPP2_PRS_RI_L3_PROTO_MASK} },
+
+	{MVPP2_PRS_FL_NON_IP_UNTAG, {MVPP2_PRS_RI_VLAN_NONE,
+				     MVPP2_PRS_RI_VLAN_MASK} },
+
+	{MVPP2_PRS_FL_NON_IP_TAG, {0, 0} },
+};
+
+/* Array of bitmask to indicate flow id attribute */
+static int mv_pp2x_prs_flow_id_attr_tbl[MVPP2_PRS_FL_LAST];
+
+/* Update parser tcam and sram hw entries */
+int mv_pp2x_prs_hw_write(struct mv_pp2x_hw *hw, struct mv_pp2x_prs_entry *pe)
+{
+	int i;
+
+	if (pe->index > MVPP2_PRS_TCAM_SRAM_SIZE - 1)
+		return -EINVAL;
+
+	/* Clear entry invalidation bit */
+	pe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] &= ~MVPP2_PRS_TCAM_INV_MASK;
+
+	/* Write tcam index - indirect access */
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_IDX_REG, pe->index);
+	for (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)
+		mv_pp2x_write(hw, MVPP2_PRS_TCAM_DATA_REG(i), pe->tcam.word[i]);
+
+	/* Write sram index - indirect access */
+	mv_pp2x_write(hw, MVPP2_PRS_SRAM_IDX_REG, pe->index);
+	for (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)
+		mv_pp2x_write(hw, MVPP2_PRS_SRAM_DATA_REG(i), pe->sram.word[i]);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_write);
+
+/* Read tcam entry from hw */
+int mv_pp2x_prs_hw_read(struct mv_pp2x_hw *hw, struct mv_pp2x_prs_entry *pe)
+{
+	int i;
+
+	if (pe->index > MVPP2_PRS_TCAM_SRAM_SIZE - 1)
+		return -EINVAL;
+
+	/* Write tcam index - indirect access */
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_IDX_REG, pe->index);
+
+	pe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] = mv_pp2x_read(hw,
+			      MVPP2_PRS_TCAM_DATA_REG(MVPP2_PRS_TCAM_INV_WORD));
+	if (pe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] & MVPP2_PRS_TCAM_INV_MASK)
+		return MVPP2_PRS_TCAM_ENTRY_INVALID;
+
+	for (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)
+		pe->tcam.word[i] = mv_pp2x_read(hw, MVPP2_PRS_TCAM_DATA_REG(i));
+
+	/* Write sram index - indirect access */
+	mv_pp2x_write(hw, MVPP2_PRS_SRAM_IDX_REG, pe->index);
+	for (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)
+		pe->sram.word[i] = mv_pp2x_read(hw, MVPP2_PRS_SRAM_DATA_REG(i));
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_read);
+
+void mv_pp2x_prs_sw_clear(struct mv_pp2x_prs_entry *pe)
+{
+	memset(pe, 0, sizeof(struct mv_pp2x_prs_entry));
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_clear);
+
+/* Invalidate tcam hw entry */
+void mv_pp2x_prs_hw_inv(struct mv_pp2x_hw *hw, int index)
+{
+	/* Write index - indirect access */
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_IDX_REG, index);
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_DATA_REG(MVPP2_PRS_TCAM_INV_WORD),
+		      MVPP2_PRS_TCAM_INV_MASK);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_inv);
+
+/* Enable shadow table entry and set its lookup ID */
+static void mv_pp2x_prs_shadow_set(struct mv_pp2x_hw *hw, int index, int lu)
+{
+	hw->prs_shadow[index].valid = true;
+	hw->prs_shadow[index].lu = lu;
+}
+
+/* Update ri fields in shadow table entry */
+static void mv_pp2x_prs_shadow_ri_set(struct mv_pp2x_hw *hw, int index,
+				      unsigned int ri,
+				      unsigned int ri_mask)
+{
+	hw->prs_shadow[index].ri_mask = ri_mask;
+	hw->prs_shadow[index].ri = ri;
+}
+
+/* Update lookup field in tcam sw entry */
+void mv_pp2x_prs_tcam_lu_set(struct mv_pp2x_prs_entry *pe, unsigned int lu)
+{
+	int enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_LU_BYTE);
+
+	pe->tcam.byte[MVPP2_PRS_TCAM_LU_BYTE] = lu;
+	pe->tcam.byte[enable_off] = MVPP2_PRS_LU_MASK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_tcam_lu_set);
+
+/* Update mask for single port in tcam sw entry */
+void mv_pp2x_prs_tcam_port_set(struct mv_pp2x_prs_entry *pe,
+			       unsigned int port, bool add)
+{
+	int enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_PORT_BYTE);
+
+	if (add)
+		pe->tcam.byte[enable_off] &= ~(1 << port);
+	else
+		pe->tcam.byte[enable_off] |= 1 << port;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_tcam_port_set);
+
+/* Update port map in tcam sw entry */
+void mv_pp2x_prs_tcam_port_map_set(struct mv_pp2x_prs_entry *pe,
+				   unsigned int ports)
+{
+	unsigned char port_mask = MVPP2_PRS_PORT_MASK;
+	int enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_PORT_BYTE);
+
+	pe->tcam.byte[MVPP2_PRS_TCAM_PORT_BYTE] = 0;
+	pe->tcam.byte[enable_off] &= ~port_mask;
+	pe->tcam.byte[enable_off] |= ~ports & MVPP2_PRS_PORT_MASK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_tcam_port_map_set);
+
+/* Obtain port map from tcam sw entry */
+static unsigned int mv_pp2x_prs_tcam_port_map_get(struct mv_pp2x_prs_entry *pe)
+{
+	int enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_PORT_BYTE);
+
+	return ~(pe->tcam.byte[enable_off]) & MVPP2_PRS_PORT_MASK;
+}
+
+/* Set byte of data and its enable bits in tcam sw entry */
+void mv_pp2x_prs_tcam_data_byte_set(struct mv_pp2x_prs_entry *pe,
+				    unsigned int offs,
+				    unsigned char byte,
+				    unsigned char enable)
+{
+	pe->tcam.byte[TCAM_DATA_BYTE(offs)] = byte;
+	pe->tcam.byte[TCAM_DATA_MASK(offs)] = enable;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_tcam_data_byte_set);
+
+/* Get byte of data and its enable bits from tcam sw entry */
+static void mv_pp2x_prs_tcam_data_byte_get(struct mv_pp2x_prs_entry *pe,
+					   unsigned int offs,
+					   unsigned char *byte,
+					   unsigned char *enable)
+{
+	*byte = pe->tcam.byte[TCAM_DATA_BYTE(offs)];
+	*enable = pe->tcam.byte[TCAM_DATA_MASK(offs)];
+}
+
+/* Set dword of data and its enable bits in tcam sw entry */
+static void mv_pp2x_prs_tcam_data_dword_set(struct mv_pp2x_prs_entry *pe,
+					    unsigned int offs,
+					    unsigned int word,
+					    unsigned int enable)
+{
+	int index, offset;
+	unsigned char byte, byteMask;
+
+	for (index = 0; index < 4; index++) {
+		offset = (offs * 4) + index;
+		byte = ((unsigned char *) &word)[index];
+		byteMask = ((unsigned char *) &enable)[index];
+		mv_pp2x_prs_tcam_data_byte_set(pe, offset, byte, byteMask);
+	}
+}
+
+/* Get dword of data and its enable bits from tcam sw entry */
+static void mv_pp2x_prs_tcam_data_dword_get(struct mv_pp2x_prs_entry *pe,
+					    unsigned int offs,
+					    unsigned int *word,
+					    unsigned int *enable)
+{
+	int index, offset;
+	unsigned char byte, mask;
+
+	for (index = 0; index < 4; index++) {
+		offset = (offs * 4) + index;
+		mv_pp2x_prs_tcam_data_byte_get(pe, offset,  &byte, &mask);
+		((unsigned char *) word)[index] = byte;
+		((unsigned char *) enable)[index] = mask;
+	}
+}
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Compare tcam data bytes with a pattern */
+static bool mv_pp2x_prs_tcam_data_cmp(struct mv_pp2x_prs_entry *pe, int offs,
+				      u16 data)
+{
+	int off = TCAM_DATA_BYTE(offs);
+	u16 tcam_data;
+
+	tcam_data = (8 << pe->tcam.byte[off + 1]) | pe->tcam.byte[off];
+	if (tcam_data != data)
+		return false;
+	return true;
+}
+#endif
+
+/* Update ai bits in tcam sw entry */
+void mv_pp2x_prs_tcam_ai_update(struct mv_pp2x_prs_entry *pe,
+				unsigned int bits,
+				unsigned int enable)
+{
+	int i, ai_idx = MVPP2_PRS_TCAM_AI_BYTE;
+
+	for (i = 0; i < MVPP2_PRS_AI_BITS; i++) {
+
+		if (!(enable & BIT(i)))
+			continue;
+
+		if (bits & BIT(i))
+			pe->tcam.byte[ai_idx] |= 1 << i;
+		else
+			pe->tcam.byte[ai_idx] &= ~(1 << i);
+	}
+
+	pe->tcam.byte[MVPP2_PRS_TCAM_EN_OFFS(ai_idx)] |= enable;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_tcam_ai_update);
+
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Get ai bits from tcam sw entry */
+static int mv_pp2x_prs_tcam_ai_get(struct mv_pp2x_prs_entry *pe)
+{
+	return pe->tcam.byte[MVPP2_PRS_TCAM_AI_BYTE];
+}
+#endif
+
+/* Set ethertype in tcam sw entry */
+static void mv_pp2x_prs_match_etype(struct mv_pp2x_prs_entry *pe, int offset,
+				    unsigned short ethertype)
+{
+	mv_pp2x_prs_tcam_data_byte_set(pe, offset + 0, ethertype >> 8, 0xff);
+	mv_pp2x_prs_tcam_data_byte_set(pe, offset + 1, ethertype & 0xff, 0xff);
+}
+
+/* Set bits in sram sw entry */
+static void mv_pp2x_prs_sram_bits_set(struct mv_pp2x_prs_entry *pe, int bit_num,
+				      int val)
+{
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(bit_num)] |= (val << (bit_num % 8));
+}
+
+/* Clear bits in sram sw entry */
+static void mv_pp2x_prs_sram_bits_clear(struct mv_pp2x_prs_entry *pe,
+					int bit_num, int val)
+{
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(bit_num)] &= ~(val << (bit_num % 8));
+}
+
+/* Update ri bits in sram sw entry */
+void mv_pp2x_prs_sram_ri_update(struct mv_pp2x_prs_entry *pe,
+				unsigned int bits, unsigned int mask)
+{
+	unsigned int i;
+
+	for (i = 0; i < MVPP2_PRS_SRAM_RI_CTRL_BITS; i++) {
+		int ri_off = MVPP2_PRS_SRAM_RI_OFFS;
+
+		if (!(mask & BIT(i)))
+			continue;
+
+		if (bits & BIT(i))
+			mv_pp2x_prs_sram_bits_set(pe, ri_off + i, 1);
+		else
+			mv_pp2x_prs_sram_bits_clear(pe, ri_off + i, 1);
+
+		mv_pp2x_prs_sram_bits_set(pe,
+			MVPP2_PRS_SRAM_RI_CTRL_OFFS + i, 1);
+	}
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sram_ri_update);
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Obtain ri bits from sram sw entry */
+static int mv_pp2x_prs_sram_ri_get(struct mv_pp2x_prs_entry *pe)
+{
+	return pe->sram.word[MVPP2_PRS_SRAM_RI_WORD];
+}
+#endif
+
+/* Update ai bits in sram sw entry */
+void mv_pp2x_prs_sram_ai_update(struct mv_pp2x_prs_entry *pe,
+				unsigned int bits, unsigned int mask)
+{
+	unsigned int i;
+	int ai_off = MVPP2_PRS_SRAM_AI_OFFS;
+
+	for (i = 0; i < MVPP2_PRS_SRAM_AI_CTRL_BITS; i++) {
+
+		if (!(mask & BIT(i)))
+			continue;
+
+		if (bits & BIT(i))
+			mv_pp2x_prs_sram_bits_set(pe, ai_off + i, 1);
+		else
+			mv_pp2x_prs_sram_bits_clear(pe, ai_off + i, 1);
+
+		mv_pp2x_prs_sram_bits_set(pe,
+			MVPP2_PRS_SRAM_AI_CTRL_OFFS + i, 1);
+	}
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sram_ai_update);
+
+/* Read ai bits from sram sw entry */
+static int mv_pp2x_prs_sram_ai_get(struct mv_pp2x_prs_entry *pe)
+{
+	u8 bits;
+	int ai_off = MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_AI_OFFS);
+	int ai_en_off = ai_off + 1;
+	int ai_shift = MVPP2_PRS_SRAM_AI_OFFS % 8;
+
+	bits = (pe->sram.byte[ai_off] >> ai_shift) |
+	       (pe->sram.byte[ai_en_off] << (8 - ai_shift));
+
+	return bits;
+}
+
+/* In sram sw entry set lookup ID field of the tcam key to be used in the next
+ * lookup interation
+ */
+void mv_pp2x_prs_sram_next_lu_set(struct mv_pp2x_prs_entry *pe,
+				  unsigned int lu)
+{
+	int sram_next_off = MVPP2_PRS_SRAM_NEXT_LU_OFFS;
+
+	mv_pp2x_prs_sram_bits_clear(pe, sram_next_off,
+				    MVPP2_PRS_SRAM_NEXT_LU_MASK);
+	mv_pp2x_prs_sram_bits_set(pe, sram_next_off, lu);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sram_next_lu_set);
+
+/* In the sram sw entry set sign and value of the next lookup offset
+ * and the offset value generated to the classifier
+ */
+static void mv_pp2x_prs_sram_shift_set(struct mv_pp2x_prs_entry *pe, int shift,
+				       unsigned int op)
+{
+	/* Set sign */
+	if (shift < 0) {
+		mv_pp2x_prs_sram_bits_set(pe,
+			MVPP2_PRS_SRAM_SHIFT_SIGN_BIT, 1);
+		shift = 0 - shift;
+	} else {
+		mv_pp2x_prs_sram_bits_clear(pe,
+			MVPP2_PRS_SRAM_SHIFT_SIGN_BIT, 1);
+	}
+
+	/* Set value */
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_SHIFT_OFFS)] =
+						   (unsigned char)shift;
+
+	/* Reset and set operation */
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS,
+				    MVPP2_PRS_SRAM_OP_SEL_SHIFT_MASK);
+	mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS, op);
+
+	/* Set base offset as current */
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS, 1);
+}
+
+/* In the sram sw entry set sign and value of the user defined offset
+ * generated to the classifier
+ */
+static void mv_pp2x_prs_sram_offset_set(struct mv_pp2x_prs_entry *pe,
+					unsigned int type, int offset,
+					unsigned int op)
+{
+	/* Set sign */
+	if (offset < 0) {
+		mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_UDF_SIGN_BIT, 1);
+		offset = 0 - offset;
+	} else {
+		mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_UDF_SIGN_BIT, 1);
+	}
+
+	/* Set value */
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_UDF_OFFS,
+				    MVPP2_PRS_SRAM_UDF_MASK);
+	mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_UDF_OFFS, offset);
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_UDF_OFFS +
+					MVPP2_PRS_SRAM_UDF_BITS)] &=
+					~(MVPP2_PRS_SRAM_UDF_MASK >>
+					(8 - (MVPP2_PRS_SRAM_UDF_OFFS % 8)));
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_UDF_OFFS +
+					MVPP2_PRS_SRAM_UDF_BITS)] |=
+					(offset >> (8 -
+					(MVPP2_PRS_SRAM_UDF_OFFS % 8)));
+
+	/* Set offset type */
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_UDF_TYPE_OFFS,
+				    MVPP2_PRS_SRAM_UDF_TYPE_MASK);
+	mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_UDF_TYPE_OFFS, type);
+
+	/* Set offset operation */
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_MASK);
+	mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS, op);
+
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS +
+					MVPP2_PRS_SRAM_OP_SEL_UDF_BITS)] &=
+					~(MVPP2_PRS_SRAM_OP_SEL_UDF_MASK >>
+					(8 -
+					(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8)));
+
+	pe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS +
+					MVPP2_PRS_SRAM_OP_SEL_UDF_BITS)] |=
+					(op >> (8 -
+					(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8)));
+
+	/* Set base offset as current */
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS, 1);
+}
+
+/* Find parser flow entry */
+static struct mv_pp2x_prs_entry *mv_pp2x_prs_flow_find(struct mv_pp2x_hw *hw,
+						       int flow,
+						       unsigned int ri,
+						       unsigned int ri_mask)
+{
+	struct mv_pp2x_prs_entry *pe;
+	int tid;
+	unsigned int dword, enable;
+
+	pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+	if (!pe)
+		return NULL;
+	mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_FLOWS);
+
+	/* Go through the all entires with MVPP2_PRS_LU_FLOWS */
+	for (tid = MVPP2_PRS_TCAM_SRAM_SIZE - 1; tid >= 0; tid--) {
+		u8 bits;
+
+		if (!hw->prs_shadow[tid].valid ||
+		    hw->prs_shadow[tid].lu != MVPP2_PRS_LU_FLOWS)
+			continue;
+
+		pe->index = tid;
+		mv_pp2x_prs_hw_read(hw, pe);
+
+		/* Check result info, because there maybe several
+		 * TCAM lines to generate the same flow
+		 */
+		mv_pp2x_prs_tcam_data_dword_get(pe, 0, &dword, &enable);
+		if ((dword != ri) || (enable != ri_mask))
+			continue;
+
+		bits = mv_pp2x_prs_sram_ai_get(pe);
+
+		/* Sram store classification lookup ID in AI bits [5:0] */
+		if ((bits & MVPP2_PRS_FLOW_ID_MASK) == flow)
+			return pe;
+	}
+	kfree(pe);
+
+	return NULL;
+}
+
+/* Return first free tcam index, seeking from start to end */
+static int mv_pp2x_prs_tcam_first_free(struct mv_pp2x_hw *hw,
+				       unsigned char start,
+				       unsigned char end)
+{
+	int tid;
+
+	/*pr_crit("mv_pp2x_prs_tcam_first_free start=%d, end=%d, caller=%pS\n",
+	 * start, end, __builtin_return_address(0));
+	 */
+	if (start > end)
+		swap(start, end);
+
+	if (end >= MVPP2_PRS_TCAM_SRAM_SIZE)
+		end = MVPP2_PRS_TCAM_SRAM_SIZE - 1;
+
+	for (tid = start; tid <= end; tid++) {
+		if (!hw->prs_shadow[tid].valid)
+			return tid;
+	}
+	pr_crit("Out of TCAM Entries !!: %s(%d)\n", __FILENAME__, __LINE__);
+	return -EINVAL;
+}
+
+/* Enable/disable dropping all mac da's */
+static void mv_pp2x_prs_mac_drop_all_set(struct mv_pp2x_hw *hw,
+					 int port, bool add)
+{
+	struct mv_pp2x_prs_entry pe;
+
+	if (hw->prs_shadow[MVPP2_PE_DROP_ALL].valid) {
+		/* Entry exist - update port only */
+		pe.index = MVPP2_PE_DROP_ALL;
+		mv_pp2x_prs_hw_read(hw, &pe);
+	} else {
+		/* Entry doesn't exist - create new */
+		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
+		pe.index = MVPP2_PE_DROP_ALL;
+
+		/* Non-promiscuous mode for all ports - DROP unknown packets */
+		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DROP_MASK,
+					 MVPP2_PRS_RI_DROP_MASK);
+
+		mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+		mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MAC);
+
+		/* Mask all ports */
+		mv_pp2x_prs_tcam_port_map_set(&pe, 0);
+	}
+
+	/* Update port mask */
+	mv_pp2x_prs_tcam_port_set(&pe, port, add);
+
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
+/* Set port to promiscuous mode */
+void mv_pp2x_prs_mac_promisc_set(struct mv_pp2x_hw *hw, int port, bool add)
+{
+	struct mv_pp2x_prs_entry pe;
+
+	/* Promiscous mode - Accept unknown packets */
+
+	if (hw->prs_shadow[MVPP2_PE_MAC_PROMISCUOUS].valid) {
+		/* Entry exist - update port only */
+		pe.index = MVPP2_PE_MAC_PROMISCUOUS;
+		mv_pp2x_prs_hw_read(hw, &pe);
+	} else {
+		/* Entry doesn't exist - create new */
+		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
+		pe.index = MVPP2_PE_MAC_PROMISCUOUS;
+
+		/* Continue - set next lookup */
+		mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_DSA);
+
+		/* Set result info bits */
+		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L2_UCAST,
+					 MVPP2_PRS_RI_L2_CAST_MASK);
+
+		/* Shift to ethertype */
+		mv_pp2x_prs_sram_shift_set(&pe, 2 * ETH_ALEN,
+					 MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+		/* Mask all ports */
+		mv_pp2x_prs_tcam_port_map_set(&pe, 0);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MAC);
+	}
+
+	/* Update port mask */
+	mv_pp2x_prs_tcam_port_set(&pe, port, add);
+
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
+/* Accept multicast */
+void mv_pp2x_prs_mac_multi_set(struct mv_pp2x_hw *hw, int port, int index,
+			       bool add)
+{
+	struct mv_pp2x_prs_entry pe;
+	unsigned char da_mc;
+
+	/* Ethernet multicast address first byte is
+	 * 0x01 for IPv4 and 0x33 for IPv6
+	 */
+	da_mc = (index == MVPP2_PE_MAC_MC_ALL) ? 0x01 : 0x33;
+
+	if (hw->prs_shadow[index].valid) {
+		/* Entry exist - update port only */
+		pe.index = index;
+		mv_pp2x_prs_hw_read(hw, &pe);
+	} else {
+		/* Entry doesn't exist - create new */
+		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
+		pe.index = index;
+
+		/* Continue - set next lookup */
+		mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_DSA);
+
+		/* Set result info bits */
+		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L2_MCAST,
+					   MVPP2_PRS_RI_L2_CAST_MASK);
+
+		/* Update tcam entry data first byte */
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 0, da_mc, 0xff);
+
+		/* Shift to ethertype */
+		mv_pp2x_prs_sram_shift_set(&pe, 2 * ETH_ALEN,
+					 MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+		/* Mask all ports */
+		mv_pp2x_prs_tcam_port_map_set(&pe, 0);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MAC);
+	}
+
+	/* Update port mask */
+	mv_pp2x_prs_tcam_port_set(&pe, port, add);
+
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
+/* Set entry for dsa packets */
+static void mv_pp2x_prs_dsa_tag_set(struct mv_pp2x_hw *hw, int port, bool add,
+				    bool tagged, bool extend)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid, shift;
+
+	if (extend) {
+		tid = tagged ? MVPP2_PE_EDSA_TAGGED : MVPP2_PE_EDSA_UNTAGGED;
+		shift = 8;
+	} else {
+		tid = tagged ? MVPP2_PE_DSA_TAGGED : MVPP2_PE_DSA_UNTAGGED;
+		shift = 4;
+	}
+
+	if (hw->prs_shadow[tid].valid) {
+		/* Entry exist - update port only */
+		pe.index = tid;
+		mv_pp2x_prs_hw_read(hw, &pe);
+	} else {
+		/* Entry doesn't exist - create new */
+		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);
+		pe.index = tid;
+
+		/* Shift 4 bytes if DSA tag or 8 bytes in case of EDSA tag*/
+		mv_pp2x_prs_sram_shift_set(&pe, shift,
+					   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_DSA);
+
+		if (tagged) {
+			/* Set tagged bit in DSA tag */
+			mv_pp2x_prs_tcam_data_byte_set(&pe, 0,
+					MVPP2_PRS_TCAM_DSA_TAGGED_BIT,
+					MVPP2_PRS_TCAM_DSA_TAGGED_BIT);
+			/* Clear all ai bits for next iteration */
+			mv_pp2x_prs_sram_ai_update(&pe, 0,
+						   MVPP2_PRS_SRAM_AI_MASK);
+			/* If packet is tagged continue check vlans */
+			mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);
+		} else {
+			/* Set result info bits to 'no vlans' */
+			mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,
+						   MVPP2_PRS_RI_VLAN_MASK);
+			mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);
+		}
+
+		/* Mask all ports */
+		mv_pp2x_prs_tcam_port_map_set(&pe, 0);
+	}
+
+	/* Update port mask */
+	mv_pp2x_prs_tcam_port_set(&pe, port, add);
+
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Set entry for dsa ethertype */
+static void mv_pp2x_prs_dsa_tag_ethertype_set(struct mv_pp2x_hw *hw, int port,
+					      bool add, bool tagged,
+					      bool extend)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid, shift, port_mask;
+
+	if (extend) {
+		tid = tagged ? MVPP2_PE_ETYPE_EDSA_TAGGED :
+		      MVPP2_PE_ETYPE_EDSA_UNTAGGED;
+		port_mask = 0;
+		shift = 8;
+	} else {
+		tid = tagged ? MVPP2_PE_ETYPE_DSA_TAGGED :
+		      MVPP2_PE_ETYPE_DSA_UNTAGGED;
+		port_mask = MVPP2_PRS_PORT_MASK;
+		shift = 4;
+	}
+
+	if (hw->prs_shadow[tid].valid) {
+		/* Entry exist - update port only */
+		pe.index = tid;
+		mv_pp2x_prs_hw_read(hw, &pe);
+	} else {
+		/* Entry doesn't exist - create new */
+		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);
+		pe.index = tid;
+
+		/* Set ethertype */
+		mv_pp2x_prs_match_etype(&pe, 0, ETH_P_EDSA);
+		mv_pp2x_prs_match_etype(&pe, 2, 0);
+
+		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DSA_MASK,
+					   MVPP2_PRS_RI_DSA_MASK);
+		/* Shift ethertype + 2 byte reserved + tag*/
+		mv_pp2x_prs_sram_shift_set(&pe, 2 + MVPP2_ETH_TYPE_LEN + shift,
+					   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_DSA);
+
+		if (tagged) {
+			/* Set tagged bit in DSA tag */
+			mv_pp2x_prs_tcam_data_byte_set(&pe,
+						     MVPP2_ETH_TYPE_LEN + 2 + 3,
+						 MVPP2_PRS_TCAM_DSA_TAGGED_BIT,
+						 MVPP2_PRS_TCAM_DSA_TAGGED_BIT);
+			/* Clear all ai bits for next iteration */
+			mv_pp2x_prs_sram_ai_update(&pe, 0,
+						   MVPP2_PRS_SRAM_AI_MASK);
+			/* If packet is tagged continue check vlans */
+			mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);
+		} else {
+			/* Set result info bits to 'no vlans' */
+			mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,
+						   MVPP2_PRS_RI_VLAN_MASK);
+			mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);
+		}
+		/* Mask/unmask all ports, depending on dsa type */
+		mv_pp2x_prs_tcam_port_map_set(&pe, port_mask);
+	}
+
+	/* Update port mask */
+	mv_pp2x_prs_tcam_port_set(&pe, port, add);
+
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+#endif
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Search for existing single/triple vlan entry */
+static struct mv_pp2x_prs_entry *mv_pp2x_prs_vlan_find(struct mv_pp2x_hw *hw,
+						       unsigned short tpid,
+						       int ai)
+{
+	struct mv_pp2x_prs_entry *pe;
+	int tid;
+
+	pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+	if (!pe)
+		return NULL;
+	mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);
+
+	/* Go through the all entries with MVPP2_PRS_LU_VLAN */
+	for (tid = MVPP2_PE_FIRST_FREE_TID;
+	     tid <= MVPP2_PE_LAST_FREE_TID; tid++) {
+		unsigned int ri_bits, ai_bits;
+		bool match;
+
+		if (!hw->prs_shadow[tid].valid ||
+		    hw->prs_shadow[tid].lu != MVPP2_PRS_LU_VLAN)
+			continue;
+
+		pe->index = tid;
+
+		mv_pp2x_prs_hw_read(hw, pe);
+		match = mv_pp2x_prs_tcam_data_cmp(pe, 0, swab16(tpid));
+		if (!match)
+			continue;
+
+		/* Get vlan type */
+		ri_bits = mv_pp2x_prs_sram_ri_get(pe);
+		ri_bits &= MVPP2_PRS_RI_VLAN_MASK;
+
+		/* Get current ai value from tcam */
+		ai_bits = mv_pp2x_prs_tcam_ai_get(pe);
+		/* Clear double vlan bit */
+		ai_bits &= ~MVPP2_PRS_DBL_VLAN_AI_BIT;
+
+		if (ai != ai_bits)
+			continue;
+
+		if (ri_bits == MVPP2_PRS_RI_VLAN_SINGLE ||
+		    ri_bits == MVPP2_PRS_RI_VLAN_TRIPLE)
+			return pe;
+	}
+	kfree(pe);
+
+	return NULL;
+}
+#endif
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Add/update single/triple vlan entry */
+static int mv_pp2x_prs_vlan_add(struct mv_pp2x_hw *hw, unsigned short tpid,
+				int ai, unsigned int port_map)
+{
+	struct mv_pp2x_prs_entry *pe;
+	int tid_aux, tid;
+	int ret = 0;
+
+	pe = mv_pp2x_prs_vlan_find(hw, tpid, ai);
+
+	if (!pe) {
+		/* Create new tcam entry */
+		tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_LAST_FREE_TID,
+						  MVPP2_PE_FIRST_FREE_TID);
+		if (tid < 0)
+			return tid;
+
+		pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+		if (!pe)
+			return -ENOMEM;
+
+		/* Get last double vlan tid */
+		for (tid_aux = MVPP2_PE_LAST_FREE_TID;
+		     tid_aux >= MVPP2_PE_FIRST_FREE_TID; tid_aux--) {
+			unsigned int ri_bits;
+
+			if (!hw->prs_shadow[tid_aux].valid ||
+			    hw->prs_shadow[tid_aux].lu != MVPP2_PRS_LU_VLAN)
+				continue;
+
+			pe->index = tid_aux;
+			mv_pp2x_prs_hw_read(hw, pe);
+			ri_bits = mv_pp2x_prs_sram_ri_get(pe);
+			if ((ri_bits & MVPP2_PRS_RI_VLAN_MASK) ==
+			    MVPP2_PRS_RI_VLAN_DOUBLE)
+				break;
+		}
+
+		if (tid <= tid_aux) {
+			ret = -EINVAL;
+			goto error;
+		}
+
+		memset(pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);
+		pe->index = tid;
+
+		mv_pp2x_prs_match_etype(pe, 0, tpid);
+
+		mv_pp2x_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_L2);
+		/* Shift 4 bytes - skip 1 vlan tag */
+		mv_pp2x_prs_sram_shift_set(pe, MVPP2_VLAN_TAG_LEN,
+					   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+		/* Clear all ai bits for next iteration */
+		mv_pp2x_prs_sram_ai_update(pe, 0, MVPP2_PRS_SRAM_AI_MASK);
+
+		if (ai == MVPP2_PRS_SINGLE_VLAN_AI) {
+			mv_pp2x_prs_sram_ri_update(pe, MVPP2_PRS_RI_VLAN_SINGLE,
+						   MVPP2_PRS_RI_VLAN_MASK);
+		} else {
+			ai |= MVPP2_PRS_DBL_VLAN_AI_BIT;
+			mv_pp2x_prs_sram_ri_update(pe, MVPP2_PRS_RI_VLAN_TRIPLE,
+						   MVPP2_PRS_RI_VLAN_MASK);
+		}
+		mv_pp2x_prs_tcam_ai_update(pe, ai, MVPP2_PRS_SRAM_AI_MASK);
+
+		mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_VLAN);
+	}
+	/* Update ports' mask */
+	mv_pp2x_prs_tcam_port_map_set(pe, port_map);
+
+	mv_pp2x_prs_hw_write(hw, pe);
+
+error:
+	kfree(pe);
+
+	return ret;
+}
+#endif
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Get first free double vlan ai number */
+static int mv_pp2x_prs_double_vlan_ai_free_get(struct mv_pp2x_hw *hw)
+{
+	int i;
+
+	for (i = 1; i < MVPP2_PRS_DBL_VLANS_MAX; i++) {
+		if (!hw->prs_double_vlans[i])
+			return i;
+	}
+
+	return -EINVAL;
+}
+#endif
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Search for existing double vlan entry */
+static struct mv_pp2x_prs_entry *mv_pp2x_prs_double_vlan_find(
+	struct mv_pp2x_hw *hw, unsigned short tpid1, unsigned short tpid2)
+{
+	struct mv_pp2x_prs_entry *pe;
+	int tid;
+
+	pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+	if (!pe)
+		return NULL;
+	mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);
+
+	/* Go through the all entries with MVPP2_PRS_LU_VLAN */
+	for (tid = MVPP2_PE_FIRST_FREE_TID;
+	     tid <= MVPP2_PE_LAST_FREE_TID; tid++) {
+		unsigned int ri_mask;
+		bool match;
+
+		if (!hw->prs_shadow[tid].valid ||
+		    hw->prs_shadow[tid].lu != MVPP2_PRS_LU_VLAN)
+			continue;
+
+		pe->index = tid;
+		mv_pp2x_prs_hw_read(hw, pe);
+
+		match = mv_pp2x_prs_tcam_data_cmp(pe, 0, swab16(tpid1))
+			&& mv_pp2x_prs_tcam_data_cmp(pe, 4, swab16(tpid2));
+
+		if (!match)
+			continue;
+
+		ri_mask = mv_pp2x_prs_sram_ri_get(pe) & MVPP2_PRS_RI_VLAN_MASK;
+		if (ri_mask == MVPP2_PRS_RI_VLAN_DOUBLE)
+			return pe;
+	}
+	kfree(pe);
+
+	return NULL;
+}
+#endif
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Add or update double vlan entry */
+static int mv_pp2x_prs_double_vlan_add(struct mv_pp2x_hw *hw,
+				       unsigned short tpid1,
+				       unsigned short tpid2,
+				       unsigned int port_map)
+{
+	struct mv_pp2x_prs_entry *pe;
+	int tid_aux, tid, ai, ret = 0;
+
+	pe = mv_pp2x_prs_double_vlan_find(hw, tpid1, tpid2);
+
+	if (!pe) {
+		/* Create new tcam entry */
+		tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+						  MVPP2_PE_LAST_FREE_TID);
+		if (tid < 0)
+			return tid;
+
+		pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+		if (!pe)
+			return -ENOMEM;
+
+		/* Set ai value for new double vlan entry */
+		ai = mv_pp2x_prs_double_vlan_ai_free_get(hw);
+		if (ai < 0) {
+			ret = ai;
+			goto error;
+		}
+
+		/* Get first single/triple vlan tid */
+		for (tid_aux = MVPP2_PE_FIRST_FREE_TID;
+		     tid_aux <= MVPP2_PE_LAST_FREE_TID; tid_aux++) {
+			unsigned int ri_bits;
+
+			if (!hw->prs_shadow[tid_aux].valid ||
+			    hw->prs_shadow[tid_aux].lu != MVPP2_PRS_LU_VLAN)
+				continue;
+
+			pe->index = tid_aux;
+			mv_pp2x_prs_hw_read(hw, pe);
+			ri_bits = mv_pp2x_prs_sram_ri_get(pe);
+			ri_bits &= MVPP2_PRS_RI_VLAN_MASK;
+			if (ri_bits == MVPP2_PRS_RI_VLAN_SINGLE ||
+			    ri_bits == MVPP2_PRS_RI_VLAN_TRIPLE)
+				break;
+		}
+
+		if (tid >= tid_aux) {
+			ret = -ERANGE;
+			goto error;
+		}
+
+		memset(pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);
+		pe->index = tid;
+
+		hw->prs_double_vlans[ai] = true;
+
+		mv_pp2x_prs_match_etype(pe, 0, tpid1);
+		mv_pp2x_prs_match_etype(pe, 4, tpid2);
+
+		mv_pp2x_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_VLAN);
+		/* Shift 8 bytes - skip 2 vlan tags */
+		mv_pp2x_prs_sram_shift_set(pe, 2 * MVPP2_VLAN_TAG_LEN,
+					   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+		mv_pp2x_prs_sram_ri_update(pe, MVPP2_PRS_RI_VLAN_DOUBLE,
+					   MVPP2_PRS_RI_VLAN_MASK);
+		mv_pp2x_prs_sram_ai_update(pe, ai | MVPP2_PRS_DBL_VLAN_AI_BIT,
+					   MVPP2_PRS_SRAM_AI_MASK);
+
+		mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_VLAN);
+	}
+
+	/* Update ports' mask */
+	mv_pp2x_prs_tcam_port_map_set(pe, port_map);
+	mv_pp2x_prs_hw_write(hw, pe);
+
+error:
+	kfree(pe);
+	return ret;
+}
+#endif
+
+/* IPv4 header parsing for fragmentation and L4 offset */
+static int mv_pp2x_prs_ip4_proto(struct mv_pp2x_hw *hw, unsigned short proto,
+				 unsigned int ri, unsigned int ri_mask)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid;
+
+	if ((proto != IPPROTO_TCP) && (proto != IPPROTO_UDP) &&
+	    (proto != IPPROTO_IGMP))
+		return -EINVAL;
+
+	/* Not fragmented packet */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	pe.index = tid;
+
+	/* Set next lu to IPv4 */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_sram_shift_set(&pe, 12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Set L4 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,
+				    sizeof(struct iphdr) - 4,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+	mv_pp2x_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				   MVPP2_PRS_IPV4_DIP_AI_BIT);
+	mv_pp2x_prs_sram_ri_update(&pe, ri | MVPP2_PRS_RI_IP_FRAG_FALSE,
+				   ri_mask | MVPP2_PRS_RI_IP_FRAG_MASK);
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 2, 0x00,
+				       MVPP2_PRS_TCAM_PROTO_MASK_L);
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 3, 0x00,
+				       MVPP2_PRS_TCAM_PROTO_MASK);
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 5, proto,
+				       MVPP2_PRS_TCAM_PROTO_MASK);
+	mv_pp2x_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Fragmented packet */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	pe.index = tid;
+	/* Clear ri before updating */
+	pe.sram.word[MVPP2_PRS_SRAM_RI_WORD] = 0x0;
+	pe.sram.word[MVPP2_PRS_SRAM_RI_CTRL_WORD] = 0x0;
+	mv_pp2x_prs_sram_ri_update(&pe, ri, ri_mask);
+	mv_pp2x_prs_sram_ri_update(&pe, ri | MVPP2_PRS_RI_IP_FRAG_TRUE,
+				   ri_mask | MVPP2_PRS_RI_IP_FRAG_MASK);
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 2, 0x00, 0x0);
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 3, 0x00, 0x0);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+
+/* IPv4 L3 multicast or broadcast */
+static int mv_pp2x_prs_ip4_cast(struct mv_pp2x_hw *hw, unsigned short l3_cast)
+{
+	struct mv_pp2x_prs_entry pe;
+	int mask, tid;
+
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	pe.index = tid;
+
+	switch (l3_cast) {
+	case MVPP2_PRS_L3_MULTI_CAST:
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 0, MVPP2_PRS_IPV4_MC,
+					       MVPP2_PRS_IPV4_MC_MASK);
+		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_MCAST,
+					   MVPP2_PRS_RI_L3_ADDR_MASK);
+		break;
+	case  MVPP2_PRS_L3_BROAD_CAST:
+		mask = MVPP2_PRS_IPV4_BC_MASK;
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 0, mask, mask);
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 1, mask, mask);
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 2, mask, mask);
+		mv_pp2x_prs_tcam_data_byte_set(&pe, 3, mask, mask);
+		mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_BCAST,
+					   MVPP2_PRS_RI_L3_ADDR_MASK);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				   MVPP2_PRS_IPV4_DIP_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Set entries for protocols over IPv6  */
+static int mv_pp2x_prs_ip6_proto(struct mv_pp2x_hw *hw, unsigned short proto,
+				 unsigned int ri, unsigned int ri_mask)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid;
+
+	if ((proto != IPPROTO_TCP) && (proto != IPPROTO_UDP) &&
+	    (proto != IPPROTO_ICMPV6) && (proto != IPPROTO_IPIP))
+		return -EINVAL;
+
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	pe.index = tid;
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, ri, ri_mask);
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,
+				    sizeof(struct ipv6hdr) - 6,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 0, proto,
+			MVPP2_PRS_TCAM_PROTO_MASK);
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,
+				 MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Write HW */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+#endif
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* IPv6 L3 multicast entry */
+static int mv_pp2x_prs_ip6_cast(struct mv_pp2x_hw *hw, unsigned short l3_cast)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid;
+
+	if (l3_cast != MVPP2_PRS_L3_MULTI_CAST)
+		return -EINVAL;
+
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	pe.index = tid;
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_MCAST,
+				   MVPP2_PRS_RI_L3_ADDR_MASK);
+	mv_pp2x_prs_sram_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,
+				   MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+	/* Shift back to IPv6 NH */
+	mv_pp2x_prs_sram_shift_set(&pe, -18, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 0, MVPP2_PRS_IPV6_MC,
+				       MVPP2_PRS_IPV6_MC_MASK);
+	mv_pp2x_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+#endif
+
+/* Parser per-port initialization */
+void mv_pp2x_prs_hw_port_init(struct mv_pp2x_hw *hw, int port, int lu_first,
+			      int lu_max, int offset)
+{
+	u32 val;
+
+	/* Set lookup ID */
+	val = mv_pp2x_read(hw, MVPP2_PRS_INIT_LOOKUP_REG);
+	val &= ~MVPP2_PRS_PORT_LU_MASK(port);
+	val |=  MVPP2_PRS_PORT_LU_VAL(port, lu_first);
+	mv_pp2x_write(hw, MVPP2_PRS_INIT_LOOKUP_REG, val);
+
+	/* Set maximum number of loops for packet received from port */
+	val = mv_pp2x_read(hw, MVPP2_PRS_MAX_LOOP_REG(port));
+	val &= ~MVPP2_PRS_MAX_LOOP_MASK(port);
+	val |= MVPP2_PRS_MAX_LOOP_VAL(port, lu_max);
+	mv_pp2x_write(hw, MVPP2_PRS_MAX_LOOP_REG(port), val);
+
+	/* Set initial offset for packet header extraction for the first
+	 * searching loop
+	 */
+	val = mv_pp2x_read(hw, MVPP2_PRS_INIT_OFFS_REG(port));
+	val &= ~MVPP2_PRS_INIT_OFF_MASK(port);
+	val |= MVPP2_PRS_INIT_OFF_VAL(port, offset);
+	mv_pp2x_write(hw, MVPP2_PRS_INIT_OFFS_REG(port), val);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_hw_port_init);
+
+/* Default flow entries initialization for all ports */
+static void mv_pp2x_prs_def_flow_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+	int port;
+
+	for (port = 0; port < MVPP2_MAX_PORTS; port++) {
+		memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+		mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+		pe.index = MVPP2_PE_FIRST_DEFAULT_FLOW - port;
+
+		/* Mask all ports */
+		mv_pp2x_prs_tcam_port_map_set(&pe, 0);
+
+		/* Set flow ID*/
+		mv_pp2x_prs_sram_ai_update(&pe, port, MVPP2_PRS_FLOW_ID_MASK);
+		mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);
+
+		/* Update shadow table and hw entry */
+		mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_FLOWS);
+
+		/*pr_crit("mv_pp2x_prs_def_flow_init: port(%d), index(%d)\n",
+		 * port, pe.index);
+		 */
+		mv_pp2x_prs_hw_write(hw, &pe);
+	}
+}
+
+/* Set default entry for Marvell Header field */
+static void mv_pp2x_prs_mh_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+
+	pe.index = MVPP2_PE_MH_DEFAULT;
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MH);
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_MH_SIZE,
+				 MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_MAC);
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MH);
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
+/* Set default entires (place holder) for promiscuous, non-promiscuous and
+ * multicast MAC addresses
+ */
+static void mv_pp2x_prs_mac_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+
+	/* Non-promiscuous mode for all ports - DROP unknown packets */
+	pe.index = MVPP2_PE_MAC_NON_PROMISCUOUS;
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);
+
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DROP_MASK,
+				   MVPP2_PRS_RI_DROP_MASK);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MAC);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* place holders only - no ports */
+	mv_pp2x_prs_mac_drop_all_set(hw, 0, false);
+	mv_pp2x_prs_mac_promisc_set(hw, 0, false);
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	mv_pp2x_prs_mac_multi_set(hw, MVPP2_PE_MAC_MC_ALL, 0, false);
+	mv_pp2x_prs_mac_multi_set(hw, MVPP2_PE_MAC_MC_IP6, 0, false);
+#endif
+}
+
+/* Set default entries for various types of dsa packets */
+static void mv_pp2x_prs_dsa_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	/* None tagged EDSA entry - place holder */
+	mv_pp2x_prs_dsa_tag_set(hw, 0, false, MVPP2_PRS_UNTAGGED,
+				MVPP2_PRS_EDSA);
+
+	/* Tagged EDSA entry - place holder */
+	mv_pp2x_prs_dsa_tag_set(hw, 0, false, MVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);
+
+	/* None tagged DSA entry - place holder */
+	mv_pp2x_prs_dsa_tag_set(hw, 0, false, MVPP2_PRS_UNTAGGED,
+				MVPP2_PRS_DSA);
+
+	/* Tagged DSA entry - place holder */
+	mv_pp2x_prs_dsa_tag_set(hw, 0, false, MVPP2_PRS_TAGGED, MVPP2_PRS_DSA);
+
+	/* None tagged EDSA ethertype entry - place holder*/
+	mv_pp2x_prs_dsa_tag_ethertype_set(hw, 0, false,
+					  MVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);
+
+	/* Tagged EDSA ethertype entry - place holder*/
+	mv_pp2x_prs_dsa_tag_ethertype_set(hw, 0, false,
+					  MVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);
+
+	/* None tagged DSA ethertype entry */
+	mv_pp2x_prs_dsa_tag_ethertype_set(hw, 0, true,
+					  MVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);
+
+	/* Tagged DSA ethertype entry */
+	mv_pp2x_prs_dsa_tag_ethertype_set(hw, 0, true,
+					  MVPP2_PRS_TAGGED, MVPP2_PRS_DSA);
+#endif
+
+	/* Set default entry, in case DSA or EDSA tag not found */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);
+	pe.index = MVPP2_PE_DSA_DEFAULT;
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);
+
+	/* Shift 0 bytes */
+	mv_pp2x_prs_sram_shift_set(&pe, 0, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_MAC);
+
+	/* Clear all sram ai bits for next iteration */
+	mv_pp2x_prs_sram_ai_update(&pe, 0, MVPP2_PRS_SRAM_AI_MASK);
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	mv_pp2x_prs_hw_write(hw, &pe);
+}
+
+/* Match basic ethertypes */
+static int mv_pp2x_prs_etype_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid;
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	/* Ethertype: PPPoE */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, ETH_P_PPP_SES);
+
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_PPPOE_HDR_SIZE,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_PPPOE);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_PPPOE_MASK,
+				   MVPP2_PRS_RI_PPPOE_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = false;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_PPPOE_MASK,
+				  MVPP2_PRS_RI_PPPOE_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+#endif
+	/* Ethertype: ARP */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, ETH_P_ARP);
+
+	/* Generate flow in the next iteration*/
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_ARP,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+	/* Set L3 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = true;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_L3_ARP,
+				  MVPP2_PRS_RI_L3_PROTO_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	/* Ethertype: LBTD */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, MVPP2_IP_LBDT_TYPE);
+
+	/* Generate flow in the next iteration*/
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_CPU_CODE_RX_SPEC |
+				   MVPP2_PRS_RI_UDF3_RX_SPECIAL,
+				   MVPP2_PRS_RI_CPU_CODE_MASK |
+				   MVPP2_PRS_RI_UDF3_MASK);
+	/* Set L3 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = true;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_CPU_CODE_RX_SPEC |
+				  MVPP2_PRS_RI_UDF3_RX_SPECIAL,
+				  MVPP2_PRS_RI_CPU_CODE_MASK |
+				  MVPP2_PRS_RI_UDF3_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+#endif
+
+	/* Ethertype: IPv4 without options */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, ETH_P_IP);
+	mv_pp2x_prs_tcam_data_byte_set(&pe, MVPP2_ETH_TYPE_LEN,
+				       MVPP2_PRS_IPV4_HEAD |
+				       MVPP2_PRS_IPV4_IHL,
+				       MVPP2_PRS_IPV4_HEAD_MASK |
+				       MVPP2_PRS_IPV4_IHL_MASK);
+
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+	/* Skip eth_type + 4 bytes of IP header */
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Set L3 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = false;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_L3_IP4,
+				  MVPP2_PRS_RI_L3_PROTO_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Ethertype: IPv4 with options */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	pe.index = tid;
+
+	/* Clear tcam data before updating */
+	pe.tcam.byte[TCAM_DATA_BYTE(MVPP2_ETH_TYPE_LEN)] = 0x0;
+	pe.tcam.byte[TCAM_DATA_MASK(MVPP2_ETH_TYPE_LEN)] = 0x0;
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, MVPP2_ETH_TYPE_LEN,
+				       MVPP2_PRS_IPV4_HEAD,
+				       MVPP2_PRS_IPV4_HEAD_MASK);
+
+	/* Clear ri before updating */
+	pe.sram.word[MVPP2_PRS_SRAM_RI_WORD] = 0x0;
+	pe.sram.word[MVPP2_PRS_SRAM_RI_CTRL_WORD] = 0x0;
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4_OPT,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = false;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_L3_IP4_OPT,
+				  MVPP2_PRS_RI_L3_PROTO_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+
+	/* Ethertype: IPv6 without options */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, ETH_P_IPV6);
+
+	/* Skip DIP of IPV6 header */
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 8 +
+				   MVPP2_MAX_L3_ADDR_SIZE,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP6,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+	/* Set L3 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = false;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_L3_IP6,
+				  MVPP2_PRS_RI_L3_PROTO_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Default entry for MVPP2_PRS_LU_L2 - Unknown ethtype */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);
+	pe.index = MVPP2_PE_ETH_TYPE_UN;
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Generate flow in the next iteration*/
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UN,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+	/* Set L3 offset even it's unknown L3 */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_L2);
+	hw->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;
+	hw->prs_shadow[pe.index].finish = true;
+	mv_pp2x_prs_shadow_ri_set(hw, pe.index, MVPP2_PRS_RI_L3_UN,
+				  MVPP2_PRS_RI_L3_PROTO_MASK);
+	mv_pp2x_prs_hw_write(hw, &pe);
+#endif
+	return 0;
+}
+
+/* Configure vlan entries and detect up to 2 successive VLAN tags.
+ * Possible options:
+ * 0x8100, 0x88A8
+ * 0x8100, 0x8100
+ * 0x8100
+ * 0x88A8
+ */
+static int mv_pp2x_prs_vlan_init(struct platform_device *pdev,
+				 struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	int err;
+#endif
+
+	hw->prs_double_vlans = devm_kcalloc(&pdev->dev, sizeof(bool),
+					    MVPP2_PRS_DBL_VLANS_MAX,
+					    GFP_KERNEL);
+	if (!hw->prs_double_vlans)
+		return -ENOMEM;
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	/* Double VLAN: 0x8100, 0x88A8 */
+	err = mv_pp2x_prs_double_vlan_add(hw, ETH_P_8021Q, ETH_P_8021AD,
+					  MVPP2_PRS_PORT_MASK);
+	if (err)
+		return err;
+
+	/* Double VLAN: 0x8100, 0x8100 */
+	err = mv_pp2x_prs_double_vlan_add(hw, ETH_P_8021Q, ETH_P_8021Q,
+					  MVPP2_PRS_PORT_MASK);
+	if (err)
+		return err;
+
+	/* Single VLAN: 0x88a8 */
+	err = mv_pp2x_prs_vlan_add(hw, ETH_P_8021AD, MVPP2_PRS_SINGLE_VLAN_AI,
+				   MVPP2_PRS_PORT_MASK);
+	if (err)
+		return err;
+
+	/* Single VLAN: 0x8100 */
+	err = mv_pp2x_prs_vlan_add(hw, ETH_P_8021Q, MVPP2_PRS_SINGLE_VLAN_AI,
+				   MVPP2_PRS_PORT_MASK);
+	if (err)
+		return err;
+
+	/* Set default double vlan entry */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_VLAN);
+	pe.index = MVPP2_PE_VLAN_DBL;
+
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);
+	/* Clear ai for next iterations */
+	mv_pp2x_prs_sram_ai_update(&pe, 0, MVPP2_PRS_SRAM_AI_MASK);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_DOUBLE,
+				   MVPP2_PRS_RI_VLAN_MASK);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_DBL_VLAN_AI_BIT,
+				   MVPP2_PRS_DBL_VLAN_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_VLAN);
+	mv_pp2x_prs_hw_write(hw, &pe);
+#endif
+	/* Set default vlan none entry */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_VLAN);
+	pe.index = MVPP2_PE_VLAN_NONE;
+
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,
+				   MVPP2_PRS_RI_VLAN_MASK);
+
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_VLAN);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Set entries for PPPoE ethertype */
+static int mv_pp2x_prs_pppoe_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid;
+
+	/* IPv4 over PPPoE with options */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_PPPOE);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, PPP_IP);
+
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4_OPT,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+	/* Skip eth_type + 4 bytes of IP header */
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Set L3 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_PPPOE);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* IPv4 over PPPoE without options */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	pe.index = tid;
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, MVPP2_ETH_TYPE_LEN,
+				       MVPP2_PRS_IPV4_HEAD | MVPP2_PRS_IPV4_IHL,
+				       MVPP2_PRS_IPV4_HEAD_MASK |
+				       MVPP2_PRS_IPV4_IHL_MASK);
+
+	/* Clear ri before updating */
+	pe.sram.word[MVPP2_PRS_SRAM_RI_WORD] = 0x0;
+	pe.sram.word[MVPP2_PRS_SRAM_RI_CTRL_WORD] = 0x0;
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_PPPOE);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* IPv6 over PPPoE */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_PPPOE);
+	pe.index = tid;
+
+	mv_pp2x_prs_match_etype(&pe, 0, PPP_IPV6);
+
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP6,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+	/* Skip eth_type + 4 bytes of IPv6 header */
+	mv_pp2x_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Set L3 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_PPPOE);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Non-IP over PPPoE */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_PPPOE);
+	pe.index = tid;
+
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UN,
+				   MVPP2_PRS_RI_L3_PROTO_MASK);
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	/* Set L3 offset even if it's unknown L3 */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,
+				    MVPP2_ETH_TYPE_LEN,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_PPPOE);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+#endif
+
+/* Initialize entries for IPv4 */
+static int mv_pp2x_prs_ip4_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+	int err;
+
+	/* Set entries for TCP, UDP and IGMP over IPv4 */
+	err = mv_pp2x_prs_ip4_proto(hw, IPPROTO_TCP, MVPP2_PRS_RI_L4_TCP,
+				    MVPP2_PRS_RI_L4_PROTO_MASK);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err)
+		return err;
+
+	err = mv_pp2x_prs_ip4_proto(hw, IPPROTO_UDP, MVPP2_PRS_RI_L4_UDP,
+				    MVPP2_PRS_RI_L4_PROTO_MASK);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err)
+		return err;
+
+	err = mv_pp2x_prs_ip4_proto(hw, IPPROTO_IGMP,
+				    MVPP2_PRS_RI_CPU_CODE_RX_SPEC |
+				    MVPP2_PRS_RI_UDF3_RX_SPECIAL,
+				    MVPP2_PRS_RI_CPU_CODE_MASK |
+				    MVPP2_PRS_RI_UDF3_MASK);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err)
+		return err;
+
+	/* IPv4 Broadcast */
+	err = mv_pp2x_prs_ip4_cast(hw, MVPP2_PRS_L3_BROAD_CAST);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err)
+		return err;
+
+	/* IPv4 Multicast */
+	err = mv_pp2x_prs_ip4_cast(hw, MVPP2_PRS_L3_MULTI_CAST);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err)
+		return err;
+
+	/* Default IPv4 entry for unknown protocols */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	pe.index = MVPP2_PE_IP4_PROTO_UN;
+
+	/* Set next lu to IPv4 */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_sram_shift_set(&pe, 12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+	/* Set L4 offset */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,
+				    sizeof(struct iphdr) - 4,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+	mv_pp2x_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				   MVPP2_PRS_IPV4_DIP_AI_BIT);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,
+				   MVPP2_PRS_RI_L4_PROTO_MASK);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Default IPv4 entry for unicast address */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);
+	pe.index = MVPP2_PE_IP4_ADDR_UN;
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UCAST,
+				   MVPP2_PRS_RI_L3_ADDR_MASK);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,
+				   MVPP2_PRS_IPV4_DIP_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	return 0;
+}
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+/* Initialize entries for IPv6 */
+static int mv_pp2x_prs_ip6_init(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_prs_entry pe;
+	int tid, err;
+
+	/* Set entries for TCP, UDP and ICMP over IPv6 */
+	err = mv_pp2x_prs_ip6_proto(hw, IPPROTO_TCP,
+				    MVPP2_PRS_RI_L4_TCP,
+				    MVPP2_PRS_RI_L4_PROTO_MASK);
+	if (err)
+		return err;
+
+	err = mv_pp2x_prs_ip6_proto(hw, IPPROTO_UDP,
+				    MVPP2_PRS_RI_L4_UDP,
+				    MVPP2_PRS_RI_L4_PROTO_MASK);
+	if (err)
+		return err;
+
+	err = mv_pp2x_prs_ip6_proto(hw, IPPROTO_ICMPV6,
+				    MVPP2_PRS_RI_CPU_CODE_RX_SPEC |
+				    MVPP2_PRS_RI_UDF3_RX_SPECIAL,
+				    MVPP2_PRS_RI_CPU_CODE_MASK |
+				    MVPP2_PRS_RI_UDF3_MASK);
+	if (err)
+		return err;
+
+	/* IPv4 is the last header. This is similar case as 6-TCP or 17-UDP */
+	/* Result Info: UDF7=1, DS lite */
+	err = mv_pp2x_prs_ip6_proto(hw, IPPROTO_IPIP,
+				    MVPP2_PRS_RI_UDF7_IP6_LITE,
+				    MVPP2_PRS_RI_UDF7_MASK);
+	if (err)
+		return err;
+
+	/* IPv6 multicast */
+	err = mv_pp2x_prs_ip6_cast(hw, MVPP2_PRS_L3_MULTI_CAST);
+	if (err)
+		return err;
+
+	/* Entry for checking hop limit */
+	tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+					  MVPP2_PE_LAST_FREE_TID);
+	if (tid < 0)
+		return tid;
+
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	pe.index = tid;
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UN |
+				   MVPP2_PRS_RI_DROP_MASK,
+				   MVPP2_PRS_RI_L3_PROTO_MASK |
+				   MVPP2_PRS_RI_DROP_MASK);
+
+	mv_pp2x_prs_tcam_data_byte_set(&pe, 1, 0x00, MVPP2_PRS_IPV6_HOP_MASK);
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,
+				   MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Default IPv6 entry for unknown protocols */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	pe.index = MVPP2_PE_IP6_PROTO_UN;
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,
+				   MVPP2_PRS_RI_L4_PROTO_MASK);
+	/* Set L4 offset relatively to our current place */
+	mv_pp2x_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,
+				    sizeof(struct ipv6hdr) - 4,
+				    MVPP2_PRS_SRAM_OP_SEL_UDF_ADD);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,
+				   MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Default IPv6 entry for unknown ext protocols */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	pe.index = MVPP2_PE_IP6_EXT_PROTO_UN;
+
+	/* Finished: go to flowid generation */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);
+	mv_pp2x_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,
+				   MVPP2_PRS_RI_L4_PROTO_MASK);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_EXT_AI_BIT,
+				   MVPP2_PRS_IPV6_EXT_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP4);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	/* Default IPv6 entry for unicast address */
+	memset(&pe, 0, sizeof(struct mv_pp2x_prs_entry));
+	mv_pp2x_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	pe.index = MVPP2_PE_IP6_ADDR_UN;
+
+	/* Finished: go to IPv6 again */
+	mv_pp2x_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UCAST,
+				   MVPP2_PRS_RI_L3_ADDR_MASK);
+	mv_pp2x_prs_sram_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,
+				   MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+	/* Shift back to IPV6 NH */
+	mv_pp2x_prs_sram_shift_set(&pe, -18, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+	mv_pp2x_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV6_NO_EXT_AI_BIT);
+	/* Unmask all ports */
+	mv_pp2x_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);
+
+	/* Update shadow table and hw entry */
+	mv_pp2x_prs_shadow_set(hw, pe.index, MVPP2_PRS_LU_IP6);
+	mv_pp2x_prs_hw_write(hw, &pe);
+
+	return 0;
+}
+#endif
+
+/* Compare MAC DA with tcam entry data */
+static bool mv_pp2x_prs_mac_range_equals(struct mv_pp2x_prs_entry *pe,
+					 const u8 *da, unsigned char *mask)
+{
+	unsigned char tcam_byte, tcam_mask;
+	int index;
+
+	for (index = 0; index < ETH_ALEN; index++) {
+		mv_pp2x_prs_tcam_data_byte_get(pe, index, &tcam_byte,
+			&tcam_mask);
+		if (tcam_mask != mask[index])
+			return false;
+
+		if ((tcam_mask & tcam_byte) != (da[index] & mask[index]))
+			return false;
+	}
+
+	return true;
+}
+
+/* Find tcam entry with matched pair <MAC DA, port> */
+static struct mv_pp2x_prs_entry *
+mv_pp2x_prs_mac_da_range_find(struct mv_pp2x_hw *hw, int pmap, const u8 *da,
+			      unsigned char *mask, int udf_type)
+{
+	struct mv_pp2x_prs_entry *pe;
+	int tid;
+
+	pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+	if (!pe)
+		return NULL;
+	mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_MAC);
+
+	/* Go through the all entires with MVPP2_PRS_LU_MAC */
+	for (tid = MVPP2_PE_FIRST_FREE_TID;
+	     tid <= MVPP2_PE_LAST_FREE_TID; tid++) {
+		unsigned int entry_pmap;
+
+		if (!hw->prs_shadow[tid].valid ||
+		    (hw->prs_shadow[tid].lu != MVPP2_PRS_LU_MAC) ||
+		    (hw->prs_shadow[tid].udf != udf_type))
+			continue;
+
+		pe->index = tid;
+		mv_pp2x_prs_hw_read(hw, pe);
+		entry_pmap = mv_pp2x_prs_tcam_port_map_get(pe);
+
+		if (mv_pp2x_prs_mac_range_equals(pe, da, mask) &&
+		    entry_pmap == pmap)
+			return pe;
+	}
+	kfree(pe);
+
+	return NULL;
+}
+
+/* Update parser's mac da entry */
+int mv_pp2x_prs_mac_da_accept(struct mv_pp2x_hw *hw, int port,
+				   const u8 *da, bool add)
+{
+	struct mv_pp2x_prs_entry *pe;
+	unsigned int pmap, len, ri;
+	unsigned char mask[ETH_ALEN] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+	int tid;
+
+	/* Scan TCAM and see if entry with this <MAC DA, port> already exist */
+	pe = mv_pp2x_prs_mac_da_range_find(hw, (1 << port), da, mask,
+					   MVPP2_PRS_UDF_MAC_DEF);
+
+	/* No such entry */
+	if (!pe) {
+		if (!add)
+			return 0;
+
+		/* Create new TCAM entry */
+		/* Find first range mac entry*/
+		for (tid = MVPP2_PE_FIRST_FREE_TID;
+		     tid <= MVPP2_PE_LAST_FREE_TID; tid++)
+			if (hw->prs_shadow[tid].valid &&
+			    (hw->prs_shadow[tid].lu == MVPP2_PRS_LU_MAC) &&
+			    (hw->prs_shadow[tid].udf ==
+						       MVPP2_PRS_UDF_MAC_RANGE))
+				break;
+
+		/* Go through the all entries from first to last */
+		tid = mv_pp2x_prs_tcam_first_free(hw, MVPP2_PE_FIRST_FREE_TID,
+						  tid - 1);
+		if (tid < 0)
+			return tid;
+
+		pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+		if (!pe)
+			return -1;
+		mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_MAC);
+		pe->index = tid;
+
+		/* Mask all ports */
+		mv_pp2x_prs_tcam_port_map_set(pe, 0);
+	}
+
+	/* Update port mask */
+	mv_pp2x_prs_tcam_port_set(pe, port, add);
+
+	/* Invalidate the entry if no ports are left enabled */
+	pmap = mv_pp2x_prs_tcam_port_map_get(pe);
+	if (pmap == 0) {
+		if (add) {
+			kfree(pe);
+			return -1;
+		}
+		mv_pp2x_prs_hw_inv(hw, pe->index);
+		hw->prs_shadow[pe->index].valid = false;
+		kfree(pe);
+		return 0;
+	}
+
+	/* Continue - set next lookup */
+	mv_pp2x_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_DSA);
+
+	/* Set match on DA */
+	len = ETH_ALEN;
+	while (len--)
+		mv_pp2x_prs_tcam_data_byte_set(pe, len, da[len], 0xff);
+
+	/* Set result info bits */
+	if (is_broadcast_ether_addr(da))
+		ri = MVPP2_PRS_RI_L2_BCAST;
+	else if (is_multicast_ether_addr(da))
+		ri = MVPP2_PRS_RI_L2_MCAST;
+	else
+		ri = MVPP2_PRS_RI_L2_UCAST | MVPP2_PRS_RI_MAC_ME_MASK;
+
+	mv_pp2x_prs_sram_ri_update(pe, ri, MVPP2_PRS_RI_L2_CAST_MASK |
+				   MVPP2_PRS_RI_MAC_ME_MASK);
+	mv_pp2x_prs_shadow_ri_set(hw, pe->index, ri, MVPP2_PRS_RI_L2_CAST_MASK |
+				  MVPP2_PRS_RI_MAC_ME_MASK);
+
+	/* Shift to ethertype */
+	mv_pp2x_prs_sram_shift_set(pe, 2 * ETH_ALEN,
+				   MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);
+
+	/* Update shadow table and hw entry */
+	hw->prs_shadow[pe->index].udf = MVPP2_PRS_UDF_MAC_DEF;
+	mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_MAC);
+	mv_pp2x_prs_hw_write(hw, pe);
+
+	kfree(pe);
+
+	return 0;
+}
+
+int mv_pp2x_prs_update_mac_da(struct net_device *dev, const u8 *da)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	int err;
+
+	/* Remove old parser entry */
+	err = mv_pp2x_prs_mac_da_accept(&port->priv->hw,
+		port->id, dev->dev_addr, false);
+	if (err)
+		return err;
+
+	/* Add new parser entry */
+	err = mv_pp2x_prs_mac_da_accept(&port->priv->hw, port->id, da, true);
+	if (err)
+		return err;
+
+	/* Set addr in the device */
+	ether_addr_copy(dev->dev_addr, da);
+
+	return 0;
+}
+
+/* Delete all port's multicast simple (not range) entries */
+void mv_pp2x_prs_mcast_del_all(struct mv_pp2x_hw *hw, int port)
+{
+	struct mv_pp2x_prs_entry pe;
+	int index, tid;
+
+	for (tid = MVPP2_PE_FIRST_FREE_TID;
+	     tid <= MVPP2_PE_LAST_FREE_TID; tid++) {
+		unsigned char da[ETH_ALEN], da_mask[ETH_ALEN];
+
+		if (!hw->prs_shadow[tid].valid ||
+		    (hw->prs_shadow[tid].lu != MVPP2_PRS_LU_MAC) ||
+		    (hw->prs_shadow[tid].udf != MVPP2_PRS_UDF_MAC_DEF))
+			continue;
+
+		/* Only simple mac entries */
+		pe.index = tid;
+		mv_pp2x_prs_hw_read(hw, &pe);
+
+		/* Read mac addr from entry */
+		for (index = 0; index < ETH_ALEN; index++)
+			mv_pp2x_prs_tcam_data_byte_get(&pe, index, &da[index],
+						       &da_mask[index]);
+
+		if (is_multicast_ether_addr(da) && !is_broadcast_ether_addr(da))
+			/* Delete this entry */
+			mv_pp2x_prs_mac_da_accept(hw, port, da, false);
+	}
+}
+
+int mv_pp2x_prs_tag_mode_set(struct mv_pp2x_hw *hw, int port, int type)
+{
+	switch (type) {
+	case MVPP2_TAG_TYPE_EDSA:
+		/* Add port to EDSA entries */
+		mv_pp2x_prs_dsa_tag_set(hw, port, true,
+					MVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, true,
+					MVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);
+		/* Remove port from DSA entries */
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_TAGGED, MVPP2_PRS_DSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);
+		break;
+
+	case MVPP2_TAG_TYPE_DSA:
+		/* Add port to DSA entries */
+		mv_pp2x_prs_dsa_tag_set(hw, port, true,
+					MVPP2_PRS_TAGGED, MVPP2_PRS_DSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, true,
+					MVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);
+		/* Remove port from EDSA entries */
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);
+		break;
+
+	case MVPP2_TAG_TYPE_MH:
+	case MVPP2_TAG_TYPE_NONE:
+		/* Remove port form EDSA and DSA entries */
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_TAGGED, MVPP2_PRS_DSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);
+		mv_pp2x_prs_dsa_tag_set(hw, port, false,
+					MVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);
+		break;
+
+	default:
+		if ((type < 0) || (type > MVPP2_TAG_TYPE_EDSA))
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* Set prs flow for the port */
+int mv_pp2x_prs_def_flow(struct mv_pp2x_port *port)
+{
+	struct mv_pp2x_prs_entry *pe;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	int tid;
+
+	pe = mv_pp2x_prs_flow_find(hw, port->id, 0, 0);
+
+	/* Such entry not exist */
+	if (!pe) {
+		/* Go through the all entires from last to first */
+		tid = mv_pp2x_prs_tcam_first_free(hw,
+						  MVPP2_PE_LAST_FREE_TID,
+						  MVPP2_PE_FIRST_FREE_TID);
+		if (tid < 0)
+			return tid;
+
+		pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+		if (!pe)
+			return -ENOMEM;
+
+		mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_FLOWS);
+		pe->index = tid;
+
+		/* Set flow ID*/
+		mv_pp2x_prs_sram_ai_update(pe, port->id,
+					   MVPP2_PRS_FLOW_ID_MASK);
+		mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_FLOWS);
+
+		/*pr_crit("mv_pp2x_prs_def_flow: index(%d) port->id\n",
+		 * pe->index, port->id);
+		 */
+	}
+
+	mv_pp2x_prs_tcam_port_map_set(pe, (1 << port->id));
+	mv_pp2x_prs_hw_write(hw, pe);
+	kfree(pe);
+
+	return 0;
+}
+
+/* Set prs dedicated flow for the port */
+int mv_pp2x_prs_flow_id_gen(struct mv_pp2x_port *port, u32 flowId,
+			    u32 res, u32 resMask)
+{
+	struct mv_pp2x_prs_entry *pe;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	int tid;
+	unsigned int pmap = 0;
+
+	pe = mv_pp2x_prs_flow_find(hw, flowId, res, resMask);
+
+	/* Such entry not exist */
+	if (!pe) {
+		pe = kzalloc(sizeof(*pe), GFP_KERNEL);
+		if (!pe)
+			return -ENOMEM;
+
+		/* Go through the all entires from last to first */
+		tid = mv_pp2x_prs_tcam_first_free(hw,
+			MVPP2_PE_LAST_FREE_TID,
+			MVPP2_PE_FIRST_FREE_TID);
+		if (tid < 0) {
+			kfree(pe);
+			return tid;
+		}
+
+		mv_pp2x_prs_tcam_lu_set(pe, MVPP2_PRS_LU_FLOWS);
+		pe->index = tid;
+
+		mv_pp2x_prs_sram_ai_update(pe, flowId, MVPP2_PRS_FLOW_ID_MASK);
+		mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);
+
+		/* Update shadow table */
+		mv_pp2x_prs_shadow_set(hw, pe->index, MVPP2_PRS_LU_FLOWS);
+
+		/*update result data and mask*/
+		mv_pp2x_prs_tcam_data_dword_set(pe, 0, res, resMask);
+	} else {
+		pmap = mv_pp2x_prs_tcam_port_map_get(pe);
+	}
+
+	mv_pp2x_prs_tcam_port_map_set(pe, (1 << port->id) | pmap);
+	mv_pp2x_prs_hw_write(hw, pe);
+	kfree(pe);
+
+	return 0;
+}
+
+int mv_pp2x_prs_flow_set(struct mv_pp2x_port *port)
+{
+	int index, ret;
+
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	for (index = 0; index < MVPP2_PRS_FL_TCAM_NUM; index++) {
+		ret = mv_pp2x_prs_flow_id_gen(port,
+			mv_pp2x_prs_flow_id_array[index].flow_id,
+			mv_pp2x_prs_flow_id_array[index].prs_result.ri,
+			mv_pp2x_prs_flow_id_array[index].prs_result.ri_mask);
+		if (ret)
+			return ret;
+	}
+#else
+
+	index = MVPP2_PRS_FL_IP4_UNTAG_NO_OPV4_OPTIONS;
+	ret = mv_pp2x_prs_flow_id_gen(port,
+			mv_pp2x_prs_flow_id_array[index].flow_id,
+			mv_pp2x_prs_flow_id_array[index].prs_result.ri,
+			mv_pp2x_prs_flow_id_array[index].prs_result.ri_mask);
+	if (ret)
+		return ret;
+
+	MVPP2_PRINT_LINE();
+
+
+	index = MVPP2_PRS_FL_NON_IP_UNTAG_INDEX;
+	ret = mv_pp2x_prs_flow_id_gen(port,
+			mv_pp2x_prs_flow_id_array[index].flow_id,
+			mv_pp2x_prs_flow_id_array[index].prs_result.ri,
+			mv_pp2x_prs_flow_id_array[index].prs_result.ri_mask);
+	if (ret)
+		return ret;
+
+	MVPP2_PRINT_LINE();
+#endif
+	return 0;
+}
+
+static void mv_pp2x_prs_flow_id_attr_set(int flow_id, int ri, int ri_mask)
+{
+	int flow_attr = 0;
+
+	flow_attr |= MVPP2_PRS_FL_ATTR_VLAN_BIT;
+	if (ri_mask & MVPP2_PRS_RI_VLAN_MASK &&
+	    (ri & MVPP2_PRS_RI_VLAN_MASK) == MVPP2_PRS_RI_VLAN_NONE)
+		flow_attr &= ~MVPP2_PRS_FL_ATTR_VLAN_BIT;
+
+	if ((ri & MVPP2_PRS_RI_L3_PROTO_MASK) == MVPP2_PRS_RI_L3_IP4 ||
+	    (ri & MVPP2_PRS_RI_L3_PROTO_MASK) == MVPP2_PRS_RI_L3_IP4_OPT ||
+	    (ri & MVPP2_PRS_RI_L3_PROTO_MASK) == MVPP2_PRS_RI_L3_IP4_OTHER)
+		flow_attr |= MVPP2_PRS_FL_ATTR_IP4_BIT;
+
+	if ((ri & MVPP2_PRS_RI_L3_PROTO_MASK) == MVPP2_PRS_RI_L3_IP6 ||
+	    (ri & MVPP2_PRS_RI_L3_PROTO_MASK) == MVPP2_PRS_RI_L3_IP6_EXT)
+		flow_attr |= MVPP2_PRS_FL_ATTR_IP6_BIT;
+
+	if ((ri & MVPP2_PRS_RI_L3_PROTO_MASK) == MVPP2_PRS_RI_L3_ARP)
+		flow_attr |= MVPP2_PRS_FL_ATTR_ARP_BIT;
+
+	if (ri & MVPP2_PRS_RI_IP_FRAG_MASK)
+		flow_attr |= MVPP2_PRS_FL_ATTR_FRAG_BIT;
+
+	if ((ri & MVPP2_PRS_RI_L4_PROTO_MASK) == MVPP2_PRS_RI_L4_TCP)
+		flow_attr |= MVPP2_PRS_FL_ATTR_TCP_BIT;
+
+	if ((ri & MVPP2_PRS_RI_L4_PROTO_MASK) == MVPP2_PRS_RI_L4_UDP)
+		flow_attr |= MVPP2_PRS_FL_ATTR_UDP_BIT;
+
+	mv_pp2x_prs_flow_id_attr_tbl[flow_id] = flow_attr;
+}
+
+/* Init lookup id attribute array */
+void mv_pp2x_prs_flow_id_attr_init(void)
+{
+	int index;
+	u32 ri, ri_mask, flow_id;
+
+	for (index = 0; index < MVPP2_PRS_FL_TCAM_NUM; index++) {
+		ri = mv_pp2x_prs_flow_id_array[index].prs_result.ri;
+		ri_mask = mv_pp2x_prs_flow_id_array[index].prs_result.ri_mask;
+		flow_id = mv_pp2x_prs_flow_id_array[index].flow_id;
+
+		mv_pp2x_prs_flow_id_attr_set(flow_id, ri, ri_mask);
+	}
+}
+
+int mv_pp2x_prs_flow_id_attr_get(int flow_id)
+{
+	return mv_pp2x_prs_flow_id_attr_tbl[flow_id];
+}
+
+/* Classifier configuration routines */
+
+/* Update classification flow table registers */
+void mv_pp2x_cls_flow_write(struct mv_pp2x_hw *hw,
+			    struct mv_pp2x_cls_flow_entry *fe)
+{
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_INDEX_REG, fe->index);
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_TBL0_REG,  fe->data[0]);
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_TBL1_REG,  fe->data[1]);
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_TBL2_REG,  fe->data[2]);
+}
+EXPORT_SYMBOL(mv_pp2x_cls_flow_write);
+
+static void mv_pp2x_cls_flow_read(struct mv_pp2x_hw *hw, int index,
+				  struct mv_pp2x_cls_flow_entry *fe)
+{
+	fe->index = index;
+	/*write index*/
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_INDEX_REG, index);
+
+	fe->data[0] = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL0_REG);
+	fe->data[1] = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL1_REG);
+	fe->data[2] = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL2_REG);
+}
+
+/* Update classification lookup table register */
+static void mv_pp2x_cls_lookup_write(struct mv_pp2x_hw *hw,
+				     struct mv_pp2x_cls_lookup_entry *le)
+{
+	u32 val;
+
+	val = (le->way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) | le->lkpid;
+	mv_pp2x_write(hw, MVPP2_CLS_LKP_INDEX_REG, val);
+	mv_pp2x_write(hw, MVPP2_CLS_LKP_TBL_REG, le->data);
+}
+
+void mv_pp2x_cls_lookup_read(struct mv_pp2x_hw *hw, int lkpid, int way,
+			     struct mv_pp2x_cls_lookup_entry *le)
+{
+	unsigned int val = 0;
+
+	/* write index reg */
+	val = (way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) | lkpid;
+	mv_pp2x_write(hw, MVPP2_CLS_LKP_INDEX_REG, val);
+	le->way = way;
+	le->lkpid = lkpid;
+	le->data = mv_pp2x_read(hw, MVPP2_CLS_LKP_TBL_REG);
+}
+
+/* Operations on flow entry */
+int mv_pp2x_cls_sw_flow_hek_num_set(struct mv_pp2x_cls_flow_entry *fe,
+				    int num_of_fields)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(num_of_fields, 0,
+	    MVPP2_CLS_FLOWS_TBL_FIELDS_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[1] &= ~MVPP2_FLOW_FIELDS_NUM_MASK;
+	fe->data[1] |= (num_of_fields << MVPP2_FLOW_FIELDS_NUM);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_hek_num_set);
+
+int mv_pp2x_cls_sw_flow_hek_set(struct mv_pp2x_cls_flow_entry *fe,
+				int field_index, int field_id)
+{
+	int num_of_fields;
+
+	/* get current num_of_fields */
+	num_of_fields = ((fe->data[1] &
+		MVPP2_FLOW_FIELDS_NUM_MASK) >> MVPP2_FLOW_FIELDS_NUM);
+
+	if (num_of_fields < (field_index+1)) {
+		pr_debug("%s:num of heks=%d ,idx(%d) out of range\n",
+			 __func__, num_of_fields, field_index);
+		return -1;
+	}
+
+	fe->data[2] &= ~MVPP2_FLOW_FIELD_MASK(field_index);
+	fe->data[2] |= (field_id <<  MVPP2_FLOW_FIELD_ID(field_index));
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_hek_set);
+
+static void mv_pp2x_cls_sw_flow_eng_set(struct mv_pp2x_cls_flow_entry *fe,
+					int engine, int is_last)
+{
+	fe->data[0] &= ~MVPP2_FLOW_LAST_MASK;
+	fe->data[0] &= ~MVPP2_FLOW_ENGINE_MASK;
+
+	fe->data[0] |= is_last;
+	fe->data[0] |= (engine << MVPP2_FLOW_ENGINE);
+	fe->data[0] |= MVPP2_FLOW_PORT_ID_SEL_MASK;
+}
+
+/* To init flow table waccording to different flow */
+static inline void mv_pp2x_cls_flow_cos(struct mv_pp2x_hw *hw,
+					struct mv_pp2x_cls_flow_entry *fe,
+					int lkpid, int cos_type)
+{
+	int hek_num, field_id, lkp_type, is_last;
+	int entry_idx = hw->cls_shadow->flow_free_start;
+
+	switch (cos_type) {
+	case MVPP2_COS_TYPE_VLAN:
+		lkp_type = MVPP2_CLS_LKP_VLAN_PRI;
+		break;
+	case MVPP2_COS_TYPE_DSCP:
+		lkp_type = MVPP2_CLS_LKP_DSCP_PRI;
+		break;
+	default:
+		lkp_type = MVPP2_CLS_LKP_DEFAULT;
+		break;
+	}
+	hek_num = 0;
+	if ((lkpid == MVPP2_PRS_FL_NON_IP_UNTAG &&
+		cos_type == MVPP2_COS_TYPE_DEF) ||
+	    (lkpid == MVPP2_PRS_FL_NON_IP_TAG &&
+		cos_type == MVPP2_COS_TYPE_VLAN))
+		is_last = 1;
+	else
+		is_last = 0;
+
+	/* Set SW */
+	memset(fe, 0, sizeof(struct mv_pp2x_cls_flow_entry));
+	mv_pp2x_cls_sw_flow_hek_num_set(fe, hek_num);
+	if (hek_num)
+		mv_pp2x_cls_sw_flow_hek_set(fe, 0, field_id);
+	mv_pp2x_cls_sw_flow_eng_set(fe, MVPP2_CLS_ENGINE_C2, is_last);
+	mv_pp2x_cls_sw_flow_extra_set(fe, lkp_type, MVPP2_CLS_FL_COS_PRI);
+	fe->index = entry_idx;
+
+	/* Write HW */
+	mv_pp2x_cls_flow_write(hw, fe);
+
+	/* Update Shadow */
+	if (cos_type == MVPP2_COS_TYPE_DEF)
+		hw->cls_shadow->flow_info[lkpid -
+			MVPP2_PRS_FL_START].flow_entry_dflt = entry_idx;
+	else if (cos_type == MVPP2_COS_TYPE_VLAN)
+		hw->cls_shadow->flow_info[lkpid -
+			MVPP2_PRS_FL_START].flow_entry_vlan = entry_idx;
+	else
+		hw->cls_shadow->flow_info[lkpid -
+			MVPP2_PRS_FL_START].flow_entry_dscp = entry_idx;
+
+	/* Update first available flow entry */
+	hw->cls_shadow->flow_free_start++;
+}
+
+/* Init flow entry for RSS hash in PP22 */
+static inline void mv_pp2x_cls_flow_rss_hash(struct mv_pp2x_hw *hw,
+					     struct mv_pp2x_cls_flow_entry *fe,
+					     int lkpid, int rss_mode)
+{
+	int field_id[4] = {0};
+	int entry_idx = hw->cls_shadow->flow_free_start;
+	int lkpid_attr = mv_pp2x_prs_flow_id_attr_get(lkpid);
+
+	/* IP4 packet */
+	if (lkpid_attr & MVPP2_PRS_FL_ATTR_IP4_BIT) {
+		field_id[0] = MVPP2_CLS_FIELD_IP4SA;
+		field_id[1] = MVPP2_CLS_FIELD_IP4DA;
+	} else if (lkpid_attr & MVPP2_PRS_FL_ATTR_IP6_BIT) {
+		field_id[0] = MVPP2_CLS_FIELD_IP6SA;
+		field_id[1] = MVPP2_CLS_FIELD_IP6DA;
+	}
+	/* L4 port */
+	field_id[2] = MVPP2_CLS_FIELD_L4SIP;
+	field_id[3] = MVPP2_CLS_FIELD_L4DIP;
+
+	/* Set SW */
+	memset(fe, 0, sizeof(struct mv_pp2x_cls_flow_entry));
+	if (rss_mode == MVPP2_RSS_HASH_2T) {
+		mv_pp2x_cls_sw_flow_hek_num_set(fe, 2);
+		mv_pp2x_cls_sw_flow_eng_set(fe, MVPP2_CLS_ENGINE_C3HA, 1);
+		mv_pp2x_cls_sw_flow_hek_set(fe, 0, field_id[0]);
+		mv_pp2x_cls_sw_flow_hek_set(fe, 1, field_id[1]);
+	} else {
+		mv_pp2x_cls_sw_flow_hek_num_set(fe, 4);
+		mv_pp2x_cls_sw_flow_hek_set(fe, 0, field_id[0]);
+		mv_pp2x_cls_sw_flow_hek_set(fe, 1, field_id[1]);
+		mv_pp2x_cls_sw_flow_hek_set(fe, 2, field_id[2]);
+		mv_pp2x_cls_sw_flow_hek_set(fe, 3, field_id[3]);
+		mv_pp2x_cls_sw_flow_eng_set(fe, MVPP2_CLS_ENGINE_C3HB, 1);
+	}
+	mv_pp2x_cls_sw_flow_extra_set(fe,
+		MVPP2_CLS_LKP_HASH, MVPP2_CLS_FL_RSS_PRI);
+	fe->index = entry_idx;
+
+	/* Update last for UDP NF flow */
+	if ((lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT) &&
+	    !(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) {
+		if (!hw->cls_shadow->flow_info[lkpid -
+			MVPP2_PRS_FL_START].flow_entry_rss1) {
+			if (rss_mode == MVPP2_RSS_HASH_2T)
+				mv_pp2x_cls_sw_flow_eng_set(fe,
+						MVPP2_CLS_ENGINE_C3HA, 0);
+			else
+				mv_pp2x_cls_sw_flow_eng_set(fe,
+						MVPP2_CLS_ENGINE_C3HB, 0);
+		}
+	}
+
+	/* Write HW */
+	mv_pp2x_cls_flow_write(hw, fe);
+
+	/* Update Shadow */
+	if (hw->cls_shadow->flow_info[lkpid -
+		MVPP2_PRS_FL_START].flow_entry_rss1 == 0)
+		hw->cls_shadow->flow_info[lkpid -
+			MVPP2_PRS_FL_START].flow_entry_rss1 = entry_idx;
+	else
+		hw->cls_shadow->flow_info[lkpid -
+			MVPP2_PRS_FL_START].flow_entry_rss2 = entry_idx;
+
+	/* Update first available flow entry */
+	hw->cls_shadow->flow_free_start++;
+}
+
+/* Init cls flow table according to different flow id */
+void mv_pp2x_cls_flow_tbl_config(struct mv_pp2x_hw *hw)
+{
+	int lkpid, rss_mode, lkpid_attr;
+	struct mv_pp2x_cls_flow_entry fe;
+
+	for (lkpid = MVPP2_PRS_FL_START; lkpid < MVPP2_PRS_FL_LAST; lkpid++) {
+		PALAD(MVPP2_PRINT_LINE());
+		/* Get lookup id attribute */
+		lkpid_attr = mv_pp2x_prs_flow_id_attr_get(lkpid);
+		/* Default rss hash is based on 5T */
+		rss_mode = MVPP2_RSS_HASH_5T;
+		/* For frag packets or non-TCP&UDP, rss must be based on 2T */
+		if ((lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT) ||
+		    !(lkpid_attr & (MVPP2_PRS_FL_ATTR_TCP_BIT |
+		    MVPP2_PRS_FL_ATTR_UDP_BIT)))
+			rss_mode = MVPP2_RSS_HASH_2T;
+
+		/* For untagged IP packets, only need default
+		 * rule and dscp rule
+		 */
+		if ((lkpid_attr & (MVPP2_PRS_FL_ATTR_IP4_BIT |
+		     MVPP2_PRS_FL_ATTR_IP6_BIT)) &&
+		    (!(lkpid_attr & MVPP2_PRS_FL_ATTR_VLAN_BIT))) {
+			/* Default rule */
+			mv_pp2x_cls_flow_cos(hw, &fe, lkpid,
+					     MVPP2_COS_TYPE_DEF);
+			/* DSCP rule */
+			mv_pp2x_cls_flow_cos(hw, &fe, lkpid,
+					     MVPP2_COS_TYPE_DSCP);
+			/* RSS hash rule */
+			if ((!(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) &&
+			    (lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT)) {
+				/* RSS hash rules for UDP rss mode update */
+				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
+							  MVPP2_RSS_HASH_2T);
+				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
+							  MVPP2_RSS_HASH_5T);
+			} else {
+				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
+							  rss_mode);
+			}
+		}
+
+		/* For tagged IP packets, only need vlan rule and dscp rule */
+		if ((lkpid_attr & (MVPP2_PRS_FL_ATTR_IP4_BIT |
+		    MVPP2_PRS_FL_ATTR_IP6_BIT)) &&
+		    (lkpid_attr & MVPP2_PRS_FL_ATTR_VLAN_BIT)) {
+			/* VLAN rule */
+			mv_pp2x_cls_flow_cos(hw, &fe, lkpid,
+					     MVPP2_COS_TYPE_VLAN);
+			/* DSCP rule */
+			mv_pp2x_cls_flow_cos(hw, &fe, lkpid,
+					     MVPP2_COS_TYPE_DSCP);
+			/* RSS hash rule */
+			if ((!(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) &&
+			    (lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT)) {
+				/* RSS hash rules for UDP rss mode update */
+				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
+							  MVPP2_RSS_HASH_2T);
+				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
+							  MVPP2_RSS_HASH_5T);
+			} else {
+				mv_pp2x_cls_flow_rss_hash(hw, &fe, lkpid,
+							  rss_mode);
+			}
+		}
+
+		/* For non-IP packets, only need default rule if untagged,
+		 * vlan rule also needed if tagged
+		 */
+		if (!(lkpid_attr & (MVPP2_PRS_FL_ATTR_IP4_BIT |
+		     MVPP2_PRS_FL_ATTR_IP6_BIT))) {
+			/* Default rule */
+			mv_pp2x_cls_flow_cos(hw, &fe, lkpid,
+					     MVPP2_COS_TYPE_DEF);
+			/* VLAN rule if tagged */
+			if (lkpid_attr & MVPP2_PRS_FL_ATTR_VLAN_BIT)
+				mv_pp2x_cls_flow_cos(hw, &fe, lkpid,
+						     MVPP2_COS_TYPE_VLAN);
+		}
+	}
+}
+
+/* Update the flow index for flow of lkpid */
+void mv_pp2x_cls_lkp_flow_set(struct mv_pp2x_hw *hw, int lkpid, int way,
+			      int flow_idx)
+{
+	struct mv_pp2x_cls_lookup_entry le;
+
+	mv_pp2x_cls_lookup_read(hw, lkpid, way, &le);
+	mv_pp2x_cls_sw_lkp_flow_set(&le, flow_idx);
+	mv_pp2x_cls_lookup_write(hw, &le);
+}
+
+int mv_pp2x_cls_lkp_port_way_set(struct mv_pp2x_hw *hw, int port, int way)
+{
+	unsigned int val;
+
+	if (mv_pp2x_range_validate(port, 0, MVPP2_MAX_PORTS - 1) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(way, 0, ONE_BIT_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	val = mv_pp2x_read(hw, MVPP2_CLS_PORT_WAY_REG);
+	if (way == 1)
+		val |= MVPP2_CLS_PORT_WAY_MASK(port);
+	else
+		val &= ~MVPP2_CLS_PORT_WAY_MASK(port);
+	mv_pp2x_write(hw, MVPP2_CLS_PORT_WAY_REG, val);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_lkp_port_way_set);
+
+int mv_pp2x_cls_hw_udf_set(struct mv_pp2x_hw *hw, int udf_no, int offs_id,
+			   int offs_bits, int size_bits)
+{
+	unsigned int regVal;
+
+	if (mv_pp2x_range_validate(offs_id, 0,
+	    MVPP2_CLS_UDF_OFFSET_ID_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(offs_bits, 0,
+	    MVPP2_CLS_UDF_REL_OFFSET_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(size_bits, 0,
+	    MVPP2_CLS_UDF_SIZE_MASK) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(udf_no, 0,
+	    MVPP2_CLS_UDF_REGS_NUM - 1) == MV_ERROR)
+		return MV_ERROR;
+
+	regVal = mv_pp2x_read(hw, MVPP2_CLS_UDF_REG(udf_no));
+	regVal &= ~MVPP2_CLS_UDF_OFFSET_ID_MASK;
+	regVal &= ~MVPP2_CLS_UDF_REL_OFFSET_MASK;
+	regVal &= ~MVPP2_CLS_UDF_SIZE_MASK;
+
+	regVal |= (offs_id << MVPP2_CLS_UDF_OFFSET_ID_OFFS);
+	regVal |= (offs_bits << MVPP2_CLS_UDF_REL_OFFSET_OFFS);
+	regVal |= (size_bits << MVPP2_CLS_UDF_SIZE_OFFS);
+
+	mv_pp2x_write(hw, MVPP2_CLS_UDF_REG(udf_no), regVal);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_udf_set);
+
+/* Init lookup decoding table with lookup id */
+void mv_pp2x_cls_lookup_tbl_config(struct mv_pp2x_hw *hw)
+{
+	int index, flow_idx;
+	int data[3];
+	struct mv_pp2x_cls_lookup_entry le;
+	struct mv_pp2x_cls_flow_info *flow_info;
+
+	memset(&le, 0, sizeof(struct mv_pp2x_cls_lookup_entry));
+	/* Enable classifier engine */
+	mv_pp2x_cls_sw_lkp_en_set(&le, 1);
+
+	for (index = 0; index < (MVPP2_PRS_FL_LAST - MVPP2_PRS_FL_START);
+		index++) {
+		flow_info = &(hw->cls_shadow->flow_info[index]);
+		PALAD(MVPP2_PRINT_LINE());
+		data[0] = MVPP2_FLOW_TBL_SIZE;
+		data[1] = MVPP2_FLOW_TBL_SIZE;
+		data[2] = MVPP2_FLOW_TBL_SIZE;
+		le.lkpid = hw->cls_shadow->flow_info[index].lkpid;
+		/* Find the min non-zero one in flow_entry_dflt,
+		 * flow_entry_vlan, and flow_entry_dscp
+		 */
+		if (flow_info->flow_entry_dflt)
+			data[0] = flow_info->flow_entry_dflt;
+		if (flow_info->flow_entry_vlan)
+			data[1] = flow_info->flow_entry_vlan;
+		if (flow_info->flow_entry_dscp)
+			data[2] = flow_info->flow_entry_dscp;
+		flow_idx = min(data[0], min(data[1], data[2]));
+
+		/* Set flow pointer index */
+		mv_pp2x_cls_sw_lkp_flow_set(&le, flow_idx);
+
+		/* Set initial rx queue */
+		mv_pp2x_cls_sw_lkp_rxq_set(&le, 0x0);
+
+		le.way = 0;
+
+		/* Update lookup ID table entry */
+		mv_pp2x_cls_lookup_write(hw, &le);
+
+		le.way = 1;
+
+		/* Update lookup ID table entry */
+		mv_pp2x_cls_lookup_write(hw, &le);
+	}
+}
+
+/* Classifier default initialization */
+int mv_pp2x_cls_init(struct platform_device *pdev, struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_cls_lookup_entry le;
+	struct mv_pp2x_cls_flow_entry fe;
+	int index;
+
+	/* Enable classifier */
+	mv_pp2x_write(hw, MVPP2_CLS_MODE_REG, MVPP2_CLS_MODE_ACTIVE_MASK);
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Clear classifier flow table */
+	memset(&fe.data, 0, MVPP2_CLS_FLOWS_TBL_DATA_WORDS);
+	for (index = 0; index < MVPP2_CLS_FLOWS_TBL_SIZE; index++) {
+		PALAD(MVPP2_PRINT_LINE());
+		fe.index = index;
+		mv_pp2x_cls_flow_write(hw, &fe);
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Clear classifier lookup table */
+	le.data = 0;
+	for (index = 0; index < MVPP2_CLS_LKP_TBL_SIZE; index++) {
+		PALAD(MVPP2_PRINT_LINE());
+		le.lkpid = index;
+		le.way = 0;
+		mv_pp2x_cls_lookup_write(hw, &le);
+
+		le.way = 1;
+		mv_pp2x_cls_lookup_write(hw, &le);
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	hw->cls_shadow = devm_kcalloc(&pdev->dev, 1,
+				      sizeof(struct mv_pp2x_cls_shadow),
+				      GFP_KERNEL);
+	if (!hw->cls_shadow)
+		return -ENOMEM;
+	PALAD(MVPP2_PRINT_LINE());
+
+	hw->cls_shadow->flow_info = devm_kcalloc(&pdev->dev,
+				(MVPP2_PRS_FL_LAST - MVPP2_PRS_FL_START),
+				sizeof(struct mv_pp2x_cls_flow_info),
+				GFP_KERNEL);
+	if (!hw->cls_shadow->flow_info)
+		return -ENOMEM;
+	MVPP2_PRINT_LINE();
+
+	/* Start from entry 1 to allocate flow table */
+	hw->cls_shadow->flow_free_start = 1;
+	for (index = 0; index < (MVPP2_PRS_FL_LAST - MVPP2_PRS_FL_START);
+		index++)
+		hw->cls_shadow->flow_info[index].lkpid = index +
+			MVPP2_PRS_FL_START;
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Init flow table */
+	mv_pp2x_cls_flow_tbl_config(hw);
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Init lookup table */
+	mv_pp2x_cls_lookup_tbl_config(hw);
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	return 0;
+}
+
+void mv_pp2x_cls_port_config(struct mv_pp2x_port *port)
+{
+	struct mv_pp2x_cls_lookup_entry le;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	u32 val;
+
+	/* Set way for the port */
+	val = mv_pp2x_read(hw, MVPP2_CLS_PORT_WAY_REG);
+	val &= ~MVPP2_CLS_PORT_WAY_MASK(port->id);
+	mv_pp2x_write(hw, MVPP2_CLS_PORT_WAY_REG, val);
+
+	/* Pick the entry to be accessed in lookup ID decoding table
+	 * according to the way and lkpid.
+	 */
+	le.lkpid = port->id;
+	le.way = 0;
+	le.data = 0;
+
+	/* Set initial CPU queue for receiving packets */
+	le.data &= ~MVPP2_CLS_LKP_TBL_RXQ_MASK;
+	le.data |= port->first_rxq;
+
+	/* Disable classification engines */
+	le.data &= ~MVPP2_CLS_LKP_TBL_LOOKUP_EN_MASK;
+
+	/* Update lookup ID table entry */
+	mv_pp2x_cls_lookup_write(hw, &le);
+}
+
+/* Set CPU queue number for oversize packets */
+void mv_pp2x_cls_oversize_rxq_set(struct mv_pp2x_port *port)
+{
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	mv_pp2x_write(hw, MVPP2_CLS_OVERSIZE_RXQ_LOW_REG(port->id),
+		      port->first_rxq & MVPP2_CLS_OVERSIZE_RXQ_LOW_MASK);
+
+}
+
+void mv_pp21_get_mac_address(struct mv_pp2x_port *port, unsigned char *addr)
+{
+	u32 mac_addr_l, mac_addr_m, mac_addr_h;
+
+	mac_addr_l = readl(port->base + MVPP2_GMAC_CTRL_1_REG);
+	mac_addr_m = readl(port->priv->hw.lms_base + MVPP2_SRC_ADDR_MIDDLE);
+	mac_addr_h = readl(port->priv->hw.lms_base + MVPP2_SRC_ADDR_HIGH);
+	addr[0] = (mac_addr_h >> 24) & 0xFF;
+	addr[1] = (mac_addr_h >> 16) & 0xFF;
+	addr[2] = (mac_addr_h >> 8) & 0xFF;
+	addr[3] = mac_addr_h & 0xFF;
+	addr[4] = mac_addr_m & 0xFF;
+	addr[5] = (mac_addr_l >> MVPP2_GMAC_SA_LOW_OFFS) & 0xFF;
+}
+
+void mv_pp2x_cause_error(struct net_device *dev, int cause)
+{
+	if (cause & MVPP2_CAUSE_FCS_ERR_MASK)
+		netdev_err(dev, "FCS error\n");
+	if (cause & MVPP2_CAUSE_RX_FIFO_OVERRUN_MASK)
+		netdev_err(dev, "rx fifo overrun error\n");
+	if (cause & MVPP2_CAUSE_TX_FIFO_UNDERRUN_MASK)
+		netdev_err(dev, "tx fifo underrun error\n");
+}
+
+/* Display more error info */
+void mv_pp2x_rx_error(struct mv_pp2x_port *port,
+		      struct mv_pp2x_rx_desc *rx_desc)
+{
+	u32 status = rx_desc->status;
+
+	switch (status & MVPP2_RXD_ERR_CODE_MASK) {
+	case MVPP2_RXD_ERR_CRC:
+		netdev_err(port->dev,
+			   "bad rx status %08x (crc error), size=%d\n",
+			   status, rx_desc->data_size);
+		break;
+	case MVPP2_RXD_ERR_OVERRUN:
+		netdev_err(port->dev,
+			   "bad rx status %08x (overrun error), size=%d\n",
+			   status, rx_desc->data_size);
+		break;
+	case MVPP2_RXD_ERR_RESOURCE:
+		netdev_err(port->dev,
+			   "bad rx status %08x (resource error), size=%d\n",
+			   status, rx_desc->data_size);
+		break;
+	}
+}
+
+/* Handle RX checksum offload */
+void mv_pp2x_rx_csum(struct mv_pp2x_port *port, u32 status,
+		     struct sk_buff *skb)
+{
+	if (((status & MVPP2_RXD_L3_IP4) &&
+	     !(status & MVPP2_RXD_IP4_HEADER_ERR)) ||
+	    (status & MVPP2_RXD_L3_IP6))
+		if (((status & MVPP2_RXD_L4_UDP) ||
+		     (status & MVPP2_RXD_L4_TCP)) &&
+		     (status & MVPP2_RXD_L4_CSUM_OK)) {
+			skb->csum = 0;
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			return;
+		}
+
+	skb->ip_summed = CHECKSUM_NONE;
+}
+
+
+/* Set the number of packets that will be received before Rx interrupt
+ * will be generated by HW.
+ */
+void mv_pp2x_rx_pkts_coal_set(struct mv_pp2x_port *port,
+			      struct mv_pp2x_rx_queue *rxq, u32 pkts)
+{
+	u32 val;
+
+	val = (pkts & MVPP2_OCCUPIED_THRESH_MASK);
+	mv_pp2x_write(&port->priv->hw, MVPP2_RXQ_NUM_REG, rxq->id);
+	mv_pp2x_write(&port->priv->hw, MVPP2_RXQ_THRESH_REG, val);
+
+	rxq->pkts_coal = pkts;
+}
+
+/* Set the time delay in usec before Rx interrupt */
+void mv_pp2x_rx_time_coal_set(struct mv_pp2x_port *port,
+			      struct mv_pp2x_rx_queue *rxq, u32 usec)
+{
+	u32 val;
+
+	val = (port->priv->hw.tclk / USEC_PER_SEC) * usec;
+	mv_pp2x_write(&port->priv->hw,
+		      MVPP2_ISR_RX_THRESHOLD_REG(rxq->id), val);
+}
+
+/* Set threshold for TX_DONE pkts coalescing */
+void mv_pp2x_tx_done_pkts_coal_set(void *arg)
+{
+	struct mv_pp2x_port *port = arg;
+	int queue;
+	u32 val;
+
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		struct mv_pp2x_tx_queue *txq = port->txqs[queue];
+
+		val = (txq->pkts_coal << MVPP2_TRANSMITTED_THRESH_OFFSET) &
+		       MVPP2_TRANSMITTED_THRESH_MASK;
+		mv_pp2x_write(&port->priv->hw, MVPP2_TXQ_NUM_REG, txq->id);
+		mv_pp2x_write(&port->priv->hw, MVPP2_TXQ_THRESH_REG, val);
+	}
+}
+
+/* Set the time delay in usec before Rx interrupt */
+void mv_pp2x_tx_done_time_coal_set(struct mv_pp2x_port *port, u32 usec)
+{
+	u32 val;
+
+	val = (port->priv->hw.tclk / USEC_PER_SEC) * usec;
+	mv_pp2x_write(&port->priv->hw,
+		      MVPP22_ISR_TX_THRESHOLD_REG(port->id), val);
+}
+
+
+/* Change maximum receive size of the port */
+void mv_pp21_gmac_max_rx_size_set(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_0_REG);
+	val &= ~MVPP2_GMAC_MAX_RX_SIZE_MASK;
+	val |= (((port->pkt_size - MVPP2_MH_SIZE) / 2) <<
+		MVPP2_GMAC_MAX_RX_SIZE_OFFS);
+	writel(val, port->base + MVPP2_GMAC_CTRL_0_REG);
+#endif
+}
+
+
+/* Set max sizes for Tx queues */
+void mv_pp2x_txp_max_tx_size_set(struct mv_pp2x_port *port)
+{
+	u32	val, size, mtu;
+	int	txq, tx_port_num;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+
+	mtu = port->pkt_size * 8;
+	if (mtu > MVPP2_TXP_MTU_MAX)
+		mtu = MVPP2_TXP_MTU_MAX;
+
+	/* WA for wrong Token bucket update: Set MTU value = 3*real MTU value */
+	mtu = 3 * mtu;
+
+	/* Indirect access to registers */
+	tx_port_num = mv_pp2x_egress_port(port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);
+
+	/* Set MTU */
+	val = mv_pp2x_read(hw, MVPP2_TXP_SCHED_MTU_REG);
+	val &= ~MVPP2_TXP_MTU_MAX;
+	val |= mtu;
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_MTU_REG, val);
+
+	/* TXP token size and all TXQs token size must be larger that MTU */
+	val = mv_pp2x_read(hw, MVPP2_TXP_SCHED_TOKEN_SIZE_REG);
+	size = val & MVPP2_TXP_TOKEN_SIZE_MAX;
+	if (size < mtu) {
+		size = mtu;
+		val &= ~MVPP2_TXP_TOKEN_SIZE_MAX;
+		val |= size;
+		mv_pp2x_write(hw, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, val);
+	}
+
+	for (txq = 0; txq < port->num_tx_queues; txq++) {
+		val = mv_pp2x_read(hw,
+				   MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq));
+		size = val & MVPP2_TXQ_TOKEN_SIZE_MAX;
+
+		if (size < mtu) {
+			size = mtu;
+			val &= ~MVPP2_TXQ_TOKEN_SIZE_MAX;
+			val |= size;
+			mv_pp2x_write(hw,
+				      MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq),
+				      val);
+		}
+	}
+}
+
+
+/* Set Tx descriptors fields relevant for CSUM calculation */
+u32 mv_pp2x_txq_desc_csum(int l3_offs, int l3_proto,
+			  int ip_hdr_len, int l4_proto)
+{
+	u32 command;
+
+	/* fields: L3_offset, IP_hdrlen, L3_type, G_IPv4_chk,
+	 * G_L4_chk, L4_type required only for checksum calculation
+	 */
+	command = (l3_offs << MVPP2_TXD_L3_OFF_SHIFT);
+	command |= (ip_hdr_len << MVPP2_TXD_IP_HLEN_SHIFT);
+	command |= MVPP2_TXD_IP_CSUM_DISABLE;
+
+	if (l3_proto == swab16(ETH_P_IP)) {
+		command &= ~MVPP2_TXD_IP_CSUM_DISABLE;	/* enable IPv4 csum */
+		command &= ~MVPP2_TXD_L3_IP6;		/* enable IPv4 */
+	} else {
+		command |= MVPP2_TXD_L3_IP6;		/* enable IPv6 */
+	}
+
+	if (l4_proto == IPPROTO_TCP) {
+		command &= ~MVPP2_TXD_L4_UDP;		/* enable TCP */
+		command &= ~MVPP2_TXD_L4_CSUM_FRAG;	/* generate L4 csum */
+	} else if (l4_proto == IPPROTO_UDP) {
+		command |= MVPP2_TXD_L4_UDP;		/* enable UDP */
+		command &= ~MVPP2_TXD_L4_CSUM_FRAG;	/* generate L4 csum */
+	} else {
+		command |= MVPP2_TXD_L4_CSUM_NOT;
+	}
+
+	return command;
+}
+
+/* Get number of sent descriptors and decrement counter.
+ * The number of sent descriptors is returned.
+ * Per-CPU access
+ */
+
+ /* Tx descriptors helper methods */
+
+/* Get number of Tx descriptors waiting to be transmitted by HW */
+int mv_pp2x_txq_pend_desc_num_get(struct mv_pp2x_port *port,
+				  struct mv_pp2x_tx_queue *txq)
+{
+	u32 val;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	mv_pp2x_write(hw, MVPP2_TXQ_NUM_REG, txq->id);
+	val = mv_pp2x_read(hw, MVPP2_TXQ_PENDING_REG);
+
+	return val & MVPP2_TXQ_PENDING_MASK;
+}
+
+/* Get pointer to next Tx descriptor to be processed (send) by HW */
+struct mv_pp2x_tx_desc *mv_pp2x_txq_next_desc_get(
+		struct mv_pp2x_aggr_tx_queue *aggr_txq)
+{
+	int tx_desc = aggr_txq->next_desc_to_proc;
+
+	aggr_txq->next_desc_to_proc = MVPP2_QUEUE_NEXT_DESC(aggr_txq, tx_desc);
+	return aggr_txq->first_desc + tx_desc;
+}
+
+/* Update HW with number of aggregated Tx descriptors to be sent */
+void mv_pp2x_aggr_txq_pend_desc_add(struct mv_pp2x_port *port, int pending)
+{
+	/* aggregated access - relevant TXQ number is written in TX desc */
+	mv_pp2x_write(&port->priv->hw, MVPP2_AGGR_TXQ_UPDATE_REG, pending);
+}
+
+int mv_pp2x_aggr_desc_num_read(struct mv_pp2x *priv, int cpu)
+{
+	u32 val = mv_pp2x_read(&priv->hw, MVPP2_AGGR_TXQ_STATUS_REG(cpu));
+
+	return(val & MVPP2_AGGR_TXQ_PENDING_MASK);
+}
+
+/* Check if there are enough free descriptors in aggregated txq.
+ * If not, update the number of occupied descriptors and repeat the check.
+ */
+int mv_pp2x_aggr_desc_num_check(struct mv_pp2x *priv,
+				struct mv_pp2x_aggr_tx_queue *aggr_txq,
+				int num)
+{
+	if ((aggr_txq->count + num) > aggr_txq->size) {
+		/* Update number of occupied aggregated Tx descriptors */
+		int cpu = smp_processor_id();
+		u32 val = mv_pp2x_read(&priv->hw,
+				MVPP2_AGGR_TXQ_STATUS_REG(cpu));
+
+		aggr_txq->count = val & MVPP2_AGGR_TXQ_PENDING_MASK;
+
+		if ((aggr_txq->count + num) > aggr_txq->size)
+			return -ENOMEM;
+	}
+
+	return 0;
+}
+
+/* Reserved Tx descriptors allocation request */
+int mv_pp2x_txq_alloc_reserved_desc(struct mv_pp2x *priv,
+				    struct mv_pp2x_tx_queue *txq, int num)
+{
+	u32 val;
+
+	val = (txq->id << MVPP2_TXQ_RSVD_REQ_Q_OFFSET) | num;
+	mv_pp2x_write(&priv->hw, MVPP2_TXQ_RSVD_REQ_REG, val);
+
+	val = mv_pp2x_read(&priv->hw, MVPP2_TXQ_RSVD_RSLT_REG);
+
+	return val & MVPP2_TXQ_RSVD_RSLT_MASK;
+}
+
+/* Set rx queue offset */
+void mv_pp2x_rxq_offset_set(struct mv_pp2x_port *port,
+			    int prxq, int offset)
+{
+	u32 val;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	/* Convert offset from bytes to units of 32 bytes */
+	offset = offset >> 5;
+
+	val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(prxq));
+	val &= ~MVPP2_RXQ_PACKET_OFFSET_MASK;
+
+	/* Offset is in */
+	val |= ((offset << MVPP2_RXQ_PACKET_OFFSET_OFFS) &
+		MVPP2_RXQ_PACKET_OFFSET_MASK);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(prxq), val);
+}
+
+/* Port configuration routines */
+
+void mv_pp21_port_mii_set(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_2_REG);
+
+	switch (port->mac_data.phy_mode) {
+	case PHY_INTERFACE_MODE_SGMII:
+		val |= MVPP2_GMAC_INBAND_AN_MASK;
+		break;
+	case PHY_INTERFACE_MODE_RGMII:
+		val |= MVPP2_GMAC_PORT_RGMII_MASK;
+	default:
+		val &= ~MVPP2_GMAC_PCS_ENABLE_MASK;
+	}
+
+	writel(val, port->base + MVPP2_GMAC_CTRL_2_REG);
+#endif
+}
+
+void mv_pp21_port_fc_adv_enable(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+	val |= MVPP2_GMAC_FC_ADV_EN;
+	writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+#endif
+}
+
+void mv_pp21_port_enable(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_0_REG);
+	val |= MVPP2_GMAC_PORT_EN_MASK;
+	val |= MVPP2_GMAC_MIB_CNTR_EN_MASK;
+	writel(val, port->base + MVPP2_GMAC_CTRL_0_REG);
+#endif
+}
+
+void mv_pp21_port_disable(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_0_REG);
+	val &= ~(MVPP2_GMAC_PORT_EN_MASK);
+	writel(val, port->base + MVPP2_GMAC_CTRL_0_REG);
+#endif
+}
+
+/* Set IEEE 802.3x Flow Control Xon Packet Transmission Mode */
+void mv_pp21_port_periodic_xon_disable(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_1_REG) &
+		    ~MVPP2_GMAC_PERIODIC_XON_EN_MASK;
+	writel(val, port->base + MVPP2_GMAC_CTRL_1_REG);
+#endif
+}
+
+/* Configure loopback port */
+void mv_pp21_port_loopback_set(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_1_REG);
+
+	if (port->mac_data.speed == 1000)
+		val |= MVPP2_GMAC_GMII_LB_EN_MASK;
+	else
+		val &= ~MVPP2_GMAC_GMII_LB_EN_MASK;
+
+	if (port->mac_data.phy_mode == PHY_INTERFACE_MODE_SGMII)
+		val |= MVPP2_GMAC_PCS_LB_EN_MASK;
+	else
+		val &= ~MVPP2_GMAC_PCS_LB_EN_MASK;
+
+	writel(val, port->base + MVPP2_GMAC_CTRL_1_REG);
+#endif
+}
+
+void mv_pp21_port_reset(struct mv_pp2x_port *port)
+{
+#ifndef CONFIG_MV_PP2_PALLADIUM
+
+	u32 val;
+
+	val = readl(port->base + MVPP2_GMAC_CTRL_2_REG) &
+		    ~MVPP2_GMAC_PORT_RESET_MASK;
+	writel(val, port->base + MVPP2_GMAC_CTRL_2_REG);
+
+	while (readl(port->base + MVPP2_GMAC_CTRL_2_REG) &
+	       MVPP2_GMAC_PORT_RESET_MASK)
+		continue;
+#endif
+}
+
+/* Refill BM pool */
+void mv_pp2x_pool_refill(struct mv_pp2x *priv, u32 pool,
+			 dma_addr_t phys_addr, struct sk_buff *cookie)
+{
+	STAT_DBG(struct mv_pp2x_bm_pool *bm_pool = &priv->bm_pools[pool]);
+
+	mv_pp2x_bm_pool_put(&priv->hw, pool, phys_addr, cookie);
+	STAT_DBG(bm_pool->stats.bm_put++);
+}
+
+/* Set pool buffer size */
+void mv_pp2x_bm_pool_bufsize_set(struct mv_pp2x_hw *hw,
+				 struct mv_pp2x_bm_pool *bm_pool, int buf_size)
+{
+	u32 val;
+
+	bm_pool->buf_size = buf_size;
+
+	val = ALIGN(buf_size, 1 << MVPP2_POOL_BUF_SIZE_OFFSET);
+	mv_pp2x_write(hw, MVPP2_POOL_BUF_SIZE_REG(bm_pool->id), val);
+}
+
+/* Attach long pool to rxq */
+void mv_pp21_rxq_long_pool_set(struct mv_pp2x_hw *hw,
+		int prxq, int long_pool)
+{
+	u32 val;
+
+	val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(prxq));
+	val &= ~MVPP21_RXQ_POOL_LONG_MASK;
+	val |= ((long_pool << MVPP21_RXQ_POOL_LONG_OFFS) &
+		    MVPP21_RXQ_POOL_LONG_MASK);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(prxq), val);
+}
+
+/* Attach short pool to rxq */
+void mv_pp21_rxq_short_pool_set(struct mv_pp2x_hw *hw,
+				int prxq, int short_pool)
+{
+	u32 val;
+
+	val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(prxq));
+	val &= ~MVPP21_RXQ_POOL_SHORT_MASK;
+	val |= ((short_pool << MVPP21_RXQ_POOL_SHORT_OFFS) &
+		    MVPP21_RXQ_POOL_SHORT_MASK);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(prxq), val);
+}
+
+
+/* Attach long pool to rxq */
+void mv_pp22_rxq_long_pool_set(struct mv_pp2x_hw *hw,
+			       int prxq, int long_pool)
+{
+	u32 val;
+
+	val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(prxq));
+	val &= ~MVPP22_RXQ_POOL_LONG_MASK;
+	val |= ((long_pool << MVPP22_RXQ_POOL_LONG_OFFS) &
+		    MVPP22_RXQ_POOL_LONG_MASK);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(prxq), val);
+}
+
+/* Attach short pool to rxq */
+void mv_pp22_rxq_short_pool_set(struct mv_pp2x_hw *hw,
+				int prxq, int short_pool)
+{
+	u32 val;
+
+	val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(prxq));
+	val &= ~MVPP22_RXQ_POOL_SHORT_MASK;
+	val |= ((short_pool << MVPP22_RXQ_POOL_SHORT_OFFS) &
+		    MVPP22_RXQ_POOL_SHORT_MASK);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(prxq), val);
+}
+
+/* Enable/disable receiving packets */
+void mv_pp2x_ingress_enable(struct mv_pp2x_port *port)
+{
+	u32 val;
+	int lrxq, queue;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	for (lrxq = 0; lrxq < port->num_rx_queues; lrxq++) {
+		queue = port->rxqs[lrxq]->id;
+		val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(queue));
+		val &= ~MVPP2_RXQ_DISABLE_MASK;
+		mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(queue), val);
+	}
+}
+
+void mv_pp2x_ingress_disable(struct mv_pp2x_port *port)
+{
+	u32 val;
+	int lrxq, queue;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	for (lrxq = 0; lrxq < port->num_rx_queues; lrxq++) {
+		queue = port->rxqs[lrxq]->id;
+		val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(queue));
+		val |= MVPP2_RXQ_DISABLE_MASK;
+		mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(queue), val);
+	}
+}
+
+void mv_pp2x_egress_enable(struct mv_pp2x_port *port)
+{
+	u32 qmap;
+	int queue;
+	int tx_port_num = mv_pp2x_egress_port(port);
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	/* Enable all initialized TXs. */
+	qmap = 0;
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		struct mv_pp2x_tx_queue *txq = port->txqs[queue];
+
+		if (txq->first_desc != NULL)
+			qmap |= (1 << queue);
+	}
+
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_Q_CMD_REG, qmap);
+
+	DBG_MSG("tx_port_num=%d qmap=0x%x\n", tx_port_num, qmap);
+}
+
+/* Disable transmit via physical egress queue
+ * - HW doesn't take descriptors from DRAM
+ */
+void mv_pp2x_egress_disable(struct mv_pp2x_port *port)
+{
+	u32 reg_data;
+	int delay;
+	int tx_port_num = mv_pp2x_egress_port(port);
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	/* Issue stop command for active channels only */
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);
+	reg_data = (mv_pp2x_read(hw, MVPP2_TXP_SCHED_Q_CMD_REG)) &
+		    MVPP2_TXP_SCHED_ENQ_MASK;
+	if (reg_data != 0)
+		mv_pp2x_write(hw, MVPP2_TXP_SCHED_Q_CMD_REG,
+			      (reg_data << MVPP2_TXP_SCHED_DISQ_OFFSET));
+
+	/* Wait for all Tx activity to terminate. */
+	delay = 0;
+	do {
+		if (delay >= MVPP2_TX_DISABLE_TIMEOUT_MSEC) {
+			netdev_warn(port->dev,
+				    "Tx stop timed out, status=0x%08x\n",
+				    reg_data);
+			break;
+		}
+		mdelay(1);
+		delay++;
+
+		/* Check port TX Command register that all
+		 * Tx queues are stopped
+		 */
+		reg_data = mv_pp2x_read(hw, MVPP2_TXP_SCHED_Q_CMD_REG);
+	} while (reg_data & MVPP2_TXP_SCHED_ENQ_MASK);
+}
+
+/* Parser default initialization */
+int mv_pp2x_prs_default_init(struct platform_device *pdev,
+		struct mv_pp2x_hw *hw)
+{
+	int err, index, i;
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Enable tcam table */
+	mv_pp2x_write(hw, MVPP2_PRS_TCAM_CTRL_REG, MVPP2_PRS_TCAM_EN_MASK);
+
+	/* Clear all tcam and sram entries */
+	for (index = 0; index < MVPP2_PRS_TCAM_SRAM_SIZE; index++) {
+		PALAD(MVPP2_PRINT_LINE());
+		mv_pp2x_write(hw, MVPP2_PRS_TCAM_IDX_REG, index);
+		for (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)
+			mv_pp2x_write(hw, MVPP2_PRS_TCAM_DATA_REG(i), 0);
+
+		mv_pp2x_write(hw, MVPP2_PRS_SRAM_IDX_REG, index);
+		for (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)
+			mv_pp2x_write(hw, MVPP2_PRS_SRAM_DATA_REG(i), 0);
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Invalidate all tcam entries */
+	for (index = 0; index < MVPP2_PRS_TCAM_SRAM_SIZE; index++) {
+		PALAD(MVPP2_PRINT_LINE());
+		mv_pp2x_prs_hw_inv(hw, index);
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	hw->prs_shadow = devm_kcalloc(&pdev->dev, MVPP2_PRS_TCAM_SRAM_SIZE,
+				      sizeof(struct mv_pp2x_prs_shadow),
+				      GFP_KERNEL);
+	PALAD(MVPP2_PRINT_LINE());
+
+	if (!hw->prs_shadow)
+		return -ENOMEM;
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Always start from lookup = 0 */
+	for (index = 0; index < MVPP2_MAX_PORTS; index++)
+		mv_pp2x_prs_hw_port_init(hw, index, MVPP2_PRS_LU_MH,
+					 MVPP2_PRS_PORT_LU_MAX, 0);
+	PALAD(MVPP2_PRINT_LINE());
+
+	mv_pp2x_prs_def_flow_init(hw);
+	PALAD(MVPP2_PRINT_LINE());
+
+	mv_pp2x_prs_mh_init(hw);
+	PALAD(MVPP2_PRINT_LINE());
+
+	mv_pp2x_prs_mac_init(hw);
+	PALAD(MVPP2_PRINT_LINE());
+
+	mv_pp2x_prs_dsa_init(hw);
+	PALAD(MVPP2_PRINT_LINE());
+
+	err = mv_pp2x_prs_etype_init(hw);
+	if (err)
+		return err;
+	PALAD(MVPP2_PRINT_LINE());
+
+	err = mv_pp2x_prs_vlan_init(pdev, hw);
+	if (err)
+		return err;
+	PALAD(MVPP2_PRINT_LINE());
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	err = mv_pp2x_prs_pppoe_init(hw);
+	if (err)
+		return err;
+
+	err = mv_pp2x_prs_ip6_init(hw);
+	if (err)
+		return err;
+#endif
+
+	err = mv_pp2x_prs_ip4_init(hw);
+	if (err)
+		return err;
+	PALAD(MVPP2_PRINT_LINE());
+	return 0;
+}
+
+/* shift to (current offset + shift) */
+int mv_pp2x_prs_sw_sram_shift_set(struct mv_pp2x_prs_entry *pe,
+				  int shift, unsigned int op)
+{
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(shift, 0 - MVPP2_PRS_SRAM_SHIFT_MASK,
+	    MVPP2_PRS_SRAM_SHIFT_MASK) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(op, 0,
+	    MVPP2_PRS_SRAM_OP_SEL_SHIFT_MASK) == MV_ERROR)
+		return MV_ERROR;
+
+	/* Set sign */
+	if (shift < 0) {
+		pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+			MVPP2_PRS_SRAM_SHIFT_SIGN_BIT)] |=
+			(1 << (MVPP2_PRS_SRAM_SHIFT_SIGN_BIT % 8));
+		shift = 0 - shift;
+	} else
+		pe->sram.byte[SRAM_BIT_TO_BYTE(
+			MVPP2_PRS_SRAM_SHIFT_SIGN_BIT)] &=
+			(~(1 << (MVPP2_PRS_SRAM_SHIFT_SIGN_BIT % 8)));
+
+	/* Set offset */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_SHIFT_OFFS)] = (unsigned char)shift;
+
+	/* Reset and Set operation */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS)] &=
+		~(MVPP2_PRS_SRAM_OP_SEL_SHIFT_MASK <<
+		(MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS % 8));
+
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS)] |=
+		(op << (MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS % 8));
+
+	/* Set base offset as current */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS)] &=
+		(~(1 << (MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS % 8)));
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_shift_set);
+
+int mv_pp2x_prs_sw_sram_shift_get(struct mv_pp2x_prs_entry *pe, int *shift)
+{
+	int sign;
+
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+	if (mv_pp2x_ptr_validate(shift) == MV_ERROR)
+		return MV_ERROR;
+
+	sign = pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_SHIFT_SIGN_BIT)] &
+		(1 << (MVPP2_PRS_SRAM_SHIFT_SIGN_BIT % 8));
+	*shift = ((int)(pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_SHIFT_OFFS)])) &
+		MVPP2_PRS_SRAM_SHIFT_MASK;
+
+	if (sign == 1)
+		*shift *= -1;
+	return MV_OK;
+}
+
+int mv_pp2x_prs_sw_sram_offset_set(struct mv_pp2x_prs_entry *pe,
+				   unsigned int type, int offset,
+				   unsigned int op)
+{
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(offset, 0 - MVPP2_PRS_SRAM_UDF_MASK,
+	    MVPP2_PRS_SRAM_UDF_MASK) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(type, 0,
+	    MVPP2_PRS_SRAM_UDF_TYPE_MASK) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(op, 0,
+		MVPP2_PRS_SRAM_OP_SEL_UDF_MASK) == MV_ERROR)
+		return MV_ERROR;
+
+	/* Set offset sign */
+	if (offset < 0) {
+		offset = 0 - offset;
+		/* set sram offset sign bit */
+		pe->sram.byte[SRAM_BIT_TO_BYTE(
+			MVPP2_PRS_SRAM_SHIFT_SIGN_BIT)] |=
+			(1 << (MVPP2_PRS_SRAM_SHIFT_SIGN_BIT % 8));
+	} else
+		pe->sram.byte[SRAM_BIT_TO_BYTE(
+			MVPP2_PRS_SRAM_SHIFT_SIGN_BIT)] &=
+			(~(1 << (MVPP2_PRS_SRAM_SHIFT_SIGN_BIT % 8)));
+
+	/* set offset value */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_OFFS)] &=
+		(~(MVPP2_PRS_SRAM_UDF_MASK <<
+		(MVPP2_PRS_SRAM_UDF_OFFS % 8)));
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_OFFS)] |=
+		(offset << (MVPP2_PRS_SRAM_UDF_OFFS % 8));
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_OFFS + MVPP2_PRS_SRAM_UDF_BITS)] &=
+		~(MVPP2_PRS_SRAM_UDF_MASK >>
+		(8 - (MVPP2_PRS_SRAM_UDF_OFFS % 8)));
+
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_OFFS + MVPP2_PRS_SRAM_UDF_BITS)] |=
+		(offset >> (8 - (MVPP2_PRS_SRAM_UDF_OFFS % 8)));
+
+	/* set offset type */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_TYPE_OFFS)] &=
+		~(MVPP2_PRS_SRAM_UDF_TYPE_MASK <<
+		(MVPP2_PRS_SRAM_UDF_TYPE_OFFS % 8));
+
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_TYPE_OFFS)] |=
+		(type << (MVPP2_PRS_SRAM_UDF_TYPE_OFFS % 8));
+
+	/* Set offset operation */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS)] &=
+		~(MVPP2_PRS_SRAM_OP_SEL_UDF_MASK <<
+		(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8));
+
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS)] |=
+		(op << (MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8));
+
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS +
+		MVPP2_PRS_SRAM_OP_SEL_UDF_BITS)] &=
+		 ~(MVPP2_PRS_SRAM_OP_SEL_UDF_MASK >>
+		 (8 - (MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8)));
+
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS +
+		MVPP2_PRS_SRAM_OP_SEL_UDF_BITS)] |=
+		(op >> (8 - (MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8)));
+
+	/* Set base offset as current */
+	pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS)] &=
+		(~(1 << (MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS % 8)));
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_offset_set);
+
+int mv_pp2x_prs_sw_sram_offset_get(struct mv_pp2x_prs_entry *pe,
+				   unsigned int *type, int *offset,
+				   unsigned int *op)
+{
+	int sign;
+
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(offset) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(type) == MV_ERROR)
+		return MV_ERROR;
+
+	*type = pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_TYPE_OFFS)] >>
+		(MVPP2_PRS_SRAM_UDF_TYPE_OFFS % 8);
+	*type &= MVPP2_PRS_SRAM_UDF_TYPE_MASK;
+
+	*offset = (pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_OFFS)] >>
+		(MVPP2_PRS_SRAM_UDF_OFFS % 8)) & 0x7f;
+	*offset |= (pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_OFFS + MVPP2_PRS_SRAM_UDF_OFFS)] <<
+		(8 - (MVPP2_PRS_SRAM_UDF_OFFS % 8))) & 0x80;
+
+	*op = (pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS)] >>
+		(MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS % 8)) & 0x7;
+	*op |= (pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS +
+		MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS)] <<
+		(8 - (MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS % 8))) & 0x18;
+
+	/* if signed bit is tes */
+	sign = pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_UDF_SIGN_BIT)] &
+		(1 << (MVPP2_PRS_SRAM_UDF_SIGN_BIT % 8));
+	if (sign != 0)
+		*offset = 1-(*offset);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_offset_get);
+
+
+int mv_pp2x_prs_sw_sram_next_lu_get(struct mv_pp2x_prs_entry *pe,
+				    unsigned int *lu)
+{
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(lu) == MV_ERROR)
+		return MV_ERROR;
+
+	*lu = pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_NEXT_LU_OFFS)];
+	*lu = ((*lu) >> MVPP2_PRS_SRAM_NEXT_LU_OFFS % 8);
+	*lu &= MVPP2_PRS_SRAM_NEXT_LU_MASK;
+	return MV_OK;
+}
+
+int mv_pp2x_prs_sram_bit_get(struct mv_pp2x_prs_entry *pe, int bitNum,
+			     unsigned int *bit)
+{
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	*bit = pe->sram.byte[MVPP2_PRS_SRAM_BIT_TO_BYTE(bitNum)]  &
+		(1 << (bitNum % 8));
+	*bit = (*bit) >> (bitNum % 8);
+	return MV_OK;
+}
+
+void mv_pp2x_prs_sw_sram_lu_done_set(struct mv_pp2x_prs_entry *pe)
+{
+	mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_lu_done_set);
+
+void mv_pp2x_prs_sw_sram_lu_done_clear(struct mv_pp2x_prs_entry *pe)
+{
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_lu_done_clear);
+
+int mv_pp2x_prs_sw_sram_lu_done_get(struct mv_pp2x_prs_entry *pe,
+				    unsigned int *bit)
+{
+	return mv_pp2x_prs_sram_bit_get(pe, MVPP2_PRS_SRAM_LU_DONE_BIT, bit);
+}
+
+void mv_pp2x_prs_sw_sram_flowid_set(struct mv_pp2x_prs_entry *pe)
+{
+	mv_pp2x_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_flowid_set);
+
+void mv_pp2x_prs_sw_sram_flowid_clear(struct mv_pp2x_prs_entry *pe)
+{
+	mv_pp2x_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);
+}
+EXPORT_SYMBOL(mv_pp2x_prs_sw_sram_flowid_clear);
+
+int mv_pp2x_prs_sw_sram_flowid_gen_get(struct mv_pp2x_prs_entry *pe,
+				       unsigned int *bit)
+{
+	return mv_pp2x_prs_sram_bit_get(pe, MVPP2_PRS_SRAM_LU_GEN_BIT, bit);
+}
+
+/* return RI and RI_UPDATE */
+int mv_pp2x_prs_sw_sram_ri_get(struct mv_pp2x_prs_entry *pe,
+			       unsigned int *bits, unsigned int *enable)
+{
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(bits) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(enable) == MV_ERROR)
+		return MV_ERROR;
+
+	*bits = pe->sram.word[MVPP2_PRS_SRAM_RI_OFFS/32];
+	*enable = pe->sram.word[MVPP2_PRS_SRAM_RI_CTRL_OFFS/32];
+	return MV_OK;
+}
+
+int mv_pp2x_prs_sw_sram_ai_get(struct mv_pp2x_prs_entry *pe,
+			       unsigned int *bits, unsigned int *enable)
+{
+	if (mv_pp2x_ptr_validate(pe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(bits) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(enable) == MV_ERROR)
+		return MV_ERROR;
+
+	*bits = (pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_AI_OFFS)] >> (MVPP2_PRS_SRAM_AI_OFFS % 8)) |
+		(pe->sram.byte[SRAM_BIT_TO_BYTE(
+			MVPP2_PRS_SRAM_AI_OFFS +
+			MVPP2_PRS_SRAM_AI_CTRL_BITS)] <<
+			(8 - (MVPP2_PRS_SRAM_AI_OFFS % 8)));
+
+	*enable = (pe->sram.byte[SRAM_BIT_TO_BYTE(
+		MVPP2_PRS_SRAM_AI_CTRL_OFFS)] >>
+		(MVPP2_PRS_SRAM_AI_CTRL_OFFS % 8)) |
+		(pe->sram.byte[SRAM_BIT_TO_BYTE(
+			MVPP2_PRS_SRAM_AI_CTRL_OFFS +
+			MVPP2_PRS_SRAM_AI_CTRL_BITS)] <<
+			(8 - (MVPP2_PRS_SRAM_AI_CTRL_OFFS % 8)));
+
+	*bits &= MVPP2_PRS_SRAM_AI_MASK;
+	*enable &= MVPP2_PRS_SRAM_AI_MASK;
+
+	return MV_OK;
+}
+
+/*#include "mvPp2ClsHw.h" */
+
+/********************************************************************/
+/***************** Classifier Top Public lkpid table APIs ********************/
+/********************************************************************/
+
+/*------------------------------------------------------------------*/
+
+int mv_pp2x_cls_hw_lkp_read(struct mv_pp2x_hw *hw, int lkpid, int way,
+			    struct mv_pp2x_cls_lookup_entry *fe)
+{
+	unsigned int regVal = 0;
+
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(way, 0, WAY_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(lkpid, 0,
+	    MVPP2_CLS_FLOWS_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	/* write index reg */
+	regVal = (way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) |
+		(lkpid << MVPP2_CLS_LKP_INDEX_LKP_OFFS);
+	mv_pp2x_write(hw, MVPP2_CLS_LKP_INDEX_REG, regVal);
+
+	fe->way = way;
+	fe->lkpid = lkpid;
+
+	fe->data = mv_pp2x_read(hw, MVPP2_CLS_LKP_TBL_REG);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_lkp_read);
+
+int mv_pp2x_cls_hw_lkp_write(struct mv_pp2x_hw *hw, int lkpid,
+			     int way, struct mv_pp2x_cls_lookup_entry *fe)
+{
+	unsigned int regVal = 0;
+
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(way, 0, 1) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(lkpid, 0,
+	    MVPP2_CLS_FLOWS_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	/* write index reg */
+	regVal = (way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) |
+		(lkpid << MVPP2_CLS_LKP_INDEX_LKP_OFFS);
+	mv_pp2x_write(hw, MVPP2_CLS_LKP_INDEX_REG, regVal);
+
+	/* write flowId reg */
+	mv_pp2x_write(hw, MVPP2_CLS_LKP_TBL_REG, fe->data);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_lkp_write);
+
+int mv_pp2x_cls_hw_lkp_print(struct mv_pp2x_hw *hw, int lkpid, int way)
+{
+	unsigned int uint32bit;
+	int int32bit;
+	struct mv_pp2x_cls_lookup_entry lkp;
+
+	if (mv_pp2x_range_validate(way, 0, WAY_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(lkpid, 0,
+	    MVPP2_CLS_FLOWS_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	mv_pp2x_cls_hw_lkp_read(hw, lkpid, way, &lkp);
+
+	DBG_MSG(" 0x%2.2x  %1.1d\t", lkp.lkpid, lkp.way);
+	mv_pp2x_cls_sw_lkp_rxq_get(&lkp, &int32bit);
+	DBG_MSG("0x%2.2x\t", int32bit);
+	mv_pp2x_cls_sw_lkp_en_get(&lkp, &int32bit);
+	DBG_MSG("%1.1d\t", int32bit);
+	mv_pp2x_cls_sw_lkp_flow_get(&lkp, &int32bit);
+	DBG_MSG("0x%3.3x\t", int32bit);
+	mv_pp2x_cls_sw_lkp_mod_get(&lkp, &int32bit);
+	DBG_MSG(" 0x%2.2x\t", int32bit);
+	mv_pp2x_cls_hw_lkp_hit_get(hw, lkp.lkpid, way, &uint32bit);
+	DBG_MSG(" 0x%8.8x\n", uint32bit);
+	DBG_MSG("\n");
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_lkp_print);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_lkp_rxq_get(struct mv_pp2x_cls_lookup_entry *lkp, int *rxq)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(rxq) == MV_ERROR)
+		return MV_ERROR;
+
+	*rxq =  (lkp->data & MVPP2_FLOWID_RXQ_MASK) >> MVPP2_FLOWID_RXQ;
+	return MV_OK;
+}
+
+int mv_pp2x_cls_sw_lkp_rxq_set(struct mv_pp2x_cls_lookup_entry *lkp, int rxq)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(rxq, 0,
+	    (1 << MVPP2_FLOWID_RXQ_BITS) - 1) == MV_ERROR)
+		return MV_ERROR;
+
+	lkp->data &= ~MVPP2_FLOWID_RXQ_MASK;
+	lkp->data |= (rxq << MVPP2_FLOWID_RXQ);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_lkp_rxq_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_lkp_en_get(struct mv_pp2x_cls_lookup_entry *lkp, int *en)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(en) == MV_ERROR)
+		return MV_ERROR;
+
+	*en = (lkp->data & MVPP2_FLOWID_EN_MASK) >> MVPP2_FLOWID_EN;
+	return MV_OK;
+}
+
+int mv_pp2x_cls_sw_lkp_en_set(struct mv_pp2x_cls_lookup_entry *lkp, int en)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(en, 0, 1) == MV_ERROR)
+		return MV_ERROR;
+
+	lkp->data &= ~MVPP2_FLOWID_EN_MASK;
+	lkp->data |= (en << MVPP2_FLOWID_EN);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_lkp_en_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_lkp_flow_get(struct mv_pp2x_cls_lookup_entry *lkp,
+				int *flow_idx)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(flow_idx) == MV_ERROR)
+		return MV_ERROR;
+
+	*flow_idx = (lkp->data & MVPP2_FLOWID_FLOW_MASK) >> MVPP2_FLOWID_FLOW;
+	return MV_OK;
+}
+
+int mv_pp2x_cls_sw_lkp_flow_set(struct mv_pp2x_cls_lookup_entry *lkp,
+		int flow_idx)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(flow_idx, 0,
+	    MVPP2_CLS_FLOWS_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	lkp->data &= ~MVPP2_FLOWID_FLOW_MASK;
+	lkp->data |= (flow_idx << MVPP2_FLOWID_FLOW);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_lkp_flow_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_lkp_mod_get(struct mv_pp2x_cls_lookup_entry *lkp,
+			       int *mod_base)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(mod_base) == MV_ERROR)
+		return MV_ERROR;
+
+	*mod_base = (lkp->data & MVPP2_FLOWID_MODE_MASK) >> MVPP2_FLOWID_MODE;
+	return MV_OK;
+}
+
+int mv_pp2x_cls_sw_lkp_mod_set(struct mv_pp2x_cls_lookup_entry *lkp,
+			       int mod_base)
+{
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	/* TODO: what is the max value of mode base */
+	if (mv_pp2x_range_validate(mod_base, 0,
+	    (1 << MVPP2_FLOWID_MODE_BITS) - 1) == MV_ERROR)
+		return MV_ERROR;
+
+	lkp->data &= ~MVPP2_FLOWID_MODE_MASK;
+	lkp->data |= (mod_base << MVPP2_FLOWID_MODE);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_lkp_mod_set);
+
+/*********************************************************************/
+/***************** Classifier Top Public flows table APIs  ********************/
+/********************************************************************/
+
+int mv_pp2x_cls_hw_flow_read(struct mv_pp2x_hw *hw, int index,
+			     struct mv_pp2x_cls_flow_entry *fe)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(index, 0,
+	    MVPP2_CLS_FLOWS_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->index = index;
+
+	/*write index*/
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_INDEX_REG, index);
+
+	fe->data[0] = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL0_REG);
+	fe->data[1] = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL1_REG);
+	fe->data[2] = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL2_REG);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_flow_read);
+
+/*----------------------------------------------------------------------*/
+/*PPv2.1 new feature MAS 3.18*/
+
+/*----------------------------------------------------------------------*/
+int mv_pp2x_cls_sw_flow_hek_get(struct mv_pp2x_cls_flow_entry *fe,
+				int *num_of_fields, int field_ids[])
+{
+	int index;
+
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(num_of_fields) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(field_ids) == MV_ERROR)
+		return MV_ERROR;
+
+	*num_of_fields = (fe->data[1] & MVPP2_FLOW_FIELDS_NUM_MASK) >>
+			MVPP2_FLOW_FIELDS_NUM;
+
+
+	for (index = 0; index < (*num_of_fields); index++)
+		field_ids[index] = ((fe->data[2] &
+			MVPP2_FLOW_FIELD_MASK(index)) >>
+			MVPP2_FLOW_FIELD_ID(index));
+
+	return MV_OK;
+}
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_flow_port_get(struct mv_pp2x_cls_flow_entry *fe,
+				 int *type, int *portid)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(type) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(portid) == MV_ERROR)
+		return MV_ERROR;
+
+	*type = (fe->data[0] & MVPP2_FLOW_PORT_TYPE_MASK) >>
+			MVPP2_FLOW_PORT_TYPE;
+	*portid = (fe->data[0] & MVPP2_FLOW_PORT_ID_MASK) >>
+			MVPP2_FLOW_PORT_ID;
+
+	return MV_OK;
+}
+
+int mv_pp2x_cls_sw_flow_port_set(struct mv_pp2x_cls_flow_entry *fe,
+				 int type, int portid)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(type, 0,
+	    ((1 << MVPP2_FLOW_PORT_TYPE_BITS) - 1)) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(portid, 0,
+	    ((1 << MVPP2_FLOW_PORT_ID_BITS) - 1)) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[0] &= ~MVPP2_FLOW_PORT_ID_MASK;
+	fe->data[0] &= ~MVPP2_FLOW_PORT_TYPE_MASK;
+
+	fe->data[0] |= (portid << MVPP2_FLOW_PORT_ID);
+	fe->data[0] |= (type << MVPP2_FLOW_PORT_TYPE);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_port_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_flow_portid_select(struct mv_pp2x_cls_flow_entry *fe,
+				      int from)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(from, 0, 1) == MV_ERROR)
+		return MV_ERROR;
+
+	if (from)
+		fe->data[0] |= MVPP2_FLOW_PORT_ID_SEL_MASK;
+	else
+		fe->data[0] &= ~MVPP2_FLOW_PORT_ID_SEL_MASK;
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_portid_select);
+
+int mv_pp2x_cls_sw_flow_pppoe_set(struct mv_pp2x_cls_flow_entry *fe, int mode)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(mode, 0, MVPP2_FLOW_PPPOE_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[0] &= ~MVPP2_FLOW_PPPOE_MASK;
+	fe->data[0] |= (mode << MVPP2_FLOW_PPPOE);
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_pppoe_set);
+
+int mv_pp2x_cls_sw_flow_vlan_set(struct mv_pp2x_cls_flow_entry *fe, int mode)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(mode, 0, MVPP2_FLOW_VLAN_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[0] &= ~MVPP2_FLOW_VLAN_MASK;
+	fe->data[0] |= (mode << MVPP2_FLOW_VLAN);
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_vlan_set);
+
+/*----------------------------------------------------------------------*/
+int mv_pp2x_cls_sw_flow_macme_set(struct mv_pp2x_cls_flow_entry *fe, int mode)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(mode, 0, MVPP2_FLOW_MACME_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[0] &= ~MVPP2_FLOW_MACME_MASK;
+	fe->data[0] |= (mode << MVPP2_FLOW_MACME);
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_macme_set);
+
+/*----------------------------------------------------------------------*/
+int mv_pp2x_cls_sw_flow_udf7_set(struct mv_pp2x_cls_flow_entry *fe, int mode)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(mode, 0, MVPP2_FLOW_UDF7_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[0] &= ~MVPP2_FLOW_UDF7_MASK;
+	fe->data[0] |= (mode << MVPP2_FLOW_UDF7);
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_udf7_set);
+
+int mv_pp2x_cls_sw_flow_seq_ctrl_set(struct mv_pp2x_cls_flow_entry *fe,
+				     int mode)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(mode, 0, MVPP2_FLOW_ENGINE_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[1] &= ~MVPP2_FLOW_SEQ_CTRL_MASK;
+	fe->data[1] |= (mode << MVPP2_FLOW_SEQ_CTRL);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_seq_ctrl_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_flow_engine_get(struct mv_pp2x_cls_flow_entry *fe,
+				   int *engine, int *is_last)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(engine) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(is_last) == MV_ERROR)
+		return MV_ERROR;
+
+	*engine = (fe->data[0] & MVPP2_FLOW_ENGINE_MASK) >> MVPP2_FLOW_ENGINE;
+	*is_last = fe->data[0] & MVPP2_FLOW_LAST_MASK;
+
+	return MV_OK;
+}
+
+/*----------------------------------------------------------------------*/
+int mv_pp2x_cls_sw_flow_engine_set(struct mv_pp2x_cls_flow_entry *fe,
+				   int engine, int is_last)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(is_last, 0, 1) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[0] &= ~MVPP2_FLOW_LAST_MASK;
+	fe->data[0] &= ~MVPP2_FLOW_ENGINE_MASK;
+
+	fe->data[0] |= is_last;
+	fe->data[0] |= (engine << MVPP2_FLOW_ENGINE);
+
+	return MV_OK;
+
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_engine_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_flow_extra_get(struct mv_pp2x_cls_flow_entry *fe,
+				  int *type, int *prio)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(type) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(prio) == MV_ERROR)
+		return MV_ERROR;
+
+	*type = (fe->data[1] & MVPP2_FLOW_LKP_TYPE_MASK) >>
+		MVPP2_FLOW_LKP_TYPE;
+	*prio = (fe->data[1] & MVPP2_FLOW_FIELD_PRIO_MASK) >>
+		MVPP2_FLOW_FIELD_PRIO;
+
+	return MV_OK;
+}
+
+int mv_pp2x_cls_sw_flow_extra_set(struct mv_pp2x_cls_flow_entry *fe,
+				  int type, int prio)
+{
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(type, 0,
+	    MVPP2_FLOW_PORT_ID_MAX) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(prio, 0,
+	    ((1 << MVPP2_FLOW_FIELD_ID_BITS) - 1)) == MV_ERROR)
+		return MV_ERROR;
+
+	fe->data[1] &= ~MVPP2_FLOW_LKP_TYPE_MASK;
+	fe->data[1] |= (type << MVPP2_FLOW_LKP_TYPE);
+
+	fe->data[1] &= ~MVPP2_FLOW_FIELD_PRIO_MASK;
+	fe->data[1] |= (prio << MVPP2_FLOW_FIELD_PRIO);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_extra_set);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_sw_flow_dump(struct mv_pp2x_cls_flow_entry *fe)
+{
+	int	int32bit_1, int32bit_2, i;
+	int	fieldsArr[MVPP2_CLS_FLOWS_TBL_FIELDS_MAX];
+	int	status = MV_OK;
+
+	if (mv_pp2x_ptr_validate(fe) == MV_ERROR)
+		return MV_ERROR;
+
+	DBG_MSG(
+	"INDEX: F[0] F[1] F[2] F[3] PRT[T  ID] ENG LAST LKP_TYP  PRIO\n");
+
+	/*index*/
+	DBG_MSG("0x%3.3x  ", fe->index);
+
+	/*filed[0] filed[1] filed[2] filed[3]*/
+	status |= mv_pp2x_cls_sw_flow_hek_get(fe, &int32bit_1, fieldsArr);
+
+	for (i = 0 ; i < MVPP2_CLS_FLOWS_TBL_FIELDS_MAX; i++)
+		if (i < int32bit_1)
+			DBG_MSG("0x%2.2x ", fieldsArr[i]);
+		else
+			DBG_MSG(" NA  ");
+
+	/*port_type port_id*/
+	status |= mv_pp2x_cls_sw_flow_port_get(fe, &int32bit_1, &int32bit_2);
+	DBG_MSG("[%1d  0x%3.3x]  ", int32bit_1, int32bit_2);
+
+	/* engine_num last_bit*/
+	status |= mv_pp2x_cls_sw_flow_engine_get(fe, &int32bit_1, &int32bit_2);
+	DBG_MSG("%1d   %1d    ", int32bit_1, int32bit_2);
+
+	/* lookup_type priority*/
+	status |= mv_pp2x_cls_sw_flow_extra_get(fe, &int32bit_1, &int32bit_2);
+	DBG_MSG("0x%2.2x    0x%2.2x", int32bit_1, int32bit_2);
+
+	DBG_MSG("\n");
+	DBG_MSG("\n");
+	DBG_MSG("       PPPEO   VLAN   MACME   UDF7   SELECT SEQ_CTRL\n");
+	DBG_MSG("         %1d      %1d      %1d       %1d      %1d      %1d\n",
+			(fe->data[0] & MVPP2_FLOW_PPPOE_MASK) >>
+					MVPP2_FLOW_PPPOE,
+			(fe->data[0] & MVPP2_FLOW_VLAN_MASK) >>
+					MVPP2_FLOW_VLAN,
+			(fe->data[0] & MVPP2_FLOW_MACME_MASK) >>
+					MVPP2_FLOW_MACME,
+			(fe->data[0] & MVPP2_FLOW_UDF7_MASK) >>
+					MVPP2_FLOW_UDF7,
+			(fe->data[0] & MVPP2_FLOW_PORT_ID_SEL_MASK) >>
+					MVPP2_FLOW_PORT_ID_SEL,
+			(fe->data[1] & MVPP2_FLOW_SEQ_CTRL_MASK) >>
+					MVPP2_FLOW_SEQ_CTRL);
+	DBG_MSG("\n");
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_flow_dump);
+
+/*----------------------------------------------------------------------*/
+
+/*----------------------------------------------------------------------*/
+/*	Classifier Top Public length change table APIs			*/
+/*----------------------------------------------------------------------*/
+
+/*----------------------------------------------------------------------*/
+/*	additional cls debug APIs					*/
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_hw_regs_dump(struct mv_pp2x_hw *hw)
+{
+	int i = 0;
+	char reg_name[100];
+
+	mv_pp2x_print_reg(hw, MVPP2_CLS_MODE_REG,
+			  "MVPP2_CLS_MODE_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_PORT_WAY_REG,
+			  "MVPP2_CLS_PORT_WAY_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_LKP_INDEX_REG,
+			  "MVPP2_CLS_LKP_INDEX_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_LKP_TBL_REG,
+			  "MVPP2_CLS_LKP_TBL_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_FLOW_INDEX_REG,
+			  "MVPP2_CLS_FLOW_INDEX_REG");
+
+	mv_pp2x_print_reg(hw, MVPP2_CLS_FLOW_TBL0_REG,
+			  "MVPP2_CLS_FLOW_TBL0_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_FLOW_TBL1_REG,
+			  "MVPP2_CLS_FLOW_TBL1_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_FLOW_TBL2_REG,
+			  "MVPP2_CLS_FLOW_TBL2_REG");
+
+
+	mv_pp2x_print_reg(hw, MVPP2_CLS_PORT_SPID_REG,
+			  "MVPP2_CLS_PORT_SPID_REG");
+
+	for (i = 0; i < MVPP2_CLS_SPID_UNI_REGS; i++) {
+		sprintf(reg_name, "MVPP2_CLS_SPID_UNI_%d_REG", i);
+		mv_pp2x_print_reg(hw, (MVPP2_CLS_SPID_UNI_BASE_REG + (4 * i)),
+				  reg_name);
+	}
+	for (i = 0; i < MVPP2_CLS_GEM_VIRT_REGS_NUM; i++) {
+		/* indirect access */
+		mv_pp2x_write(hw, MVPP2_CLS_GEM_VIRT_INDEX_REG, i);
+		sprintf(reg_name, "MVPP2_CLS_GEM_VIRT_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_CLS_GEM_VIRT_REG, reg_name);
+	}
+	for (i = 0; i < MVPP2_CLS_UDF_BASE_REGS; i++)	{
+		sprintf(reg_name, "MVPP2_CLS_UDF_REG_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_CLS_UDF_REG(i), reg_name);
+	}
+	for (i = 0; i < 16; i++) {
+		sprintf(reg_name, "MVPP2_CLS_MTU_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_CLS_MTU_REG(i), reg_name);
+	}
+	for (i = 0; i < MVPP2_MAX_PORTS; i++) {
+		sprintf(reg_name, "MVPP2_CLS_OVER_RXQ_LOW_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_CLS_OVERSIZE_RXQ_LOW_REG(i),
+				reg_name);
+	}
+	for (i = 0; i < MVPP2_MAX_PORTS; i++) {
+		sprintf(reg_name, "MVPP2_CLS_SWFWD_P2HQ_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_CLS_SWFWD_P2HQ_REG(i), reg_name);
+	}
+
+	mv_pp2x_print_reg(hw, MVPP2_CLS_SWFWD_PCTRL_REG,
+			  "MVPP2_CLS_SWFWD_PCTRL_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS_SEQ_SIZE_REG,
+			  "MVPP2_CLS_SEQ_SIZE_REG");
+
+	for (i = 0; i < MVPP2_MAX_PORTS; i++) {
+		sprintf(reg_name, "MVPP2_CLS_PCTRL_%d_REG", i);
+		mv_pp2x_print_reg(hw, MV_PP2_CLS_PCTRL_REG(i), reg_name);
+	}
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_regs_dump);
+/*----------------------------------------------------------------------*/
+static int mv_pp2x_cls_hw_flow_hit_get(struct mv_pp2x_hw *hw,
+				       int index,  unsigned int *cnt)
+{
+	if (mv_pp2x_range_validate(index, 0,
+	    MVPP2_CLS_FLOWS_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	/*set index */
+	mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, MVPP2_CNT_IDX_FLOW(index));
+
+	if (cnt)
+		*cnt = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL_HIT_REG);
+	else
+		DBG_MSG("HITS = %d\n", mv_pp2x_read(hw,
+			MVPP2_CLS_FLOW_TBL_HIT_REG));
+
+	return MV_OK;
+}
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_hw_lkp_hit_get(struct mv_pp2x_hw *hw, int lkpid, int way,
+			       unsigned int *cnt)
+{
+	if (mv_pp2x_range_validate(way, 0, 1) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(lkpid, 0,
+	    MVPP2_CLS_LKP_TBL_SIZE) == MV_ERROR)
+		return MV_ERROR;
+
+	/*set index */
+	mv_pp2x_write(hw, MVPP2_CNT_IDX_REG, MVPP2_CNT_IDX_LKP(lkpid, way));
+
+	if (cnt)
+		*cnt = mv_pp2x_read(hw, MVPP2_CLS_LKP_TBL_HIT_REG);
+	else
+		DBG_MSG("HITS: %d\n", mv_pp2x_read(hw,
+			MVPP2_CLS_LKP_TBL_HIT_REG));
+
+	return MV_OK;
+}
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_hw_flow_dump(struct mv_pp2x_hw *hw)
+{
+	int index;
+
+	struct mv_pp2x_cls_flow_entry fe;
+
+	for (index = 0; index < MVPP2_CLS_FLOWS_TBL_SIZE ; index++) {
+		mv_pp2x_cls_hw_flow_read(hw, index, &fe);
+		mv_pp2x_cls_sw_flow_dump(&fe);
+		mv_pp2x_cls_hw_flow_hit_get(hw, index, NULL);
+		DBG_MSG("-------------------------------------------------\n");
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_flow_dump);
+
+/*----------------------------------------------------------------------*/
+/*PPv2.1 new counters MAS 3.20*/
+int mv_pp2x_cls_hw_flow_hits_dump(struct mv_pp2x_hw *hw)
+{
+	int index;
+	unsigned int cnt;
+	struct mv_pp2x_cls_flow_entry fe;
+
+	for (index = 0; index < MVPP2_CLS_FLOWS_TBL_SIZE ; index++) {
+		mv_pp2x_cls_hw_flow_hit_get(hw, index, &cnt);
+		if (cnt != 0) {
+			mv_pp2x_cls_hw_flow_read(hw, index, &fe);
+			mv_pp2x_cls_sw_flow_dump(&fe);
+			DBG_MSG("HITS = %d\n", cnt);
+			DBG_MSG("\n");
+		}
+	}
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_flow_hits_dump);
+
+/*----------------------------------------------------------------------*/
+/*PPv2.1 new counters MAS 3.20*/
+int mv_pp2x_cls_hw_lkp_hits_dump(struct mv_pp2x_hw *hw)
+{
+	int index, way, entryInd;
+	unsigned int cnt;
+
+	DBG_MSG("< ID  WAY >:	HITS\n");
+	for (index = 0; index < MVPP2_CLS_LKP_TBL_SIZE ; index++)
+		for (way = 0; way < 2 ; way++)	{
+			entryInd = (way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) |
+				index;
+			mv_pp2x_cls_hw_lkp_hit_get(hw, index, way,  &cnt);
+			if (cnt != 0)
+				DBG_MSG(" 0x%2.2x  %1.1d\t0x%8.8x\n",
+					index, way, cnt);
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_lkp_hits_dump);
+
+/*----------------------------------------------------------------------*/
+int mv_pp2x_cls_sw_lkp_dump(struct mv_pp2x_cls_lookup_entry *lkp)
+{
+	int int32bit;
+	int status = 0;
+
+	if (mv_pp2x_ptr_validate(lkp) == MV_ERROR)
+		return MV_ERROR;
+
+	DBG_MSG("< ID  WAY >:	RXQ	EN	FLOW	MODE_BASE\n");
+
+	/* id */
+	DBG_MSG(" 0x%2.2x  %1.1d\t", lkp->lkpid, lkp->way);
+
+	/*rxq*/
+	status |= mv_pp2x_cls_sw_lkp_rxq_get(lkp, &int32bit);
+	DBG_MSG("0x%2.2x\t", int32bit);
+
+	/*enabe bit*/
+	status |= mv_pp2x_cls_sw_lkp_en_get(lkp, &int32bit);
+	DBG_MSG("%1.1d\t", int32bit);
+
+	/*flow*/
+	status |= mv_pp2x_cls_sw_lkp_flow_get(lkp, &int32bit);
+	DBG_MSG("0x%3.3x\t", int32bit);
+
+	/*mode*/
+	status |= mv_pp2x_cls_sw_lkp_mod_get(lkp, &int32bit);
+	DBG_MSG(" 0x%2.2x\t", int32bit);
+
+	DBG_MSG("\n");
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_sw_lkp_dump);
+
+int mv_pp2x_cls_hw_lkp_dump(struct mv_pp2x_hw *hw)
+{
+	int index, way, int32bit, ind;
+	unsigned int uint32bit;
+
+	struct mv_pp2x_cls_lookup_entry lkp;
+
+	DBG_MSG("< ID  WAY >:	RXQ	EN	FLOW	MODE_BASE  HITS\n");
+	for (index = 0; index < MVPP2_CLS_LKP_TBL_SIZE ; index++)
+		for (way = 0; way < 2 ; way++)	{
+			ind = (way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) | index;
+			mv_pp2x_cls_hw_lkp_read(hw, index, way, &lkp);
+			DBG_MSG(" 0x%2.2x  %1.1d\t", lkp.lkpid, lkp.way);
+			mv_pp2x_cls_sw_lkp_rxq_get(&lkp, &int32bit);
+			DBG_MSG("0x%2.2x\t", int32bit);
+			mv_pp2x_cls_sw_lkp_en_get(&lkp, &int32bit);
+			DBG_MSG("%1.1d\t", int32bit);
+			mv_pp2x_cls_sw_lkp_flow_get(&lkp, &int32bit);
+			DBG_MSG("0x%3.3x\t", int32bit);
+			mv_pp2x_cls_sw_lkp_mod_get(&lkp, &int32bit);
+			DBG_MSG(" 0x%2.2x\t", int32bit);
+			mv_pp2x_cls_hw_lkp_hit_get(hw, index, way, &uint32bit);
+			DBG_MSG(" 0x%8.8x\n", uint32bit);
+			DBG_MSG("\n");
+
+		}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_hw_lkp_dump);
+
+/*----------------------------------------------------------------------*/
+/*	Classifier C2 engine QoS table Public APIs			*/
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_qos_hw_read(struct mv_pp2x_hw *hw, int tbl_id,
+			       int tbl_sel, int tbl_line,
+			       struct mv_pp2x_cls_c2_qos_entry *qos)
+{
+	unsigned int regVal = 0;
+
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(tbl_sel, 0, 1) == MV_ERROR) /* one bit */
+		return MV_ERROR;
+
+	if (tbl_sel == 1) {
+		/*dscp*/
+		/* TODO define 8=DSCP_TBL_NUM  64=DSCP_TBL_LINES */
+		if (mv_pp2x_range_validate(tbl_id, 0,
+		    MVPP2_QOS_TBL_NUM_DSCP) == MV_ERROR)
+			return MV_ERROR;
+		if (mv_pp2x_range_validate(tbl_line, 0,
+		    MVPP2_QOS_TBL_LINE_NUM_DSCP) == MV_ERROR)
+			return MV_ERROR;
+	} else {
+		/*pri*/
+		/* TODO define 64=PRI_TBL_NUM  8=PRI_TBL_LINES */
+		if (mv_pp2x_range_validate(tbl_id, 0,
+		    MVPP2_QOS_TBL_NUM_PRI) == MV_ERROR)
+			return MV_ERROR;
+		if (mv_pp2x_range_validate(tbl_line, 0,
+		    MVPP2_QOS_TBL_LINE_NUM_PRI) == MV_ERROR)
+			return MV_ERROR;
+	}
+
+	qos->tbl_id = tbl_id;
+	qos->tbl_sel = tbl_sel;
+	qos->tbl_line = tbl_line;
+
+	/* write index reg */
+	regVal |= (tbl_line << MVPP2_CLS2_DSCP_PRI_INDEX_LINE_OFF);
+	regVal |= (tbl_sel << MVPP2_CLS2_DSCP_PRI_INDEX_SEL_OFF);
+	regVal |= (tbl_id << MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_OFF);
+
+	mv_pp2x_write(hw, MVPP2_CLS2_DSCP_PRI_INDEX_REG, regVal);
+
+	/* read data reg*/
+	qos->data = mv_pp2x_read(hw, MVPP2_CLS2_QOS_TBL_REG);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_hw_read);
+
+/*----------------------------------------------------------------------*/
+
+int mvPp2ClsC2QosPrioGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *prio)
+{
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(prio) == MV_ERROR)
+		return MV_ERROR;
+
+	*prio = (qos->data & MVPP2_CLS2_QOS_TBL_PRI_MASK) >>
+		MVPP2_CLS2_QOS_TBL_PRI_OFF;
+	return MV_OK;
+}
+EXPORT_SYMBOL(mvPp2ClsC2QosPrioGet);
+
+/*----------------------------------------------------------------------*/
+
+int mvPp2ClsC2QosDscpGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *dscp)
+{
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(dscp) == MV_ERROR)
+		return MV_ERROR;
+
+	*dscp = (qos->data & MVPP2_CLS2_QOS_TBL_DSCP_MASK) >>
+		MVPP2_CLS2_QOS_TBL_DSCP_OFF;
+	return MV_OK;
+}
+EXPORT_SYMBOL(mvPp2ClsC2QosDscpGet);
+
+/*----------------------------------------------------------------------*/
+
+int mvPp2ClsC2QosColorGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *color)
+{
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(color) == MV_ERROR)
+		return MV_ERROR;
+
+	*color = (qos->data & MVPP2_CLS2_QOS_TBL_COLOR_MASK) >>
+		MVPP2_CLS2_QOS_TBL_COLOR_OFF;
+	return MV_OK;
+}
+EXPORT_SYMBOL(mvPp2ClsC2QosColorGet);
+
+/*----------------------------------------------------------------------*/
+
+int mvPp2ClsC2QosGpidGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *gpid)
+{
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(gpid) == MV_ERROR)
+		return MV_ERROR;
+
+	*gpid = (qos->data & MVPP2_CLS2_QOS_TBL_GEMPORT_MASK) >>
+		MVPP2_CLS2_QOS_TBL_GEMPORT_OFF;
+	return MV_OK;
+}
+EXPORT_SYMBOL(mvPp2ClsC2QosGpidGet);
+
+/*----------------------------------------------------------------------*/
+
+int mvPp2ClsC2QosQueueGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *queue)
+{
+	if (mv_pp2x_ptr_validate(qos) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(queue) == MV_ERROR)
+		return MV_ERROR;
+
+	*queue = (qos->data & MVPP2_CLS2_QOS_TBL_QUEUENUM_MASK) >>
+		MVPP2_CLS2_QOS_TBL_QUEUENUM_OFF;
+	return MV_OK;
+}
+EXPORT_SYMBOL(mvPp2ClsC2QosQueueGet);
+
+/*----------------------------------------------------------------------*/
+/*	Classifier C2 engine TCAM table Public APIs			*/
+/*----------------------------------------------------------------------*/
+
+/* note: error is not returned if entry is invalid
+ * user should check c2->valid afer returned from this func
+ */
+int mv_pp2x_cls_c2_hw_read(struct mv_pp2x_hw *hw, int index,
+			   struct mv_pp2x_cls_c2_entry *c2)
+{
+	unsigned int regVal;
+	int	TcmIdx;
+
+	if (mv_pp2x_ptr_validate(c2) == MV_ERROR)
+		return MV_ERROR;
+
+	c2->index = index;
+
+	/* write index reg */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_IDX_REG, index);
+
+	/* read inValid bit*/
+	regVal = mv_pp2x_read(hw, MVPP2_CLS2_TCAM_INV_REG);
+	c2->inv = (regVal & MVPP2_CLS2_TCAM_INV_INVALID_MASK) >>
+		MVPP2_CLS2_TCAM_INV_INVALID_OFF;
+
+	if (c2->inv)
+		return MV_OK;
+
+	for (TcmIdx = 0; TcmIdx < MVPP2_CLS_C2_TCAM_WORDS; TcmIdx++)
+		c2->tcam.words[TcmIdx] = mv_pp2x_read(hw,
+					MVPP2_CLS2_TCAM_DATA_REG(TcmIdx));
+
+	/* read action_tbl 0x1B30 */
+	c2->sram.regs.action_tbl = mv_pp2x_read(hw, MVPP2_CLS2_ACT_DATA_REG);
+
+	/* read actions 0x1B60 */
+	c2->sram.regs.actions = mv_pp2x_read(hw, MVPP2_CLS2_ACT_REG);
+
+	/* read qos_attr 0x1B64 */
+	c2->sram.regs.qos_attr = mv_pp2x_read(hw, MVPP2_CLS2_ACT_QOS_ATTR_REG);
+
+	/* read hwf_attr 0x1B68 */
+	c2->sram.regs.hwf_attr = mv_pp2x_read(hw, MVPP2_CLS2_ACT_HWF_ATTR_REG);
+
+	/* read hwf_attr 0x1B6C */
+	c2->sram.regs.rss_attr = mv_pp2x_read(hw, MVPP2_CLS2_ACT_DUP_ATTR_REG);
+
+	/* read seq_attr 0x1B70 */
+	c2->sram.regs.seq_attr = mv_pp2x_read(hw, MVPP22_CLS2_ACT_SEQ_ATTR_REG);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hw_read);
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_sw_words_dump(struct mv_pp2x_cls_c2_entry *c2)
+{
+	int i;
+
+	if (mv_pp2x_ptr_validate(c2) == MV_ERROR)
+		return MV_ERROR;
+
+	/* TODO check size */
+	/* hw entry id */
+	DBG_MSG("[0x%3.3x] ", c2->index);
+
+	i = MVPP2_CLS_C2_TCAM_WORDS - 1;
+
+	while (i >= 0)
+		DBG_MSG("%4.4x ", (c2->tcam.words[i--]) & 0xFFFF);
+
+	DBG_MSG("| ");
+
+	DBG_MSG("%8.8x %8.8x %8.8x %8.8x %8.8x", c2->sram.words[4],
+		c2->sram.words[3], c2->sram.words[2], c2->sram.words[1],
+		c2->sram.words[0]);
+
+	/*tcam inValid bit*/
+	DBG_MSG(" %s", (c2->inv == 1) ? "[inv]" : "[valid]");
+
+	DBG_MSG("\n        ");
+
+	i = MVPP2_CLS_C2_TCAM_WORDS - 1;
+
+	while (i >= 0)
+		DBG_MSG("%4.4x ", ((c2->tcam.words[i--] >> 16)  & 0xFFFF));
+
+	DBG_MSG("\n");
+
+	return MV_OK;
+}
+
+/*----------------------------------------------------------------------*/
+
+int mvPp2ClsC2TcamByteGet(struct mv_pp2x_cls_c2_entry *c2,
+			  unsigned int offs, unsigned char *byte,
+			  unsigned char *enable)
+{
+	if (mv_pp2x_ptr_validate(c2) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(byte) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_ptr_validate(enable) == MV_ERROR)
+		return MV_ERROR;
+
+	if (mv_pp2x_range_validate(offs, 0, 8) == MV_ERROR)
+		return MV_ERROR;
+
+	*byte = c2->tcam.bytes[TCAM_DATA_BYTE(offs)];
+	*enable = c2->tcam.bytes[TCAM_DATA_MASK(offs)];
+	return MV_OK;
+}
+
+/*----------------------------------------------------------------------*/
+/*	return EQUALS if tcam_data[off]&tcam_mask[off] = byte		*/
+/*----------------------------------------------------------------------*/
+/*	Classifier C2 engine Hit counters Public APIs			*/
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_hit_cntr_is_busy(struct mv_pp2x_hw *hw)
+{
+	unsigned int regVal;
+
+	regVal = mv_pp2x_read(hw, MVPP2_CLS2_HIT_CTR_REG);
+	regVal &= MVPP2_CLS2_HIT_CTR_CLR_DONE_MASK;
+	regVal >>= MVPP2_CLS2_HIT_CTR_CLR_DONE_OFF;
+
+	return (1 - (int)regVal);
+}
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_hit_cntr_clear_all(struct mv_pp2x_hw *hw)
+{
+	int iter = 0;
+
+	/* wrirte clear bit*/
+	mv_pp2x_write(hw, MVPP2_CLS2_HIT_CTR_CLR_REG,
+		(1 << MVPP2_CLS2_HIT_CTR_CLR_CLR_OFF));
+
+	while (mv_pp2x_cls_c2_hit_cntr_is_busy(hw))
+		if (iter++ >= RETRIES_EXCEEDED) {
+			DBG_MSG("%s:Error - retries exceeded.\n", __func__);
+			return MV_ERROR;
+		}
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hit_cntr_clear_all);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_hit_cntr_read(struct mv_pp2x_hw *hw, int index, u32 *cntr)
+{
+	unsigned int value = 0;
+
+	/* write index reg */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_IDX_REG, index);
+
+	value = mv_pp2x_read(hw, MVPP2_CLS2_HIT_CTR_REG);
+
+	if (cntr)
+		*cntr = value;
+	else
+		DBG_MSG("INDEX: 0x%8.8X	VAL: 0x%8.8X\n", index, value);
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hit_cntr_read);
+
+
+/*----------------------------------------------------------------------*/
+int mv_pp2x_cls_c2_hit_cntr_dump(struct mv_pp2x_hw *hw)
+{
+	int i;
+	unsigned int cnt;
+
+	for (i = 0; i < MVPP2_CLS_C2_TCAM_SIZE; i++) {
+		mv_pp2x_cls_c2_hit_cntr_read(hw, i, &cnt);
+		if (cnt != 0)
+			DBG_MSG("INDEX: 0x%8.8X	VAL: 0x%8.8X\n", i, cnt);
+	}
+
+
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hit_cntr_dump);
+
+/*----------------------------------------------------------------------*/
+
+int mv_pp2x_cls_c2_regs_dump(struct mv_pp2x_hw *hw)
+{
+	int i;
+	char reg_name[100];
+
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_TCAM_IDX_REG,
+			  "MVPP2_CLS2_TCAM_IDX_REG");
+
+	for (i = 0; i < MVPP2_CLS_C2_TCAM_WORDS; i++) {
+		printk(reg_name, "MVPP2_CLS2_TCAM_DATA_%d_REG", i);
+		mv_pp2x_print_reg(hw, MVPP2_CLS2_TCAM_DATA_REG(i), reg_name);
+	}
+
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_TCAM_INV_REG,
+			  "MVPP2_CLS2_TCAM_INV_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_ACT_DATA_REG,
+			  "MVPP2_CLS2_ACT_DATA_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_DSCP_PRI_INDEX_REG,
+			  "MVPP2_CLS2_DSCP_PRI_INDEX_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_QOS_TBL_REG,
+			  "MVPP2_CLS2_QOS_TBL_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_ACT_REG,
+			  "MVPP2_CLS2_ACT_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_ACT_QOS_ATTR_REG,
+			  "MVPP2_CLS2_ACT_QOS_ATTR_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_ACT_HWF_ATTR_REG,
+			  "MVPP2_CLS2_ACT_HWF_ATTR_REG");
+	mv_pp2x_print_reg(hw, MVPP2_CLS2_ACT_DUP_ATTR_REG,
+			  "MVPP2_CLS2_ACT_DUP_ATTR_REG");
+	mv_pp2x_print_reg(hw, MVPP22_CLS2_ACT_SEQ_ATTR_REG,
+			  "MVPP22_CLS2_ACT_SEQ_ATTR_REG");
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_regs_dump);
+
+void mv_pp2x_cls_flow_port_add(struct mv_pp2x_hw *hw, int index, int port_id)
+{
+	u32 data;
+
+	/* Write flow index */
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_INDEX_REG, index);
+	/* Read first data with port info */
+	data = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL0_REG);
+	/* Add the port */
+	data |= ((1 << port_id) << MVPP2_FLOW_PORT_ID);
+	/* Update the register */
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_TBL0_REG, data);
+}
+
+void mv_pp2x_cls_flow_port_del(struct mv_pp2x_hw *hw, int index, int port_id)
+{
+	u32 data;
+
+	/* Write flow index */
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_INDEX_REG, index);
+	/* Read first data with port info */
+	data = mv_pp2x_read(hw, MVPP2_CLS_FLOW_TBL0_REG);
+	/* Delete the port */
+	data &= ~(((1 << port_id) << MVPP2_FLOW_PORT_ID));
+	/* Update the register */
+	mv_pp2x_write(hw, MVPP2_CLS_FLOW_TBL0_REG, data);
+}
+
+/* The function prepare a temporary flow table for lkpid flow,
+ * in order to change the original one
+ */
+void mv_pp2x_cls_flow_tbl_temp_copy(struct mv_pp2x_hw *hw, int lkpid,
+				    int *temp_flow_idx)
+{
+	struct mv_pp2x_cls_flow_entry fe;
+	int index = lkpid - MVPP2_PRS_FL_START;
+	int flow_start = hw->cls_shadow->flow_free_start;
+	struct mv_pp2x_cls_flow_info *flow_info;
+
+	flow_info = &(hw->cls_shadow->flow_info[index]);
+
+	if (flow_info->flow_entry_dflt) {
+		mv_pp2x_cls_flow_read(hw, flow_info->flow_entry_dflt, &fe);
+		fe.index = flow_start++;
+		mv_pp2x_cls_flow_write(hw, &fe);
+	}
+	if (flow_info->flow_entry_vlan) {
+		mv_pp2x_cls_flow_read(hw, flow_info->flow_entry_vlan, &fe);
+		fe.index = flow_start++;
+		mv_pp2x_cls_flow_write(hw, &fe);
+	}
+	if (flow_info->flow_entry_dscp) {
+		mv_pp2x_cls_flow_read(hw, flow_info->flow_entry_dscp, &fe);
+		fe.index = flow_start++;
+		mv_pp2x_cls_flow_write(hw, &fe);
+	}
+	if (flow_info->flow_entry_rss1) {
+		mv_pp2x_cls_flow_read(hw, flow_info->flow_entry_rss1, &fe);
+		fe.index = flow_start++;
+		mv_pp2x_cls_flow_write(hw, &fe);
+	}
+	if (flow_info->flow_entry_rss2) {
+		mv_pp2x_cls_flow_read(hw, flow_info->flow_entry_rss2, &fe);
+		fe.index = flow_start++;
+		mv_pp2x_cls_flow_write(hw, &fe);
+	}
+
+	*temp_flow_idx = hw->cls_shadow->flow_free_start;
+}
+
+/* C2 rule and Qos table */
+int mv_pp2x_cls_c2_hw_write(struct mv_pp2x_hw *hw, int index,
+			    struct mv_pp2x_cls_c2_entry *c2)
+{
+	int TcmIdx;
+
+	if (!c2 || index >= MVPP2_CLS_C2_TCAM_SIZE)
+		return -EINVAL;
+
+	c2->index = index;
+
+	/* write index reg */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_IDX_REG, index);
+
+	/* write valid bit*/
+	c2->inv = 0;
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_INV_REG,
+		((c2->inv) << MVPP2_CLS2_TCAM_INV_INVALID_OFF));
+
+	for (TcmIdx = 0; TcmIdx < MVPP2_CLS_C2_TCAM_WORDS; TcmIdx++)
+		mv_pp2x_write(hw, MVPP2_CLS2_TCAM_DATA_REG(TcmIdx),
+			c2->tcam.words[TcmIdx]);
+
+	/* write action_tbl CLSC2_ACT_DATA */
+	mv_pp2x_write(hw, MVPP2_CLS2_ACT_DATA_REG, c2->sram.regs.action_tbl);
+
+	/* write actions CLSC2_ACT */
+	mv_pp2x_write(hw, MVPP2_CLS2_ACT_REG, c2->sram.regs.actions);
+
+	/* write qos_attr CLSC2_ATTR0 */
+	mv_pp2x_write(hw, MVPP2_CLS2_ACT_QOS_ATTR_REG, c2->sram.regs.qos_attr);
+
+	/* write hwf_attr CLSC2_ATTR1 */
+	mv_pp2x_write(hw, MVPP2_CLS2_ACT_HWF_ATTR_REG, c2->sram.regs.hwf_attr);
+
+	/* write rss_attr CLSC2_ATTR2 */
+	mv_pp2x_write(hw, MVPP2_CLS2_ACT_DUP_ATTR_REG, c2->sram.regs.rss_attr);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hw_write);
+
+int mv_pp2x_cls_c2_qos_hw_write(struct mv_pp2x_hw *hw,
+				struct mv_pp2x_cls_c2_qos_entry *qos)
+{
+	unsigned int regVal = 0;
+
+	if (!qos || qos->tbl_sel > MVPP2_QOS_TBL_SEL_DSCP)
+		return -EINVAL;
+
+	if (qos->tbl_sel == MVPP2_QOS_TBL_SEL_DSCP) {
+		/*dscp*/
+		if (qos->tbl_id >=  MVPP2_QOS_TBL_NUM_DSCP ||
+			qos->tbl_line >= MVPP2_QOS_TBL_LINE_NUM_DSCP)
+			return -EINVAL;
+	} else {
+		/*pri*/
+		if (qos->tbl_id >=  MVPP2_QOS_TBL_NUM_PRI ||
+			qos->tbl_line >= MVPP2_QOS_TBL_LINE_NUM_PRI)
+			return -EINVAL;
+	}
+	/* write index reg */
+	regVal |= (qos->tbl_line << MVPP2_CLS2_DSCP_PRI_INDEX_LINE_OFF);
+	regVal |= (qos->tbl_sel << MVPP2_CLS2_DSCP_PRI_INDEX_SEL_OFF);
+	regVal |= (qos->tbl_id << MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_OFF);
+	mv_pp2x_write(hw, MVPP2_CLS2_DSCP_PRI_INDEX_REG, regVal);
+
+	/* write data reg*/
+	mv_pp2x_write(hw, MVPP2_CLS2_QOS_TBL_REG, qos->data);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_hw_write);
+
+int mv_pp2x_cls_c2_hw_inv(struct mv_pp2x_hw *hw, int index)
+{
+	if (!hw || index >= MVPP2_CLS_C2_TCAM_SIZE)
+		return -EINVAL;
+
+	/* write index reg */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_IDX_REG, index);
+
+	/* set invalid bit*/
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_INV_REG, (1 <<
+		MVPP2_CLS2_TCAM_INV_INVALID_OFF));
+
+	/* trigger */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_DATA_REG(4), 0);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hw_inv);
+
+void mv_pp2x_cls_c2_hw_inv_all(struct mv_pp2x_hw *hw)
+{
+	int index;
+
+	for (index = 0; index < MVPP2_CLS_C2_TCAM_SIZE; index++) {
+		PALAD(MVPP2_PRINT_LINE());
+		mv_pp2x_cls_c2_hw_inv(hw, index);
+	}
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_hw_inv_all);
+
+static void mv_pp2x_cls_c2_qos_hw_clear_all(struct mv_pp2x_hw *hw)
+{
+	struct mv_pp2x_cls_c2_qos_entry qos;
+
+	memset(&qos, 0, sizeof(struct mv_pp2x_cls_c2_qos_entry));
+
+	/* clear DSCP tables */
+	qos.tbl_sel = MVPP2_QOS_TBL_SEL_DSCP;
+	for (qos.tbl_id = 0; qos.tbl_id < MVPP2_QOS_TBL_NUM_DSCP;
+		qos.tbl_id++) {
+		for (qos.tbl_line = 0; qos.tbl_line <
+			MVPP2_QOS_TBL_LINE_NUM_DSCP; qos.tbl_line++) {
+			mv_pp2x_cls_c2_qos_hw_write(hw, &qos);
+			PALAD(MVPP2_PRINT_LINE());
+		}
+	}
+
+	/* clear PRIO tables */
+	qos.tbl_sel = MVPP2_QOS_TBL_SEL_PRI;
+	for (qos.tbl_id = 0; qos.tbl_id <
+		MVPP2_QOS_TBL_NUM_PRI; qos.tbl_id++)
+		for (qos.tbl_line = 0; qos.tbl_line <
+			MVPP2_QOS_TBL_LINE_NUM_PRI; qos.tbl_line++) {
+			PALAD(MVPP2_PRINT_LINE());
+			mv_pp2x_cls_c2_qos_hw_write(hw, &qos);
+		}
+}
+
+int mv_pp2x_cls_c2_qos_tbl_set(struct mv_pp2x_cls_c2_entry *c2,
+			       int tbl_id, int tbl_sel)
+{
+	if (!c2 || tbl_sel > 1)
+		return -EINVAL;
+
+	if (tbl_sel == 1) {
+		/*dscp*/
+		if (tbl_id >= MVPP2_QOS_TBL_NUM_DSCP)
+			return -EINVAL;
+	} else {
+		/*pri*/
+		if (tbl_id >= MVPP2_QOS_TBL_NUM_PRI)
+			return -EINVAL;
+	}
+	c2->sram.regs.action_tbl = (tbl_id <<
+			MVPP2_CLS2_ACT_DATA_TBL_ID_OFF) |
+			(tbl_sel << MVPP2_CLS2_ACT_DATA_TBL_SEL_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_tbl_set);
+
+int mv_pp2x_cls_c2_color_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+			     int from)
+{
+	if (!c2 || cmd > MVPP2_COLOR_ACTION_TYPE_RED_LOCK)
+		return -EINVAL;
+
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_COLOR_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_COLOR_OFF);
+
+	if (from == 1)
+		c2->sram.regs.action_tbl |= (1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_COLOR_OFF);
+	else
+		c2->sram.regs.action_tbl &= ~(1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_COLOR_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_color_set);
+
+int mv_pp2x_cls_c2_prio_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+			    int prio, int from)
+{
+	if (!c2 || cmd > MVPP2_ACTION_TYPE_UPDT_LOCK ||
+			prio >= MVPP2_QOS_TBL_LINE_NUM_PRI)
+		return -EINVAL;
+
+	/*set command*/
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_PRI_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_PRI_OFF);
+
+	/*set modify priority value*/
+	c2->sram.regs.qos_attr &= ~MVPP2_CLS2_ACT_QOS_ATTR_PRI_MASK;
+	c2->sram.regs.qos_attr |= ((prio << MVPP2_CLS2_ACT_QOS_ATTR_PRI_OFF) &
+		MVPP2_CLS2_ACT_QOS_ATTR_PRI_MASK);
+
+	if (from == 1)
+		c2->sram.regs.action_tbl |= (1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_OFF);
+	else
+		c2->sram.regs.action_tbl &= ~(1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_prio_set);
+
+int mv_pp2x_cls_c2_dscp_set(struct mv_pp2x_cls_c2_entry *c2,
+			    int cmd, int dscp, int from)
+{
+	if (!c2 || cmd > MVPP2_ACTION_TYPE_UPDT_LOCK ||
+			dscp >= MVPP2_QOS_TBL_LINE_NUM_DSCP)
+		return -EINVAL;
+
+	/*set command*/
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_DSCP_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_DSCP_OFF);
+
+	/*set modify DSCP value*/
+	c2->sram.regs.qos_attr &= ~MVPP2_CLS2_ACT_QOS_ATTR_DSCP_MASK;
+	c2->sram.regs.qos_attr |= ((dscp <<
+		MVPP2_CLS2_ACT_QOS_ATTR_DSCP_OFF) &
+		MVPP2_CLS2_ACT_QOS_ATTR_DSCP_MASK);
+
+	if (from == 1)
+		c2->sram.regs.action_tbl |= (1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_OFF);
+	else
+		c2->sram.regs.action_tbl &= ~(1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_dscp_set);
+
+int mv_pp2x_cls_c2_queue_low_set(struct mv_pp2x_cls_c2_entry *c2,
+				 int cmd, int queue, int from)
+{
+	if (!c2 || cmd > MVPP2_ACTION_TYPE_UPDT_LOCK ||
+			queue >= (1 << MVPP2_CLS2_ACT_QOS_ATTR_QL_BITS))
+		return -EINVAL;
+
+	/*set command*/
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_QL_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_QL_OFF);
+
+	/*set modify Low queue value*/
+	c2->sram.regs.qos_attr &= ~MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK;
+	c2->sram.regs.qos_attr |= ((queue <<
+			MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF) &
+			MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK);
+
+	if (from == 1)
+		c2->sram.regs.action_tbl |= (1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_OFF);
+	else
+		c2->sram.regs.action_tbl &= ~(1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_queue_low_set);
+
+int mv_pp2x_cls_c2_queue_high_set(struct mv_pp2x_cls_c2_entry *c2,
+				  int cmd, int queue, int from)
+{
+	if (!c2 || cmd > MVPP2_ACTION_TYPE_UPDT_LOCK ||
+			queue >= (1 << MVPP2_CLS2_ACT_QOS_ATTR_QH_BITS))
+		return -EINVAL;
+
+	/*set command*/
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_QH_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_QH_OFF);
+
+	/*set modify High queue value*/
+	c2->sram.regs.qos_attr &= ~MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK;
+	c2->sram.regs.qos_attr |= ((queue <<
+			MVPP2_CLS2_ACT_QOS_ATTR_QH_OFF) &
+			MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK);
+
+	if (from == 1)
+		c2->sram.regs.action_tbl |= (1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_OFF);
+	else
+		c2->sram.regs.action_tbl &= ~(1 <<
+			MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_queue_high_set);
+
+int mv_pp2x_cls_c2_forward_set(struct mv_pp2x_cls_c2_entry *c2, int cmd)
+{
+	if (!c2 || cmd > MVPP2_ACTION_TYPE_UPDT_LOCK)
+		return -EINVAL;
+
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_FRWD_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_FRWD_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_forward_set);
+
+int mv_pp2x_cls_c2_rss_set(struct mv_pp2x_cls_c2_entry *c2, int cmd, int rss_en)
+{
+	if (!c2 || cmd > MVPP2_ACTION_TYPE_UPDT_LOCK || rss_en >=
+			(1 << MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_BITS))
+		return -EINVAL;
+
+	c2->sram.regs.actions &= ~MVPP2_CLS2_ACT_RSS_MASK;
+	c2->sram.regs.actions |= (cmd << MVPP2_CLS2_ACT_RSS_OFF);
+
+	c2->sram.regs.rss_attr &= ~MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_MASK;
+	c2->sram.regs.rss_attr |= (rss_en <<
+			MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_rss_set);
+
+int mv_pp2x_cls_c2_flow_id_en(struct mv_pp2x_cls_c2_entry *c2, int flowid_en)
+{
+	if (!c2)
+		return -EINVAL;
+
+	/*set Flow ID enable or disable*/
+	if (flowid_en)
+		c2->sram.regs.actions |= (1 << MVPP2_CLS2_ACT_FLD_EN_OFF);
+	else
+		c2->sram.regs.actions &= ~(1 << MVPP2_CLS2_ACT_FLD_EN_OFF);
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_flow_id_en);
+
+int mv_pp2x_cls_c2_tcam_byte_set(struct mv_pp2x_cls_c2_entry *c2,
+				 unsigned int offs, unsigned char byte,
+				 unsigned char enable)
+{
+	if (!c2 || offs >= MVPP2_CLS_C2_TCAM_DATA_BYTES)
+		return -EINVAL;
+
+	c2->tcam.bytes[TCAM_DATA_BYTE(offs)] = byte;
+	c2->tcam.bytes[TCAM_DATA_MASK(offs)] = enable;
+
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_tcam_byte_set);
+
+int mv_pp2x_cls_c2_qos_queue_set(struct mv_pp2x_cls_c2_qos_entry *qos,
+				 u8 queue)
+{
+	if (!qos || queue >= (1 << MVPP2_CLS2_QOS_TBL_QUEUENUM_BITS))
+		return -EINVAL;
+
+	qos->data &= ~MVPP2_CLS2_QOS_TBL_QUEUENUM_MASK;
+	qos->data |= (((u32)queue) << MVPP2_CLS2_QOS_TBL_QUEUENUM_OFF);
+	return 0;
+}
+EXPORT_SYMBOL(mv_pp2x_cls_c2_qos_queue_set);
+
+static int mv_pp2x_c2_tcam_set(struct mv_pp2x_hw *hw,
+			       struct mv_pp2x_c2_add_entry *c2_add_entry,
+			       unsigned int c2_hw_idx)
+{
+	int ret_code;
+	struct mv_pp2x_cls_c2_entry c2_entry;
+	int hek_offs;
+	unsigned char hek_byte[MVPP2_CLS_C2_HEK_OFF_MAX],
+		      hek_byte_mask[MVPP2_CLS_C2_HEK_OFF_MAX];
+
+	if (!c2_add_entry || !hw || c2_hw_idx >= MVPP2_CLS_C2_TCAM_SIZE)
+		return -EINVAL;
+
+	/* Clear C2 sw data */
+	memset(&c2_entry, 0, sizeof(struct mv_pp2x_cls_c2_entry));
+
+	/* Set QOS table, selection and ID */
+	ret_code = mv_pp2x_cls_c2_qos_tbl_set(&c2_entry,
+					c2_add_entry->qos_info.qos_tbl_index,
+					c2_add_entry->qos_info.qos_tbl_type);
+	if (ret_code)
+		return ret_code;
+
+	/* Set color, cmd and source */
+	ret_code = mv_pp2x_cls_c2_color_set(&c2_entry,
+					    c2_add_entry->action.color_act,
+					    c2_add_entry->qos_info.color_src);
+	if (ret_code)
+		return ret_code;
+
+	/* Set priority(pbit), cmd, value(not from qos table) and source */
+	ret_code = mv_pp2x_cls_c2_prio_set(&c2_entry,
+					   c2_add_entry->action.pri_act,
+					   c2_add_entry->qos_value.pri,
+					   c2_add_entry->qos_info.pri_dscp_src);
+	if (ret_code)
+		return ret_code;
+
+	/* Set DSCP, cmd, value(not from qos table) and source */
+	ret_code = mv_pp2x_cls_c2_dscp_set(&c2_entry,
+					   c2_add_entry->action.dscp_act,
+					   c2_add_entry->qos_value.dscp,
+					   c2_add_entry->qos_info.pri_dscp_src);
+	if (ret_code)
+		return ret_code;
+
+	/* Set queue low, cmd, value, and source */
+	ret_code = mv_pp2x_cls_c2_queue_low_set(&c2_entry,
+						c2_add_entry->action.q_low_act,
+						c2_add_entry->qos_value.q_low,
+					     c2_add_entry->qos_info.q_low_src);
+	if (ret_code)
+		return ret_code;
+
+	/* Set queue high, cmd, value and source */
+	ret_code = mv_pp2x_cls_c2_queue_high_set(&c2_entry,
+					c2_add_entry->action.q_high_act,
+					c2_add_entry->qos_value.q_high,
+					c2_add_entry->qos_info.q_high_src);
+	if (ret_code)
+		return ret_code;
+
+	/* Set forward */
+	ret_code = mv_pp2x_cls_c2_forward_set(&c2_entry,
+					      c2_add_entry->action.frwd_act);
+	if (ret_code)
+		return ret_code;
+
+	/* Set RSS */
+	ret_code = mv_pp2x_cls_c2_rss_set(&c2_entry,
+					  c2_add_entry->action.rss_act,
+					  c2_add_entry->rss_en);
+	if (ret_code)
+		return ret_code;
+
+	/* Set flowID(not for multicast) */
+	ret_code = mv_pp2x_cls_c2_flow_id_en(&c2_entry,
+					     c2_add_entry->action.flowid_act);
+	if (ret_code)
+		return ret_code;
+
+	/* Set C2 HEK */
+	memset(hek_byte, 0, MVPP2_CLS_C2_HEK_OFF_MAX);
+	memset(hek_byte_mask, 0, MVPP2_CLS_C2_HEK_OFF_MAX);
+
+	/* HEK offs 8, lookup type, port type */
+	hek_byte[MVPP2_CLS_C2_HEK_OFF_LKP_PORT_TYPE] =
+		(c2_add_entry->port.port_type <<
+			MVPP2_CLS_C2_HEK_PORT_TYPE_OFFS) |
+		(c2_add_entry->lkp_type <<
+			MVPP2_CLS_C2_HEK_LKP_TYPE_OFFS);
+	hek_byte_mask[MVPP2_CLS_C2_HEK_OFF_LKP_PORT_TYPE] =
+			MVPP2_CLS_C2_HEK_PORT_TYPE_MASK |
+			((c2_add_entry->lkp_type_mask <<
+				MVPP2_CLS_C2_HEK_LKP_TYPE_OFFS) &
+				MVPP2_CLS_C2_HEK_LKP_TYPE_MASK);
+	/* HEK offs 9, port ID */
+	hek_byte[MVPP2_CLS_C2_HEK_OFF_PORT_ID] =
+		c2_add_entry->port.port_value;
+	hek_byte_mask[MVPP2_CLS_C2_HEK_OFF_PORT_ID] =
+		c2_add_entry->port.port_mask;
+
+	for (hek_offs = MVPP2_CLS_C2_HEK_OFF_PORT_ID; hek_offs >=
+			MVPP2_CLS_C2_HEK_OFF_BYTE0; hek_offs--) {
+		ret_code = mv_pp2x_cls_c2_tcam_byte_set(&c2_entry,
+							hek_offs,
+							hek_byte[hek_offs],
+						hek_byte_mask[hek_offs]);
+		if (ret_code)
+			return ret_code;
+	}
+
+	/* Write C2 entry data to HW */
+	ret_code = mv_pp2x_cls_c2_hw_write(hw, c2_hw_idx, &c2_entry);
+	if (ret_code)
+		return ret_code;
+
+	return 0;
+}
+
+/* C2 TCAM init */
+int mv_pp2x_c2_init(struct platform_device *pdev, struct mv_pp2x_hw *hw)
+{
+	int i;
+
+	/* Invalid all C2 and QoS entries */
+	mv_pp2x_cls_c2_hw_inv_all(hw);
+	PALAD(MVPP2_PRINT_LINE());
+
+	mv_pp2x_cls_c2_qos_hw_clear_all(hw);
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Set CLSC2_TCAM_CTRL to enable C2, or C2 does not work */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_CTRL_REG,
+		      MVPP2_CLS2_TCAM_CTRL_EN_MASK);
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Allocate mem for c2 shadow */
+	hw->c2_shadow = devm_kcalloc(&pdev->dev, 1,
+				      sizeof(struct mv_pp2x_c2_shadow),
+				      GFP_KERNEL);
+	if (!hw->c2_shadow)
+		return -ENOMEM;
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Init the rule idx to invalid value */
+	for (i = 0; i < 8; i++) {
+		hw->c2_shadow->rule_idx_info[i].vlan_pri_idx =
+			MVPP2_CLS_C2_TCAM_SIZE;
+		hw->c2_shadow->rule_idx_info[i].dscp_pri_idx =
+			MVPP2_CLS_C2_TCAM_SIZE;
+		hw->c2_shadow->rule_idx_info[i].default_rule_idx =
+			MVPP2_CLS_C2_TCAM_SIZE;
+	}
+	hw->c2_shadow->c2_tcam_free_start = 0;
+	PALAD(MVPP2_PRINT_LINE());
+
+	return 0;
+}
+
+static int mv_pp2x_c2_rule_add(struct mv_pp2x_port *port,
+			       struct mv_pp2x_c2_add_entry *c2_add_entry)
+{
+	int ret, lkp_type, c2_index = 0;
+	bool first_free_update = false;
+	struct mv_pp2x_c2_rule_idx *rule_idx;
+
+	rule_idx = &(port->priv->hw.c2_shadow->rule_idx_info[port->id]);
+
+	if (!port || !c2_add_entry)
+		return -EINVAL;
+
+	lkp_type = c2_add_entry->lkp_type;
+	/* Write rule in C2 TCAM */
+	if (lkp_type == MVPP2_CLS_LKP_VLAN_PRI) {
+		if (rule_idx->vlan_pri_idx == MVPP2_CLS_C2_TCAM_SIZE) {
+			/* If the C2 rule is new, apply a free c2 rule index */
+			c2_index =
+				port->priv->hw.c2_shadow->c2_tcam_free_start;
+			first_free_update = true;
+		} else {
+			/* If the C2 rule is exist one,
+			* take the C2 index from shadow
+			*/
+			c2_index = rule_idx->vlan_pri_idx;
+			first_free_update = false;
+		}
+	} else if (lkp_type == MVPP2_CLS_LKP_DSCP_PRI) {
+		if (rule_idx->dscp_pri_idx == MVPP2_CLS_C2_TCAM_SIZE) {
+			c2_index =
+				port->priv->hw.c2_shadow->c2_tcam_free_start;
+			first_free_update = true;
+		} else {
+			c2_index = rule_idx->dscp_pri_idx;
+			first_free_update = false;
+		}
+	} else if (lkp_type == MVPP2_CLS_LKP_DEFAULT) {
+		if (rule_idx->default_rule_idx == MVPP2_CLS_C2_TCAM_SIZE) {
+			c2_index =
+				port->priv->hw.c2_shadow->c2_tcam_free_start;
+			first_free_update = true;
+		} else {
+			c2_index = rule_idx->default_rule_idx;
+			first_free_update = false;
+		}
+	} else {
+		return -EINVAL;
+	}
+
+	/* Write C2 TCAM HW */
+	ret = mv_pp2x_c2_tcam_set(&port->priv->hw, c2_add_entry, c2_index);
+	if (ret)
+		return ret;
+
+	/* Update first free rule */
+	if (first_free_update)
+		port->priv->hw.c2_shadow->c2_tcam_free_start++;
+
+	/* Update shadow */
+	if (lkp_type == MVPP2_CLS_LKP_VLAN_PRI)
+		rule_idx->vlan_pri_idx = c2_index;
+	else if (lkp_type == MVPP2_CLS_LKP_DSCP_PRI)
+		rule_idx->dscp_pri_idx = c2_index;
+	else if (lkp_type == MVPP2_CLS_LKP_DEFAULT)
+		rule_idx->default_rule_idx = c2_index;
+
+	return 0;
+}
+
+/* Fill the qos table with queue */
+static void mv_pp2x_cls_c2_qos_tbl_fill(struct mv_pp2x_port *port,
+					u8 tbl_sel, u8 start_queue)
+{
+	struct mv_pp2x_cls_c2_qos_entry qos_entry;
+	u32 pri, line_num;
+	u8 cos_value, cos_queue, queue;
+
+
+	if (tbl_sel == MVPP2_QOS_TBL_SEL_PRI)
+		line_num = MVPP2_QOS_TBL_LINE_NUM_PRI;
+	else
+		line_num = MVPP2_QOS_TBL_LINE_NUM_DSCP;
+
+	memset(&qos_entry, 0, sizeof(struct mv_pp2x_cls_c2_qos_entry));
+	qos_entry.tbl_id = port->id;
+	qos_entry.tbl_sel = tbl_sel;
+
+	/* Fill the QoS dscp/pbit table */
+	for (pri = 0; pri < line_num; pri++) {
+		/* cos_value equal to dscp/8 or pbit value */
+		cos_value = ((tbl_sel == MVPP2_QOS_TBL_SEL_PRI) ?
+			pri : (pri/8));
+		/* each nibble of pri_map stands for a cos-value,
+		 * nibble value is the queue
+		 */
+		cos_queue = mv_pp2x_cosval_queue_map(port, cos_value);
+		qos_entry.tbl_line = pri;
+		/* map cos queue to physical queue */
+		/* Physical queue contains 2 parts: port ID and CPU ID,
+		 * CPU ID will be used in RSS
+		 */
+		queue = start_queue + cos_queue;
+		mv_pp2x_cls_c2_qos_queue_set(&qos_entry, queue);
+		PALAD(MVPP2_PRINT_LINE());
+		mv_pp2x_cls_c2_qos_hw_write(&port->priv->hw, &qos_entry);
+	}
+}
+
+/* C2 rule set */
+int mv_pp2x_cls_c2_rule_set(struct mv_pp2x_port *port, u8 start_queue)
+{
+	struct mv_pp2x_c2_add_entry c2_init_entry;
+	int ret;
+	u8 cos_value, cos_queue, queue, lkp_type;
+
+	/* QoS of pbit rule */
+	for (lkp_type = MVPP2_CLS_LKP_VLAN_PRI; lkp_type <=
+			MVPP2_CLS_LKP_DEFAULT; lkp_type++) {
+		memset(&c2_init_entry, 0, sizeof(struct mv_pp2x_c2_add_entry));
+
+		/* Port info */
+		c2_init_entry.port.port_type = MVPP2_SRC_PORT_TYPE_PHY;
+		c2_init_entry.port.port_value = (1 << port->id);
+		c2_init_entry.port.port_mask = 0xff;
+		/* Lookup type */
+		c2_init_entry.lkp_type = lkp_type;
+		c2_init_entry.lkp_type_mask = 0x3F;
+		/* Action info */
+		c2_init_entry.action.color_act =
+				MVPP2_COLOR_ACTION_TYPE_NO_UPDT_LOCK;
+		c2_init_entry.action.pri_act =
+				MVPP2_ACTION_TYPE_NO_UPDT_LOCK;
+		c2_init_entry.action.dscp_act =
+				MVPP2_ACTION_TYPE_NO_UPDT_LOCK;
+		c2_init_entry.action.q_low_act =
+				MVPP2_ACTION_TYPE_UPDT_LOCK;
+		c2_init_entry.action.q_high_act =
+				MVPP2_ACTION_TYPE_UPDT_LOCK;
+		if (port->priv->pp2_version == PPV22)
+			c2_init_entry.action.rss_act =
+				MVPP2_ACTION_TYPE_UPDT_LOCK;
+		/* To CPU */
+		c2_init_entry.action.frwd_act =
+				MVPP2_FRWD_ACTION_TYPE_SWF_LOCK;
+
+		/* QoS info */
+		if (lkp_type != MVPP2_CLS_LKP_DEFAULT) {
+			/* QoS info from C2 QoS table */
+			/* Set the QoS table index equal to port ID */
+			c2_init_entry.qos_info.qos_tbl_index = port->id;
+			c2_init_entry.qos_info.q_low_src =
+					MVPP2_QOS_SRC_DSCP_PBIT_TBL;
+			c2_init_entry.qos_info.q_high_src =
+					MVPP2_QOS_SRC_DSCP_PBIT_TBL;
+			if (lkp_type == MVPP2_CLS_LKP_VLAN_PRI) {
+				c2_init_entry.qos_info.qos_tbl_type =
+					MVPP2_QOS_TBL_SEL_PRI;
+				mv_pp2x_cls_c2_qos_tbl_fill(port,
+							MVPP2_QOS_TBL_SEL_PRI,
+							start_queue);
+			} else if (lkp_type == MVPP2_CLS_LKP_DSCP_PRI) {
+				c2_init_entry.qos_info.qos_tbl_type =
+					MVPP2_QOS_TBL_SEL_DSCP;
+				mv_pp2x_cls_c2_qos_tbl_fill(port,
+							MVPP2_QOS_TBL_SEL_DSCP,
+							start_queue);
+			}
+		} else {
+			/* QoS info from C2 action table */
+			c2_init_entry.qos_info.q_low_src =
+					MVPP2_QOS_SRC_ACTION_TBL;
+			c2_init_entry.qos_info.q_high_src =
+					MVPP2_QOS_SRC_ACTION_TBL;
+			cos_value = port->priv->pp2_cfg.cos_cfg.default_cos;
+			cos_queue = mv_pp2x_cosval_queue_map(port, cos_value);
+			/* map to physical queue */
+			/* Physical queue contains 2 parts: port ID and CPU ID,
+			 * CPU ID will be used in RSS
+			 */
+			queue = start_queue + cos_queue;
+			c2_init_entry.qos_value.q_low = ((u16)queue) &
+				((1 << MVPP2_CLS2_ACT_QOS_ATTR_QL_BITS) - 1);
+			c2_init_entry.qos_value.q_high = ((u16)queue) >>
+					MVPP2_CLS2_ACT_QOS_ATTR_QL_BITS;
+		}
+		/* RSS En in PP22 */
+		c2_init_entry.rss_en = port->priv->pp2_cfg.rss_cfg.rss_en;
+
+		/* Add rule to C2 TCAM */
+		PALAD(MVPP2_PRINT_LINE());
+		ret = mv_pp2x_c2_rule_add(port, &c2_init_entry);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+/* The function get the queue in the C2 rule with input index */
+u8 mv_pp2x_cls_c2_rule_queue_get(struct mv_pp2x_hw *hw, u32 rule_idx)
+{
+	u32 regVal;
+	u8 queue;
+
+	/* Write index reg */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_IDX_REG, rule_idx);
+
+	/* Read Reg CLSC2_ATTR0 */
+	regVal = mv_pp2x_read(hw, MVPP2_CLS2_ACT_QOS_ATTR_REG);
+	queue = (regVal & (MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK |
+			MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK)) >>
+			MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF;
+	return queue;
+}
+
+/* The function set the qos queue in one C2 rule */
+void mv_pp2x_cls_c2_rule_queue_set(struct mv_pp2x_hw *hw, u32 rule_idx,
+				   u8 queue)
+{
+	u32 regVal;
+
+	/* Write index reg */
+	mv_pp2x_write(hw, MVPP2_CLS2_TCAM_IDX_REG, rule_idx);
+
+	/* Read Reg CLSC2_ATTR0 */
+	regVal = mv_pp2x_read(hw, MVPP2_CLS2_ACT_QOS_ATTR_REG);
+	/* Update Value */
+	regVal &= (~(MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK |
+			MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK));
+	regVal |= (((u32)queue) << MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF);
+
+	/* Write Reg CLSC2_ATTR0 */
+	mv_pp2x_write(hw, MVPP2_CLS2_ACT_QOS_ATTR_REG, regVal);
+}
+
+/* The function get the queue in the pbit table entry */
+u8 mv_pp2x_cls_c2_pbit_tbl_queue_get(struct mv_pp2x_hw *hw, u8 tbl_id,
+				     u8 tbl_line)
+{
+	u8 queue;
+	u32 regVal = 0;
+
+	/* write index reg */
+	regVal |= (tbl_line << MVPP2_CLS2_DSCP_PRI_INDEX_LINE_OFF);
+	regVal |= (MVPP2_QOS_TBL_SEL_PRI <<
+			MVPP2_CLS2_DSCP_PRI_INDEX_SEL_OFF);
+	regVal |= (tbl_id << MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_OFF);
+	mv_pp2x_write(hw, MVPP2_CLS2_DSCP_PRI_INDEX_REG, regVal);
+	/* Read Reg CLSC2_DSCP_PRI */
+	regVal = mv_pp2x_read(hw, MVPP2_CLS2_QOS_TBL_REG);
+	queue = (regVal &  MVPP2_CLS2_QOS_TBL_QUEUENUM_MASK) >>
+			MVPP2_CLS2_QOS_TBL_QUEUENUM_OFF;
+
+	return queue;
+}
+
+/* The function set the queue in the pbit table entry */
+void mv_pp2x_cls_c2_pbit_tbl_queue_set(struct mv_pp2x_hw *hw,
+				       u8 tbl_id, u8 tbl_line, u8 queue)
+{
+	u32 regVal = 0;
+
+	/* write index reg */
+	regVal |= (tbl_line << MVPP2_CLS2_DSCP_PRI_INDEX_LINE_OFF);
+	regVal |=
+		(MVPP2_QOS_TBL_SEL_PRI << MVPP2_CLS2_DSCP_PRI_INDEX_SEL_OFF);
+	regVal |= (tbl_id << MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_OFF);
+	mv_pp2x_write(hw, MVPP2_CLS2_DSCP_PRI_INDEX_REG, regVal);
+
+	/* Read Reg CLSC2_DSCP_PRI */
+	regVal = mv_pp2x_read(hw, MVPP2_CLS2_QOS_TBL_REG);
+	regVal &= (~MVPP2_CLS2_QOS_TBL_QUEUENUM_MASK);
+	regVal |= (((u32)queue) << MVPP2_CLS2_QOS_TBL_QUEUENUM_OFF);
+
+	/* Write Reg CLSC2_DSCP_PRI */
+	mv_pp2x_write(hw, MVPP2_CLS2_QOS_TBL_REG, regVal);
+}
+
+/* RSS */
+/* The function will set rss table entry */
+int mv_pp22_rss_tbl_entry_set(struct mv_pp2x_hw *hw,
+			      struct mv_pp22_rss_entry *rss)
+{
+	unsigned int regVal = 0;
+
+	if (!rss || rss->sel > MVPP22_RSS_ACCESS_TBL)
+		return -EINVAL;
+
+	if (rss->sel == MVPP22_RSS_ACCESS_POINTER) {
+		if (rss->u.pointer.rss_tbl_ptr >= MVPP22_RSS_TBL_NUM)
+			return -EINVAL;
+		/* Write index */
+		regVal |= rss->u.pointer.rxq_idx <<
+				MVPP22_RSS_IDX_RXQ_NUM_OFF;
+		mv_pp2x_write(hw, MVPP22_RSS_IDX_REG, regVal);
+		/* Write entry */
+		regVal &= (~MVPP22_RSS_RXQ2RSS_TBL_POINT_MASK);
+		regVal |= rss->u.pointer.rss_tbl_ptr <<
+				MVPP22_RSS_RXQ2RSS_TBL_POINT_OFF;
+		mv_pp2x_write(hw, MVPP22_RSS_RXQ2RSS_TBL_REG, regVal);
+	} else if (rss->sel == MVPP22_RSS_ACCESS_TBL) {
+		if (rss->u.entry.tbl_id >= MVPP22_RSS_TBL_NUM ||
+		    rss->u.entry.tbl_line >= MVPP22_RSS_TBL_LINE_NUM ||
+		    rss->u.entry.width >= MVPP22_RSS_WIDTH_MAX)
+			return -EINVAL;
+		/* Write index */
+		regVal |= (rss->u.entry.tbl_line <<
+				MVPP22_RSS_IDX_ENTRY_NUM_OFF |
+			   rss->u.entry.tbl_id << MVPP22_RSS_IDX_TBL_NUM_OFF);
+		mv_pp2x_write(hw, MVPP22_RSS_IDX_REG, regVal);
+		/* Write entry */
+		regVal &= (~MVPP22_RSS_TBL_ENTRY_MASK);
+		regVal |= (rss->u.entry.rxq << MVPP22_RSS_TBL_ENTRY_OFF);
+		mv_pp2x_write(hw, MVPP22_RSS_TBL_ENTRY_REG, regVal);
+		regVal &= (~MVPP22_RSS_WIDTH_MASK);
+		regVal |= (rss->u.entry.width << MVPP22_RSS_WIDTH_OFF);
+		mv_pp2x_write(hw, MVPP22_RSS_WIDTH_REG, regVal);
+	}
+	return 0;
+}
+
+/* The function will get rss table entry */
+int mv_pp22_rss_tbl_entry_get(struct mv_pp2x_hw *hw,
+			      struct mv_pp22_rss_entry *rss)
+{
+	unsigned int regVal = 0;
+
+	if (!rss || rss->sel > MVPP22_RSS_ACCESS_TBL)
+		return -EINVAL;
+
+	if (rss->sel == MVPP22_RSS_ACCESS_POINTER) {
+		/* Read entry */
+		rss->u.pointer.rss_tbl_ptr =
+			mv_pp2x_read(hw, MVPP22_RSS_RXQ2RSS_TBL_REG) &
+				     MVPP22_RSS_RXQ2RSS_TBL_POINT_MASK;
+	} else if (rss->sel == MVPP22_RSS_ACCESS_TBL) {
+		if (rss->u.entry.tbl_id >= MVPP22_RSS_TBL_NUM ||
+		    rss->u.entry.tbl_line >= MVPP22_RSS_TBL_LINE_NUM)
+			return -EINVAL;
+		/* Read index */
+		regVal |= (rss->u.entry.tbl_line <<
+				MVPP22_RSS_IDX_ENTRY_NUM_OFF |
+			   rss->u.entry.tbl_id <<
+				MVPP22_RSS_IDX_TBL_NUM_OFF);
+		mv_pp2x_write(hw, MVPP22_RSS_IDX_REG, regVal);
+		/* Read entry */
+		rss->u.entry.rxq = mv_pp2x_read(hw,
+						MVPP22_RSS_TBL_ENTRY_REG) &
+						MVPP22_RSS_TBL_ENTRY_MASK;
+		rss->u.entry.width = mv_pp2x_read(hw,
+						  MVPP22_RSS_WIDTH_REG) &
+						  MVPP22_RSS_WIDTH_MASK;
+	}
+	return 0;
+}
+
+int mv_pp22_rss_hw_dump(struct mv_pp2x_hw *hw)
+{
+	int tbl_id, tbl_line;
+
+	struct mv_pp22_rss_entry rss_entry;
+
+	memset(&rss_entry, 0, sizeof(struct mv_pp22_rss_entry));
+
+	rss_entry.sel = MVPP22_RSS_ACCESS_TBL;
+
+	for (tbl_id = 0; tbl_id < MVPP22_RSS_TBL_NUM; tbl_id++) {
+		DBG_MSG("\n-------- RSS TABLE %d-----------\n", tbl_id);
+		DBG_MSG("HASH	QUEUE	WIDTH\n");
+
+		for (tbl_line = 0; tbl_line < MVPP22_RSS_TBL_LINE_NUM;
+			tbl_line++) {
+			rss_entry.u.entry.tbl_id = tbl_id;
+			rss_entry.u.entry.tbl_line = tbl_line;
+			mv_pp22_rss_tbl_entry_get(hw, &rss_entry);
+			DBG_MSG("0x%2.2x\t", rss_entry.u.entry.tbl_line);
+			DBG_MSG("0x%2.2x\t", rss_entry.u.entry.rxq);
+			DBG_MSG("0x%2.2x", rss_entry.u.entry.width);
+			DBG_MSG("\n");
+		}
+	}
+	return MV_OK;
+}
+EXPORT_SYMBOL(mv_pp22_rss_hw_dump);
+
+/* The function allocate a rss table for each phisical rxq,
+ * they have same cos priority
+ */
+int mv_pp22_rss_rxq_set(struct mv_pp2x_port *port, u32 cos_width)
+{
+	int rxq;
+	struct mv_pp22_rss_entry rss_entry;
+	int cos_mask = ((1 << cos_width) - 1);
+
+	if (port->priv->pp2_version != PPV22)
+		return 0;
+
+	memset(&rss_entry, 0, sizeof(struct mv_pp22_rss_entry));
+
+	rss_entry.sel = MVPP22_RSS_ACCESS_POINTER;
+
+	for (rxq = 0; rxq < port->num_rx_queues; rxq++) {
+		PALAD(MVPP2_PRINT_LINE());
+		rss_entry.u.pointer.rxq_idx = port->rxqs[rxq]->id;
+		rss_entry.u.pointer.rss_tbl_ptr =
+				port->rxqs[rxq]->id & cos_mask;
+		if (mv_pp22_rss_tbl_entry_set(&port->priv->hw, &rss_entry))
+			return -1;
+	}
+
+	return 0;
+}
+
+void mv_pp22_rss_c2_enable(struct mv_pp2x_port *port, bool en)
+{
+	int lkp_type, regVal;
+	int c2_index[MVPP2_CLS_LKP_MAX];
+	struct mv_pp2x_c2_rule_idx *rule_idx;
+
+	rule_idx = &(port->priv->hw.c2_shadow->rule_idx_info[port->id]);
+
+	/* Get the C2 index from shadow */
+	c2_index[MVPP2_CLS_LKP_VLAN_PRI] = rule_idx->vlan_pri_idx;
+	c2_index[MVPP2_CLS_LKP_DSCP_PRI] = rule_idx->dscp_pri_idx;
+	c2_index[MVPP2_CLS_LKP_DEFAULT] = rule_idx->default_rule_idx;
+
+	for (lkp_type = 0; lkp_type < MVPP2_CLS_LKP_MAX; lkp_type++) {
+		PALAD(MVPP2_PRINT_LINE());
+		/* For lookup type of MVPP2_CLS_LKP_HASH,
+		 * there is no corresponding C2 rule, so skip it
+		 */
+		if (lkp_type == MVPP2_CLS_LKP_HASH)
+			continue;
+		/* write index reg */
+		mv_pp2x_write(&port->priv->hw, MVPP2_CLS2_TCAM_IDX_REG,
+			      c2_index[lkp_type]);
+		/* Update rss_attr in reg CLSC2_ATTR2 */
+		regVal = mv_pp2x_read(&port->priv->hw,
+				      MVPP2_CLS2_ACT_DUP_ATTR_REG);
+		if (en)
+			regVal |= MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_MASK;
+		else
+			regVal &= (~MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_MASK);
+
+		mv_pp2x_write(&port->priv->hw,
+			      MVPP2_CLS2_ACT_DUP_ATTR_REG, regVal);
+	}
+}
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
new file mode 100644
index 0000000..3b4c648
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw.h
@@ -0,0 +1,763 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifndef _MVPP2_HW_H_
+#define _MVPP2_HW_H_
+
+#include <linux/kernel.h>
+#include <linux/skbuff.h>
+#include <linux/printk.h>
+
+#include <linux/platform_device.h>
+
+#if 1
+void mv_pp2x_write(struct mv_pp2x_hw *hw, u32 offset, u32 data);
+u32 mv_pp2x_read(struct mv_pp2x_hw *hw, u32 offset);
+
+#else
+static inline void mv_pp2x_write(struct mv_pp2x_hw *hw, u32 offset, u32 data)
+{
+#if 1
+	if (smp_processor_id() != 0)
+		pr_emerg_once("Received mv_pp2x_write(%d) from CPU1 !!\n",
+			offset);
+#endif
+	pr_info("mv_pp2x_write(%p)\n", hw->cpu_base[smp_processor_id()] +
+		offset);
+	writel(data, hw->cpu_base[smp_processor_id()] + offset);
+}
+
+static inline u32 mv_pp2x_read(struct mv_pp2x_hw *hw, u32 offset)
+{
+#if 1
+	if (smp_processor_id() != 0)
+		pr_emerg_once("Received mv_pp2x_read(%d) from CPU1 !!\n",
+			offset);
+#endif
+	pr_info("mv_pp2x_read(%p)\n", hw->cpu_base[smp_processor_id()] +
+		offset);
+
+	return readl(hw->cpu_base[smp_processor_id()] + offset);
+}
+#endif
+static inline void mv_pp22_thread_write(struct mv_pp2x_hw *hw, u32 sw_thread,
+					     u32 offset, u32 data)
+{
+	writel(data, hw->base + sw_thread*MVPP2_ADDR_SPACE_SIZE + offset);
+}
+
+static inline u32 mv_pp22_thread_read(struct mv_pp2x_hw *hw, u32 sw_thread,
+					    u32 offset)
+{
+	return readl(hw->base + sw_thread*MVPP2_ADDR_SPACE_SIZE + offset);
+}
+
+static inline void mv_pp21_isr_rx_group_write(struct mv_pp2x_hw *hw, int port,
+						    int num_rx_queues)
+{
+	mv_pp2x_write(hw, MVPP21_ISR_RXQ_GROUP_REG(port), num_rx_queues);
+}
+
+static inline void mv_pp22_isr_rx_group_write(struct mv_pp2x_hw *hw, int port,
+						    int sub_group,
+						    int start_queue,
+						    int num_rx_queues)
+{
+	int val;
+
+	val = (port << MVPP22_ISR_RXQ_GROUP_INDEX_GROUP_OFFSET) | sub_group;
+	mv_pp2x_write(hw, MVPP22_ISR_RXQ_GROUP_INDEX_REG, val);
+	val = (num_rx_queues << MVPP22_ISR_RXQ_SUB_GROUP_SIZE_OFFSET) |
+		start_queue;
+	mv_pp2x_write(hw, MVPP22_ISR_RXQ_SUB_GROUP_CONFIG_REG, val);
+
+}
+
+/* Get number of physical egress port */
+static inline int mv_pp2x_egress_port(struct mv_pp2x_port *port)
+{
+	return MVPP2_MAX_TCONT + port->id;
+}
+
+/* Get number of physical TXQ */
+static inline int mv_pp2x_txq_phys(int port, int txq)
+{
+	return (MVPP2_MAX_TCONT + port) * MVPP2_MAX_TXQ + txq;
+}
+
+/* Rx descriptors helper methods */
+
+/* Get number of Rx descriptors occupied by received packets */
+static inline int mv_pp2x_rxq_received(struct mv_pp2x_port *port, int rxq_id)
+{
+	u32 val = mv_pp2x_read(&port->priv->hw, MVPP2_RXQ_STATUS_REG(rxq_id));
+
+	return val & MVPP2_RXQ_OCCUPIED_MASK;
+}
+
+/* Get number of Rx descriptors occupied by received packets */
+static inline int mv_pp2x_rxq_free(struct mv_pp2x_port *port, int rxq_id)
+{
+	u32 val = mv_pp2x_read(&port->priv->hw, MVPP2_RXQ_STATUS_REG(rxq_id));
+
+	return (val & (MVPP2_RXQ_NON_OCCUPIED_MASK >>
+		MVPP2_RXQ_NON_OCCUPIED_OFFSET));
+}
+
+/* Update Rx queue status with the number of occupied and available
+ * Rx descriptor slots.
+ */
+static inline void mv_pp2x_rxq_status_update(struct mv_pp2x_port *port,
+						    int rxq_id,
+						    int used_count,
+						    int free_count)
+{
+	/* Decrement the number of used descriptors and increment count
+	 * increment the number of free descriptors.
+	 */
+	u32 val = used_count | (free_count << MVPP2_RXQ_NUM_NEW_OFFSET);
+
+	mv_pp2x_write(&port->priv->hw,
+		      MVPP2_RXQ_STATUS_UPDATE_REG(rxq_id), val);
+}
+
+/* Get pointer to next RX descriptor to be processed by SW */
+static inline struct mv_pp2x_rx_desc *
+mv_pp2x_rxq_next_desc_get(struct mv_pp2x_rx_queue *rxq)
+{
+	int rx_desc = rxq->next_desc_to_proc;
+
+	rxq->next_desc_to_proc = MVPP2_QUEUE_NEXT_DESC(rxq, rx_desc);
+	prefetch(rxq->first_desc + rxq->next_desc_to_proc);
+	return (rxq->first_desc + rx_desc);
+}
+
+/* Mask the current CPU's Rx/Tx interrupts */
+static inline void mv_pp2x_interrupts_mask(void *arg)
+{
+	struct mv_pp2x_port *port = arg;
+
+	mv_pp2x_write(&(port->priv->hw), MVPP2_ISR_RX_TX_MASK_REG(port->id), 0);
+}
+
+/* Unmask the current CPU's Rx/Tx interrupts */
+static inline void mv_pp2x_interrupts_unmask(void *arg)
+{
+	struct mv_pp2x_port *port = arg;
+	u32 val;
+
+	val = MVPP2_CAUSE_MISC_SUM_MASK | MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK;
+	if (port->priv->pp2xdata->interrupt_tx_done == true)
+		val |= MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;
+
+	mv_pp2x_write(&(port->priv->hw),
+		      MVPP2_ISR_RX_TX_MASK_REG(port->id), val);
+}
+
+static inline void mv_pp2x_shared_thread_interrupts_mask(
+		struct mv_pp2x_port *port)
+{
+	struct queue_vector *q_vec = &port->q_vector[0];
+	int i;
+
+	if (port->priv->pp2xdata->multi_addr_space == false)
+		return;
+
+	for (i = 0; i < port->num_qvector; i++) {
+		if (q_vec[i].qv_type == MVPP2_SHARED)
+			mv_pp22_thread_write(&port->priv->hw,
+					     q_vec[i].sw_thread_id,
+					    MVPP2_ISR_RX_TX_MASK_REG(port->id),
+					    0);
+	}
+}
+
+/* Unmask the shared CPU's Rx interrupts */
+static inline void mv_pp2x_shared_thread_interrupts_unmask(
+		struct mv_pp2x_port *port)
+{
+	struct queue_vector *q_vec = &port->q_vector[0];
+	int i;
+
+	if (port->priv->pp2xdata->multi_addr_space == false)
+		return;
+
+	for (i = 0; i < port->num_qvector; i++) {
+		if (q_vec[i].qv_type == MVPP2_SHARED)
+			mv_pp22_thread_write(&port->priv->hw,
+					     q_vec[i].sw_thread_id,
+					    MVPP2_ISR_RX_TX_MASK_REG(port->id),
+					  MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK);
+	}
+}
+
+static inline struct mv_pp2x_rx_queue *mv_pp2x_get_rx_queue(
+		struct mv_pp2x_port *port, u32 cause)
+{
+	int rx_queue = fls(cause) - 1;
+
+	return port->rxqs[rx_queue];
+}
+
+static inline struct mv_pp2x_tx_queue *mv_pp2x_get_tx_queue(
+		struct mv_pp2x_port *port, u32 cause)
+{
+	int tx_queue = fls(cause) - 1;
+
+	return port->txqs[tx_queue];
+}
+
+static inline struct sk_buff *mv_pp2x_bm_virt_addr_get(struct mv_pp2x_hw *hw,
+							      u32 pool)
+{
+	uintptr_t val = 0;
+
+	mv_pp2x_read(hw, MVPP2_BM_PHY_ALLOC_REG(pool));
+/*TODO: Validate this is  correct CONFIG_XXX for (sk_buff *),
+ * it is a kmem_cache address (YuvalC).
+ */
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	val = mv_pp2x_read(hw, MVPP22_BM_PHY_VIRT_HIGH_ALLOC_REG);
+	val &= MVPP22_BM_VIRT_HIGH_ALLOC_MASK;
+	val <<= (32 - MVPP22_BM_VIRT_HIGH_ALLOC_OFFSET);
+#endif
+	val |= mv_pp2x_read(hw, MVPP2_BM_VIRT_ALLOC_REG);
+	return((struct sk_buff *)val);
+}
+
+static inline void mv_pp2x_bm_hw_pool_create(struct mv_pp2x_hw *hw,
+						      u32 pool,
+						      dma_addr_t pool_addr,
+						      int size)
+{
+	u32 val;
+
+	mv_pp2x_write(hw, MVPP2_BM_POOL_BASE_ADDR_REG(pool),
+		      lower_32_bits(pool_addr));
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+	mv_pp2x_write(hw, MVPP22_BM_POOL_BASE_ADDR_HIGH_REG,
+		      (upper_32_bits(pool_addr) & MVPP22_ADDR_HIGH_MASK));
+#endif
+	mv_pp2x_write(hw, MVPP2_BM_POOL_SIZE_REG(pool), size);
+
+	val = mv_pp2x_read(hw, MVPP2_BM_POOL_CTRL_REG(pool));
+	val |= MVPP2_BM_START_MASK;
+	mv_pp2x_write(hw, MVPP2_BM_POOL_CTRL_REG(pool), val);
+}
+
+/* Release buffer to BM */
+static inline void mv_pp2x_bm_pool_put(struct mv_pp2x_hw *hw, u32 pool,
+					      dma_addr_t buf_phys_addr,
+					      struct sk_buff *buf_virt_addr)
+{
+
+/*TODO: Validate this is  correct CONFIG_XXX for (sk_buff *),
+ * it is a kmem_cache address (YuvalC).
+ */
+#ifdef CONFIG_64BIT /*CONFIG_PHYS_ADDR_T_64BIT*/
+	u32 val = 0;
+
+	val = (upper_32_bits((uintptr_t)buf_virt_addr) &
+		MVPP22_ADDR_HIGH_MASK) << MVPP22_BM_VIRT_HIGH_RLS_OFFST;
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+	val |= (upper_32_bits(buf_phys_addr) &
+		MVPP22_ADDR_HIGH_MASK) << MVPP22_BM_PHY_HIGH_RLS_OFFSET;
+
+#endif
+	mv_pp2x_write(hw, MVPP22_BM_PHY_VIRT_HIGH_RLS_REG, val);
+#endif
+
+	mv_pp2x_write(hw, MVPP2_BM_VIRT_RLS_REG,
+		      lower_32_bits((uintptr_t)buf_virt_addr));
+	mv_pp2x_write(hw, MVPP2_BM_PHY_RLS_REG(pool),
+		      lower_32_bits(buf_phys_addr));
+
+}
+
+/* Release multicast buffer */
+static inline void mv_pp2x_bm_pool_mc_put(struct mv_pp2x_port *port, int pool,
+						   u32 buf_phys_addr,
+						   u32 buf_virt_addr,
+						   int mc_id)
+{
+	u32 val = 0;
+
+	val |= (mc_id & MVPP21_BM_MC_ID_MASK);
+	mv_pp2x_write(&(port->priv->hw), MVPP21_BM_MC_RLS_REG, val);
+	/*TODO : YuvalC, this is just workaround to compile.
+	 * Need to handle mv_pp2x_buff_hdr_rx().
+	 */
+	mv_pp2x_bm_pool_put(&(port->priv->hw), pool,
+			    (dma_addr_t)(buf_phys_addr |
+			    MVPP2_BM_PHY_RLS_MC_BUFF_MASK),
+			    (struct sk_buff *)(u64)(buf_virt_addr));
+}
+
+static inline void mv_pp2x_port_interrupts_enable(struct mv_pp2x_port *port)
+{
+	int sw_thread_mask = 0, i;
+	struct queue_vector *q_vec = &port->q_vector[0];
+
+	for (i = 0; i < port->num_qvector; i++)
+		sw_thread_mask |= q_vec[i].sw_thread_mask;
+#if !defined(CONFIG_MV_PP2_POLLING)
+	mv_pp2x_write(&port->priv->hw, MVPP2_ISR_ENABLE_REG(port->id),
+		      MVPP2_ISR_ENABLE_INTERRUPT(sw_thread_mask));
+#endif
+}
+
+static inline void mv_pp2x_port_interrupts_disable(struct mv_pp2x_port *port)
+{
+	int sw_thread_mask = 0, i;
+	struct queue_vector *q_vec = &port->q_vector[0];
+
+	for (i = 0; i < port->num_qvector; i++)
+		sw_thread_mask |= q_vec[i].sw_thread_mask;
+
+	mv_pp2x_write(&port->priv->hw, MVPP2_ISR_ENABLE_REG(port->id),
+		      MVPP2_ISR_DISABLE_INTERRUPT(sw_thread_mask));
+}
+
+
+static inline void mv_pp2x_qvector_interrupt_enable(struct queue_vector *q_vec)
+{
+#if !defined(CONFIG_MV_PP2_POLLING)
+	struct mv_pp2x_port *port = q_vec->parent;
+
+	mv_pp2x_write(&port->priv->hw, MVPP2_ISR_ENABLE_REG(port->id),
+		      MVPP2_ISR_ENABLE_INTERRUPT(q_vec->sw_thread_mask));
+#endif
+}
+
+static inline void mv_pp2x_qvector_interrupt_disable(struct queue_vector *q_vec)
+{
+	struct mv_pp2x_port *port = q_vec->parent;
+
+	mv_pp2x_write(&port->priv->hw, MVPP2_ISR_ENABLE_REG(port->id),
+		      MVPP2_ISR_DISABLE_INTERRUPT(q_vec->sw_thread_mask));
+
+}
+
+static inline int mv_pp2x_txq_sent_desc_proc(struct mv_pp2x_port *port,
+						  struct mv_pp2x_tx_queue *txq)
+{
+	u32 val;
+
+	/* Reading status reg resets transmitted descriptor counter */
+	if (port->priv->pp2_version == PPV21) {
+		val = mv_pp2x_read(&(port->priv->hw),
+				   MVPP21_TXQ_SENT_REG(txq->id));
+		return (val & MVPP21_TRANSMITTED_COUNT_MASK) >>
+			MVPP21_TRANSMITTED_COUNT_OFFSET;
+		}
+	else {
+		val = mv_pp2x_read(&(port->priv->hw),
+				   MVPP22_TXQ_SENT_REG(txq->id));
+		return (val & MVPP22_TRANSMITTED_COUNT_MASK) >>
+			MVPP22_TRANSMITTED_COUNT_OFFSET;
+		}
+
+}
+
+static inline void mv_pp2x_txq_desc_put(struct mv_pp2x_tx_queue *txq)
+{
+	if (txq->next_desc_to_proc == 0)
+		txq->next_desc_to_proc = txq->last_desc - 1;
+	else
+		txq->next_desc_to_proc--;
+}
+
+
+static inline void mv_pp2x_txq_sent_counter_clear(void *arg)
+{
+	struct mv_pp2x_port *port = arg;
+	int queue;
+
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		int id = port->txqs[queue]->id;
+
+		if (port->priv->pp2_version == PPV21)
+			mv_pp2x_read(&(port->priv->hw),
+				     MVPP21_TXQ_SENT_REG(id));
+		else
+			mv_pp2x_read(&(port->priv->hw),
+				     MVPP22_TXQ_SENT_REG(id));
+	}
+}
+
+static inline struct sk_buff *mv_pp21_rxdesc_cookie_get(
+		struct mv_pp2x_rx_desc *rx_desc)
+{
+	return((struct sk_buff *)((uintptr_t)rx_desc->u.pp21.buf_cookie));
+}
+
+static inline dma_addr_t mv_pp21_rxdesc_phys_addr_get(
+		struct mv_pp2x_rx_desc *rx_desc)
+{
+	return (dma_addr_t)rx_desc->u.pp21.buf_phys_addr;
+}
+
+/*YuvalC: Below functions are intended to support both aarch64 & aarch32 */
+static inline struct sk_buff *mv_pp22_rxdesc_cookie_get(
+		struct mv_pp2x_rx_desc *rx_desc)
+{
+	return((struct sk_buff *)((uintptr_t)
+		(rx_desc->u.pp22.buf_cookie_bm_qset_cls_info &
+		DMA_BIT_MASK(40))));
+}
+
+static inline dma_addr_t mv_pp22_rxdesc_phys_addr_get(
+		struct mv_pp2x_rx_desc *rx_desc)
+{
+	return((dma_addr_t)
+		(rx_desc->u.pp22.buf_phys_addr_key_hash &
+		DMA_BIT_MASK(40)));
+}
+
+static inline struct sk_buff *mv_pp21_txdesc_cookie_get(
+		struct mv_pp2x_tx_desc *tx_desc)
+{
+	return((struct sk_buff *)((uintptr_t)tx_desc->u.pp21.buf_cookie));
+}
+
+static inline dma_addr_t mv_pp21_txdesc_phys_addr_get(
+		struct mv_pp2x_tx_desc *tx_desc)
+{
+	return (dma_addr_t)tx_desc->u.pp21.buf_phys_addr;
+}
+
+static inline struct sk_buff *mv_pp22_txdesc_cookie_get(
+		struct mv_pp2x_tx_desc *tx_desc)
+{
+	return((struct sk_buff *)((uintptr_t)
+		(tx_desc->u.pp22.buf_cookie_bm_qset_hw_cmd3 &
+		DMA_BIT_MASK(40))));
+}
+
+static inline dma_addr_t mv_pp22_txdesc_phys_addr_get(
+		struct mv_pp2x_tx_desc *tx_desc)
+{
+	return((dma_addr_t)
+		(tx_desc->u.pp22.buf_phys_addr_hw_cmd2 & DMA_BIT_MASK(40)));
+}
+
+static inline dma_addr_t mv_pp2x_txdesc_phys_addr_get(
+	enum mvppv2_version pp2_ver, struct mv_pp2x_tx_desc *tx_desc)
+{
+	if (pp2_ver == PPV21)
+		return mv_pp21_txdesc_phys_addr_get(tx_desc);
+
+	return mv_pp22_txdesc_phys_addr_get(tx_desc);
+}
+
+
+static inline void mv_pp21_txdesc_phys_addr_set(dma_addr_t phys_addr,
+	struct mv_pp2x_tx_desc *tx_desc)
+{
+	tx_desc->u.pp21.buf_phys_addr = phys_addr;
+}
+
+static inline void mv_pp22_txdesc_phys_addr_set(dma_addr_t phys_addr,
+	struct mv_pp2x_tx_desc *tx_desc)
+{
+	u64 *buf_phys_addr_p = &tx_desc->u.pp22.buf_phys_addr_hw_cmd2;
+
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+	*buf_phys_addr_p &= ~(DMA_BIT_MASK(40));
+	*buf_phys_addr_p |= phys_addr & DMA_BIT_MASK(40);
+#else
+	*((dma_addr_t *)buf_phys_addr_p) = phys_addr;
+	*((u8 *)buf_phys_addr_p + sizeof(dma_addr_t)) = 0; /*5th byte*/
+
+	/*pr_crit("phys_addr=%x, buf_phys_addr_hw_cmd2=%d\n",
+	* phys_addr, tx_desc->u.pp22.buf_phys_addr_hw_cmd2);
+	*/
+#endif
+}
+
+static inline void mv_pp2x_txdesc_phys_addr_set(enum mvppv2_version pp2_ver,
+	dma_addr_t phys_addr, struct mv_pp2x_tx_desc *tx_desc)
+{
+	if (pp2_ver == PPV21)
+		mv_pp21_txdesc_phys_addr_set(phys_addr, tx_desc);
+	else
+		mv_pp22_txdesc_phys_addr_set(phys_addr, tx_desc);
+}
+
+int mv_pp2x_ptr_validate(const void *ptr);
+int mv_pp2x_range_validate(int value, int min, int max);
+
+int mv_pp2x_prs_hw_read(struct mv_pp2x_hw *hw, struct mv_pp2x_prs_entry *pe);
+
+int mv_pp2x_prs_default_init(struct platform_device *pdev,
+				  struct mv_pp2x_hw *hw);
+void mv_pp2x_prs_mac_promisc_set(struct mv_pp2x_hw *hw, int port, bool add);
+void mv_pp2x_prs_mac_multi_set(struct mv_pp2x_hw *hw, int port, int index,
+				      bool add);
+int mv_pp2x_prs_mac_da_accept(struct mv_pp2x_hw *hw, int port,
+				      const u8 *da, bool add);
+int mv_pp2x_prs_def_flow(struct mv_pp2x_port *port);
+int mv_pp2x_prs_flow_set(struct mv_pp2x_port *port);
+void mv_pp2x_prs_mcast_del_all(struct mv_pp2x_hw *hw, int port);
+int mv_pp2x_prs_tag_mode_set(struct mv_pp2x_hw *hw, int port, int type);
+int mv_pp2x_prs_update_mac_da(struct net_device *dev, const u8 *da);
+void mv_pp2x_prs_flow_id_attr_init(void);
+int mv_pp2x_prs_flow_id_attr_get(int flow_id);
+
+int mv_pp2x_cls_init(struct platform_device *pdev, struct mv_pp2x_hw *hw);
+void mv_pp2x_cls_port_config(struct mv_pp2x_port *port);
+void mv_pp2x_cls_config(struct mv_pp2x_hw *hw);
+void mv_pp2x_cls_oversize_rxq_set(struct mv_pp2x_port *port);
+void mv_pp2x_cls_lookup_read(struct mv_pp2x_hw *hw, int lkpid, int way,
+				   struct mv_pp2x_cls_lookup_entry *le);
+void mv_pp2x_cls_flow_tbl_temp_copy(struct mv_pp2x_hw *hw, int lkpid,
+					    int *temp_flow_idx);
+void mv_pp2x_cls_lkp_flow_set(struct mv_pp2x_hw *hw, int lkpid, int way,
+				    int flow_idx);
+void mv_pp2x_cls_flow_port_add(struct mv_pp2x_hw *hw, int index, int port_id);
+void mv_pp2x_cls_flow_port_del(struct mv_pp2x_hw *hw, int index, int port_id);
+
+void mv_pp2x_txp_max_tx_size_set(struct mv_pp2x_port *port);
+void mv_pp2x_tx_done_time_coal_set(struct mv_pp2x_port *port, u32 usec);
+void mv_pp21_gmac_max_rx_size_set(struct mv_pp2x_port *port);
+
+int mv_pp2x_txq_pend_desc_num_get(struct mv_pp2x_port *port,
+					    struct mv_pp2x_tx_queue *txq);
+u32 mv_pp2x_txq_desc_csum(int l3_offs, int l3_proto,
+				  int ip_hdr_len, int l4_proto);
+struct mv_pp2x_tx_desc *mv_pp2x_txq_next_desc_get(
+		struct mv_pp2x_aggr_tx_queue *aggr_txq);
+int mv_pp2x_txq_alloc_reserved_desc(struct mv_pp2x *priv,
+					    struct mv_pp2x_tx_queue *txq,
+					    int num);
+void mv_pp2x_aggr_txq_pend_desc_add(struct mv_pp2x_port *port, int pending);
+int mv_pp2x_aggr_desc_num_read(struct mv_pp2x *priv, int cpu);
+int mv_pp2x_aggr_desc_num_check(struct mv_pp2x *priv,
+					struct mv_pp2x_aggr_tx_queue *aggr_txq,
+					int num);
+void mv_pp2x_rxq_offset_set(struct mv_pp2x_port *port,
+				 int prxq, int offset);
+void mv_pp2x_bm_pool_bufsize_set(struct mv_pp2x_hw *hw,
+					 struct mv_pp2x_bm_pool *bm_pool,
+					 int buf_size);
+void mv_pp2x_pool_refill(struct mv_pp2x *priv, u32 pool,
+			    dma_addr_t phys_addr, struct sk_buff *cookie);
+
+void mv_pp21_rxq_long_pool_set(struct mv_pp2x_hw *hw,
+				     int prxq, int long_pool);
+void mv_pp21_rxq_short_pool_set(struct mv_pp2x_hw *hw,
+				     int prxq, int short_pool);
+
+void mv_pp22_rxq_long_pool_set(struct mv_pp2x_hw *hw,
+				     int prxq, int long_pool);
+void mv_pp22_rxq_short_pool_set(struct mv_pp2x_hw *hw,
+				     int prxq, int short_pool);
+
+void mv_pp21_port_mii_set(struct mv_pp2x_port *port);
+void mv_pp21_port_fc_adv_enable(struct mv_pp2x_port *port);
+void mv_pp21_port_enable(struct mv_pp2x_port *port);
+void mv_pp21_port_disable(struct mv_pp2x_port *port);
+
+void mv_pp2x_ingress_enable(struct mv_pp2x_port *port);
+void mv_pp2x_ingress_disable(struct mv_pp2x_port *port);
+void mv_pp2x_egress_enable(struct mv_pp2x_port *port);
+void mv_pp2x_egress_disable(struct mv_pp2x_port *port);
+
+void mv_pp21_port_periodic_xon_disable(struct mv_pp2x_port *port);
+void mv_pp21_port_loopback_set(struct mv_pp2x_port *port);
+void mv_pp21_port_reset(struct mv_pp2x_port *port);
+
+void mv_pp2x_rx_pkts_coal_set(struct mv_pp2x_port *port,
+				    struct mv_pp2x_rx_queue *rxq, u32 pkts);
+void mv_pp2x_rx_time_coal_set(struct mv_pp2x_port *port,
+				   struct mv_pp2x_rx_queue *rxq, u32 usec);
+void mv_pp2x_tx_done_pkts_coal_set(void *arg);
+void mv_pp2x_cause_error(struct net_device *dev, int cause);
+void mv_pp2x_rx_error(struct mv_pp2x_port *port,
+			  struct mv_pp2x_rx_desc *rx_desc);
+void mv_pp2x_rx_csum(struct mv_pp2x_port *port, u32 status,
+			   struct sk_buff *skb);
+void mv_pp21_get_mac_address(struct mv_pp2x_port *port, unsigned char *addr);
+
+int mv_pp2x_c2_init(struct platform_device *pdev, struct mv_pp2x_hw *hw);
+
+int mv_pp2x_prs_sw_sram_shift_set(struct mv_pp2x_prs_entry *pe, int shift,
+					  unsigned int op);
+int mv_pp2x_prs_sw_sram_shift_get(struct mv_pp2x_prs_entry *pe, int *shift);
+int mv_pp2x_prs_sw_sram_next_lu_get(struct mv_pp2x_prs_entry *pe,
+					     unsigned int *lu);
+int mv_pp2x_prs_sram_bit_get(struct mv_pp2x_prs_entry *pe, int bitNum,
+				   unsigned int *bit);
+int mv_pp2x_prs_sw_sram_lu_done_get(struct mv_pp2x_prs_entry *pe,
+					      unsigned int *bit);
+int mv_pp2x_prs_sw_sram_flowid_gen_get(struct mv_pp2x_prs_entry *pe,
+						 unsigned int *bit);
+int mv_pp2x_prs_sw_sram_ri_get(struct mv_pp2x_prs_entry *pe,
+				       unsigned int *bits,
+				       unsigned int *enable);
+int mv_pp2x_prs_sw_sram_ai_get(struct mv_pp2x_prs_entry *pe,
+				       unsigned int *bits,
+				       unsigned int *enable);
+int mv_pp2x_prs_sw_sram_offset_set(struct mv_pp2x_prs_entry *pe,
+					   unsigned int type,
+					   int offset, unsigned int op);
+int mv_pp2x_prs_sw_sram_offset_get(struct mv_pp2x_prs_entry *pe,
+					   unsigned int *type,
+					   int *offset, unsigned int *op);
+void mv_pp2x_prs_hw_port_init(struct mv_pp2x_hw *hw, int port,
+				    int lu_first, int lu_max, int offset);
+void mv_pp2x_prs_sw_clear(struct mv_pp2x_prs_entry *pe);
+void mv_pp2x_prs_hw_inv(struct mv_pp2x_hw *hw, int index);
+void mv_pp2x_prs_tcam_lu_set(struct mv_pp2x_prs_entry *pe, unsigned int lu);
+void mv_pp2x_prs_tcam_port_set(struct mv_pp2x_prs_entry *pe,
+				      unsigned int port, bool add);
+void mv_pp2x_prs_tcam_port_map_set(struct mv_pp2x_prs_entry *pe,
+					    unsigned int ports);
+void mv_pp2x_prs_tcam_data_byte_set(struct mv_pp2x_prs_entry *pe,
+					    unsigned int offs,
+					    unsigned char byte,
+					    unsigned char enable);
+void mv_pp2x_prs_tcam_ai_update(struct mv_pp2x_prs_entry *pe,
+					unsigned int bits,
+					unsigned int enable);
+void mv_pp2x_prs_sram_ri_update(struct mv_pp2x_prs_entry *pe,
+					unsigned int bits, unsigned int mask);
+void mv_pp2x_prs_sram_ai_update(struct mv_pp2x_prs_entry *pe,
+					unsigned int bits, unsigned int mask);
+void mv_pp2x_prs_sram_next_lu_set(struct mv_pp2x_prs_entry *pe,
+					unsigned int lu);
+void mv_pp2x_prs_sw_sram_lu_done_set(struct mv_pp2x_prs_entry *pe);
+void mv_pp2x_prs_sw_sram_lu_done_clear(struct mv_pp2x_prs_entry *pe);
+void mv_pp2x_prs_sw_sram_flowid_set(struct mv_pp2x_prs_entry *pe);
+void mv_pp2x_prs_sw_sram_flowid_clear(struct mv_pp2x_prs_entry *pe);
+int mv_pp2x_prs_hw_write(struct mv_pp2x_hw *hw, struct mv_pp2x_prs_entry *pe);
+int mv_pp2x_cls_hw_lkp_read(struct mv_pp2x_hw *hw, int lkpid, int way,
+				   struct mv_pp2x_cls_lookup_entry *fe);
+int mv_pp2x_cls_hw_lkp_write(struct mv_pp2x_hw *hw, int lkpid, int way,
+				   struct mv_pp2x_cls_lookup_entry *fe);
+int mv_pp2x_cls_lkp_port_way_set(struct mv_pp2x_hw *hw, int port, int way);
+int mv_pp2x_cls_hw_lkp_print(struct mv_pp2x_hw *hw, int lkpid, int way);
+int mv_pp2x_cls_sw_lkp_rxq_get(struct mv_pp2x_cls_lookup_entry *lkp, int *rxq);
+int mv_pp2x_cls_sw_lkp_rxq_set(struct mv_pp2x_cls_lookup_entry *fe, int rxq);
+int mv_pp2x_cls_sw_lkp_en_get(struct mv_pp2x_cls_lookup_entry *lkp, int *en);
+int mv_pp2x_cls_sw_lkp_en_set(struct mv_pp2x_cls_lookup_entry *lkp, int en);
+int mv_pp2x_cls_sw_lkp_flow_get(struct mv_pp2x_cls_lookup_entry *lkp,
+				       int *flow_idx);
+int mv_pp2x_cls_sw_lkp_flow_set(struct mv_pp2x_cls_lookup_entry *lkp,
+				       int flow_idx);
+int mv_pp2x_cls_sw_lkp_mod_get(struct mv_pp2x_cls_lookup_entry *lkp,
+				       int *mod_base);
+int mv_pp2x_cls_sw_lkp_mod_set(struct mv_pp2x_cls_lookup_entry *lkp,
+				       int mod_base);
+int mv_pp2x_cls_hw_flow_read(struct mv_pp2x_hw *hw, int index,
+				    struct mv_pp2x_cls_flow_entry *fe);
+int mv_pp2x_cls_sw_flow_dump(struct mv_pp2x_cls_flow_entry *fe);
+int mv_pp2x_cls_hw_regs_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_hw_lkp_hit_get(struct mv_pp2x_hw *hw, int lkpid, int way,
+				     unsigned int *cnt);
+int mv_pp2x_cls_hw_flow_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_hw_flow_hits_dump(struct mv_pp2x_hw *hw);
+void mv_pp2x_cls_flow_write(struct mv_pp2x_hw *hw,
+				 struct mv_pp2x_cls_flow_entry *fe);
+int mv_pp2x_cls_sw_flow_port_set(struct mv_pp2x_cls_flow_entry *fe,
+					int type, int portid);
+int mv_pp2x_cls_sw_flow_hek_num_set(struct mv_pp2x_cls_flow_entry *fe,
+					      int num_of_fields);
+int mv_pp2x_cls_sw_flow_hek_set(struct mv_pp2x_cls_flow_entry *fe,
+					int field_index, int field_id);
+int mv_pp2x_cls_sw_flow_portid_select(struct mv_pp2x_cls_flow_entry *fe,
+					     int from);
+int mv_pp2x_cls_sw_flow_pppoe_set(struct mv_pp2x_cls_flow_entry *fe, int mode);
+int mv_pp2x_cls_sw_flow_vlan_set(struct mv_pp2x_cls_flow_entry *fe, int mode);
+int mv_pp2x_cls_sw_flow_macme_set(struct mv_pp2x_cls_flow_entry *fe, int mode);
+int mv_pp2x_cls_sw_flow_udf7_set(struct mv_pp2x_cls_flow_entry *fe, int mode);
+int mv_pp2x_cls_sw_flow_seq_ctrl_set(struct mv_pp2x_cls_flow_entry *fe,
+					    int mode);
+int mv_pp2x_cls_sw_flow_engine_set(struct mv_pp2x_cls_flow_entry *fe,
+					   int engine, int is_last);
+int mv_pp2x_cls_sw_flow_extra_set(struct mv_pp2x_cls_flow_entry *fe,
+					 int type, int prio);
+int mv_pp2x_cls_hw_lkp_hits_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_sw_lkp_dump(struct mv_pp2x_cls_lookup_entry *lkp);
+int mv_pp2x_cls_hw_lkp_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_hw_udf_set(struct mv_pp2x_hw *hw, int udf_no, int offs_id,
+				 int offs_bits, int size_bits);
+int mv_pp2x_cls_c2_qos_hw_read(struct mv_pp2x_hw *hw, int tbl_id, int tbl_sel,
+					int tbl_line,
+					struct mv_pp2x_cls_c2_qos_entry *qos);
+int mv_pp2x_cls_c2_qos_hw_write(struct mv_pp2x_hw *hw,
+					struct mv_pp2x_cls_c2_qos_entry *qos);
+int mvPp2ClsC2QosPrioGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *prio);
+int mvPp2ClsC2QosDscpGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *dscp);
+int mvPp2ClsC2QosColorGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *color);
+int mvPp2ClsC2QosGpidGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *gpid);
+int mvPp2ClsC2QosQueueGet(struct mv_pp2x_cls_c2_qos_entry *qos, int *queue);
+int mv_pp2x_cls_c2_qos_tbl_set(struct mv_pp2x_cls_c2_entry *c2, int tbl_id,
+				     int tbl_sel);
+int mv_pp2x_cls_c2_hw_write(struct mv_pp2x_hw *hw, int index,
+				  struct mv_pp2x_cls_c2_entry *c2);
+int mv_pp2x_cls_c2_hw_read(struct mv_pp2x_hw *hw, int index,
+				  struct mv_pp2x_cls_c2_entry *c2);
+int mv_pp2x_cls_c2_sw_words_dump(struct mv_pp2x_cls_c2_entry *c2);
+int mv_pp2x_cls_c2_hit_cntr_clear_all(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_c2_hit_cntr_read(struct mv_pp2x_hw *hw, int index, u32 *cntr);
+int mv_pp2x_cls_c2_hit_cntr_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_c2_regs_dump(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_c2_rule_set(struct mv_pp2x_port *port, u8 start_queue);
+u8 mv_pp2x_cls_c2_rule_queue_get(struct mv_pp2x_hw *hw, u32 rule_idx);
+void mv_pp2x_cls_c2_rule_queue_set(struct mv_pp2x_hw *hw, u32 rule_idx,
+					   u8 queue);
+u8 mv_pp2x_cls_c2_pbit_tbl_queue_get(struct mv_pp2x_hw *hw, u8 tbl_id,
+					     u8 tbl_line);
+void mv_pp2x_cls_c2_pbit_tbl_queue_set(struct mv_pp2x_hw *hw, u8 tbl_id,
+					      u8 tbl_line, u8 queue);
+int mv_pp2x_cls_c2_hw_inv(struct mv_pp2x_hw *hw, int index);
+void mv_pp2x_cls_c2_hw_inv_all(struct mv_pp2x_hw *hw);
+int mv_pp2x_cls_c2_tcam_byte_set(struct mv_pp2x_cls_c2_entry *c2,
+					unsigned int offs,
+					unsigned char byte,
+					unsigned char enable);
+int mv_pp2x_cls_c2_qos_queue_set(struct mv_pp2x_cls_c2_qos_entry *qos,
+					 u8 queue);
+int mv_pp2x_cls_c2_color_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+				   int from);
+int mv_pp2x_cls_c2_prio_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+				 int prio, int from);
+int mv_pp2x_cls_c2_dscp_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+				  int dscp, int from);
+int mv_pp2x_cls_c2_queue_low_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+					 int queue, int from);
+int mv_pp2x_cls_c2_queue_high_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+					  int queue, int from);
+int mv_pp2x_cls_c2_forward_set(struct mv_pp2x_cls_c2_entry *c2, int cmd);
+int mv_pp2x_cls_c2_rss_set(struct mv_pp2x_cls_c2_entry *c2, int cmd,
+				 int rss_en);
+int mv_pp2x_cls_c2_flow_id_en(struct mv_pp2x_cls_c2_entry *c2,
+				    int flowid_en);
+
+int mv_pp22_rss_tbl_entry_set(struct mv_pp2x_hw *hw,
+				struct mv_pp22_rss_entry *rss);
+int mv_pp22_rss_tbl_entry_get(struct mv_pp2x_hw *hw,
+				struct mv_pp22_rss_entry *rss);
+
+int mv_pp22_rss_rxq_set(struct mv_pp2x_port *port, u32 cos_width);
+
+void mv_pp22_rss_c2_enable(struct mv_pp2x_port *port, bool en);
+int mv_pp22_rss_hw_dump(struct mv_pp2x_hw *hw);
+
+#endif /* _MVPP2_HW_H_ */
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
new file mode 100644
index 0000000..72520a3
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_hw_type.h
@@ -0,0 +1,2269 @@
+/*
+* ***************************************************************************
+* Copyright (C) 2016 Marvell International Ltd.
+* ***************************************************************************
+* This program is free software: you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the Free
+* Software Foundation, either version 2 of the License, or any later version.
+*
+* This program is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+* GNU General Public License for more details.
+*
+* You should have received a copy of the GNU General Public License
+* along with this program.  If not, see <http://www.gnu.org/licenses/>.
+* ***************************************************************************
+*/
+
+#ifndef _MVPP2_HW_TYPE_H_
+#define _MVPP2_HW_TYPE_H_
+
+#include <linux/kernel.h>
+#include <linux/skbuff.h>
+#include <linux/bitops.h>
+
+#define CREATE_MASK(pos, len)		GENMASK((pos)+(len)-1, (pos))
+#define CREATE_MASK_ULL(pos, len)	GENMASK_ULL((pos)+(len)-1, (pos))
+
+#define AUTO_MASK(reg_name)	CREATE_MASK(reg_name##_OFFS, reg_name##_SIZE)
+
+/*All PPV22 Addresses are 40-bit */
+#define MVPP22_ADDR_HIGH_SIZE			8
+#define MVPP22_ADDR_HIGH_MASK		((1<<MVPP22_ADDR_HIGH_SIZE) - 1)
+
+/*PPV22 ADDRESS SPACE */
+#define MVPP2_ADDR_SPACE_SIZE			(64*1024)
+
+/*TODO*/
+/*AXI_BRIDGE*/
+/*AXI_CONTEXT*/
+/*Top Regfile*/
+
+#define MVPP21_DESC_ADDR_SHIFT		0 /*Applies to RXQ, AGGR_TXQ*/
+#define MVPP22_DESC_ADDR_SHIFT		(9-1) /*Applies to RXQ, AGGR_TXQ*/
+
+/* RX Fifo Registers */
+#define MVPP2_RX_DATA_FIFO_SIZE_REG(port)	(0x00 + 4 * (port))
+#define MVPP2_RX_ATTR_FIFO_SIZE_REG(port)	(0x20 + 4 * (port))
+#define MVPP2_RX_MIN_PKT_SIZE_REG		0x60
+#define MVPP2_RX_FIFO_INIT_REG			0x64
+
+/* RX DMA Top Registers */
+#define MVPP2_RX_CTRL_REG(port)			(0x140 + 4 * (port))
+#define MVPP2_RX_LOW_LATENCY_PKT_SIZE(s)	(((s) & 0xfff) << 16)
+#define MVPP2_RX_USE_PSEUDO_FOR_CSUM_MASK	BIT(31)
+#define MVPP2_POOL_BUF_SIZE_REG(pool)		(0x180 + 4 * (pool))
+#define MVPP2_POOL_BUF_SIZE_OFFSET		5
+
+/* RXQ_CONFIG_REF Generic+PPV21+PPV22 */
+#define MVPP2_RXQ_CONFIG_REG(rxq)		(0x800 + 4 * (rxq))
+#define MVPP2_SNOOP_PKT_SIZE_MASK		0x1ff
+#define MVPP2_SNOOP_BUF_HDR_MASK		BIT(9)
+#define MVPP2_RXQ_PACKET_OFFSET_OFFS		28
+#define MVPP2_RXQ_PACKET_OFFSET_MASK		0x70000000
+#define MVPP2_RXQ_DISABLE_MASK			BIT(31)
+
+#define MVPP21_RXQ_POOL_SHORT_OFFS		20
+#define MVPP21_RXQ_POOL_SHORT_MASK		0x700000
+#define MVPP21_RXQ_POOL_LONG_OFFS		24
+#define MVPP21_RXQ_POOL_LONG_MASK		0x7000000
+
+#define MVPP22_RXQ_POOL_SHORT_OFFS		20
+#define MVPP22_RXQ_POOL_SHORT_MASK		0xf00000
+#define MVPP22_RXQ_POOL_LONG_OFFS		24
+#define MVPP22_RXQ_POOL_LONG_MASK		0xf000000
+#define MVPP22_RXQ_LLC_DEP_HDR_SIZE		0xf000
+#define MVPP22_RXQ_LLC_DEP_ENABLE		BIT(16)
+
+#define MVPP21_ETH_RX_HWQ_REG(txq)		(0xc00 + 4 * (txq))
+#define MVPP21_ETH_RX_HWQ_POOL_SHORT_OFFS	0
+#define MVPP21_ETH_RX_HWQ_POOL_SHORT_MASK	0x7
+#define MVPP21_ETH_RX_HWQ_POOL_LONG_OFFS	4
+#define MVPP21_ETH_RX_HWQ_POOL_LONG_MASK	0x70
+#define MVPP21_ETH_RX_HWQ_DISABLE_MASK		BIT(31)
+#define MVPP22_ETH_RX_HWQ_REG(txq)		(0xe00 + 4 * (txq))
+#define MVPP22_ETH_RX_HWQ_POOL_SHORT_OFFS	0
+#define MVPP22_ETH_RX_HWQ_POOL_SHORT_MASK	0xf
+#define MVPP22_ETH_RX_HWQ_POOL_LONG_OFFS	4
+#define MVPP22_ETH_RX_HWQ_POOL_LONG_MASK	0xf0
+#define MVPP22_ETH_RX_HWQ_DISABLE_MASK		BIT(31)
+
+#define MVPP22_RX_HWF_SNOOP_REG			(0x178)
+#define MVPP22_RX_HWF_SNOOP_ENABLE		(BIT(0))
+
+/* AXI Bridge Registers */
+#define MVPP22_AXI_BM_WR_ATTR_REG		0x4100
+#define MVPP22_AXI_BM_RD_ATTR_REG		0x4104
+#define MVPP22_AXI_AGGRQ_DESCR_RD_ATTR_REG	0x4110
+#define MVPP22_AXI_TXQ_DESCR_WR_ATTR_REG	0x4114
+#define MVPP22_AXI_TXQ_DESCR_RD_ATTR_REG	0x4118
+#define MVPP22_AXI_RXQ_DESCR_WR_ATTR_REG	0x411c
+#define MVPP22_AXI_RX_DATA_WR_ATTR_REG		0x4120
+#define MVPP22_AXI_TX_DATA_RD_ATTR_REG		0x4130
+
+#define MVPP22_AXI_ATTR_CACHE_OFFS		0
+#define MVPP22_AXI_ATTR_CACHE_SIZE		4
+#define MVPP22_AXI_ATTR_CACHE_MASK	AUTO_MASK(MVPP22_AXI_ATTR_CACHE)
+
+#define MVPP22_AXI_ATTR_QOS_OFFS		4
+#define MVPP22_AXI_ATTR_QOS_SIZE		4
+#define MVPP22_AXI_ATTR_QOS_MASK	AUTO_MASK(MVPP22_AXI_ATTR_QOS)
+
+#define MVPP22_AXI_ATTR_TC_OFFS			8
+#define MVPP22_AXI_ATTR_TC_SIZE			4
+#define MVPP22_AXI_ATTR_TC_MASK		AUTO_MASK(MVPP22_AXI_ATTR_TC)
+
+#define MVPP22_AXI_ATTR_DOMAIN_OFFS		12
+#define MVPP22_AXI_ATTR_DOMAIN_SIZE		2
+#define MVPP22_AXI_ATTR_DOMAIN_MASK	AUTO_MASK(MVPP22_AXI_ATTR_DOMAIN)
+
+#define MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT		BIT(16)
+
+/* Parser Registers */
+#define MVPP2_PRS_INIT_LOOKUP_REG		0x1000
+#define MVPP2_PRS_PORT_LU_MAX			0xf
+#define MVPP2_PRS_MAX_LOOP_MIN			0x1
+#define MVPP2_PRS_PORT_LU_MASK(port)		(0xff << ((port) * 4))
+#define MVPP2_PRS_PORT_LU_VAL(port, val)	((val) << ((port) * 4))
+#define MVPP2_PRS_INIT_OFFS_REG(port)		(0x1004 + ((port) & 4))
+#define MVPP2_PRS_INIT_OFF_MASK(port)		(0x3f << (((port) % 4) * 8))
+#define MVPP2_PRS_INIT_OFF_VAL(port, val)	((val) << (((port) % 4) * 8))
+#define MVPP2_PRS_INIT_OFF_BITS			6
+#define MVPP2_PRS_INIT_OFF_MAX		((1 << MVPP2_PRS_INIT_OFF_BITS) - 1)
+#define MVPP2_PRS_MAX_LOOP_REG(port)		(0x100c + ((port) & 4))
+#define MVPP2_PRS_MAX_LOOP_MASK(port)		(0xff << (((port) % 4) * 8))
+#define MVPP2_PRS_MAX_LOOP_VAL(port, val)	((val) << (((port) % 4) * 8))
+#define MVPP2_PRS_TCAM_IDX_REG			0x1100
+#define MVPP2_PRS_TCAM_DATA_REG(idx)		(0x1104 + (idx) * 4)
+#define MVPP2_PRS_TCAM_INV_MASK			BIT(31)
+#define MVPP2_PRS_SRAM_IDX_REG			0x1200
+#define MVPP2_PRS_SRAM_DATA_REG(idx)		(0x1204 + (idx) * 4)
+
+#define MVPP2_PRS_EXP_REG			0x1214
+#define MVPP2_PRS_EXP_MISS			0
+#define MVPP2_PRS_EXP_EXEED			1
+#define MVPP2_PRS_EXP_OF			2
+
+#define MVPP2_PRS_TCAM_CTRL_REG			0x1230
+#define MVPP2_PRS_TCAM_EN_MASK			BIT(0)
+#define MVPP2_PRS_INTR_CAUSE_REG		(0x1020)
+#define MVPP2_PRS_INTR_MASK_REG			(0x1024)
+
+/*PPv2.1 MASS 3.20 new feature */
+#define MVPP2_PRS_TCAM_HIT_IDX_REG		0x1240
+/*----------------------------------------------------------------------*/
+/*PPv2.1 MASS 3.20 new feature */
+#define MVPP2_PRS_TCAM_HIT_CNT_REG		0x1244
+#define MVPP2_PRS_TCAM_HIT_CNT_BITS		16
+#define MVPP2_PRS_TCAM_HIT_CNT_OFFS		0
+#define MVPP2_PRS_TCAM_HIT_CNT_MASK		\
+				(((1 << MVPP2_PRS_TCAM_HIT_CNT_BITS) - 1) << \
+				MVPP2_PRS_TCAM_HIT_CNT_OFFS)
+
+/* Classifier Registers */
+#define MVPP2_CLS_MODE_REG			0x1800
+#define MVPP2_CLS_MODE_ACTIVE_MASK		BIT(0)
+#define MVPP2_CLS_PORT_WAY_REG			0x1810
+#define MVPP2_CLS_PORT_WAY_MASK(port)		(1 << (port))
+#define MVPP2_CLS_LKP_INDEX_REG			0x1814
+#define MVPP2_CLS_LKP_INDEX_WAY_OFFS		6
+#define MVPP2_CLS_LKP_INDEX_LKP_OFFS		0
+#define MVPP2_CLS_LKP_TBL_REG			0x1818
+#define MVPP2_CLS_LKP_TBL_RXQ_MASK		0xff
+#define MVPP2_CLS_LKP_TBL_LOOKUP_EN_MASK	BIT(25)
+#define MVPP22_CLS_LKP_TBL_SEL_REG		0x181c
+#define MVPP22_CLS_LKP_TBL_SEL_CDT_MASK		BIT(0)
+#define MVPP22_CLS_LKP_TBL_SEL_FDT_MASK		BIT(1)
+#define MVPP2_CLS_FLOW_INDEX_REG		0x1820
+#define MVPP2_CLS_FLOW_TBL0_REG			0x1824
+#define MVPP2_CLS_FLOW_TBL1_REG			0x1828
+#define MVPP2_CLS_FLOW_TBL2_REG			0x182c
+
+
+#define MVPP2_CLS_PORT_SPID_REG			0x1830
+
+#define MVPP2_CLS_PORT_SPID_BITS		2
+#define MVPP2_CLS_PORT_SPID_MAX			\
+					((1 << MVPP2_CLS_PORT_SPID_BITS) - 1)
+#define MVPP2_CLS_PORT_SPID_MASK(port)		((MVPP2_CLS_PORT_SPID_MAX) << \
+					((port) * MVPP2_CLS_PORT_SPID_BITS))
+#define MVPP2_CLS_PORT_SPID_VAL(port, val)	((val) << \
+					((port) * MVPP2_CLS_PORT_SPID_BITS))
+
+/* PORT - SPID types */
+#define MVPP2_PORT_SPID_MH			0
+#define MVPP2_PORT_SPID_EXT_SWITCH		1
+#define MVPP2_PORT_SPID_CAS_SWITCH		2
+#define MVPP2_PORT_SPID_PORT_TRUNK		3
+/*----------------------------------------------------------------------*/
+
+#define MVPP2_CLS_SPID_UNI_BASE_REG		0x1840
+#define MVPP2_CLS_SPID_UNI_REG(spid)		(MVPP2_CLS_SPID_UNI_BASE_REG + \
+						(((spid) >> 3) * 4))
+
+#define MVPP2_CLS_SPID_MAX			31
+#define MVPP2_CLS_SPID_UNI_REGS			4
+#define MVPP2_CLS_SPID_UNI_BITS			3
+#define MVPP2_CLS_SPID_UNI_FIXED_BITS		4
+#define MVPP2_CLS_SPID_UNI_MAX			((1 << \
+						MVPP2_CLS_SPID_UNI_BITS) - 1)
+#define MVPP2_CLS_SPID_UNI_OFFS(spid)		(((spid) % 8) * \
+						MVPP2_CLS_SPID_UNI_FIXED_BITS)
+#define MVPP2_CLS_SPID_UNI_MASK(spid)		((MVPP2_CLS_SPID_UNI_MAX) << \
+					(MVPP2_CLS_SPID_UNI_OFFS(spid)))
+#define MVPP2_CLS_SPID_UNI_VAL(spid, val)	((val) << \
+					(MVPP2_CLS_SPID_UNI_OFFS(spid)))
+
+/*----------------------------------------------------------------------*/
+#define MVPP2_CLS_GEM_VIRT_INDEX_REG		0x1A00
+#define MVPP2_CLS_GEM_VIRT_INDEX_BITS		(7)
+#define MVPP2_CLS_GEM_VIRT_INDEX_MAX		(((1 << \
+				MVPP2_CLS_GEM_VIRT_INDEX_BITS) - 1) << 0)
+
+/*----------------------------------------------------------------------*/
+
+/* indirect rd/wr via index GEM_VIRT_INDEX */
+#define MVPP2_CLS_GEM_VIRT_REGS_NUM		128
+#define MVPP2_CLS_GEM_VIRT_REG			0x1A04
+
+#define MVPP2_CLS_GEM_VIRT_BITS			12
+#define MVPP2_CLS_GEM_VIRT_MAX			((1 << \
+					MVPP2_CLS_GEM_VIRT_BITS) - 1)
+#define MVPP2_CLS_GEM_VIRT_MASK			(((1 << \
+					MVPP2_CLS_GEM_VIRT_BITS) - 1) << 0)
+
+/*----------------------------------------------------------------------*/
+#define MVPP2_CLS_UDF_BASE_REG			0x1860
+#define MVPP2_CLS_UDF_REG(index)		(MVPP2_CLS_UDF_BASE_REG + \
+						((index) * 4)) /*index <=63*/
+#define MVPP2_CLS_UDF_REGS_NUM			64
+
+#define MVPP2_CLS_UDF_BASE_REGS			8
+#define MVPP2_CLS_UDF_OFFSET_ID_OFFS		0
+#define MVPP2_CLS_UDF_OFFSET_ID_BITS		4
+#define MVPP2_CLS_UDF_OFFSET_ID_MAX		((1 << \
+					MVPP2_CLS_UDF_OFFSET_ID_BITS) - 1)
+#define MVPP2_CLS_UDF_OFFSET_ID_MASK		\
+	((MVPP2_CLS_UDF_OFFSET_ID_MAX) << MVPP2_CLS_UDF_OFFSET_ID_OFFS)
+
+#define MVPP2_CLS_UDF_OFFSET_PACKET		0
+#define MVPP2_CLS_UDF_OFFSET_L3			1
+#define MVPP2_CLS_UDF_OFFSET_L4			4
+#define MVPP2_CLS_UDF_OFFSET_OUTVLAN		8
+#define MVPP2_CLS_UDF_OFFSET_INVLAN		9
+#define MVPP2_CLS_UDF_OFFSET_ETHTYPE		0xa
+
+#define MVPP2_CLS_UDF_REL_OFFSET_OFFS		4
+#define MVPP2_CLS_UDF_REL_OFFSET_BITS		11
+#define MVPP2_CLS_UDF_REL_OFFSET_MAX		((1 << \
+					MVPP2_CLS_UDF_REL_OFFSET_BITS) - 1)
+#define MVPP2_CLS_UDF_REL_OFFSET_MASK		\
+	((MVPP2_CLS_UDF_REL_OFFSET_MAX) << MVPP2_CLS_UDF_REL_OFFSET_OFFS)
+
+#define MVPP2_CLS_UDF_SIZE_OFFS			16
+#define MVPP2_CLS_UDF_SIZE_BITS			8
+#define MVPP2_CLS_UDF_SIZE_MAX			((1 << \
+					MVPP2_CLS_UDF_SIZE_BITS) - 1)
+#define MVPP2_CLS_UDF_SIZE_MASK			(((1 << \
+		MVPP2_CLS_UDF_SIZE_BITS) - 1) << MVPP2_CLS_UDF_SIZE_OFFS)
+/*----------------------------------------------------------------------*/
+
+#define MVPP2_CLS_MTU_BASE_REG			0x1900
+/*  in PPv2.1 (feature MAS 3.7) num indicate an mtu reg index
+ * in PPv2.0 num (<=31) indicate eport number , 0-15 pon txq,  16-23 ethernet
+ */
+#define MVPP2_CLS_MTU_REG(num)			(MVPP2_CLS_MTU_BASE_REG + \
+						((num) * 4))
+#define MVPP2_CLS_MTU_OFFS			0
+#define MVPP2_CLS_MTU_BITS			16
+#define MVPP2_CLS_MTU_MAX			((1 << \
+					MVPP2_CLS_MTU_BITS) - 1)
+#define MVPP2_CLS_MTU_MASK			(((1 << \
+			MVPP2_CLS_MTU_BITS) - 1) << MVPP2_CLS_MTU_OFFS)
+/*----------------------------------------------------------------------*/
+
+#define MVPP2_CLS_OVERSIZE_RXQ_LOW_REG(port)	(0x1980 + ((port) * 4))
+#define MVPP2_CLS_OVERSIZE_RXQ_LOW_BITS		3
+#define MVPP2_CLS_OVERSIZE_RXQ_LOW_MASK		0x7
+#define MVPP2_CLS_SWFWD_P2HQ_REG(port)		(0x19b0 + ((port) * 4))
+#define MVPP2_CLS_SWFWD_PCTRL_REG		0x19d0
+#define MVPP2_CLS_SWFWD_PCTRL_MASK(port)	(1 << (port))
+
+/*PPv2.1 new feature MAS 3.14*/
+#define MVPP2_CLS_SEQ_SIZE_REG			0x19D4
+#define MVPP2_CLS_SEQ_SIZE_BITS			4
+#define MVPP2_CLS_SEQ_INDEX_MAX			7
+#define MVPP2_CLS_SEQ_SIZE_MAX			8
+#define MVPP2_CLS_SEQ_SIZE_MASK(index)		\
+				(((1 << MVPP2_CLS_SEQ_SIZE_BITS) - 1) << \
+				(MVPP2_CLS_SEQ_SIZE_BITS * (index)))
+#define MVPP2_CLS_SEQ_SIZE_VAL(index, val)	((val) << ((index) * \
+						MVPP2_CLS_SEQ_SIZE_BITS))
+
+/*PPv2.1 new register MAS 3.18*/
+#define MVPP2_CLS_PCTRL_BASE_REG		0x1880
+#define MV_PP2_CLS_PCTRL_REG(port)		(MVPP2_CLS_PCTRL_BASE_REG + \
+						4 * (port))
+#define MVPP2_CLS_PCTRL_MH_OFFS			0
+#define MVPP2_CLS_PCTRL_MH_BITS			16
+#define MVPP2_CLS_PCTRL_MH_MASK			(((1 << \
+		MVPP2_CLS_PCTRL_MH_BITS) - 1) << MVPP2_CLS_PCTRL_MH_OFFS)
+
+#define MVPP2_CLS_PCTRL_VIRT_EN_OFFS		16
+#define MVPP2_CLS_PCTRL_VIRT_EN_MASK		(1 << \
+					MVPP2_CLS_PCTRL_VIRT_EN_OFFS)
+
+#define MVPP2_CLS_PCTRL_UNI_EN_OFFS		17
+#define MVPP2_CLS_PCTRL_UNI_EN_MASK		(1 << \
+					MVPP2_CLS_PCTRL_UNI_EN_OFFS)
+
+/*----------------------------------------------------------------------*/
+
+#define MV_PP2_OVERRUN_DROP_REG(port)		(0x7000 + 4 * (port))
+#define MV_PP2_CLS_DROP_REG(port)		(0x7020 + 4 * (port))
+
+#define MVPP2_CNT_IDX_REG			0x7040
+/* LKP counters index */
+#define MVPP2_CNT_IDX_LKP(lkp, way)		((way) << 6 | (lkp))
+/* Flow counters index */
+#define MVPP2_CNT_IDX_FLOW(index)		(index)
+/* TX counters index */
+#define MVPP2_CNT_IDX_TX(port, txq)		(((16+port) << 3) | (txq))
+
+#define MVPP2_TX_DESC_ENQ_REG			0x7100
+#define MVPP2_TX_DESC_ENQ_TO_DRAM_REG		0x7104
+#define MVPP2_TX_BUF_ENQ_TO_DRAM_REG		0x7108
+#define MVPP2_TX_DESC_HWF_ENQ_REG		0x710c
+#define MVPP2_RX_DESC_ENQ_REG			0x7120
+#define MVPP2_TX_PKT_DQ_REG			0x7130
+
+#define MVPP2_TX_PKT_FULLQ_DROP_REG		0x7200
+#define MVPP2_TX_PKT_EARLY_DROP_REG		0x7204
+#define MVPP2_TX_PKT_BM_DROP_REG		0x7208
+#define MVPP2_TX_PKT_BM_MC_DROP_REG		0x720c
+#define MVPP2_RX_PKT_FULLQ_DROP_REG		0x7220
+#define MVPP2_RX_PKT_EARLY_DROP_REG		0x7224
+#define MVPP2_RX_PKT_BM_DROP_REG		0x7228
+
+#define MVPP2_BM_DROP_CNTR_REG(pool)		(0x7300 + 4 * (pool))
+#define MVPP2_BM_MC_DROP_CNTR_REG(pool)		(0x7340 + 4 * (pool))
+
+#define MVPP2_PLCR_GREEN_CNTR_REG(plcr)		(0x7400 + 4 * (plcr))
+#define MVPP2_PLCR_YELLOW_CNTR_REG(plcr)	(0x7500 + 4 * (plcr))
+#define MVPP2_PLCR_RED_CNTR_REG(plcr)		(0x7600 + 4 * (plcr))
+
+#define MVPP2_CLS_LKP_TBL_HIT_REG		0x7700
+#define MVPP2_CLS_FLOW_TBL_HIT_REG		0x7704
+#define MVPP2_CLS4_TBL_HIT_REG			0x7708
+
+#define MVPP2_V1_OVERFLOW_MC_DROP_REG		0x770c
+
+/* Classifier C2 Engine Registers */
+#define MVPP2_CLS2_TCAM_IDX_REG			0x1B00
+#define MVPP2_CLS2_TCAM_DATA_REG(idx)		(0x1B10 + (idx) * 4)
+#define MVPP2_CLS2_TCAM_INV_REG			0x1B24
+#define MVPP2_CLS2_TCAM_INV_INVALID_OFF		31
+#define MVPP2_CLS2_TCAM_INV_INVALID_MASK	BIT(31)
+#define MVPP2_CLS2_ACT_DATA_REG			0x1B30
+#define MVPP2_CLS2_ACT_DATA_TBL_ID_OFF		0
+#define MVPP2_CLS2_ACT_DATA_TBL_ID_MASK		0x3F
+#define MVPP2_CLS2_ACT_DATA_TBL_SEL_OFF		6
+#define MVPP2_CLS2_ACT_DATA_TBL_SEL_MASK	0x40
+#define MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_OFF	7
+#define MVPP2_CLS2_ACT_DATA_TBL_PRI_DSCP_MASK	0x80
+#define MVPP2_CLS2_ACT_DATA_TBL_GEM_ID_OFF	8
+#define MVPP2_CLS2_ACT_DATA_TBL_GEM_ID_MASK	0x100
+#define MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_OFF	9
+#define MVPP2_CLS2_ACT_DATA_TBL_LOW_Q_MASK	0x200
+#define MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_OFF	10
+#define MVPP2_CLS2_ACT_DATA_TBL_HIGH_Q_MASK	0x400
+#define MVPP2_CLS2_ACT_DATA_TBL_COLOR_OFF	11
+#define MVPP2_CLS2_ACT_DATA_TBL_COLOR_MASK	0x800
+#define MVPP2_CLS2_DSCP_PRI_INDEX_REG		0x1B40
+#define MVPP2_CLS2_DSCP_PRI_INDEX_LINE_OFF	0
+#define MVPP2_CLS2_DSCP_PRI_INDEX_LINE_BITS	6
+#define MVPP2_CLS2_DSCP_PRI_INDEX_LINE_MASK	0x0000003f
+#define MVPP2_CLS2_DSCP_PRI_INDEX_SEL_OFF	6
+#define MVPP2_CLS2_DSCP_PRI_INDEX_SEL_MASK	BIT(6)
+#define MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_OFF	8
+#define MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_BITS	6
+#define MVPP2_CLS2_DSCP_PRI_INDEX_TBL_ID_MASK	0x00003f00
+#define MVPP2_CLS2_QOS_TBL_REG			0x1B44
+#define MVPP2_CLS2_QOS_TBL_PRI_OFF		0
+#define MVPP2_CLS2_QOS_TBL_PRI_BITS		3
+#define MVPP2_CLS2_QOS_TBL_PRI_MASK		0x00000007
+#define MVPP2_CLS2_QOS_TBL_DSCP_OFF		3
+#define MVPP2_CLS2_QOS_TBL_DSCP_BITS		6
+#define MVPP2_CLS2_QOS_TBL_DSCP_MASK		0x000001f8
+#define MVPP2_CLS2_QOS_TBL_COLOR_OFF		9
+#define MVPP2_CLS2_QOS_TBL_COLOR_BITS		3
+#define MVPP2_CLS2_QOS_TBL_COLOR_MASK		0x00000e00
+#define MVPP2_CLS2_QOS_TBL_GEMPORT_OFF		12
+#define MVPP2_CLS2_QOS_TBL_GEMPORT_BITS		12
+#define MVPP2_CLS2_QOS_TBL_GEMPORT_MASK		0x00fff000
+#define MVPP2_CLS2_QOS_TBL_QUEUENUM_OFF		24
+#define MVPP2_CLS2_QOS_TBL_QUEUENUM_BITS	8
+#define MVPP2_CLS2_QOS_TBL_QUEUENUM_MASK	0xff000000
+#define MVPP2_CLS2_HIT_CTR_REG			0x1B50
+#define MVPP2_CLS2_HIT_CTR_OFF			0
+#define MVPP2_CLS2_HIT_CTR_BITS			32
+#define MVPP2_CLS2_HIT_CTR_MASK			0xffffffff
+#define MVPP2_CLS2_HIT_CTR_CLR_REG		0x1B54
+#define MVPP2_CLS2_HIT_CTR_CLR_CLR_OFF		0
+#define MVPP2_CLS2_HIT_CTR_CLR_CLR_MASK		BIT(0)
+#define MVPP2_CLS2_HIT_CTR_CLR_DONE_OFF		1
+#define MVPP2_CLS2_HIT_CTR_CLR_DONE_MASK	BIT(1)
+#define MVPP2_CLS2_ACT_REG			0x1B60
+#define MVPP2_CLS2_ACT_COLOR_OFF		0
+#define MVPP2_CLS2_ACT_COLOR_BITS		3
+#define MVPP2_CLS2_ACT_COLOR_MASK		0x00000007
+#define MVPP2_CLS2_ACT_PRI_OFF			3
+#define MVPP2_CLS2_ACT_PRI_BITS			2
+#define MVPP2_CLS2_ACT_PRI_MASK			0x00000018
+#define MVPP2_CLS2_ACT_DSCP_OFF			5
+#define MVPP2_CLS2_ACT_DSCP_BITS		2
+#define MVPP2_CLS2_ACT_DSCP_MASK		0x00000060
+#define MVPP2_CLS2_ACT_GEM_OFF			7
+#define MVPP2_CLS2_ACT_GEM_BITS			2
+#define MVPP2_CLS2_ACT_GEM_MASK			0x00000180
+#define MVPP2_CLS2_ACT_QL_OFF			9
+#define MVPP2_CLS2_ACT_QL_BITS			2
+#define MVPP2_CLS2_ACT_QL_MASK			0x00000600
+#define MVPP2_CLS2_ACT_QH_OFF			11
+#define MVPP2_CLS2_ACT_QH_BITS			2
+#define MVPP2_CLS2_ACT_QH_MASK			0x00001800
+#define MVPP2_CLS2_ACT_FRWD_OFF			13
+#define MVPP2_CLS2_ACT_FRWD_BITS		3
+#define MVPP2_CLS2_ACT_FRWD_MASK		0x0000e000
+#define MVPP2_CLS2_ACT_PLCR_OFF			16
+#define MVPP2_CLS2_ACT_PLCR_BITS		2
+#define MVPP2_CLS2_ACT_PLCR_MASK		0x00030000
+#define MVPP2_CLS2_ACT_FLD_EN_OFF		18
+#define MVPP2_CLS2_ACT_FLD_EN_BITS		1
+#define MVPP2_CLS2_ACT_FLD_EN_MASK		0x00040000
+#define MVPP2_CLS2_ACT_RSS_OFF			19
+#define MVPP2_CLS2_ACT_RSS_BITS			2
+#define MVPP2_CLS2_ACT_RSS_MASK			0x00180000
+#define MVPP2_CLS2_ACT_QOS_ATTR_REG		0x1B64
+#define MVPP2_CLS2_ACT_QOS_ATTR_PRI_OFF		0
+#define MVPP2_CLS2_ACT_QOS_ATTR_PRI_BITS	3
+#define MVPP2_CLS2_ACT_QOS_ATTR_PRI_MASK	0x00000007
+#define MVPP2_CLS2_ACT_QOS_ATTR_DSCP_OFF	3
+#define MVPP2_CLS2_ACT_QOS_ATTR_DSCP_BITS	6
+#define MVPP2_CLS2_ACT_QOS_ATTR_DSCP_MASK	0x000001f8
+#define MVPP2_CLS2_ACT_QOS_ATTR_GEM_OFF		9
+#define MVPP2_CLS2_ACT_QOS_ATTR_GEM_BITS	12
+#define MVPP2_CLS2_ACT_QOS_ATTR_GEM_MASK	0x001ffe00
+#define MVPP2_CLS2_ACT_QOS_ATTR_QL_OFF		21
+#define MVPP2_CLS2_ACT_QOS_ATTR_QL_BITS		3
+#define MVPP2_CLS2_ACT_QOS_ATTR_QL_MASK		0x00e00000
+#define MVPP2_CLS2_ACT_QOS_ATTR_QH_OFF		24
+#define MVPP2_CLS2_ACT_QOS_ATTR_QH_BITS		5
+#define MVPP2_CLS2_ACT_QOS_ATTR_QH_MASK		0x1f000000
+#define MVPP2_CLS2_ACT_HWF_ATTR_REG		0x1B68
+#define MVPP2_CLS2_ACT_HWF_ATTR_DPTR_OFF	1
+#define MVPP2_CLS2_ACT_HWF_ATTR_DPTR_BITS	15
+#define MVPP2_CLS2_ACT_HWF_ATTR_DPTR_MASK	0x0000fffe
+#define MVPP2_CLS2_ACT_HWF_ATTR_IPTR_OFF	16
+#define MVPP2_CLS2_ACT_HWF_ATTR_IPTR_BITS	8
+#define MVPP2_CLS2_ACT_HWF_ATTR_IPTR_MASK	0x00ff0000
+#define MVPP2_CLS2_ACT_HWF_ATTR_L4CHK_OFF	24
+#define MVPP2_CLS2_ACT_HWF_ATTR_L4CHK_BITS	1
+#define MVPP2_CLS2_ACT_HWF_ATTR_L4CHK_MASK	0x01000000
+#define MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_OFF	25
+#define MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_BITS	4
+#define MVPP2_CLS2_ACT_HWF_ATTR_MTUIDX_MASK	0x1e000000
+#define MVPP2_CLS2_ACT_DUP_ATTR_REG		0x1B6C
+#define MVPP2_CLS2_ACT_DUP_ATTR_DUPID_OFF	0
+#define MVPP2_CLS2_ACT_DUP_ATTR_DUPID_BITS	8
+#define MVPP2_CLS2_ACT_DUP_ATTR_DUPID_MASK	0x000000ff
+#define MVPP2_CLS2_ACT_DUP_ATTR_DUPCNT_OFF	8
+#define MVPP2_CLS2_ACT_DUP_ATTR_DUPCNT_BITS	4
+#define MVPP2_CLS2_ACT_DUP_ATTR_DUPCNT_MASK	0x00000f00
+#define MVPP2_CLS2_ACT_DUP_ATTR_PLCRID_OFF	24
+#define MVPP2_CLS2_ACT_DUP_ATTR_PLCRID_BITS	5
+#define MVPP2_CLS2_ACT_DUP_ATTR_PLCRID_MASK	0x1f000000
+#define MVPP2_CLS2_ACT_DUP_ATTR_PLCRBK_OFF	29
+#define MVPP2_CLS2_ACT_DUP_ATTR_PLCRBK_BITS	1
+#define MVPP2_CLS2_ACT_DUP_ATTR_PLCRBK_MASK	0x20000000
+#define MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_OFF	30
+#define MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_BITS	1
+#define MVPP2_CLS2_ACT_DUP_ATTR_RSSEN_MASK	0x40000000
+#define MVPP21_CLS2_ACT_SEQ_ATTR_REG		0x1B70
+#define MVPP21_CLS2_ACT_SEQ_ATTR_ID		0
+#define MVPP21_CLS2_ACT_SEQ_ATTR_ID_BITS	8
+#define MVPP21_CLS2_ACT_SEQ_ATTR_ID_MASK	0x000000ff
+#define MVPP21_CLS2_ACT_SEQ_ATTR_MISS_OFF	8
+#define MVPP21_CLS2_ACT_SEQ_ATTR_MISS_BITS	1
+#define MVPP21_CLS2_ACT_SEQ_ATTR_MISS_MASK	0x00000100
+#define MVPP22_CLS2_ACT_SEQ_ATTR_REG		0x1B70
+#define MVPP22_CLS2_ACT_SEQ_ATTR_ID		0
+#define MVPP22_CLS2_ACT_SEQ_ATTR_ID_BITS	16
+#define MVPP22_CLS2_ACT_SEQ_ATTR_ID_MASK	0x0000ffff
+#define MVPP22_CLS2_ACT_SEQ_ATTR_MISS_OFF	16
+#define MVPP22_CLS2_ACT_SEQ_ATTR_MISS_BITS	1
+#define MVPP22_CLS2_ACT_SEQ_ATTR_MISS_MASK	0x0001000
+#define MVPP2_CLS2_TCAM_CFG0_REG		0x1b80
+#define MVPP2_CLS2_TCAM_CFG0_EN_OFF		0
+#define MVPP2_CLS2_TCAM_CFG0_EN_MASK		0x00000001
+#define MVPP2_CLS2_TCAM_CFG0_SIZE_OFF		1
+#define MVPP2_CLS2_TCAM_CFG0_SIZE_MASK		0x0000001e
+#define MVPP2_CLS2_TCAM_CTRL_REG		0x1B90
+#define MVPP2_CLS2_TCAM_CTRL_EN_OFF		0
+#define MVPP2_CLS2_TCAM_CTRL_EN_MASK		0x0000001
+
+/* Classifier C2 QOS Table (DSCP/PRI Table) */
+#define MVPP2_QOS_TBL_LINE_NUM_PRI		8
+#define MVPP2_QOS_TBL_NUM_PRI			64
+#define MVPP2_QOS_TBL_LINE_NUM_DSCP		64
+#define MVPP2_QOS_TBL_NUM_DSCP			8
+
+/*------------------Classifier C3 Top Registers---------------------------*/
+#define MVPP2_CLS3_KEY_CTRL_REG		0x1C10
+#define KEY_CTRL_L4				0
+#define KEY_CTRL_L4_BITS			3
+#define KEY_CTRL_L4_MAX				((1 << \
+					KEY_CTRL_L4_BITS) - 1)
+#define KEY_CTRL_L4_MASK			(((1 << \
+				KEY_CTRL_L4_BITS) - 1) << KEY_CTRL_L4)
+#define KEY_CTRL_LKP_TYPE			4
+#define KEY_CTRL_LKP_TYPE_BITS			6
+#define KEY_CTRL_LKP_TYPE_MAX			((1 << \
+					KEY_CTRL_LKP_TYPE_BITS) - 1)
+#define KEY_CTRL_LKP_TYPE_MASK			(((1 << \
+			KEY_CTRL_LKP_TYPE_BITS) - 1) << KEY_CTRL_LKP_TYPE)
+#define KEY_CTRL_PRT_ID_TYPE			12
+#define KEY_CTRL_PRT_ID_TYPE_BITS		2
+#define KEY_CTRL_PRT_ID_TYPE_MAX		((1 << \
+					KEY_CTRL_PRT_ID_TYPE_BITS) - 1)
+#define KEY_CTRL_PRT_ID_TYPE_MASK		((KEY_CTRL_PRT_ID_TYPE_MAX) << \
+					KEY_CTRL_PRT_ID_TYPE)
+#define KEY_CTRL_PRT_ID				16
+#define KEY_CTRL_PRT_ID_BITS			8
+#define KEY_CTRL_PRT_ID_MAX			((1 << \
+					KEY_CTRL_PRT_ID_BITS) - 1)
+#define KEY_CTRL_PRT_ID_MASK			(((1 << \
+			KEY_CTRL_PRT_ID_BITS) - 1) << KEY_CTRL_PRT_ID)
+#define KEY_CTRL_HEK_SIZE			24
+#define KEY_CTRL_HEK_SIZE_BITS			6
+#define KEY_CTRL_HEK_SIZE_MAX			36
+#define KEY_CTRL_HEK_SIZE_MASK			(((1 << \
+			KEY_CTRL_HEK_SIZE_BITS) - 1) << KEY_CTRL_HEK_SIZE)
+
+#define MVPP2_CLS3_KEY_HEK_REG(reg_num)		(0x1C34 - 4 * (reg_num))
+
+#define MVPP2_CLS3_QRY_ACT_REG			0x1C40
+#define MVPP2_CLS3_QRY_ACT			0
+
+#define MVPP2_CLS3_QRY_RES_HASH_REG(hash)	(0x1C50 + 4 * (hash))
+#define MVPP2_CLS3_HASH_BANKS_NUM		8
+
+#define MVPP2_CLS3_INIT_HIT_CNT_REG		0x1C80
+#define MVPP2_CLS3_INIT_HIT_CNT_OFFS		6
+#define MVPP2_CLS3_INIT_HIT_CNT_BITS		18
+#define MVPP2_CLS3_INIT_HIT_CNT_MASK		(((1 << \
+	MVPP2_CLS3_INIT_HIT_CNT_BITS) - 1) << MVPP2_CLS3_INIT_HIT_CNT_OFFS)
+#define MVPP2_CLS3_INIT_HIT_CNT_MAX		((1 << \
+					MVPP2_CLS3_INIT_HIT_CNT_BITS) - 1)
+
+#define MVPP2_CLS3_HASH_OP_REG			0x1C84
+#define MVPP2_CLS3_HASH_OP_TBL_ADDR		0
+#define MVPP2_CLS3_HASH_OP_TBL_ADDR_BITS	12
+#define MVPP2_CLS3_HASH_OP_TBL_ADDR_MAX		((1 << \
+					MVPP2_CLS3_HASH_OP_TBL_ADDR_BITS) - 1)
+#define MVPP2_CLS3_HASH_OP_TBL_ADDR_MASK	\
+	((MVPP2_CLS3_HASH_OP_TBL_ADDR_MAX) << MVPP2_CLS3_HASH_OP_TBL_ADDR)
+#define MVPP2_CLS3_MISS_PTR			12
+#define MVPP2_CLS3_MISS_PTR_MASK		(1 << MVPP2_CLS3_MISS_PTR)
+#define MVPP2_CLS3_HASH_OP_DEL			14
+#define MVPP2_CLS3_HASH_OP_ADD			15
+#define MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR		16
+#define MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR_BITS	8
+#define MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR_MAX	((1 << \
+				MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR_BITS) - 1)
+#define MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR_MASK	\
+				((MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR_MAX) << \
+				MVPP2_CLS3_HASH_OP_EXT_TBL_ADDR)
+
+#define MVPP2_CLS3_STATE_REG			0x1C8C
+#define MVPP2_CLS3_STATE_CPU_DONE		0
+#define MVPP2_CLS3_STATE_CPU_DONE_MASK		(1 << \
+					MVPP2_CLS3_STATE_CPU_DONE)
+#define MVPP2_CLS3_STATE_CLEAR_CTR_DONE		1
+#define MVPP2_CLS3_STATE_CLEAR_CTR_DONE_MASK	(1 << \
+					MVPP2_CLS3_STATE_CLEAR_CTR_DONE)
+#define MVPP2_CLS3_STATE_SC_DONE		2
+#define MVPP2_CLS3_STATE_SC_DONE_MASK		(1 << MVPP2_CLS3_STATE_SC_DONE)
+#define MVPP2_CLS3_STATE_OCCIPIED		8
+#define MVPP2_CLS3_STATE_OCCIPIED_BITS		8
+#define MVPP2_CLS3_STATE_OCCIPIED_MASK		(((1 << \
+	MVPP2_CLS3_STATE_OCCIPIED_BITS) - 1) << MVPP2_CLS3_STATE_OCCIPIED)
+
+#define MVPP2_CLS3_STATE_SC_STATE		16
+#define MVPP2_CLS3_STATE_SC_STATE_BITS		2
+#define MVPP2_CLS3_STATE_SC_STATE_MASK		(((1 << \
+	MVPP2_CLS3_STATE_SC_STATE_BITS) - 1) << MVPP2_CLS3_STATE_SC_STATE)
+
+/* SCAN STATUS
+ * 0 - scan compleat
+ * 1 -	hit counter clear
+ * 3 - scan wait
+ * 4 - scan in progress
+ */
+
+#define MVPP2_CLS3_STATE_NO_OF_SC_RES		20
+#define MVPP2_CLS3_STATE_NO_OF_SC_RES_BITS	9
+#define MVPP2_CLS3_STATE_NO_OF_SC_RES_MASK	(((1 << \
+				MVPP2_CLS3_STATE_NO_OF_SC_RES_BITS) - 1) << \
+				MVPP2_CLS3_STATE_NO_OF_SC_RES)
+
+#define MVPP2_CLS3_DB_INDEX_REG			0x1C90
+#define MVPP2_CLS3_DB_MISS_OFFS			12
+#define MVPP2_CLS3_DB_MISS_MASK			(1 << MVPP2_CLS3_DB_MISS_OFFS)
+
+						/* 0-3 valid val*/
+#define MVPP2_CLS3_HASH_DATA_REG(num)		(0x1CA0 + 4 * (num))
+#define MVPP2_CLS3_HASH_DATA_REG_NUM		4
+#define MVPP2_CLS3_HASH_EXT_DATA_REG(num)	(0x1CC0 + 4 * (num))
+#define MVPP2_CLS3_HASH_EXT_DATA_REG_NUM	7
+
+#define MVPP2_CLS3_CLEAR_COUNTERS_REG		0x1D00
+#define MVPP2_CLS3_CLEAR_COUNTERS		0
+#define MVPP2_CLS3_CLEAR_COUNTERS_BITS		7
+#define MVPP2_CLS3_CLEAR_ALL			0x3f
+#define MVPP2_CLS3_CLEAR_COUNTERS_MAX		0x3F
+#define MVPP2_CLS3_CLEAR_COUNTERS_MASK		\
+					((MVPP2_CLS3_CLEAR_COUNTERS_MAX) << \
+					MVPP2_CLS3_CLEAR_COUNTERS)
+
+#define MVPP2_CLS3_HIT_COUNTER_REG		0x1D08
+#define MVPP2_CLS3_HIT_COUNTER			0
+#define MVPP2_CLS3_HIT_COUNTER_BITS		24
+#define MVPP2_CLS3_HIT_COUNTER_MAX		((1 << \
+					MVPP2_CLS3_HIT_COUNTER_BITS) - 1)
+#define MVPP2_CLS3_HIT_COUNTER_MASK		\
+		((MVPP2_CLS3_HIT_COUNTER_MAX) << MVPP2_CLS3_HIT_COUNTER)
+
+#define MVPP2_CLS3_SC_PROP_REG			0x1D10
+#define MVPP2_CLS3_SC_PROP_TH_MODE		0
+#define MVPP2_CLS3_SC_PROP_TH_MODE_MASK		(1 << \
+					MVPP2_CLS3_SC_PROP_TH_MODE)
+#define MVPP2_CLS3_SC_PROP_CLEAR		1
+#define MVPP2_CLS3_SC_PROP_CLEAR_MASK		(1 << \
+					MVPP2_CLS3_SC_PROP_CLEAR)
+#define MVPP2_CLS3_SC_PROP_LKP_TYPE_EN		3
+#define MVPP2_CLS3_SC_PROP_LKP_TYPE_EN_MASK	(1 << \
+					MVPP2_CLS3_SC_PROP_LKP_TYPE_EN)
+#define MVPP2_CLS3_SC_PROP_LKP_TYPE		4
+#define MVPP2_CLS3_SC_PROP_LKP_TYPE_BITS	6
+#define MVPP2_CLS3_SC_PROP_LKP_TYPE_MAX		((1 << \
+					MVPP2_CLS3_SC_PROP_LKP_TYPE_BITS) - 1)
+#define MVPP2_CLS3_SC_PROP_LKP_TYPE_MASK	\
+	((MVPP2_CLS3_SC_PROP_LKP_TYPE_MAX) << MVPP2_CLS3_SC_PROP_LKP_TYPE)
+#define MVPP2_CLS3_SC_PROP_START_ENTRY		16
+#define MVPP2_CLS3_SC_PROP_START_ENTRY_MASK	\
+	((MVPP2_CLS3_HASH_OP_TBL_ADDR_MAX) << MVPP2_CLS3_SC_PROP_START_ENTRY)
+
+#define MVPP2_CLS3_SC_PROP_VAL_REG		0x1D14
+#define MVPP2_CLS3_SC_PROP_VAL_DELAY		0
+#define MVPP2_CLS3_SC_PROP_VAL_DELAY_BITS	16
+#define MVPP2_CLS3_SC_PROP_VAL_DELAY_MAX	((1 << \
+					MVPP2_CLS3_SC_PROP_VAL_DELAY_BITS) - 1)
+#define MVPP2_CLS3_SC_PROP_VAL_DELAY_MASK	\
+	(MVPP2_CLS3_SC_PROP_VAL_DELAY_MAX << MVPP2_CLS3_SC_PROP_VAL_DELAY)
+
+#define MVPP2_CLS3_SC_TH_REG			0x1D18
+#define MVPP2_CLS3_SC_TH			4
+#define MVPP2_CLS3_SC_TH_BITS			20
+#define MVPP2_CLS3_SC_TH_MAX			((1 << \
+					MVPP2_CLS3_SC_TH_BITS) - 1)
+#define MVPP2_CLS3_SC_TH_MASK			(((1 << \
+			MVPP2_CLS3_SC_TH_BITS) - 1) << MVPP2_CLS3_SC_TH)
+
+#define MVPP2_CLS3_SC_TIMER_REG			0x1D1c
+#define MVPP2_CLS3_SC_TIMER			0
+#define MVPP2_CLS3_SC_TIMER_BITS		16
+#define MVPP2_CLS3_SC_TIMER_MASK		\
+		(((1 << MVPP2_CLS3_SC_TIMER_BITS) - 1) << MVPP2_CLS3_SC_TIMER)
+
+#define MVPP2_CLS3_SC_ACT_REG			0x1D20
+#define MVPP2_CLS3_SC_ACT			0
+
+#define MVPP2_CLS3_SC_INDEX_REG			0x1D28
+#define MVPP2_CLS3_SC_INDEX			0
+
+#define MVPP2_CLS3_SC_RES_REG			0x1D2C
+#define MVPP2_CLS3_SC_RES_ENTRY			0
+#define MVPP2_CLS3_SC_RES_ENTRY_MASK		\
+		((MVPP2_CLS3_HASH_OP_TBL_ADDR_MAX) << MVPP2_CLS3_SC_RES_ENTRY)
+#define MVPP2_CLS3_SC_RES_CTR			12
+#define MVPP2_CLS3_SC_RES_CTR_MASK		\
+		((MVPP2_CLS3_HIT_COUNTER_MAX) << MVPP2_CLS3_SC_RES_CTR)
+
+#define MVPP2_CLS3_ACT_REG			0x1D40
+
+#define MVPP2_CLS3_ACT_QOS_ATTR_REG		0x1D44
+
+#define MVPP2_CLS3_ACT_HWF_ATTR_REG		0x1D48
+
+#define MVPP2_CLS3_ACT_DUP_ATTR_REG		0x1D4C
+#define MVPP2_CLS3_ACT_SEQ_L_ATTR_REG		0x1D50
+#define MVPP2_CLS3_ACT_SEQ_H_ATTR_REG		0x1D54
+#define MVPP2_CLS3_ACT_SEQ_SIZE			38
+
+/* Descriptor Manager Top Registers */
+#define MVPP2_RXQ_NUM_REG			0x2040
+
+#define MVPP2_RXQ_DESC_ADDR_REG			0x2044
+#define MVPP21_RXQ_DESC_ADDR_SHIFT		MVPP21_DESC_ADDR_SHIFT
+#define MVPP21_RXQ_DESC_ADDR_MASK		0xfffffe00
+
+#define MVPP22_RXQ_DESC_ADDR_SHIFT		MVPP22_DESC_ADDR_SHIFT
+#define MVPP22_RXQ_DESC_ADDR_MASK		0xfffffffe
+
+#define MVPP2_RXQ_DESC_SIZE_REG			0x2048
+#define MVPP2_RXQ_DESC_SIZE_MASK		0x3ff0
+#define MVPP2_RXQ_STATUS_UPDATE_REG(rxq)	(0x3000 + 4 * (rxq))
+#define MVPP2_RXQ_NUM_PROCESSED_OFFSET		0
+#define MVPP2_RXQ_NUM_NEW_OFFSET		16
+#define MVPP2_RXQ_STATUS_REG(rxq)		(0x3400 + 4 * (rxq))
+#define MVPP2_RXQ_OCCUPIED_MASK			0x3fff
+#define MVPP2_RXQ_NON_OCCUPIED_OFFSET		16
+#define MVPP2_RXQ_NON_OCCUPIED_MASK		0x3fff0000
+#define MVPP2_RXQ_THRESH_REG			0x204c
+#define MVPP2_OCCUPIED_THRESH_OFFSET		0
+#define MVPP2_OCCUPIED_THRESH_MASK		0x3fff
+#define MVPP2_RXQ_INDEX_REG			0x2050
+#define MVPP2_TXQ_NUM_REG			0x2080
+#define MVPP2_TXQ_DESC_ADDR_LOW_REG		0x2084
+#define MVPP2_TXQ_DESC_ADDR_LOW_SHIFT		0
+#define MVPP2_TXQ_DESC_ADDR_LOW_MASK		0xfffffe00
+#define MVPP22_TXQ_DESC_ADDR_HIGH_REG		0x20a8
+#define MVPP22_TXQ_DESC_ADDR_HIGH_MASK		0xff
+#define MVPP2_TXQ_DESC_SIZE_REG			0x2088
+#define MVPP2_TXQ_DESC_HWF_SIZE_REG		0x208c
+#define MVPP2_TXQ_DESC_SIZE_MASK		0x3ff0
+#define MVPP2_AGGR_TXQ_UPDATE_REG		0x2090
+#define MVPP2_TXQ_THRESH_REG			0x2094
+#define MVPP2_TRANSMITTED_THRESH_OFFSET		16
+#define MVPP2_TRANSMITTED_THRESH_MASK		0x3fff0000
+#define MVPP2_TXQ_INDEX_REG			0x2098
+#define MVPP2_TXQ_PREF_BUF_REG			0x209c
+#define MVPP2_PREF_BUF_PTR(desc)		((desc) & 0xfff)
+#define MVPP2_PREF_BUF_SIZE_4			(BIT(12) | BIT(13))
+#define MVPP2_PREF_BUF_SIZE_16			(BIT(12) | BIT(14))
+#define MVPP2_PREF_BUF_THRESH(val)		((val) << 17)
+#define MVPP2_TXQ_DRAIN_EN_MASK			BIT(31)
+#define MVPP2_TXQ_PENDING_REG			0x20a0
+#define MVPP2_TXQ_PENDING_MASK			0x3fff
+#define MVPP2_TXQ_INT_STATUS_REG		0x20a4
+
+#define MVPP21_TXQ_SENT_REG(txq)		(0x3c00 + 4 * (txq))
+#define MVPP21_TRANSMITTED_COUNT_OFFSET		16
+#define MVPP21_TRANSMITTED_COUNT_MASK		0x3fff0000
+#define MVPP22_TXQ_SENT_REG(txq)		(0x3e00 + 4 * (txq-128))
+#define MVPP22_TRANSMITTED_COUNT_OFFSET		16
+#define MVPP22_TRANSMITTED_COUNT_MASK		0x3fff0000
+
+#define MVPP2_TXQ_RSVD_REQ_REG			0x20b0
+#define MVPP2_TXQ_RSVD_REQ_Q_OFFSET		16
+#define MVPP2_TXQ_RSVD_RSLT_REG			0x20b4
+#define MVPP2_TXQ_RSVD_RSLT_MASK		0x3fff
+#define MVPP2_TXQ_RSVD_CLR_REG			0x20b8
+#define MVPP2_TXQ_RSVD_CLR_OFFSET		16
+#define MVPP2_AGGR_TXQ_DESC_ADDR_REG(cpu)	(0x2100 + 4 * (cpu))
+#define MVPP21_AGGR_TXQ_DESC_ADDR_SHIFT		MVPP21_DESC_ADDR_SHIFT
+#define MVPP21_AGGR_TXQ_DESC_ADDR_MASK		0xfffffe00
+#define MVPP22_AGGR_TXQ_DESC_ADDR_SHIFT		MVPP22_DESC_ADDR_SHIFT
+#define MVPP22_AGGR_TXQ_DESC_ADDR_MASK		0xfffffffe
+
+
+#define MVPP2_AGGR_TXQ_DESC_SIZE_REG(cpu)	(0x2140 + 4 * (cpu))
+#define MVPP2_AGGR_TXQ_DESC_SIZE_MASK		0x3ff0
+#define MVPP2_AGGR_TXQ_STATUS_REG(cpu)		(0x2180 + 4 * (cpu))
+#define MVPP2_AGGR_TXQ_PENDING_MASK		0x3fff
+#define MVPP2_AGGR_TXQ_INDEX_REG(cpu)		(0x21c0 + 4 * (cpu))
+
+/* MBUS bridge registers */
+#define MVPP2_WIN_BASE(w)			(0x4000 + ((w) << 2))
+#define MVPP2_WIN_SIZE(w)			(0x4020 + ((w) << 2))
+#define MVPP2_WIN_REMAP(w)			(0x4040 + ((w) << 2))
+#define MVPP2_BASE_ADDR_ENABLE			0x4060
+
+/* Interrupt Cause and Mask registers */
+#define MVPP22_ISR_TX_THRESHOLD_REG(port)	(0x5140 + 4 * (port))
+#define MVPP2_ISR_RX_THRESHOLD_REG(rxq)		(0x5200 + 4 * (rxq))
+#define MVPP21_ISR_RXQ_GROUP_REG(port)		(0x5400 + 4 * (port))
+#define MVPP22_ISR_RXQ_GROUP_INDEX_REG		0x5400
+#define MVPP22_ISR_RXQ_GROUP_INDEX_SUBGROUP_MASK 0xf
+#define MVPP22_ISR_RXQ_GROUP_INDEX_GROUP_MASK   0x380
+#define MVPP22_ISR_RXQ_GROUP_INDEX_GROUP_OFFSET 7
+
+#define MVPP22_ISR_RXQ_GROUP_INDEX_SUBGROUP_MASK 0xf
+#define MVPP22_ISR_RXQ_GROUP_INDEX_GROUP_MASK   0x380
+
+#define MVPP22_ISR_RXQ_SUB_GROUP_CONFIG_REG	0x5404
+#define MVPP22_ISR_RXQ_SUB_GROUP_STARTQ_MASK	0x1f
+#define MVPP22_ISR_RXQ_SUB_GROUP_SIZE_MASK	0xf00
+#define MVPP22_ISR_RXQ_SUB_GROUP_SIZE_OFFSET	8
+
+#define MVPP2_ISR_ENABLE_REG(port)		(0x5420 + 4 * (port))
+#define MVPP2_ISR_ENABLE_INTERRUPT(mask)	((mask) & 0xffff)
+#define MVPP2_ISR_DISABLE_INTERRUPT(mask)	(((mask) << 16) & 0xffff0000)
+#define MVPP2_ISR_RX_TX_CAUSE_REG(eth_port)	(0x5480 + 4 * (eth_port))
+#define MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK	0xffff
+#define MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK	0xff0000
+#define MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_OFFSET	16
+
+#define MVPP2_CAUSE_RX_FIFO_OVERRUN_MASK	BIT(24)
+#define MVPP2_CAUSE_FCS_ERR_MASK		BIT(25)
+#define MVPP2_CAUSE_TX_FIFO_UNDERRUN_MASK	BIT(26)
+#define MVPP2_CAUSE_TX_EXCEPTION_SUM_MASK	BIT(29)
+#define MVPP2_CAUSE_RX_EXCEPTION_SUM_MASK	BIT(30)
+#define MVPP2_CAUSE_MISC_SUM_MASK		BIT(31)
+
+#define MVPP2_ISR_RX_TX_MASK_REG(port)		(0x54a0 + 4 * (port))
+#define MVPP2_ISR_PON_RX_TX_MASK_REG		0x54bc
+#define MVPP2_PON_CAUSE_RXQ_OCCUP_DESC_ALL_MASK	0xffff
+#define MVPP2_PON_CAUSE_TXP_OCCUP_DESC_ALL_MASK	0x3fc00000
+#define MVPP2_PON_CAUSE_MISC_SUM_MASK		BIT(31)
+
+#define MV_PP21_ISR_RX_ERR_CAUSE_REG(port)	(0x5500 + 4 * (port))
+#define MV_PP21_ISR_RX_ERR_CAUSE_NONOCC_MASK	0xffff
+#define MV_PP21_ISR_RX_ERR_CAUSE_DESC_RES_MASK	0xffff0000
+#define MV_PP21_ISR_RX_ERR_MASK_REG(port)	(0x5520 + 4 * (port))
+
+#define MV_PP22_ISR_RX_ERR_CAUSE_REG(port)	(0x5500 + 4 * (port))
+#define MV_PP22_ISR_RX_ERR_CAUSE_NONOCC_MASK	0x00ff
+#define MV_PP22_ISR_RX_ERR_CAUSE_DESC_RES_MASK	0xff0000
+#define MV_PP22_ISR_RX_ERR_MASK_REG(port)	(0x5520 + 4 * (port))
+
+#define MV_PP2_ISR_TX_ERR_CAUSE_REG(eth_port)	(0x5540 + 4 * (eth_port))
+#define MV_PP2_ISR_TX_ERR_MASK_REG(eth_port)	(0x5560 + 4 * (eth_port))
+
+#define MVPP2_ISR_MISC_CAUSE_REG		0x55b0
+#define MVPP2_ISR_MISC_MASK_REG			0x55b4
+
+#define MVPP22_ISR_NO_BUF_CAUSE_REG		0x55b8
+#define MVPP22_ISR_NO_BUF_MASK_REG		0x55bc
+
+/* Buffer Manager registers */
+#define MVPP2_BM_POOL_BASE_ADDR_REG(pool)	(0x6000 + ((pool) * 4))
+#define MVPP2_BM_POOL_BASE_ADDR_MASK		0xfffff80
+#define MVPP2_BM_POOL_SIZE_REG(pool)		(0x6040 + ((pool) * 4))
+#define MVPP21_BM_POOL_SIZE_MASK		0xfff0
+#define MVPP21_BM_POOL_SIZE_OFFSET		4
+
+#define MVPP2_BM_POOL_READ_PTR_REG(pool)	(0x6080 + ((pool) * 4))
+#define MVPP21_BM_POOL_READ_PTR_REG		MVPP2_BM_POOL_READ_PTR_REG
+
+#define MVPP21_BM_POOL_GET_READ_PTR_MASK	0xfff0
+#define MVPP2_BM_POOL_PTRS_NUM_REG(pool)	(0x60c0 + ((pool) * 4))
+#define MVPP21_BM_POOL_PTRS_NUM_REG		MVPP2_BM_POOL_PTRS_NUM_REG
+
+#define MVPP21_BM_POOL_PTRS_NUM_MASK		0xfff0
+
+#define MVPP22_BM_POOL_SIZE_MASK		0xfff8
+#define MVPP22_BM_POOL_SIZE_OFFSET		3
+
+/* Use PPV21 Pool Size both for PPV21/PPV22, deliberately ignore PPV22 */
+#define MVPP2_BM_POOL_SIZE_MASK			MVPP21_BM_POOL_SIZE_MASK
+#define MVPP2_BM_POOL_SIZE_OFFSET		MVPP21_BM_POOL_SIZE_OFFSET
+#undef  MVPP22_BM_POOL_SIZE_MASK
+#undef	MVPP22_BM_POOL_SIZE_OFFSET
+
+#define MVPP22_BM_POOL_READ_PTR_REG		MVPP2_BM_POOL_READ_PTR_REG
+#define MVPP22_BM_POOL_GET_READ_PTR_MASK	0xfff8
+#define MVPP22_BM_POOL_PTRS_NUM_REG		MVPP2_BM_POOL_PTRS_NUM_REG
+#define MVPP22_BM_POOL_PTRS_NUM_MASK		0xfff8
+
+#define MVPP2_BM_BPPI_READ_PTR_REG(pool)	(0x6100 + ((pool) * 4))
+#define MVPP2_BM_BPPI_PTRS_NUM_REG(pool)	(0x6140 + ((pool) * 4))
+#define MVPP2_BM_BPPI_PTR_NUM_MASK		0x7ff
+#define MVPP2_BM_BPPI_PREFETCH_FULL_MASK	BIT(16)
+#define MVPP2_BM_POOL_CTRL_REG(pool)		(0x6200 + ((pool) * 4))
+#define MVPP2_BM_START_MASK			BIT(0)
+#define MVPP2_BM_STOP_MASK			BIT(1)
+#define MVPP2_BM_STATE_MASK			BIT(4)
+#define MVPP2_BM_LOW_THRESH_OFFS		8
+#define MVPP2_BM_LOW_THRESH_MASK		0x7f00
+#define MVPP2_BM_LOW_THRESH_VALUE(val)		((val) << \
+						MVPP2_BM_LOW_THRESH_OFFS)
+#define MVPP2_BM_HIGH_THRESH_OFFS		16
+#define MVPP2_BM_HIGH_THRESH_MASK		0x7f0000
+#define MVPP2_BM_HIGH_THRESH_VALUE(val)		((val) << \
+						MVPP2_BM_HIGH_THRESH_OFFS)
+#define MVPP2_BM_INTR_CAUSE_REG(pool)		(0x6240 + ((pool) * 4))
+#define MVPP2_BM_RELEASED_DELAY_MASK		BIT(0)
+#define MVPP2_BM_ALLOC_FAILED_MASK		BIT(1)
+#define MVPP2_BM_BPPE_EMPTY_MASK		BIT(2)
+#define MVPP2_BM_BPPE_FULL_MASK			BIT(3)
+#define MVPP2_BM_AVAILABLE_BP_LOW_MASK		BIT(4)
+#define MVPP2_BM_INTR_MASK_REG(pool)		(0x6280 + ((pool) * 4))
+
+#define MVPP21_BM_UNUSED_PTR_THRESH_REG(pool)	(0x62c0 + ((pool) * 4))
+#define MVPP21_BM_UNUSED_PTR_THRESH_MASK	0xfff0
+#define MVPP22_BM_UNUSED_PTR_THRESH_REG(pool)	(0x62c0 + ((pool) * 4))
+#define MVPP22_BM_UNUSED_PTR_THRESH_MASK	0xfff8
+
+#define MVPP22_BM_POOL_BASE_ADDR_HIGH_REG	0x6310
+#define MVPP22_BM_POOL_BASE_ADDR_HIGH_MASK	0xff
+
+#define MVPP2_BM_PHY_ALLOC_REG(pool)		(0x6400 + ((pool) * 4))
+#define MVPP2_BM_PHY_ALLOC_GRNTD_MASK		BIT(0)
+#define MVPP2_BM_VIRT_ALLOC_REG			0x6440
+
+#define MVPP22_BM_PHY_VIRT_HIGH_ALLOC_REG	0x6444
+#define MVPP22_BM_PHY_HIGH_ALLOC_OFFSET		0
+#define MVPP22_BM_VIRT_HIGH_ALLOC_OFFSET	8
+#define MVPP22_BM_VIRT_HIGH_ALLOC_MASK		0xff00
+
+#define MVPP2_BM_PHY_RLS_REG(pool)		(0x6480 + ((pool) * 4))
+#define MVPP2_BM_PHY_RLS_MC_BUFF_MASK		BIT(0)
+#define MVPP2_BM_PHY_RLS_PRIO_EN_MASK		BIT(1)
+#define MVPP2_BM_PHY_RLS_GRNTD_MASK		BIT(2)
+
+#define MVPP2_BM_VIRT_RLS_REG			0x64c0
+
+#define MVPP21_BM_MC_RLS_REG			0x64c4 /* Not a mixup */
+#define MVPP21_BM_MC_ID_MASK			0xfff
+#define MVPP21_BM_FORCE_RELEASE_MASK		BIT(12)
+
+#define MVPP22_BM_PHY_VIRT_HIGH_RLS_REG		0x64c4 /* Not a mixup */
+
+#define MVPP22_BM_PHY_HIGH_RLS_OFFSET		0
+#define MVPP22_BM_VIRT_HIGH_RLS_OFFST		8
+
+#define MVPP22_BM_MC_RLS_REG			0x64d4 /* Not a mixup */
+#define MVPP22_BM_MC_ID_MASK			0xfff
+#define MVPP22_BM_FORCE_RELEASE_MASK		BIT(12)
+
+#define MVPP2_BM_PRIO_CTRL_REG			0x6800
+
+#define MVPP2_BM_PRIO_IDX_REG			0x6810
+#define MVPP2_BM_PRIO_IDX_BITS			8
+#define MVPP2_BM_PRIO_IDX_MAX			255
+#define MVPP2_BM_PRIO_IDX_MASK			0xff
+
+#define MVPP2_BM_CPU_QSET_REG			0x6814
+
+#define MVPP2_BM_CPU_SHORT_QSET_OFFS		0
+#define MVPP2_BM_CPU_SHORT_QSET_MASK		(0x7f << \
+					MVPP2_BM_CPU_SHORT_QSET_OFFS)
+
+#define MVPP2_BM_CPU_LONG_QSET_OFFS		8
+#define MVPP2_BM_CPU_LONG_QSET_MASK		(0x7f << \
+					MVPP2_BM_CPU_LONG_QSET_OFFS)
+
+#define MVPP2_BM_HWF_QSET_REG			0x6818
+
+#define MVPP2_BM_HWF_SHORT_QSET_OFFS		0
+#define MVPP2_BM_HWF_SHORT_QSET_MASK		(0x7f << \
+					MVPP2_BM_HWF_SHORT_QSET_OFFS)
+
+#define MVPP2_BM_HWF_LONG_QSET_OFFS		8
+#define MVPP2_BM_HWF_LONG_QSET_MASK		(0x7f << \
+					MVPP2_BM_HWF_LONG_QSET_OFFS)
+
+#define MVPP2_BM_QSET_SET_MAX_REG		0x6820
+
+#define MVPP2_BM_QSET_MAX_SHARED_OFFS		0
+#define MVPP2_BM_QSET_MAX_GRNTD_OFFS		16
+
+#define MVPP2_BM_QSET_MAX_SHARED_MASK		(0xffff << \
+					MVPP2_BM_QSET_MAX_SHARED_OFFS)
+#define MVPP2_BM_QSET_MAX_GRNTD_MASK		(0xffff << \
+					MVPP2_BM_QSET_MAX_GRNTD_OFFS)
+
+
+#define MVPP2_BM_QSET_SET_CNTRS_REG		0x6824
+
+/* TX Scheduler registers */
+#define MVPP2_TXP_SCHED_PORT_INDEX_REG		0x8000
+#define MVPP2_TXP_SCHED_Q_CMD_REG		0x8004
+#define MVPP2_TXP_SCHED_ENQ_MASK		0xff
+#define MVPP2_TXP_SCHED_DISQ_OFFSET		8
+#define MVPP2_TXP_SCHED_CMD_1_REG		0x8010
+#define MVPP2_TXP_SCHED_FIXED_PRIO_REG		0x8014
+#define MVPP2_TXP_SCHED_PERIOD_REG		0x8018
+#define MVPP2_TXP_SCHED_MTU_REG			0x801c
+#define MVPP2_TXP_MTU_MAX			0x7FFFF
+#define MVPP2_TXP_SCHED_REFILL_REG		0x8020
+#define MVPP2_TXP_REFILL_TOKENS_OFFS		0
+#define MVPP2_TXP_REFILL_TOKENS_MAX		0x7FFFF
+#define MVPP2_TXP_REFILL_TOKENS_ALL_MASK	0x7ffff
+#define MVPP2_TXP_REFILL_TOKENS_MASK(val)	((val) << \
+					MVPP2_TXP_REFILL_TOKENS_OFFS)
+#define MVPP2_TXP_REFILL_PERIOD_MAX		0x3FF
+#define MVPP2_TXP_REFILL_PERIOD_ALL_MASK	0x3ff00000
+#define MVPP2_TXP_REFILL_PERIOD_MASK(v)		((v) << 20)
+#define MVPP2_TXP_SCHED_TOKEN_SIZE_REG		0x8024
+#define MVPP2_TXP_SCHED_TOKEN_CNTR_REG		0x8028
+#define MVPP2_TXP_TOKEN_SIZE_MAX		0xffffffff
+#define MVPP2_TXQ_SCHED_REFILL_REG(q)		(0x8040 + ((q) << 2))
+#define MVPP2_TXQ_REFILL_TOKENS_OFFS		0
+#define MVPP2_TXQ_REFILL_TOKENS_MAX		0x7FFFF
+#define MVPP2_TXQ_REFILL_TOKENS_ALL_MASK	0x7ffff
+#define MVPP2_TXQ_REFILL_TOKENS_MASK(val)	((val) << \
+					MVPP2_TXQ_REFILL_TOKENS_OFFS)
+#define MVPP2_TXQ_REFILL_PERIOD_MAX		0x3FF
+#define MVPP2_TXQ_REFILL_PERIOD_ALL_MASK	0x3ff00000
+#define MVPP2_TXQ_REFILL_PERIOD_MASK(v)		((v) << 20)
+#define MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(q)	(0x8060 + ((q) << 2))
+#define MVPP2_TXQ_TOKEN_SIZE_MAX		0x7fffffff
+#define MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(q)	(0x8080 + ((q) << 2))
+#define MVPP2_TXQ_TOKEN_CNTR_MAX		0xffffffff
+/* Transmit Queue Arbiter Configuration (TQxAC) */
+#define MVPP2_TXQ_SCHED_WRR_REG(q)		(0x80A0 + ((q) << 2))
+#define MVPP2_TXQ_WRR_WEIGHT_OFFS		0
+#define MVPP2_TXQ_WRR_WEIGHT_MAX		0xFF
+#define MVPP2_TXQ_WRR_WEIGHT_ALL_MASK		(MVPP2_TXQ_WRR_WEIGHT_MAX << \
+					MVPP2_TXQ_WRR_WEIGHT_OFFS)
+#define MVPP2_TXQ_WRR_WEIGHT_MASK(weigth)	((weigth) << \
+					MVPP2_TXQ_WRR_WEIGHT_OFFS)
+#define MVPP2_TXQ_WRR_BYTE_COUNT_OFFS		8
+#define MVPP2_TXQ_WRR_BYTE_COUNT_MASK		(0x3FFFF << \
+					MVPP2_TXQ_WRR_BYTE_COUNT_OFFS)
+
+/* TX general registers */
+#define MVPP2_TX_SNOOP_REG			0x8800
+#define MVPP2_TX_SNOOP_EN_MASK			BIT(0)
+#define MVPP2_TX_SNOOP_EN_MASK			BIT(0)
+#define MVPP22_TX_SNOOP_HWF_EN_MASK		BIT(1)
+
+#define MVPP21_TX_FIFO_THRESH_REG		0x8804
+#define MVPP21_TX_FIFO_THRESH_MASK		0x7ff
+#define MVPP22_TX_FIFO_THRESH_REG(eth_tx_port)	(0x8840 + ((eth_tx_port) << 2))
+#define MVPP22_TX_FIFO_THRESH_MASK		0x3fff
+
+#define MVPP22_TX_FIFO_SIZE_REG(eth_tx_port)	(0x8860 + ((eth_tx_port) << 2))
+#define MVPP22_TX_FIFO_SIZE_MASK		0xf
+
+#define MVPP2_TX_PORT_FLUSH_REG			0x8810
+#define MVPP2_TX_PORT_FLUSH_MASK(port)		(1 << (port))
+
+						/* Same for PPv21/PPv22 */
+#define MVPP2_TX_BAD_FCS_CNTR_REG(eth_tx_port)	(0x8940 + ((eth_tx_port) << 2))
+						/* Same for PPv21/PPv22 */
+#define MVPP2_TX_DROP_CNTR_REG(eth_tx_port)	(0x8980 + ((eth_tx_port) << 2))
+
+
+#define MVPP2_TX_ETH_DSEC_THRESH_REG(eth_tx_port)(0x8a40 + \
+					((eth_tx_port) << 2))
+#define MVPP2_TX_ETH_DSEC_THRESH_MASK		0x7f0
+
+#define MVPP22_TX_EGR_PIPE_DELAY_REG(eth_tx_port)(0x8a80 + \
+					((eth_tx_port) << 2))
+#define MVPP22_TX_EGR_PIPE_DELAY_MASK		0x3fff
+#define MVPP22_TX_PTP_DISPATCH_ENABLE_MASK	BIT(30)
+
+#define MVPP22_TX_PORT_SHORT_HDR_REG		0x8ac0
+#define MVPP22_TX_PORT_SHORT_HDR_MASK		0x7f
+
+/* LMS registers */
+#define MVPP2_SRC_ADDR_MIDDLE			0x24
+#define MVPP2_SRC_ADDR_HIGH			0x28
+#define MVPP2_PHY_AN_CFG0_REG			0x34
+#define MVPP2_PHY_AN_STOP_SMI0_MASK		BIT(7)
+#define MVPP2_MIB_COUNTERS_BASE(port)		(0x1000 + ((port) >> 1) * \
+						0x400 + (port) * 0x400)
+#define MVPP2_MIB_LATE_COLLISION		0x7c
+#define MVPP2_ISR_SUM_MASK_REG			0x220c
+#define MVPP2_MNG_EXTENDED_GLOBAL_CTRL_REG	0x305c
+#define MVPP2_EXT_GLOBAL_CTRL_DEFAULT		0x27
+
+/* Per-port registers */
+#define MVPP2_GMAC_CTRL_0_REG			0x0
+#define MVPP2_GMAC_PORT_EN_MASK			BIT(0)
+#define MVPP2_GMAC_MAX_RX_SIZE_OFFS		2
+#define MVPP2_GMAC_MAX_RX_SIZE_MASK		0x7ffc
+#define MVPP2_GMAC_MIB_CNTR_EN_MASK		BIT(15)
+#define MVPP2_GMAC_CTRL_1_REG			0x4
+#define MVPP2_GMAC_PERIODIC_XON_EN_MASK		BIT(1)
+#define MVPP2_GMAC_GMII_LB_EN_MASK		BIT(5)
+#define MVPP2_GMAC_PCS_LB_EN_BIT		6
+#define MVPP2_GMAC_PCS_LB_EN_MASK		BIT(6)
+#define MVPP2_GMAC_SA_LOW_OFFS			7
+#define MVPP2_GMAC_CTRL_2_REG			0x8
+#define MVPP2_GMAC_INBAND_AN_MASK		BIT(0)
+#define MVPP2_GMAC_PCS_ENABLE_MASK		BIT(3)
+#define MVPP2_GMAC_PORT_RGMII_MASK		BIT(4)
+#define MVPP2_GMAC_PORT_RESET_MASK		BIT(6)
+#define MVPP2_GMAC_AUTONEG_CONFIG		0xc
+#define MVPP2_GMAC_FORCE_LINK_DOWN		BIT(0)
+#define MVPP2_GMAC_FORCE_LINK_PASS		BIT(1)
+#define MVPP2_GMAC_CONFIG_MII_SPEED		BIT(5)
+#define MVPP2_GMAC_CONFIG_GMII_SPEED		BIT(6)
+#define MVPP2_GMAC_AN_SPEED_EN			BIT(7)
+#define MVPP2_GMAC_FC_ADV_EN			BIT(9)
+#define MVPP2_GMAC_CONFIG_FULL_DUPLEX		BIT(12)
+#define MVPP2_GMAC_AN_DUPLEX_EN			BIT(13)
+#define MVPP2_GMAC_PORT_FIFO_CFG_1_REG		0x1c
+#define MVPP2_GMAC_TX_FIFO_MIN_TH_OFFS	6
+#define MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK	0x1fc0
+#define MVPP2_GMAC_TX_FIFO_MIN_TH_MASK(v)	(((v) << 6) & \
+					MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK)
+
+#define MVPP2_CAUSE_TXQ_SENT_DESC_ALL_MASK	0xff
+
+/* The two bytes Marvell header. Either contains a special value used
+ * by Marvell switches when a specific hardware mode is enabled (not
+ * supported by this driver) or is filled automatically by zeroes on
+ * the RX side. Those two bytes being at the front of the Ethernet
+ * header, they allow to have the IP header aligned on a 4 bytes
+ * boundary automatically: the hardware skips those two bytes on its
+ * own.
+ */
+#define MVPP2_MH_SIZE			2
+#define MVPP2_ETH_TYPE_LEN		2
+#define MVPP2_PPPOE_HDR_SIZE		8
+#define MVPP2_VLAN_TAG_LEN		4
+
+/* Lbtd 802.3 type */
+#define MVPP2_IP_LBDT_TYPE		0xfffa
+
+#define MVPP2_CPU_D_CACHE_LINE_SIZE	32
+#define MVPP2_TX_CSUM_MAX_SIZE		9800
+
+/* Timeout constants */
+#define MVPP2_TX_DISABLE_TIMEOUT_MSEC	1000
+#define MVPP2_TX_PENDING_TIMEOUT_MSEC	1000
+
+#define MVPP2_TX_MTU_MAX		0x7ffff
+
+/* Maximum number of T-CONTs of PON port */
+#define MVPP2_MAX_TCONT			16
+
+/* Maximum number of supported ports */
+#define MVPP2_MAX_PORTS			4
+
+/* Maximum number of TXQs used by single port */
+#define MVPP2_MAX_TXQ			8
+
+/* Maximum number of RXQs used by single port */
+#define MVPP2_MAX_RXQ			8
+
+/* Dfault number of RXQs in use */
+#define MVPP2_DEFAULT_RXQ		4
+
+/* Total number of RXQs available to all ports */
+#define MVPP2_RXQ_TOTAL_NUM		(MVPP2_MAX_PORTS * MVPP2_MAX_RXQ)
+
+#define MVPP2_TXQ_TOTAL_NUM		(128/*pon*/ + \
+					MVPP2_MAX_PORTS*MVPP2_MAX_TXQ/*eth*/)
+
+/* Max number of Rx descriptors */
+#define MVPP2_MAX_RXD			1024
+
+/* Max number of Tx descriptors */
+#define MVPP2_MAX_TXD			1024
+
+/* Amount of Tx descriptors that can be reserved at once by CPU */
+#define MVPP2_CPU_DESC_CHUNK		64
+
+/* Max number of Tx descriptors in each aggregated queue */
+#define MVPP2_AGGR_TXQ_SIZE		256
+
+/* Descriptor aligned size */
+#define MVPP2_DESC_ALIGNED_SIZE		32
+#define MVPP2_DESC_Q_ALIGN		512
+
+#define MVPP2_DESCQ_MEM_SIZE(descs)	(descs * MVPP2_DESC_ALIGNED_SIZE + \
+					MVPP2_DESC_Q_ALIGN)
+#define MVPP2_DESCQ_MEM_ALIGN(mem)	(ALIGN(mem, MVPP2_DESC_Q_ALIGN))
+
+/* Descriptor alignment mask */
+#define MVPP2_TX_DESC_ALIGN		(MVPP2_DESC_ALIGNED_SIZE - 1)
+
+/* RX FIFO constants */
+#define MVPP2_RX_FIFO_PORT_DATA_SIZE	0x2000
+#define MVPP2_RX_FIFO_PORT_ATTR_SIZE	0x80
+#define MVPP2_RX_FIFO_PORT_MIN_PKT	0x80
+
+/* RX buffer constants */
+#define MVPP2_SKB_SHINFO_SIZE \
+	SKB_DATA_ALIGN(sizeof(struct skb_shared_info))
+
+#define MVPP2_RX_PKT_SIZE(mtu) \
+	ALIGN((mtu) + MVPP2_MH_SIZE + MVPP2_VLAN_TAG_LEN + \
+	      ETH_HLEN + ETH_FCS_LEN, MVPP2_CPU_D_CACHE_LINE_SIZE)
+
+#define MVPP2_RX_BUF_SIZE(pkt_size)	((pkt_size) + NET_SKB_PAD)
+#define MVPP2_RX_TOTAL_SIZE(buf_size)	((buf_size) + MVPP2_SKB_SHINFO_SIZE)
+#define MVPP2_RX_MAX_PKT_SIZE(total_size) \
+	((total_size) - NET_SKB_PAD - MVPP2_SKB_SHINFO_SIZE)
+
+#define MVPP2_BIT_TO_BYTE(bit)		((bit) / 8)
+
+/* IPv6 max L3 address size */
+#define MVPP2_MAX_L3_ADDR_SIZE		16
+
+/* Port flags */
+#define MVPP2_F_LOOPBACK		BIT(0)
+#define MVPP2_F_IFCAP_NETMAP    BIT(1)
+
+/* Marvell tag types */
+enum mv_pp2x_tag_type {
+	MVPP2_TAG_TYPE_NONE = 0,
+	MVPP2_TAG_TYPE_MH   = 1,
+	MVPP2_TAG_TYPE_DSA  = 2,
+	MVPP2_TAG_TYPE_EDSA = 3,
+	MVPP2_TAG_TYPE_VLAN = 4,
+	MVPP2_TAG_TYPE_LAST = 5
+};
+
+/* Parser constants */
+#ifdef CONFIG_MV_PP2_PALLADIUM
+#define MVPP2_PRS_TCAM_SRAM_SIZE	32
+#else
+#define MVPP2_PRS_TCAM_SRAM_SIZE	256
+#endif
+#define MVPP2_PRS_TCAM_WORDS		6
+#define MVPP2_PRS_SRAM_WORDS		4
+#define MVPP2_PRS_FLOW_ID_SIZE		64
+#define MVPP2_PRS_FLOW_ID_MASK		0x3f
+#define MVPP2_PRS_TCAM_ENTRY_VALID	0
+#define MVPP2_PRS_TCAM_ENTRY_INVALID	1
+#define MVPP2_PRS_TCAM_DSA_TAGGED_BIT	BIT(5)
+#define MVPP2_PRS_IPV4_HEAD		0x40
+#define MVPP2_PRS_IPV4_HEAD_MASK	0xf0
+#define MVPP2_PRS_IPV4_MC		0xe0
+#define MVPP2_PRS_IPV4_MC_MASK		0xf0
+#define MVPP2_PRS_IPV4_BC_MASK		0xff
+#define MVPP2_PRS_IPV4_IHL		0x5
+#define MVPP2_PRS_IPV4_IHL_MASK		0xf
+#define MVPP2_PRS_IPV6_MC		0xff
+#define MVPP2_PRS_IPV6_MC_MASK		0xff
+#define MVPP2_PRS_IPV6_HOP_MASK		0xff
+#define MVPP2_PRS_TCAM_PROTO_MASK	0xff
+#define MVPP2_PRS_TCAM_PROTO_MASK_L	0x3f
+#define MVPP2_PRS_DBL_VLANS_MAX		100
+
+/* Tcam structure:
+ * - lookup ID - 4 bits
+ * - port ID - 1 byte
+ * - additional information - 1 byte
+ * - header data - 8 bytes
+ * The fields are represented by MVPP2_PRS_TCAM_DATA_REG(5)->(0).
+ */
+#define MVPP2_PRS_AI_BITS			8
+#define MVPP2_PRS_PORT_MASK			0xff
+#define MVPP2_PRS_LU_MASK			0xf
+#define MVPP2_PRS_TCAM_AI_BYTE			16
+#define MVPP2_PRS_TCAM_PORT_BYTE		17
+#define MVPP2_PRS_TCAM_LU_BYTE			20
+#define MVPP2_PRS_TCAM_EN_OFFS(offs)		((offs) + 2)
+#define MVPP2_PRS_TCAM_INV_WORD			5
+#define MVPP2_PRS_TCAM_INV_MASK			BIT(31)
+
+/* Tcam entries ID */
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+#define MVPP2_PE_DROP_ALL		0
+#define MVPP2_PE_FIRST_FREE_TID		1
+#define MVPP2_PE_LAST_FREE_TID		(MVPP2_PRS_TCAM_SRAM_SIZE - 31)
+#define MVPP2_PE_IP6_EXT_PROTO_UN	(MVPP2_PRS_TCAM_SRAM_SIZE - 30)
+#define MVPP2_PE_MAC_MC_IP6		(MVPP2_PRS_TCAM_SRAM_SIZE - 29)
+#define MVPP2_PE_IP6_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 28)
+#define MVPP2_PE_IP4_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 27)
+#define MVPP2_PE_LAST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 26)
+#define MVPP2_PE_FIRST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 19)
+#define MVPP2_PE_EDSA_TAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 18)
+#define MVPP2_PE_EDSA_UNTAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 17)
+#define MVPP2_PE_DSA_TAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 16)
+#define MVPP2_PE_DSA_UNTAGGED		(MVPP2_PRS_TCAM_SRAM_SIZE - 15)
+#define MVPP2_PE_ETYPE_EDSA_TAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 14)
+#define MVPP2_PE_ETYPE_EDSA_UNTAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 13)
+#define MVPP2_PE_ETYPE_DSA_TAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 12)
+#define MVPP2_PE_ETYPE_DSA_UNTAGGED	(MVPP2_PRS_TCAM_SRAM_SIZE - 11)
+#define MVPP2_PE_MH_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 10)
+#define MVPP2_PE_DSA_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 9)
+#define MVPP2_PE_IP6_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 8)
+#define MVPP2_PE_IP4_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 7)
+#define MVPP2_PE_ETH_TYPE_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 6)
+#define MVPP2_PE_VLAN_DBL		(MVPP2_PRS_TCAM_SRAM_SIZE - 5)
+#define MVPP2_PE_VLAN_NONE		(MVPP2_PRS_TCAM_SRAM_SIZE - 4)
+#define MVPP2_PE_MAC_MC_ALL		(MVPP2_PRS_TCAM_SRAM_SIZE - 3)
+#define MVPP2_PE_MAC_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 2)
+#define MVPP2_PE_MAC_NON_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 1)
+#else
+#define MVPP2_PE_DROP_ALL		0
+#define MVPP2_PE_FIRST_FREE_TID		1
+#define MVPP2_PE_LAST_FREE_TID		(MVPP2_PRS_TCAM_SRAM_SIZE - 14)
+
+#define MVPP2_PE_IP4_PROTO_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 13)
+#define MVPP2_PE_IP4_ADDR_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 12)
+
+#define MVPP2_PE_LAST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 11)
+#define MVPP2_PE_FIRST_DEFAULT_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 8)
+#define MVPP2_PE_MH_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 7)
+#define MVPP2_PE_DSA_DEFAULT		(MVPP2_PRS_TCAM_SRAM_SIZE - 6)
+#define MVPP2_PE_ETH_TYPE_UN		(MVPP2_PRS_TCAM_SRAM_SIZE - 5)
+#define MVPP2_PE_VLAN_NONE		(MVPP2_PRS_TCAM_SRAM_SIZE - 4)
+#define MVPP2_PE_MAC_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 3)
+#define MVPP2_PE_MAC_NON_PROMISCUOUS	(MVPP2_PRS_TCAM_SRAM_SIZE - 2)
+#define MVPP2_PE_PALLADIUM_DUMMY_FLOW	(MVPP2_PRS_TCAM_SRAM_SIZE - 1)
+
+#define MVPP2_PE_IP6_EXT_PROTO_UN	(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_MAC_MC_IP6		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_IP6_ADDR_UN		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_EDSA_TAGGED		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_EDSA_UNTAGGED		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_DSA_TAGGED		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_DSA_UNTAGGED		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_ETYPE_EDSA_TAGGED	(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_ETYPE_EDSA_UNTAGGED	(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_ETYPE_DSA_TAGGED	(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_ETYPE_DSA_UNTAGGED	(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_IP6_PROTO_UN		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_VLAN_DBL		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+#define MVPP2_PE_MAC_MC_ALL		(MVPP2_PE_PALLADIUM_DUMMY_FLOW)
+
+#endif
+
+/* Sram structure
+ * The fields are represented by MVPP2_PRS_TCAM_DATA_REG(3)->(0).
+ */
+#define MVPP2_PRS_SRAM_RI_OFFS			0
+#define MVPP2_PRS_SRAM_RI_WORD			0
+#define MVPP2_PRS_SRAM_RI_BITS			32
+#define MVPP2_PRS_SRAM_RI_CTRL_OFFS		32
+#define MVPP2_PRS_SRAM_RI_CTRL_WORD		1
+#define MVPP2_PRS_SRAM_RI_CTRL_BITS		32
+#define MVPP2_PRS_SRAM_SHIFT_OFFS		64
+#define MVPP2_PRS_SRAM_SHIFT_BITS		8
+#define MVPP2_PRS_SRAM_SHIFT_SIGN_BIT		72
+#define MVPP2_PRS_SRAM_UDF_OFFS			73
+#define MVPP2_PRS_SRAM_UDF_BITS			8
+#define MVPP2_PRS_SRAM_UDF_MASK			0xff
+#define MVPP2_PRS_SRAM_UDF_SIGN_BIT		81
+#define MVPP2_PRS_SRAM_UDF_TYPE_OFFS		82
+#define MVPP2_PRS_SRAM_UDF_TYPE_MASK		0x7
+#define MVPP2_PRS_SRAM_UDF_TYPE_L3		1
+#define MVPP2_PRS_SRAM_UDF_TYPE_L4		4
+#define MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS	85
+#define MVPP2_PRS_SRAM_OP_SEL_SHIFT_BITS	2
+#define MVPP2_PRS_SRAM_OP_SEL_BITS		5
+#define MVPP2_PRS_SRAM_OP_SEL_SHIFT_MASK	0x3
+#define MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD		1
+#define MVPP2_PRS_SRAM_OP_SEL_SHIFT_IP4_ADD	2
+#define MVPP2_PRS_SRAM_OP_SEL_SHIFT_IP6_ADD	3
+#define MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS		87
+#define MVPP2_PRS_SRAM_OP_SEL_UDF_BITS		2
+#define MVPP2_PRS_SRAM_OP_SEL_UDF_MASK		0x3
+#define MVPP2_PRS_SRAM_OP_SEL_UDF_ADD		0
+#define MVPP2_PRS_SRAM_OP_SEL_UDF_IP4_ADD	2
+#define MVPP2_PRS_SRAM_OP_SEL_UDF_IP6_ADD	3
+#define MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS		89
+#define MVPP2_PRS_SRAM_AI_OFFS			90
+#define MVPP2_PRS_SRAM_AI_CTRL_OFFS		98
+#define MVPP2_PRS_SRAM_AI_CTRL_BITS		8
+#define MVPP2_PRS_SRAM_AI_MASK			0xff
+#define MVPP2_PRS_SRAM_NEXT_LU_OFFS		106
+#define MVPP2_PRS_SRAM_NEXT_LU_MASK		0xf
+#define MVPP2_PRS_SRAM_LU_DONE_BIT		110
+#define MVPP2_PRS_SRAM_LU_GEN_BIT		111
+
+/* Sram result info bits assignment */
+#define MVPP2_PRS_RI_MAC_ME_MASK		0x1
+#define MVPP2_PRS_RI_DSA_MASK			0x2
+#define MVPP2_PRS_RI_VLAN_OFFS			2
+#define MVPP2_PRS_RI_VLAN_MASK			0xc
+#define MVPP2_PRS_RI_VLAN_NONE			0x0
+#define MVPP2_PRS_RI_VLAN_SINGLE		BIT(2)
+#define MVPP2_PRS_RI_VLAN_DOUBLE		BIT(3)
+#define MVPP2_PRS_RI_VLAN_TRIPLE		(BIT(2) | BIT(3))
+#define MVPP2_PRS_RI_CPU_CODE_MASK		0x70
+#define MVPP2_PRS_RI_CPU_CODE_RX_SPEC		BIT(4)
+#define MVPP2_PRS_RI_L2_CAST_OFFS		9
+#define MVPP2_PRS_RI_L2_CAST_MASK		0x600
+#define MVPP2_PRS_RI_L2_UCAST			0x0
+#define MVPP2_PRS_RI_L2_MCAST			BIT(9)
+#define MVPP2_PRS_RI_L2_BCAST			BIT(10)
+#define MVPP2_PRS_RI_PPPOE_MASK			0x800
+#define MVPP2_PRS_RI_L3_PROTO_MASK		0x7000
+#define MVPP2_PRS_RI_L3_UN			0x0
+#define MVPP2_PRS_RI_L3_IP4			BIT(12)
+#define MVPP2_PRS_RI_L3_IP4_OPT			BIT(13)
+#define MVPP2_PRS_RI_L3_IP4_OTHER		(BIT(12) | BIT(13))
+#define MVPP2_PRS_RI_L3_IP6			BIT(14)
+#define MVPP2_PRS_RI_L3_IP6_EXT			(BIT(12) | BIT(14))
+#define MVPP2_PRS_RI_L3_ARP			(BIT(13) | BIT(14))
+#define MVPP2_PRS_RI_L3_ADDR_MASK		0x18000
+#define MVPP2_PRS_RI_L3_UCAST			0x0
+#define MVPP2_PRS_RI_L3_MCAST			BIT(15)
+#define MVPP2_PRS_RI_L3_BCAST			(BIT(15) | BIT(16))
+#define MVPP2_PRS_RI_IP_FRAG_MASK		0x20000
+#define MVPP2_PRS_RI_IP_FRAG_TRUE		BIT(17)
+#define MVPP2_PRS_RI_IP_FRAG_FALSE		0x0
+#define MVPP2_PRS_RI_UDF3_MASK			0x300000
+#define MVPP2_PRS_RI_UDF3_RX_SPECIAL		BIT(21)
+#define MVPP2_PRS_RI_L4_PROTO_MASK		0x1c00000
+#define MVPP2_PRS_RI_L4_TCP			BIT(22)
+#define MVPP2_PRS_RI_L4_UDP			BIT(23)
+#define MVPP2_PRS_RI_L4_OTHER			(BIT(22) | BIT(23))
+#define MVPP2_PRS_RI_UDF7_MASK			0x60000000
+#define MVPP2_PRS_RI_UDF7_IP6_LITE		BIT(29)
+#define MVPP2_PRS_RI_DROP_MASK			0x80000000
+
+/* Sram additional info bits assignment */
+#define MVPP2_PRS_IPV4_DIP_AI_BIT		BIT(0)
+#define MVPP2_PRS_IPV6_NO_EXT_AI_BIT		BIT(0)
+#define MVPP2_PRS_IPV6_EXT_AI_BIT		BIT(1)
+#define MVPP2_PRS_IPV6_EXT_AH_AI_BIT		BIT(2)
+#define MVPP2_PRS_IPV6_EXT_AH_LEN_AI_BIT	BIT(3)
+#define MVPP2_PRS_IPV6_EXT_AH_L4_AI_BIT		BIT(4)
+#define MVPP2_PRS_SINGLE_VLAN_AI		0
+#define MVPP2_PRS_DBL_VLAN_AI_BIT		BIT(7)
+
+#define MVPP2_PRS_SRAM_BIT_TO_BYTE(_bit_)	HW_BYTE_OFFS((_bit_) / 8)
+#define MVPP2_PRS_SRAM_SHIFT_MASK		((1 << \
+					MVPP2_PRS_SRAM_SHIFT_BITS) - 1)
+
+/* DSA/EDSA type */
+#define MVPP2_PRS_TAGGED		true
+#define MVPP2_PRS_UNTAGGED		false
+#define MVPP2_PRS_EDSA			true
+#define MVPP2_PRS_DSA			false
+
+/* lkpid table structure	*/
+#define MVPP2_FLOWID_RXQ		0
+#define MVPP2_FLOWID_RXQ_BITS		8
+#define MVPP2_FLOWID_RXQ_MASK		(((1 << \
+			MVPP2_FLOWID_RXQ_BITS) - 1) << MVPP2_FLOWID_RXQ)
+
+#define MVPP2_FLOWID_MODE		8
+#define MVPP2_FLOWID_MODE_BITS		8
+#define MVPP2_FLOWID_MODE_MASK		(((1 << \
+			MVPP2_FLOWID_MODE_BITS) - 1) << MVPP2_FLOWID_MODE)
+#define MVPP2_FLOWID_MODE_MAX		((1 << MVPP2_FLOWID_MODE_BITS) - 1)
+
+#define MVPP2_FLOWID_FLOW		16
+#define MVPP2_FLOWID_FLOW_BITS		9
+#define MVPP2_FLOWID_FLOW_MASK		(((1 << \
+			MVPP2_FLOWID_FLOW_BITS) - 1) << MVPP2_FLOWID_FLOW)
+
+#define MVPP2_FLOWID_EN			25 /*one bit */
+#define MVPP2_FLOWID_EN_MASK		(1 << MVPP2_FLOWID_EN)
+
+/* flow table structure */
+#define MVPP2_FLOW_TBL_SIZE		512
+/*-------------------------  DWORD 0  --------------------------------- */
+#define MVPP2_FLOW_LAST			0
+#define MVPP2_FLOW_LAST_MASK		1 /*one bit*/
+
+#define MVPP2_FLOW_ENGINE		1
+#define MVPP2_FLOW_ENGINE_BITS		3
+#define MVPP2_FLOW_ENGINE_MASK		(((1 << \
+			MVPP2_FLOW_ENGINE_BITS) - 1) << MVPP2_FLOW_ENGINE)
+#define MVPP2_FLOW_ENGINE_MAX		7 /* valid value 1 - 7 */
+
+#define MVPP2_FLOW_PORT_ID		4
+#define MVPP2_FLOW_PORT_ID_BITS		8
+#define MVPP2_FLOW_PORT_ID_MASK		(((1 << \
+			MVPP2_FLOW_PORT_ID_BITS) - 1) << MVPP2_FLOW_PORT_ID)
+#define MVPP2_FLOW_PORT_ID_MAX		((1 << MVPP2_FLOW_PORT_ID_BITS) - 1)
+
+#define MVPP2_FLOW_PORT_TYPE		12
+#define MVPP2_FLOW_PORT_TYPE_BITS	2
+#define MVPP2_FLOW_PORT_TYPE_MASK	(((1 << \
+		MVPP2_FLOW_PORT_TYPE_BITS) - 1) << MVPP2_FLOW_PORT_TYPE)
+#define MVPP2_FLOW_PORT_TYPE_MAX	2 /* valid value 0 - 2 */
+
+#define MVPP2_FLOW_PPPOE		14
+#define MVPP2_FLOW_PPPOE_BITS		2
+#define MVPP2_FLOW_PPPOE_MASK		(((1 << \
+			MVPP2_FLOW_PPPOE_BITS) - 1) << MVPP2_FLOW_PPPOE)
+#define MVPP2_FLOW_PPPOE_MAX		2 /* valid value 0 - 2 */
+
+#define MVPP2_FLOW_VLAN			16
+#define MVPP2_FLOW_VLAN_BITS		3
+#define MVPP2_FLOW_VLAN_MASK		(((1 << \
+			MVPP2_FLOW_VLAN_BITS) - 1) << MVPP2_FLOW_VLAN)
+#define MVPP2_FLOW_VLAN_MAX		((1 << MVPP2_FLOW_VLAN_BITS) - 1)
+
+#define MVPP2_FLOW_MACME		19
+#define MVPP2_FLOW_MACME_BITS		2
+#define MVPP2_FLOW_MACME_MASK		(((1 << \
+			MVPP2_FLOW_MACME_BITS) - 1) << MVPP2_FLOW_MACME)
+#define MVPP2_FLOW_MACME_MAX		2 /* valid value 0 - 2 */
+
+#define MVPP2_FLOW_UDF7			21
+#define MVPP2_FLOW_UDF7_BITS		2
+#define MVPP2_FLOW_UDF7_MASK		(((1 << \
+			MVPP2_FLOW_UDF7_BITS) - 1) << MVPP2_FLOW_UDF7)
+#define MVPP2_FLOW_UDF7_MAX		((1 << MVPP2_FLOW_UDF7_BITS) - 1)
+
+#define MVPP2_FLOW_PORT_ID_SEL		23
+#define MVPP2_FLOW_PORT_ID_SEL_MASK	(1 << MVPP2_FLOW_PORT_ID_SEL)
+
+/*-----------------------  DWORD 1  ------------------------------------ */
+
+#define MVPP2_FLOW_FIELDS_NUM		0
+#define MVPP2_FLOW_FIELDS_NUM_BITS	3
+#define MVPP2_FLOW_FIELDS_NUM_MASK	(((1 << \
+		MVPP2_FLOW_FIELDS_NUM_BITS) - 1) << MVPP2_FLOW_FIELDS_NUM)
+#define MVPP2_FLOW_FIELDS_NUM_MAX	4 /*valid vaue 0 - 4 */
+
+#define MVPP2_FLOW_LKP_TYPE		3
+#define MVPP2_FLOW_LKP_TYPE_BITS	6
+#define MVPP2_FLOW_LKP_TYPE_MASK	(((1 << \
+		MVPP2_FLOW_LKP_TYPE_BITS) - 1) << MVPP2_FLOW_LKP_TYPE)
+#define MVPP2_FLOW_LKP_TYPE_MAX		((1 << MVPP2_FLOW_LKP_TYPE_BITS) - 1)
+
+#define MVPP2_FLOW_FIELD_PRIO		9
+#define MVPP2_FLOW_FIELD_PRIO_BITS	6
+#define MVPP2_FLOW_FIELD_PRIO_MASK	(((1 << \
+		MVPP2_FLOW_FIELD_PRIO_BITS) - 1) << MVPP2_FLOW_FIELD_PRIO)
+#define MVPP2_FLOW_FIELD_PRIO_MAX	((1 << MVPP2_FLOW_FIELD_PRIO_BITS) - 1)
+
+#define MVPP2_FLOW_SEQ_CTRL		15
+#define MVPP2_FLOW_SEQ_CTRL_BITS	3
+#define MVPP2_FLOW_SEQ_CTRL_MASK	(((1 << \
+		MVPP2_FLOW_SEQ_CTRL_BITS) - 1) << MVPP2_FLOW_SEQ_CTRL)
+#define MVPP2_FLOW_SEQ_CTRL_MAX		4
+
+/*-------------------------  DWORD 2  ---------------------------------- */
+#define MVPP2_FLOW_FIELD0_ID		0
+#define MVPP2_FLOW_FIELD1_ID		6
+#define MVPP2_FLOW_FIELD2_ID		12
+#define MVPP2_FLOW_FIELD3_ID		18
+
+#define MVPP2_FLOW_FIELD_ID_BITS	6
+#define MVPP2_FLOW_FIELD_ID(num)	(MVPP2_FLOW_FIELD0_ID + \
+					(MVPP2_FLOW_FIELD_ID_BITS * (num)))
+#define MVPP2_FLOW_FIELD_MASK(num)	(((1 << \
+	MVPP2_FLOW_FIELD_ID_BITS) - 1) << (MVPP2_FLOW_FIELD_ID_BITS * (num)))
+#define MVPP2_FLOW_FIELD_MAX		((1 << MVPP2_FLOW_FIELD_ID_BITS) - 1)
+
+/* lookup id attribute define */
+#define MVPP2_PRS_FL_ATTR_VLAN_BIT	BIT(0)
+#define MVPP2_PRS_FL_ATTR_IP4_BIT	BIT(1)
+#define MVPP2_PRS_FL_ATTR_IP6_BIT	BIT(2)
+#define MVPP2_PRS_FL_ATTR_ARP_BIT	BIT(3)
+#define MVPP2_PRS_FL_ATTR_FRAG_BIT	BIT(4)
+#define MVPP2_PRS_FL_ATTR_TCP_BIT	BIT(5)
+#define MVPP2_PRS_FL_ATTR_UDP_BIT	BIT(6)
+
+/* PP22 RSS Registers */
+#define MVPP22_RSS_IDX_REG			0x1500
+#define MVPP22_RSS_IDX_ENTRY_NUM_OFF		0
+#define MVPP22_RSS_IDX_ENTRY_NUM_MASK		0x1F
+#define MVPP22_RSS_IDX_TBL_NUM_OFF		8
+#define MVPP22_RSS_IDX_TBL_NUM_MASK		0x700
+#define MVPP22_RSS_IDX_RXQ_NUM_OFF		16
+#define MVPP22_RSS_IDX_RXQ_NUM_MASK		0xFF0000
+#define MVPP22_RSS_RXQ2RSS_TBL_REG		0x1504
+#define MVPP22_RSS_RXQ2RSS_TBL_POINT_OFF	0
+#define MVPP22_RSS_RXQ2RSS_TBL_POINT_MASK	0x7
+#define MVPP22_RSS_TBL_ENTRY_REG		0x1508
+#define MVPP22_RSS_TBL_ENTRY_OFF		0
+#define MVPP22_RSS_TBL_ENTRY_MASK		0xFF
+#define MVPP22_RSS_WIDTH_REG			0x150c
+#define MVPP22_RSS_WIDTH_OFF			0
+#define MVPP22_RSS_WIDTH_MASK			0xF
+#define MVPP22_RSS_HASH_SEL_REG			0x1510
+#define MVPP22_RSS_HASH_SEL_OFF			0
+#define MVPP22_RSS_HASH_SEL_MASK		0x1
+/* RSS consant */
+#define MVPP22_RSS_TBL_NUM			8
+#define MVPP22_RSS_TBL_LINE_NUM			32
+#define MVPP22_RSS_WIDTH_MAX			8
+
+/* MAC entries, shadow udf */
+enum mv_pp2x_prs_udf {
+	MVPP2_PRS_UDF_MAC_DEF,
+	MVPP2_PRS_UDF_MAC_RANGE,
+	MVPP2_PRS_UDF_L2_DEF,
+	MVPP2_PRS_UDF_L2_DEF_COPY,
+	MVPP2_PRS_UDF_L2_USER,
+};
+
+/* Lookup ID */
+enum mv_pp2x_prs_lookup {
+	MVPP2_PRS_LU_MH,
+	MVPP2_PRS_LU_MAC,
+	MVPP2_PRS_LU_DSA,
+	MVPP2_PRS_LU_VLAN,
+	MVPP2_PRS_LU_L2,
+	MVPP2_PRS_LU_PPPOE,
+	MVPP2_PRS_LU_IP4,
+	MVPP2_PRS_LU_IP6,
+	MVPP2_PRS_LU_FLOWS,
+	MVPP2_PRS_LU_LAST,
+};
+
+/* L3 cast enum */
+enum mv_pp2x_prs_l3_cast {
+	MVPP2_PRS_L3_UNI_CAST,
+	MVPP2_PRS_L3_MULTI_CAST,
+	MVPP2_PRS_L3_BROAD_CAST
+};
+
+/* Packet flow ID */
+enum mv_pp2x_prs_flow {
+	MVPP2_PRS_FL_START = 8,
+	MVPP2_PRS_FL_IP4_TCP_NF_UNTAG = MVPP2_PRS_FL_START,
+	MVPP2_PRS_FL_IP4_UDP_NF_UNTAG,
+	MVPP2_PRS_FL_IP4_TCP_NF_TAG,
+	MVPP2_PRS_FL_IP4_UDP_NF_TAG,
+	MVPP2_PRS_FL_IP6_TCP_NF_UNTAG,
+	MVPP2_PRS_FL_IP6_UDP_NF_UNTAG,
+	MVPP2_PRS_FL_IP6_TCP_NF_TAG,
+	MVPP2_PRS_FL_IP6_UDP_NF_TAG,
+	MVPP2_PRS_FL_IP4_TCP_FRAG_UNTAG,
+	MVPP2_PRS_FL_IP4_UDP_FRAG_UNTAG,
+	MVPP2_PRS_FL_IP4_TCP_FRAG_TAG,
+	MVPP2_PRS_FL_IP4_UDP_FRAG_TAG,
+	MVPP2_PRS_FL_IP6_TCP_FRAG_UNTAG,
+	MVPP2_PRS_FL_IP6_UDP_FRAG_UNTAG,
+	MVPP2_PRS_FL_IP6_TCP_FRAG_TAG,
+	MVPP2_PRS_FL_IP6_UDP_FRAG_TAG,
+	MVPP2_PRS_FL_IP4_UNTAG, /* non-TCP, non-UDP, same for below */
+	MVPP2_PRS_FL_IP4_TAG,
+	MVPP2_PRS_FL_IP6_UNTAG,
+	MVPP2_PRS_FL_IP6_TAG,
+	MVPP2_PRS_FL_NON_IP_UNTAG,
+	MVPP2_PRS_FL_NON_IP_TAG,
+	MVPP2_PRS_FL_LAST,
+	MVPP2_PRS_FL_TCAM_NUM = 52,	/* The parser TCAM lines needed to
+					*generate flow ID
+					*/
+};
+
+enum mv_pp2x_cls_engine_num {
+	MVPP2_CLS_ENGINE_C2 = 1,
+	MVPP2_CLS_ENGINE_C3A,
+	MVPP2_CLS_ENGINE_C3B,
+	MVPP2_CLS_ENGINE_C4,
+	MVPP2_CLS_ENGINE_C3HA = 6,
+	MVPP2_CLS_ENGINE_C3HB,
+};
+
+enum mv_pp2x_cls_lkp_type {
+	MVPP2_CLS_LKP_HASH = 0,
+	MVPP2_CLS_LKP_VLAN_PRI,
+	MVPP2_CLS_LKP_DSCP_PRI,
+	MVPP2_CLS_LKP_DEFAULT,
+	MVPP2_CLS_LKP_MAX,
+};
+
+enum mv_pp2x_cls_fl_pri {
+	MVPP2_CLS_FL_COS_PRI = 0,
+	MVPP2_CLS_FL_RSS_PRI,
+};
+
+enum mv_pp2x_cls_filed_id {
+	MVPP2_CLS_FIELD_IP4SA = 0x10,
+	MVPP2_CLS_FIELD_IP4DA = 0x11,
+	MVPP2_CLS_FIELD_IP6SA = 0x17,
+	MVPP2_CLS_FIELD_IP6DA = 0x18,
+	MVPP2_CLS_FIELD_L4SIP = 0x1D,
+	MVPP2_CLS_FIELD_L4DIP = 0x1E,
+};
+
+enum mv_pp2x_cos_type {
+	MVPP2_COS_TYPE_DEF = 0,
+	MVPP2_COS_TYPE_VLAN,
+	MVPP2_COS_TYPE_DSCP,
+};
+
+enum mv_pp2x_rss_hash_mode {
+	MVPP2_RSS_HASH_2T = 0,
+	MVPP2_RSS_HASH_5T,
+};
+
+struct mv_pp2x_prs_result_info {
+	u32 ri;
+	u32 ri_mask;
+};
+
+struct mv_pp2x_prs_flow_id {
+	u32 flow_id;
+	struct mv_pp2x_prs_result_info prs_result;
+};
+
+/* Classifier constants */
+#define MVPP2_CLS_FLOWS_TBL_SIZE	512
+#define MVPP2_CLS_FLOWS_TBL_DATA_WORDS	3
+#define MVPP2_CLS_FLOWS_TBL_FIELDS_MAX	4
+
+#define MVPP2_CLS_LKP_TBL_SIZE		64
+
+/* BM cookie (32 bits) definition */
+#define MVPP2_BM_COOKIE_POOL_OFFS	8
+#define MVPP2_BM_COOKIE_CPU_OFFS	24
+
+/* BM short pool packet size
+ * These value assure that for SWF the total number
+ * of bytes allocated for each buffer will be 512
+ */
+#define MVPP2_BM_SHORT_PKT_SIZE		MVPP2_RX_MAX_PKT_SIZE(512)
+#define MVPP2_BM_LONG_PKT_SIZE		MVPP2_RX_MAX_PKT_SIZE(2048)
+#define MVPP2_BM_JUMBO_PKT_SIZE		9600 /*FIXME: What is max. size ? */
+
+enum mv_pp2x_bm_pool_log_num {
+	MVPP2_BM_SWF_SHORT_POOL,
+	MVPP2_BM_SWF_LONG_POOL,
+	MVPP2_BM_SWF_JUMBO_POOL,
+	MVPP2_BM_SWF_POOL_OUT_OF_RANGE
+};
+
+/* The mv_pp2x_tx_desc and mv_pp2x_rx_desc structures describe the
+ * layout of the transmit and reception DMA descriptors, and their
+ * layout is therefore defined by the hardware design
+ */
+
+#define MVPP2_TXD_L3_OFF_SHIFT		0
+#define MVPP2_TXD_IP_HLEN_SHIFT		8
+#define MVPP2_TXD_L4_CSUM_FRAG		BIT(13)
+#define MVPP2_TXD_L4_CSUM_NOT		BIT(14)
+#define MVPP2_TXD_IP_CSUM_DISABLE	BIT(15)
+#define MVPP2_TXD_PADDING_DISABLE	BIT(23)
+#define MVPP2_TXD_L4_UDP		BIT(24)
+#define MVPP2_TXD_L3_IP6		BIT(26)
+#define MVPP2_TXD_L_DESC		BIT(28)
+#define MVPP2_TXD_F_DESC		BIT(29)
+
+#define MVPP2_RXD_ERR_SUMMARY		BIT(15)
+#define MVPP2_RXD_ERR_CODE_MASK		(BIT(13) | BIT(14))
+#define MVPP2_RXD_ERR_CRC		0x0
+#define MVPP2_RXD_ERR_OVERRUN		BIT(13)
+#define MVPP2_RXD_ERR_RESOURCE		(BIT(13) | BIT(14))
+#define MVPP2_RXD_BM_POOL_ID_OFFS	16
+#define MVPP2_RXD_BM_POOL_ID_MASK	(BIT(16) | BIT(17) | BIT(18))
+#define MVPP2_RXD_HWF_SYNC		BIT(21)
+#define MVPP2_RXD_L4_CSUM_OK		BIT(22)
+#define MVPP2_RXD_IP4_HEADER_ERR	BIT(24)
+#define MVPP2_RXD_L4_TCP		BIT(25)
+#define MVPP2_RXD_L4_UDP		BIT(26)
+#define MVPP2_RXD_L3_IP4		BIT(28)
+#define MVPP2_RXD_L3_IP6		BIT(30)
+#define MVPP2_RXD_BUF_HDR		BIT(31)
+/* Sub fields of "parserInfo" field */
+#define MVPP2_RXD_LKP_ID_OFFS		0
+#define MVPP2_RXD_LKP_ID_BITS		6
+#define MVPP2_RXD_LKP_ID_MASK		(((1 << \
+		MVPP2_RXD_LKP_ID_BITS) - 1) << MVPP2_RXD_LKP_ID_OFFS)
+#define MVPP2_RXD_CPU_CODE_OFFS		6
+#define MVPP2_RXD_CPU_CODE_BITS		3
+#define MVPP2_RXD_CPU_CODE_MASK		(((1 << \
+		MVPP2_RXD_CPU_CODE_BITS) - 1) << MVPP2_RXD_CPU_CODE_OFFS)
+#define MVPP2_RXD_PPPOE_BIT		9
+#define MVPP2_RXD_PPPOE_MASK		(1 << MVPP2_RXD_PPPOE_BIT)
+#define MVPP2_RXD_L3_CAST_OFFS		10
+#define MVPP2_RXD_L3_CAST_BITS		2
+#define MVPP2_RXD_L3_CAST_MASK		(((1 << \
+		MVPP2_RXD_L3_CAST_BITS) - 1) << MVPP2_RXD_L3_CAST_OFFS)
+#define MVPP2_RXD_L2_CAST_OFFS		12
+#define MVPP2_RXD_L2_CAST_BITS		2
+#define MVPP2_RXD_L2_CAST_MASK		(((1 << \
+		MVPP2_RXD_L2_CAST_BITS) - 1) << MVPP2_RXD_L2_CAST_OFFS)
+#define MVPP2_RXD_VLAN_INFO_OFFS	14
+#define MVPP2_RXD_VLAN_INFO_BITS	2
+#define MVPP2_RXD_VLAN_INFO_MASK	(((1 << \
+		MVPP2_RXD_VLAN_INFO_BITS) - 1) << MVPP2_RXD_VLAN_INFO_OFFS)
+/* Bits of "bmQset" field */
+#define MVPP2_RXD_BUFF_QSET_NUM_OFFS	0
+#define MVPP2_RXD_BUFF_QSET_NUM_MASK	(0x7f << MVPP2_RXD_BUFF_QSET_NUM_OFFS)
+#define MVPP2_RXD_BUFF_TYPE_OFFS	7
+#define MVPP2_RXD_BUFF_TYPE_MASK	(0x1 << MVPP2_RXD_BUFF_TYPE_OFFS)
+/* Bits of "status" field */
+#define MVPP2_RXD_L3_OFFSET_OFFS	0
+#define MVPP2_RXD_L3_OFFSET_MASK	(0x7F << MVPP2_RXD_L3_OFFSET_OFFS)
+#define MVPP2_RXD_IP_HLEN_OFFS		8
+#define MVPP2_RXD_IP_HLEN_MASK		(0x1F << MVPP2_RXD_IP_HLEN_OFFS)
+#define MVPP2_RXD_ES_BIT		15
+#define MVPP2_RXD_ES_MASK		(1 << MVPP2_RXD_ES_BIT)
+#define MVPP2_RXD_HWF_SYNC_BIT		21
+#define MVPP2_RXD_HWF_SYNC_MASK		(1 << MVPP2_RXD_HWF_SYNC_BIT)
+#define MVPP2_RXD_L4_CHK_OK_BIT		22
+#define MVPP2_RXD_L4_CHK_OK_MASK	(1 << MVPP2_RXD_L4_CHK_OK_BIT)
+#define MVPP2_RXD_IP_FRAG_BIT		23
+#define MVPP2_RXD_IP_FRAG_MASK		(1 << MVPP2_RXD_IP_FRAG_BIT)
+#define MVPP2_RXD_IP4_HEADER_ERR_BIT	24
+#define MVPP2_RXD_IP4_HEADER_ERR_MASK	(1 << MVPP2_RXD_IP4_HEADER_ERR_BIT)
+#define MVPP2_RXD_L4_OFFS		25
+#define MVPP2_RXD_L4_MASK		(7 << MVPP2_RXD_L4_OFFS)
+/* Value 0 - N/A, 3-7 - User Defined */
+#define MVPP2_RXD_L3_OFFS		28
+#define MVPP2_RXD_L3_MASK		(7 << MVPP2_RXD_L3_OFFS)
+/* Value 0 - N/A, 6-7 - User Defined */
+#define MVPP2_RXD_L3_IP4_OPT		(2 << MVPP2_RXD_L3_OFFS)
+#define MVPP2_RXD_L3_IP4_OTHER		(3 << MVPP2_RXD_L3_OFFS)
+#define MVPP2_RXD_L3_IP6_EXT		(5 << MVPP2_RXD_L3_OFFS)
+#define MVPP2_RXD_BUF_HDR_BIT		31
+#define MVPP2_RXD_BUF_HDR_MASK		(1 << MVPP2_RXD_BUF_HDR_BIT)
+/* status field MACROs */
+#define MVPP2_RXD_L3_IS_IP4(status)		(((status) & \
+				MVPP2_RXD_L3_MASK) == MVPP2_RXD_L3_IP4)
+#define MVPP2_RXD_L3_IS_IP4_OPT(status)		(((status) & \
+				MVPP2_RXD_L3_MASK) == MVPP2_RXD_L3_IP4_OPT)
+#define MVPP2_RXD_L3_IS_IP4_OTHER(status)	(((status) & \
+				MVPP2_RXD_L3_MASK) == MVPP2_RXD_L3_IP4_OTHER)
+#define MVPP2_RXD_L3_IS_IP6(status)		(((status) & \
+				MVPP2_RXD_L3_MASK) == MVPP2_RXD_L3_IP6)
+#define MVPP2_RXD_L3_IS_IP6_EXT(status)		(((status) & \
+				MVPP2_RXD_L3_MASK) == MVPP2_RXD_L3_IP6_EXT)
+#define MVPP2_RXD_L4_IS_UDP(status)		(((status) & \
+				MVPP2_RXD_L4_MASK) == MVPP2_RXD_L4_UDP)
+#define MVPP2_RXD_L4_IS_TCP(status)		(((status) & \
+				MVPP2_RXD_L4_MASK) == MVPP2_RXD_L4_TCP)
+#define MVPP2_RXD_IP4_HDR_ERR(status)		((status) & \
+				MVPP2_RXD_IP4_HEADER_ERR_MASK)
+#define MVPP2_RXD_IP4_FRG(status)		((status) & \
+				MVPP2_RXD_IP_FRAG_MASK)
+#define MVPP2_RXD_L4_CHK_OK(status)		((status) & \
+				MVPP2_RXD_L4_CHK_OK_MASK)
+
+struct pp21_specific_tx_desc {
+	u32 buf_phys_addr;	/* physical addr of transmitted buffer	*/
+	u32 buf_cookie;	/* cookie for access to TX buffer in tx path */
+	u32 rsrvd_hw_cmd[3];	/* hw_cmd (for future use, BM, PON, PNC) */
+	u32 rsrvd1;		/* reserved (for future use)		*/
+};
+
+struct pp22_specific_tx_desc {
+	u64 rsrvd_hw_cmd1;	/* hw_cmd (BM, PON, PNC) */
+	u64 buf_phys_addr_hw_cmd2;
+	u64 buf_cookie_bm_qset_hw_cmd3;
+		/* cookie for access to RX buffer in rx path */
+		/* cookie for access to RX buffer in rx path */
+		/* bm_qset (for future use, BM)		*/
+		/* classify_info (for future use, PnC)	*/
+};
+
+union pp2x_specific_tx_desc {
+	struct pp21_specific_tx_desc pp21;
+	struct pp22_specific_tx_desc pp22;
+};
+
+struct mv_pp2x_tx_desc {
+	u32 command;		/* Options used by HW for packet xmitting */
+	u8  packet_offset;	/* the offset from the buffer beginning	*/
+	u8  phys_txq;		/* destination queue ID			*/
+	u16 data_size;		/* data size of transmitted packet in bytes */
+	union pp2x_specific_tx_desc u;
+};
+
+struct pp21_specific_rx_desc {
+	u32 buf_phys_addr;	/* physical address of the buffer */
+	u32 buf_cookie;	/* cookie for access to RX buffer in rx path */
+	u16 rsrvd_gem;		/* gem_port_id (for future use, PON) */
+	u16 rsrvd_l4csum;	/* csum_l4 (for future use, PnC) */
+	u8  rsrvd_bm_qset;	/* bm_qset (for future use, BM) */
+	u8  rsrvd1;
+	u16 rsrvd_cls_info;	/* classify_info (for future use, PnC)	*/
+	u32 rsrvd_flow_id;	/* flow_id (for future use, PnC) */
+	u32 rsrvd_abs;
+};
+struct pp22_specific_rx_desc {
+	u16 rsrvd_gem;		/* gem_port_id (for future use, PON)	*/
+	u16 rsrvd_l4csum;	/* csum_l4 (for future use, PnC)	*/
+	u32 rsrvd_timestamp;
+	u64 buf_phys_addr_key_hash;
+	u64 buf_cookie_bm_qset_cls_info;
+	/* cookie for access to RX buffer in rx path */
+	/* bm_qset (for future use, BM)		*/
+	/* classify_info (for future use, PnC)	*/
+};
+
+union pp2x_specific_rx_desc {
+	struct pp21_specific_rx_desc pp21;
+	struct pp22_specific_rx_desc pp22;
+};
+
+struct mv_pp2x_rx_desc {
+	u32 status;		/* info about received packet */
+	u16 rsrvd_parser;	/* parser_info (for future use, PnC)	*/
+	u16 data_size;		/* size of received packet in bytes	*/
+	union pp2x_specific_rx_desc u;
+};
+
+union mv_pp2x_prs_tcam_entry {
+	u32 word[MVPP2_PRS_TCAM_WORDS];
+	u8  byte[MVPP2_PRS_TCAM_WORDS * 4];
+};
+
+union mv_pp2x_prs_sram_entry {
+	u32 word[MVPP2_PRS_SRAM_WORDS];
+	u8  byte[MVPP2_PRS_SRAM_WORDS * 4];
+};
+
+struct mv_pp2x_prs_entry {
+	u32 index;
+	union mv_pp2x_prs_tcam_entry tcam;
+	union mv_pp2x_prs_sram_entry sram;
+};
+
+struct mv_pp2x_prs_shadow {
+	bool valid;
+	bool finish;
+
+	/* Lookup ID */
+	int lu;
+
+	/* User defined offset */
+	int udf;
+
+	/* Result info */
+	u32 ri;
+	u32 ri_mask;
+};
+
+struct mv_pp2x_cls_flow_entry {
+	u32 index;
+	u32 data[MVPP2_CLS_FLOWS_TBL_DATA_WORDS];
+};
+
+struct mv_pp2x_cls_lookup_entry {
+	u32 lkpid;
+	u32 way;
+	u32 data;
+};
+
+struct mv_pp2x_cls_flow_info {
+	u32 lkpid;
+	/* The flow table entry index of CoS default rule */
+	u32 flow_entry_dflt;
+	/* The flow table entry index of CoS VLAN rule */
+	u32 flow_entry_vlan;
+	/* The flow table entry index of CoS DSCP rule */
+	u32 flow_entry_dscp;
+	/* The flow table entry index of RSS rule */
+	u32 flow_entry_rss1;
+	/* The flow table entry index of RSS rule for UDP packet to
+	 * update hash mode
+	 */
+	u32 flow_entry_rss2;
+};
+
+struct mv_pp2x_cls_shadow {
+	struct mv_pp2x_cls_flow_info *flow_info;
+	u32 flow_free_start; /* The start of free entry index in flow table */
+	/* TODO: does need a spin_lock for flow_free_start? */
+};
+
+/* Classifier engine2 and QoS structure */
+
+/* C2  constants */
+#ifdef CONFIG_MV_PP2_PALLADIUM
+#define MVPP2_CLS_C2_TCAM_SIZE			32
+#else
+#define MVPP2_CLS_C2_TCAM_SIZE			256
+#endif
+#define MVPP2_CLS_C2_TCAM_WORDS			5
+#define MVPP2_CLS_C2_TCAM_DATA_BYTES		10
+#define MVPP2_CLS_C2_SRAM_WORDS			5
+#define MVPP2_CLS_C2_HEK_LKP_TYPE_OFFS		0
+#define MVPP2_CLS_C2_HEK_LKP_TYPE_BITS		6
+#define MVPP2_CLS_C2_HEK_LKP_TYPE_MASK		(0x3F << \
+					MVPP2_CLS_C2_HEK_LKP_TYPE_OFFS)
+#define MVPP2_CLS_C2_HEK_PORT_TYPE_OFFS		6
+#define MVPP2_CLS_C2_HEK_PORT_TYPE_BITS		2
+#define MVPP2_CLS_C2_HEK_PORT_TYPE_MASK		(0x3 << \
+					MVPP2_CLS_C2_HEK_PORT_TYPE_OFFS)
+#define MVPP2_CLS_C2_QOS_DSCP_TBL_SIZE		64
+#define MVPP2_CLS_C2_QOS_PRIO_TBL_SIZE		8
+#define MVPP2_CLS_C2_QOS_DSCP_TBL_NUM		8
+#define MVPP2_CLS_C2_QOS_PRIO_TBL_NUM		64
+
+struct mv_pp2x_cls_c2_entry {
+	u32          index;
+	bool         inv;
+	union {
+		u32	words[MVPP2_CLS_C2_TCAM_WORDS];
+		u8	bytes[MVPP2_CLS_C2_TCAM_WORDS * 4];
+	} tcam;
+	union {
+		u32	words[MVPP2_CLS_C2_SRAM_WORDS];
+		struct {
+			u32 action_tbl; /* 0x1B30 */
+			u32 actions;    /* 0x1B60 */
+			u32 qos_attr;   /* 0x1B64*/
+			u32 hwf_attr;   /* 0x1B68 */
+			u32 rss_attr;   /* 0x1B6C */
+			u32 seq_attr;   /* 0x1B70 */
+		} regs;
+	} sram;
+};
+
+enum mv_pp2x_cls2_hek_offs {
+	MVPP2_CLS_C2_HEK_OFF_BYTE0 = 0,
+	MVPP2_CLS_C2_HEK_OFF_BYTE1,
+	MVPP2_CLS_C2_HEK_OFF_BYTE2,
+	MVPP2_CLS_C2_HEK_OFF_BYTE3,
+	MVPP2_CLS_C2_HEK_OFF_BYTE4,
+	MVPP2_CLS_C2_HEK_OFF_BYTE5,
+	MVPP2_CLS_C2_HEK_OFF_BYTE6,
+	MVPP2_CLS_C2_HEK_OFF_BYTE7,
+	MVPP2_CLS_C2_HEK_OFF_LKP_PORT_TYPE,
+	MVPP2_CLS_C2_HEK_OFF_PORT_ID,
+	MVPP2_CLS_C2_HEK_OFF_MAX
+};
+
+struct mv_pp2x_cls_c2_qos_entry {
+	u32 tbl_id;
+	u32 tbl_sel;
+	u32 tbl_line;
+	u32 data;
+};
+
+enum mv_pp2x_src_port_type {
+	MVPP2_SRC_PORT_TYPE_PHY,
+	MVPP2_SRC_PORT_TYPE_UNI,
+	MVPP2_SRC_PORT_TYPE_VIR
+};
+
+struct mv_pp2x_src_port {
+	enum mv_pp2x_src_port_type	port_type;
+	unsigned int			port_value;
+	unsigned int			port_mask;
+};
+
+enum mv_pp2x_qos_tbl_sel {
+	MVPP2_QOS_TBL_SEL_PRI = 0,
+	MVPP2_QOS_TBL_SEL_DSCP,
+};
+
+enum mv_pp2x_qos_src_tbl {
+	MVPP2_QOS_SRC_ACTION_TBL = 0,
+	MVPP2_QOS_SRC_DSCP_PBIT_TBL,
+};
+
+struct mv_pp2x_engine_qos_info {
+	/* dscp pri table or none */
+	enum mv_pp2x_qos_tbl_sel	qos_tbl_type;
+	/* dscp or pri table index */
+	unsigned int		qos_tbl_index;
+	/* policer id, 0xffff do not assign policer */
+	unsigned short		policer_id;
+	/* pri/dscp comes from qos or act tbl */
+	enum mv_pp2x_qos_src_tbl	pri_dscp_src;
+	/* gemport comes from qos or act tbl */
+	enum mv_pp2x_qos_src_tbl	gemport_src;
+	enum mv_pp2x_qos_src_tbl	q_low_src;
+	enum mv_pp2x_qos_src_tbl	q_high_src;
+	enum mv_pp2x_qos_src_tbl	color_src;
+};
+
+enum mv_pp2x_color_action_type {
+	/* Do not update color */
+	MVPP2_COLOR_ACTION_TYPE_NO_UPDT = 0,
+	/* Do not update color and lock */
+	MVPP2_COLOR_ACTION_TYPE_NO_UPDT_LOCK,
+	/* Update to green */
+	MVPP2_COLOR_ACTION_TYPE_GREEN,
+	/* Update to green and lock */
+	MVPP2_COLOR_ACTION_TYPE_GREEN_LOCK,
+	/* Update to yellow */
+	MVPP2_COLOR_ACTION_TYPE_YELLOW,
+	/* Update to yellow */
+	MVPP2_COLOR_ACTION_TYPE_YELLOW_LOCK,
+	/* Update to red */
+	MVPP2_COLOR_ACTION_TYPE_RED,
+	/* Update to red and lock */
+	MVPP2_COLOR_ACTION_TYPE_RED_LOCK,
+};
+
+enum mv_pp2x_general_action_type {
+	/* The field will be not updated */
+	MVPP2_ACTION_TYPE_NO_UPDT,
+	/* The field will be not updated and lock */
+	MVPP2_ACTION_TYPE_NO_UPDT_LOCK,
+	/* The field will be updated */
+	MVPP2_ACTION_TYPE_UPDT,
+	/* The field will be updated and lock */
+	MVPP2_ACTION_TYPE_UPDT_LOCK,
+};
+
+enum mv_pp2x_flowid_action_type {
+	/* FlowID is disable */
+	MVPP2_ACTION_FLOWID_DISABLE = 0,
+	/* FlowID is enable */
+	MVPP2_ACTION_FLOWID_ENABLE,
+};
+
+enum mv_pp2x_frwd_action_type {
+	/* The decision will be not updated */
+	MVPP2_FRWD_ACTION_TYPE_NO_UPDT,
+	/* The decision is not updated, and following no change to it */
+	MVPP2_FRWD_ACTION_TYPE_NO_UPDT_LOCK,
+	/* The packet to CPU (Software Forwarding) */
+	MVPP2_FRWD_ACTION_TYPE_SWF,
+	 /* The packet to CPU, and following no change to it */
+	MVPP2_FRWD_ACTION_TYPE_SWF_LOCK,
+	/* The packet to one transmit port (Hardware Forwarding) */
+	MVPP2_FRWD_ACTION_TYPE_HWF,
+	/* The packet to one tx port, and following no change to it */
+	MVPP2_FRWD_ACTION_TYPE_HWF_LOCK,
+	/* The pkt to one tx port, and maybe internal packets is used */
+	MVPP2_FRWD_ACTION_TYPE_HWF_LOW_LATENCY,
+	/* Same to above, but following no change to it*/
+	MVPP2_FRWD_ACTION_TYPE_HWF_LOW_LATENCY_LOCK,
+};
+
+struct mv_pp2x_engine_pkt_action {
+	enum mv_pp2x_color_action_type		color_act;
+	enum mv_pp2x_general_action_type		pri_act;
+	enum mv_pp2x_general_action_type		dscp_act;
+	enum mv_pp2x_general_action_type		gemp_act;
+	enum mv_pp2x_general_action_type		q_low_act;
+	enum mv_pp2x_general_action_type		q_high_act;
+	enum mv_pp2x_general_action_type		rss_act;
+	enum mv_pp2x_flowid_action_type		flowid_act;
+	enum mv_pp2x_frwd_action_type		frwd_act;
+};
+
+struct mv_pp2x_qos_value {
+	unsigned short		pri;
+	unsigned short		dscp;
+	unsigned short		gemp;
+	unsigned short		q_low;
+	unsigned short		q_high;
+};
+
+struct mv_pp2x_engine_pkt_mod {
+	unsigned int	mod_cmd_idx;
+	unsigned int	mod_data_idx;
+	unsigned int	l4_chksum_update_flag;
+};
+
+struct mv_pp2x_duplicate_info {
+	/* pkt duplication flow id */
+	unsigned int	flow_id;
+	/* pkt duplication count */
+	unsigned int	flow_cnt;
+};
+
+/* The logic C2 entry, easy to understand and use */
+struct mv_pp2x_c2_add_entry {
+	struct mv_pp2x_src_port		port;
+	unsigned char			lkp_type;
+	unsigned char			lkp_type_mask;
+	/* priority in this look_type */
+	unsigned int			priority;
+	/* all the qos input */
+	struct mv_pp2x_engine_qos_info	qos_info;
+	/* update&lock info */
+	struct mv_pp2x_engine_pkt_action	action;
+	/* pri/dscp/gemport/qLow/qHigh */
+	struct mv_pp2x_qos_value		qos_value;
+	/* PMT cmd_idx and data_idx */
+	struct mv_pp2x_engine_pkt_mod	pkt_mod;
+	/* RSS enable or disable */
+	int				rss_en;
+	/* pkt duplication flow info */
+	struct mv_pp2x_duplicate_info	flow_info;
+};
+
+struct mv_pp2x_c2_rule_idx {
+	/* The TCAM rule index for VLAN pri check with QoS pbit table */
+	u32 vlan_pri_idx;
+	/* The TCAM rule index for DSCP check with QoS dscp table */
+	u32 dscp_pri_idx;
+	/* The default rule for flow untagged and non-IP */
+	u32 default_rule_idx;
+};
+
+struct mv_pp2x_c2_shadow {
+	int c2_tcam_free_start;
+	/* Per src port */
+	struct mv_pp2x_c2_rule_idx rule_idx_info[8];
+};
+
+/* BM specific defines */
+#ifdef MVPP2_DEBUG
+struct mv_pp2x_bm_pool_stats {
+	u32 skb_alloc_oom;
+	u32 skb_alloc_ok;
+	u32 bm_put;
+};
+#endif
+
+struct mv_pp2x_bm_pool {
+	/* Pool number in the range 0-7 */
+	int id;
+
+	/*Logical id, equals to index in parent priv */
+	enum mv_pp2x_bm_pool_log_num  log_id;
+
+	/* Buffer Pointers Pool External (BPPE) size */
+	int size;
+	/* Number of buffers for this pool */
+	int buf_num;
+	/* Pool buffer size */
+	int buf_size;
+	/* Packet size */
+	int pkt_size;
+
+	/* BPPE virtual base address */
+	void *virt_addr;
+	/* BPPE physical base address */
+	dma_addr_t phys_addr;
+
+	/* Ports using BM pool */
+	u32 port_map;
+
+	/* Occupied buffers indicator */
+	atomic_t in_use;
+	int in_use_thresh;
+	/* Stats info */
+#ifdef MVPP2_DEBUG
+	struct mv_pp2x_bm_pool_stats stats;
+#endif
+};
+
+struct mv_pp2x_buff_hdr {
+	u32 next_buff_phys_addr;
+	u32 next_buff_virt_addr;
+	u16 byte_count;
+	u16 info;
+	u8  reserved1;		/* bm_qset (for future use, BM) */
+};
+
+/* Buffer header info bits */
+#define MVPP2_B_HDR_INFO_MC_ID_MASK	0xfff
+#define MVPP2_B_HDR_INFO_MC_ID(info)	((info) & MVPP2_B_HDR_INFO_MC_ID_MASK)
+#define MVPP2_B_HDR_INFO_LAST_OFFS	12
+#define MVPP2_B_HDR_INFO_LAST_MASK	BIT(12)
+#define MVPP2_B_HDR_INFO_IS_LAST(info) \
+	   ((info & MVPP2_B_HDR_INFO_LAST_MASK) >> MVPP2_B_HDR_INFO_LAST_OFFS)
+
+
+/* Macroes */
+#define MVPP2_RX_DESC_POOL(rx_desc)	((rx_desc->status & \
+		MVPP2_RXD_BM_POOL_ID_MASK) >> MVPP2_RXD_BM_POOL_ID_OFFS)
+
+/* RSS related definetions */
+enum mv_pp22_rss_access_sel {
+	MVPP22_RSS_ACCESS_POINTER,
+	MVPP22_RSS_ACCESS_TBL,
+};
+
+/* Structure dexcribe RXQ and corresponding rss table */
+struct mv_pp22_rss_tbl_ptr {
+	u8 rxq_idx;
+	u8 rss_tbl_ptr;
+};
+
+/* Normal RSS entry */
+struct mv_pp22_rss_tbl_entry {
+	u8 tbl_id;
+	u8 tbl_line;
+	u8 width;
+	u8 rxq;
+};
+
+union mv_pp22_rss_access_entry {
+	struct mv_pp22_rss_tbl_ptr pointer;
+	struct mv_pp22_rss_tbl_entry entry;
+};
+
+struct mv_pp22_rss_entry {
+	enum mv_pp22_rss_access_sel sel;
+	union mv_pp22_rss_access_entry u;
+};
+
+#endif /*_MVPP2_HW_TYPE_H_*/
+
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
new file mode 100644
index 0000000..e06eef0
--- /dev/null
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
@@ -0,0 +1,5029 @@
+ /*
+ * ***************************************************************************
+ * Copyright (C) 2016 Marvell International Ltd.
+ * ***************************************************************************
+ * This program is free software: you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation, either version 2 of the License, or any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ * ***************************************************************************
+ */
+
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+#include <linux/inetdevice.h>
+#include <linux/mbus.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/cpumask.h>
+#include <linux/kallsyms.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+
+#include <linux/phy.h>
+#include <linux/clk.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include <uapi/linux/ppp_defs.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <asm/cacheflush.h>
+#include <linux/dma-mapping.h>
+
+#ifdef CONFIG_MV_PP2_FPGA
+#include <linux/pci.h>
+#endif
+#include "mv_pp2x.h"
+#include "mv_pp2x_hw.h"
+#include "mv_gop110_hw.h"
+#include "mv_pp2x_debug.h"
+
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+#include <if_mv_pp2x_netmap.h>
+#endif
+
+#define MVPP2_SKB_TEST_SIZE 64
+#define MVPP2_ADDRESS 0xf2000000
+#define CPN110_ADDRESS_SPACE_SIZE (16*1024*1024)
+
+/* Declaractions */
+u8 mv_pp2x_num_cos_queues = 4;
+static u8 mv_pp2x_queue_mode = MVPP2_QDIST_MULTI_MODE;
+static u8 rss_mode;
+static u8 default_cpu;
+static u8 cos_classifer;
+static u32 pri_map;
+static u8 default_cos;
+static bool jumbo_pool;
+static u16 rx_queue_size = MVPP2_MAX_RXD;
+static u16 tx_queue_size = MVPP2_MAX_TXD;
+static u16 buffer_scaling = 100;
+static u32 port_cpu_bind_map;
+static u8 first_bm_pool;
+static u8 first_addr_space;
+static u8 first_log_rxq_queue;
+
+u32 debug_param;
+
+#if defined(CONFIG_MV_PP2_POLLING)
+#ifdef CONFIG_MV_PP2_PALLADIUM
+#define MV_PP2_FPGA_PERODIC_TIME 2
+#else
+#define MV_PP2_FPGA_PERODIC_TIME 10
+#endif
+#endif
+#ifdef CONFIG_MV_PP2_FPGA
+#define MAC_PORT0_OFFSET       0x104000
+#endif
+#ifdef CONFIG_MV_PP2_PALLADIUM
+#define MAC_PORT0_OFFSET       0x130E00
+#endif
+
+#if defined(CONFIG_MV_PP2_FPGA) || defined(CONFIG_MV_PP2_PALLADIUM)
+void *mv_pp2_vfpga_address;
+#endif
+
+#if defined(CONFIG_MV_PP2_POLLING)
+struct timer_list cpu_poll_timer;
+static int cpu_poll_timer_ref_cnt;
+static void mv_pp22_cpu_timer_callback(unsigned long data);
+#endif
+
+module_param_named(num_cos_queues, mv_pp2x_num_cos_queues, byte, S_IRUGO);
+MODULE_PARM_DESC(num_cos_queues, "Set number of cos_queues (1-8), def=4");
+
+module_param_named(queue_mode, mv_pp2x_queue_mode, byte, S_IRUGO);
+MODULE_PARM_DESC(queue_mode, "Set queue_mode (single=0, multi=1)");
+
+module_param(rss_mode, byte, S_IRUGO);
+MODULE_PARM_DESC(rss_mode, "Set rss_mode (UDP_2T=0, UDP_5T=1)");
+
+module_param(default_cpu, byte, S_IRUGO);
+MODULE_PARM_DESC(default_cpu, "Set default CPU for non RSS frames");
+
+module_param(cos_classifer, byte, S_IRUGO);
+MODULE_PARM_DESC(cos_classifer,
+	"Cos Classifier (vlan_pri=0, dscp=1, vlan_dscp=2, dscp_vlan=3)");
+
+module_param(pri_map, uint, S_IRUGO);
+MODULE_PARM_DESC(pri_map, "Set priority_map, nibble for each cos.");
+
+module_param(default_cos, byte, S_IRUGO);
+MODULE_PARM_DESC(default_cos, "Set default cos (0-(num_cose_queues-1)).");
+
+module_param(jumbo_pool, bool, S_IRUGO);
+MODULE_PARM_DESC(jumbo_pool, "no_jumbo_support(0), jumbo_support(1)");
+
+module_param(rx_queue_size, ushort, S_IRUGO);
+MODULE_PARM_DESC(rx_queue_size, "Rx queue size");
+
+module_param(tx_queue_size, ushort, S_IRUGO);
+MODULE_PARM_DESC(tx_queue_size, "Tx queue size");
+
+module_param(buffer_scaling, ushort, S_IRUGO);
+MODULE_PARM_DESC(buffer_scaling, "Buffer scaling (TBD)");
+
+module_param(debug_param, uint, S_IRUGO);
+MODULE_PARM_DESC(debug_param,
+	"Ad-hoc parameter, which can be used for various debug operations.");
+
+/*TODO:  Below module_params will not go to ML. Created for testing. */
+
+module_param(port_cpu_bind_map, uint, S_IRUGO);
+MODULE_PARM_DESC(port_cpu_bind_map,
+	"Set default port-to-cpu binding, nibble for each port. Relevant when queue_mode=multi-mode & rss is disabled");
+
+module_param(first_bm_pool, byte, S_IRUGO);
+MODULE_PARM_DESC(first_bm_pool, "First used buffer pool (0-11)");
+
+module_param(first_addr_space, byte, S_IRUGO);
+MODULE_PARM_DESC(first_addr_space, "First used PPV22 address space (0-8)");
+
+module_param(first_log_rxq_queue, byte, S_IRUGO);
+MODULE_PARM_DESC(first_log_rxq_queue, "First logical rx_queue (0-31)");
+
+/* Number of RXQs used by single port */
+static int mv_pp2x_rxq_number;
+/* Number of TXQs used by single port */
+static int mv_pp2x_txq_number;
+
+struct mv_pp2x_pool_attributes mv_pp2x_pools[] = {
+	{
+		.description =  "short",
+		/*.pkt_size    =	MVPP2_BM_SHORT_PKT_SIZE, */
+		.buf_num     =  MVPP2_BM_SHORT_BUF_NUM,
+	},
+	{
+		.description =  "long",
+		/*.pkt_size    =	MVPP2_BM_LONG_PKT_SIZE,*/
+		.buf_num     =  MVPP2_BM_LONG_BUF_NUM,
+	},
+	{
+		.description =	"jumbo",
+		/*.pkt_size    =	MVPP2_BM_JUMBO_PKT_SIZE, */
+		.buf_num     =  MVPP2_BM_JUMBO_BUF_NUM,
+	}
+};
+
+#if defined(CONFIG_NETMAP) || defined(CONFIG_NETMAP_MODULE)
+void *mv_pp2x_vfpga_address_get(void)
+{
+	return mv_pp2_vfpga_address;
+}
+#endif
+
+static void mv_pp2x_txq_inc_get(struct mv_pp2x_txq_pcpu *txq_pcpu)
+{
+	txq_pcpu->txq_get_index++;
+	if (txq_pcpu->txq_get_index == txq_pcpu->size)
+		txq_pcpu->txq_get_index = 0;
+}
+
+static void mv_pp2x_txq_inc_put(enum mvppv2_version pp2_ver,
+			      struct mv_pp2x_txq_pcpu *txq_pcpu,
+			      struct sk_buff *skb,
+			      struct mv_pp2x_tx_desc *tx_desc)
+{
+	txq_pcpu->tx_skb[txq_pcpu->txq_put_index] = skb;
+	if (skb)
+		txq_pcpu->tx_buffs[txq_pcpu->txq_put_index] =
+				mv_pp2x_txdesc_phys_addr_get(pp2_ver, tx_desc);
+	txq_pcpu->txq_put_index++;
+	if (txq_pcpu->txq_put_index == txq_pcpu->size)
+		txq_pcpu->txq_put_index = 0;
+#if defined(__BIG_ENDIAN)
+	if (pp2_ver == PPV21)
+		mv_pp21_tx_desc_swap(tx_desc);
+	else
+		mv_pp22_tx_desc_swap(tx_desc);
+#endif /* __BIG_ENDIAN */
+}
+
+static inline u8 mv_pp2x_first_pool_get(struct mv_pp2x *priv)
+{
+	return priv->pp2_cfg.first_bm_pool;
+}
+
+static inline u8 mv_pp2x_num_pools_get(struct mv_pp2x *priv)
+{
+	return((priv->pp2_cfg.jumbo_pool == true) ? 3 : 2);
+}
+
+static inline u8 mv_pp2x_last_pool_get(struct mv_pp2x *priv)
+{
+	return(mv_pp2x_first_pool_get(priv) + mv_pp2x_num_pools_get(priv));
+}
+
+static inline int mv_pp2x_pool_pkt_size_get(
+		enum mv_pp2x_bm_pool_log_num  log_id)
+{
+	return mv_pp2x_pools[log_id].pkt_size;
+}
+
+static inline int mv_pp2x_pool_buf_num_get(
+		enum mv_pp2x_bm_pool_log_num  log_id)
+{
+	return mv_pp2x_pools[log_id].buf_num;
+}
+
+static inline const char *
+	mv_pp2x_pool_description_get(
+		enum mv_pp2x_bm_pool_log_num  log_id)
+{
+	return mv_pp2x_pools[log_id].description;
+}
+
+/* Buffer Manager configuration routines */
+
+/* Create pool */
+static int mv_pp2x_bm_pool_create(struct platform_device *pdev,
+					 struct mv_pp2x_hw *hw,
+					 struct mv_pp2x_bm_pool *bm_pool,
+					 int size)
+{
+	int size_bytes;
+
+	/* Driver enforces size= x16 both for PPv21 and for PPv22, even though
+	 *    PPv22 HW allows size= x8
+	 */
+	if (!IS_ALIGNED(size, (1<<MVPP21_BM_POOL_SIZE_OFFSET)))
+		return -EINVAL;
+
+	/*YuvalC: Two pointers per buffer, existing bug fixed. */
+	size_bytes = 2 * sizeof(uintptr_t) * size;
+	bm_pool->virt_addr = dma_alloc_coherent(&pdev->dev, size_bytes,
+						&bm_pool->phys_addr,
+						GFP_KERNEL);
+	if (!bm_pool->virt_addr)
+		return -ENOMEM;
+
+	if (!IS_ALIGNED((uintptr_t)bm_pool->virt_addr,
+			MVPP2_BM_POOL_PTR_ALIGN)) {
+		dma_free_coherent(&pdev->dev, size_bytes, bm_pool->virt_addr,
+				  bm_pool->phys_addr);
+		dev_err(&pdev->dev, "BM pool %d is not %d bytes aligned\n",
+			bm_pool->id, MVPP2_BM_POOL_PTR_ALIGN);
+		return -ENOMEM;
+	}
+
+	mv_pp2x_bm_hw_pool_create(hw, bm_pool->id, bm_pool->phys_addr, size);
+
+	bm_pool->size = size;
+	bm_pool->pkt_size = mv_pp2x_pool_pkt_size_get(bm_pool->log_id);
+	bm_pool->buf_num = 0;
+	mv_pp2x_bm_pool_bufsize_set(hw, bm_pool,
+				    MVPP2_RX_BUF_SIZE(bm_pool->pkt_size));
+	atomic_set(&bm_pool->in_use, 0);
+
+	return 0;
+}
+
+void mv_pp2x_bm_bufs_free(struct mv_pp2x *priv, struct mv_pp2x_bm_pool *bm_pool,
+				int buf_num)
+{
+	int i;
+
+	if (buf_num > bm_pool->buf_num) {
+		WARN(1, "Pool does not have so many bufs pool(%d) bufs(%d)\n",
+		     bm_pool->id, buf_num);
+		buf_num = bm_pool->buf_num;
+
+	}
+	for (i = 0; i < buf_num; i++) {
+		struct sk_buff *vaddr;
+
+		/* Get buffer virtual address (indirect access) */
+		vaddr = mv_pp2x_bm_virt_addr_get(&priv->hw, bm_pool->id);
+		if (!vaddr)
+			break;
+#ifdef CONFIG_64BIT
+		dev_kfree_skb_any(
+			(struct sk_buff *)(priv->pp2xdata->skb_base_addr |
+			(uintptr_t)vaddr));
+#else
+		dev_kfree_skb_any(vaddr);
+#endif
+	}
+
+	/* Update BM driver with number of buffers removed from pool */
+	bm_pool->buf_num -= i;
+}
+
+
+/* Cleanup pool */
+static int mv_pp2x_bm_pool_destroy(struct platform_device *pdev,
+					  struct mv_pp2x *priv,
+					  struct mv_pp2x_bm_pool *bm_pool)
+{
+	u32 val;
+	int size_bytes;
+
+	mv_pp2x_bm_bufs_free(priv, bm_pool, bm_pool->buf_num);
+	if (bm_pool->buf_num) {
+		WARN(1, "cannot free all buffers in pool %d, buf_num left %d\n",
+		     bm_pool->id,
+		     bm_pool->buf_num);
+		return 0;
+	}
+
+	val = mv_pp2x_read(&priv->hw, MVPP2_BM_POOL_CTRL_REG(bm_pool->id));
+	val |= MVPP2_BM_STOP_MASK;
+	mv_pp2x_write(&priv->hw, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);
+
+	size_bytes = 2 * sizeof(uintptr_t) * bm_pool->size;
+	dma_free_coherent(&pdev->dev, size_bytes, bm_pool->virt_addr,
+			  bm_pool->phys_addr);
+	mv_pp2x_bm_pool_bufsize_set(&priv->hw, bm_pool, 0);
+	return 0;
+}
+
+static int mv_pp2x_bm_pools_init(struct platform_device *pdev,
+				       struct mv_pp2x *priv,
+				       u8 first_pool, u8 num_pools)
+{
+	int i, err, size;
+	struct mv_pp2x_bm_pool *bm_pool;
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	/* Create all pools with maximum size */
+	size = MVPP2_BM_POOL_SIZE_MAX;
+	for (i = 0; i < num_pools; i++) {
+		bm_pool = &priv->bm_pools[i];
+		bm_pool->log_id = i;
+		bm_pool->id = first_pool + i;
+		err = mv_pp2x_bm_pool_create(pdev, hw, bm_pool, size);
+		if (err)
+			goto err_unroll_pools;
+	}
+	priv->num_pools = num_pools;
+	return 0;
+
+err_unroll_pools:
+	dev_err(&pdev->dev, "failed to create BM pool %d, size %d\n", i, size);
+	for (i = i - 1; i >= 0; i--)
+		mv_pp2x_bm_pool_destroy(pdev, priv, &priv->bm_pools[i]);
+		return err;
+}
+
+static int mv_pp2x_bm_init(struct platform_device *pdev, struct mv_pp2x *priv)
+{
+	int i, err;
+	u8 first_pool = mv_pp2x_first_pool_get(priv);
+	u8 num_pools = mv_pp2x_num_pools_get(priv);
+
+	for (i = first_pool; i < (first_pool + num_pools); i++) {
+		/* Mask BM all interrupts */
+		mv_pp2x_write(&priv->hw, MVPP2_BM_INTR_MASK_REG(i), 0);
+		/* Clear BM cause register */
+		mv_pp2x_write(&priv->hw, MVPP2_BM_INTR_CAUSE_REG(i), 0);
+	}
+
+	/* Allocate and initialize BM pools */
+	priv->bm_pools = devm_kcalloc(&pdev->dev, num_pools,
+				      sizeof(struct mv_pp2x_bm_pool),
+				      GFP_KERNEL);
+	if (!priv->bm_pools)
+		return -ENOMEM;
+
+	err = mv_pp2x_bm_pools_init(pdev, priv, first_pool, num_pools);
+	if (err < 0)
+		return err;
+	return 0;
+}
+
+/* Allocate skb for BM pool */
+static struct sk_buff *mv_pp2x_skb_alloc(struct mv_pp2x_port *port,
+					      struct mv_pp2x_bm_pool *bm_pool,
+					      dma_addr_t *buf_phys_addr,
+					      gfp_t gfp_mask)
+{
+	struct sk_buff *skb;
+	dma_addr_t phys_addr;
+
+	gfp_mask |= GFP_DMA;
+	skb = __dev_alloc_skb(bm_pool->pkt_size, gfp_mask);
+	if (!skb) {
+		pr_crit_once("%s skb alloc failed\n", __func__);
+		STAT_DBG(bm_pool->stats.skb_alloc_oom++);
+		return NULL;
+	}
+	STAT_DBG(bm_pool->stats.skb_alloc_ok++);
+	phys_addr = dma_map_single(port->dev->dev.parent, skb->head,
+				   MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),
+				   DMA_FROM_DEVICE);
+#ifdef MVPP2_DEBUG
+	pr_crit_once("dev_ptr:%p, dev_name:%s, sizeof(dma_addr_t):%ld",
+		     port->dev->dev.parent, port->dev->dev.parent->init_name,
+		     sizeof(dma_addr_t));
+	pr_crit_once("sizeof(long):%ld, phys_addr:%llx, size:%d\n",
+		     sizeof(long),
+		     phys_addr, MVPP2_RX_BUF_SIZE(bm_pool->pkt_size));
+
+#endif
+	if (unlikely(dma_mapping_error(port->dev->dev.parent, phys_addr))) {
+		MVPP2_PRINT_2LINE();
+		dev_kfree_skb_any(skb);
+		return NULL;
+	}
+	*buf_phys_addr = phys_addr;
+
+	return skb;
+}
+
+/* Allocate buffers for the pool */
+int mv_pp2x_bm_bufs_add(struct mv_pp2x_port *port,
+			       struct mv_pp2x_bm_pool *bm_pool, int buf_num)
+{
+	struct sk_buff *skb;
+	int i, buf_size, total_size;
+	dma_addr_t phys_addr;
+
+	buf_size = MVPP2_RX_BUF_SIZE(bm_pool->pkt_size);
+	total_size = MVPP2_RX_TOTAL_SIZE(buf_size);
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	if (buf_num < 0 ||
+	    (buf_num + bm_pool->buf_num > bm_pool->size)) {
+		netdev_err(port->dev,
+			   "cannot allocate %d buffers for pool %d\n",
+			   buf_num, bm_pool->id);
+
+		PALAD(MVPP2_PRINT_LINE());
+		return 0;
+	}
+
+	MVPP2_PRINT_VAR(buf_num);
+
+	for (i = 0; i < buf_num; i++) {
+		skb = mv_pp2x_skb_alloc(port, bm_pool, &phys_addr, GFP_KERNEL);
+		if (!skb) {
+			PALAD(MVPP2_PRINT_LINE());
+			break;
+		}
+		if ((i & 0xF) == 0)
+			PALAD(MVPP2_PRINT_LINE());
+
+		mv_pp2x_pool_refill(port->priv, (u32)bm_pool->id,
+				    phys_addr, skb);
+		if ((i & 0xF) == 0)
+			PALAD(MVPP2_PRINT_LINE());
+	}
+
+	/* Update BM driver with number of buffers added to pool */
+	bm_pool->buf_num += i;
+	bm_pool->in_use_thresh = bm_pool->buf_num / 4;
+
+	netdev_dbg(port->dev,
+		   "%s pool %d: pkt_size=%4d, buf_size=%4d, total_size=%4d\n",
+		   mv_pp2x_pool_description_get(bm_pool->log_id),
+		   bm_pool->id, bm_pool->pkt_size, buf_size, total_size);
+
+	netdev_dbg(port->dev,
+		   "%s pool %d: %d of %d buffers added\n",
+		   mv_pp2x_pool_description_get(bm_pool->log_id),
+		   bm_pool->id, i, buf_num);
+	return i;
+}
+
+static int mv_pp2x_bm_buf_calc(enum mv_pp2x_bm_pool_log_num log_pool,
+				     u32 port_map)
+{
+	/*TODO: Code algo based  on
+	 * port_map/num_rx_queues/num_tx_queues/queue_sizes
+	 */
+	int num_ports = hweight_long(port_map);
+
+	return(num_ports * mv_pp2x_pool_buf_num_get(log_pool));
+}
+
+/* Notify the driver that BM pool is being used as specific type and return the
+ * pool pointer on success
+ */
+
+static struct mv_pp2x_bm_pool *mv_pp2x_bm_pool_use_internal(
+	struct mv_pp2x_port *port, enum mv_pp2x_bm_pool_log_num log_pool,
+	bool add_port)
+{
+	int pkts_num, add_num, num;
+	struct mv_pp2x_bm_pool *pool = &port->priv->bm_pools[log_pool];
+
+	if (log_pool < MVPP2_BM_SWF_SHORT_POOL ||
+	    log_pool > MVPP2_BM_SWF_JUMBO_POOL) {
+		netdev_err(port->dev, "pool does not exist\n");
+		return NULL;
+	}
+	MVPP2_PRINT_LINE();
+
+	if (add_port) {
+		pkts_num = mv_pp2x_bm_buf_calc(log_pool,
+					       pool->port_map |
+					       (1 << port->id));
+		MVPP2_PRINT_VAR(pkts_num);
+	} else {
+		pkts_num = mv_pp2x_bm_buf_calc(log_pool,
+					       pool->port_map &
+					       ~(1 << port->id));
+	}
+
+	add_num = pkts_num - pool->buf_num;
+	MVPP2_PRINT_VAR(add_num);
+
+	/* Allocate buffers for this pool */
+	if (add_num > 0) {
+		num = mv_pp2x_bm_bufs_add(port, pool, add_num);
+		if (num != add_num) {
+			WARN(1, "pool %d: %d of %d allocated\n",
+			     pool->id, num, pkts_num);
+			/* We need to undo the bufs_add() allocations */
+			return NULL;
+		}
+	} else if (add_num < 0) {
+		mv_pp2x_bm_bufs_free(port->priv, pool, -add_num);
+	}
+
+	return pool;
+}
+
+static struct mv_pp2x_bm_pool *mv_pp2x_bm_pool_use(
+			struct mv_pp2x_port *port,
+			enum mv_pp2x_bm_pool_log_num log_pool)
+{
+	return mv_pp2x_bm_pool_use_internal(port, log_pool, true);
+}
+
+static struct mv_pp2x_bm_pool *mv_pp2x_bm_pool_stop_use(
+			struct mv_pp2x_port *port,
+			enum mv_pp2x_bm_pool_log_num log_pool)
+{
+	return mv_pp2x_bm_pool_use_internal(port, log_pool, false);
+}
+
+/* Initialize pools for swf */
+static int mv_pp2x_swf_bm_pool_init(struct mv_pp2x_port *port)
+{
+	int rxq;
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+
+	MVPP2_PRINT_LINE();
+
+	if (!port->pool_long) {
+		MVPP2_PRINT_LINE();
+		port->pool_long =
+		       mv_pp2x_bm_pool_use(port, MVPP2_BM_SWF_LONG_POOL);
+		if (!port->pool_long)
+			return -ENOMEM;
+		port->pool_long->port_map |= (1 << port->id);
+		MVPP2_PRINT_LINE();
+
+		for (rxq = 0; rxq < port->num_rx_queues; rxq++) {
+			port->priv->pp2xdata->mv_pp2x_rxq_long_pool_set(hw,
+				port->rxqs[rxq]->id, port->pool_long->id);
+		}
+	}
+	MVPP2_PRINT_LINE();
+
+	if (!port->pool_short) {
+		MVPP2_PRINT_LINE();
+		port->pool_short =
+			mv_pp2x_bm_pool_use(port, MVPP2_BM_SWF_SHORT_POOL);
+		if (!port->pool_short)
+			return -ENOMEM;
+
+		port->pool_short->port_map |= (1 << port->id);
+		MVPP2_PRINT_LINE();
+
+		for (rxq = 0; rxq < port->num_rx_queues; rxq++)
+			port->priv->pp2xdata->mv_pp2x_rxq_short_pool_set(hw,
+			port->rxqs[rxq]->id, port->pool_short->id);
+	}
+	MVPP2_PRINT_LINE();
+
+	return 0;
+}
+
+static int mv_pp2x_bm_update_mtu(struct net_device *dev, int mtu)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct mv_pp2x_bm_pool *old_port_pool = port->pool_long;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	enum mv_pp2x_bm_pool_log_num new_log_pool;
+	enum mv_pp2x_bm_pool_log_num old_log_pool = old_port_pool->log_id;
+	int rxq;
+	int pkt_size = MVPP2_RX_PKT_SIZE(mtu);
+
+	if (pkt_size > MVPP2_BM_LONG_PKT_SIZE)
+		new_log_pool = MVPP2_BM_SWF_JUMBO_POOL;
+	else
+		new_log_pool = MVPP2_BM_SWF_LONG_POOL;
+
+	if (new_log_pool != old_log_pool) {
+		/* Add port to new pool */
+		port->pool_long = mv_pp2x_bm_pool_use(port, new_log_pool);
+		if (!port->pool_long)
+			return -ENOMEM;
+		port->pool_long->port_map |= (1 << port->id);
+		for (rxq = 0; rxq < port->num_rx_queues; rxq++)
+			port->priv->pp2xdata->mv_pp2x_rxq_long_pool_set(hw,
+			port->rxqs[rxq]->id, port->pool_long->id);
+
+		/* Remove port from old pool */
+		mv_pp2x_bm_pool_stop_use(port, old_log_pool);
+		old_port_pool->port_map &= ~(1 << port->id);
+	}
+
+	dev->mtu = mtu;
+	netdev_update_features(dev);
+	return 0;
+}
+
+/* Set defaults to the MVPP2 port */
+static void mv_pp2x_defaults_set(struct mv_pp2x_port *port)
+{
+	int tx_port_num, val, queue, ptxq, lrxq;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	/* Configure port to loopback if needed */
+	if (port->flags & MVPP2_F_LOOPBACK) {
+		if (port->priv->pp2_version == PPV21)
+			mv_pp21_port_loopback_set(port);
+	}
+
+#ifdef CONFIG_MV_PP2_FPGA
+	writel(0x8be4, port->base);
+	writel(0xc200, port->base + 0x8);
+	writel(0x3, port->base + 0x90);
+
+	writel(0x902A, port->base + 0xC);    /*force link to 100Mb*/
+	writel(0x8be5, port->base);          /*enable port        */
+#endif
+	/* Disable Legacy WRR, Disable EJP, Release from reset */
+	tx_port_num = mv_pp2x_egress_port(port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG,
+		      tx_port_num);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_CMD_1_REG, 0);
+
+	/* Close bandwidth for all queues */
+	for (queue = 0; queue < MVPP2_MAX_TXQ; queue++) {
+		ptxq = mv_pp2x_txq_phys(port->id, queue);
+		mv_pp2x_write(hw,
+			      MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(ptxq), 0);
+	}
+
+	/* Set refill period to 1 usec, refill tokens
+	 * and bucket size to maximum
+	 */
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PERIOD_REG,
+		      hw->tclk / USEC_PER_SEC);
+	val = mv_pp2x_read(hw, MVPP2_TXP_SCHED_REFILL_REG);
+	val &= ~MVPP2_TXP_REFILL_PERIOD_ALL_MASK;
+	val |= MVPP2_TXP_REFILL_PERIOD_MASK(1);
+	val |= MVPP2_TXP_REFILL_TOKENS_ALL_MASK;
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_REFILL_REG, val);
+	val = MVPP2_TXP_TOKEN_SIZE_MAX;
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, val);
+
+	/* Set MaximumLowLatencyPacketSize value to 256 */
+	mv_pp2x_write(hw, MVPP2_RX_CTRL_REG(port->id),
+		      MVPP2_RX_USE_PSEUDO_FOR_CSUM_MASK |
+		      MVPP2_RX_LOW_LATENCY_PKT_SIZE(256));
+
+	/* Disable Rx cache snoop */
+	for (lrxq = 0; lrxq < port->num_rx_queues; lrxq++) {
+		queue = port->rxqs[lrxq]->id;
+		val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(queue));
+
+		val &= ~MVPP2_SNOOP_PKT_SIZE_MASK;
+		val &= ~MVPP2_SNOOP_BUF_HDR_MASK;
+		mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(queue), val);
+	}
+
+	/* At default, mask all interrupts to all present cpus */
+	mv_pp2x_port_interrupts_disable(port);
+}
+
+/* Check if there are enough reserved descriptors for transmission.
+ * If not, request chunk of reserved descriptors and check again.
+ */
+static int mv_pp2x_txq_reserved_desc_num_proc(
+					struct mv_pp2x *priv,
+					struct mv_pp2x_tx_queue *txq,
+					struct mv_pp2x_txq_pcpu *txq_pcpu,
+					int num)
+{
+	int req, cpu, desc_count;
+
+	if (txq_pcpu->reserved_num >= num)
+		return 0;
+
+	/* Not enough descriptors reserved! Update the reserved descriptor
+	 * count and check again.
+	 */
+
+	desc_count = 0;
+	/* Compute total of used descriptors */
+	for_each_online_cpu(cpu) {
+		struct mv_pp2x_txq_pcpu *txq_pcpu_aux;
+
+		txq_pcpu_aux = per_cpu_ptr(txq->pcpu, cpu);
+		desc_count += txq_pcpu_aux->count;
+		desc_count += txq_pcpu_aux->reserved_num;
+	}
+
+	req = max(MVPP2_CPU_DESC_CHUNK, num - txq_pcpu->reserved_num);
+	desc_count += req;
+
+	if (desc_count >
+	   (txq->size - (num_active_cpus() * MVPP2_CPU_DESC_CHUNK)))
+		return -ENOMEM;
+
+	txq_pcpu->reserved_num += mv_pp2x_txq_alloc_reserved_desc(priv, txq,
+								  req);
+
+	/* OK, the descriptor cound has been updated: check again. */
+	if (txq_pcpu->reserved_num < num)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/* Release the last allocated Tx descriptor. Useful to handle DMA
+ * mapping failures in the Tx path.
+ */
+
+/* Free Tx queue skbuffs */
+static void mv_pp2x_txq_bufs_free(struct mv_pp2x_port *port,
+				       struct mv_pp2x_tx_queue *txq,
+				       struct mv_pp2x_txq_pcpu *txq_pcpu,
+				       int num)
+{
+	int i;
+
+	for (i = 0; i < num; i++) {
+		dma_addr_t buf_phys_addr =
+				    txq_pcpu->tx_buffs[txq_pcpu->txq_get_index];
+		struct sk_buff *skb = txq_pcpu->tx_skb[txq_pcpu->txq_get_index];
+
+		mv_pp2x_txq_inc_get(txq_pcpu);
+
+		if (!skb)
+			continue;
+
+		dma_unmap_single(port->dev->dev.parent, buf_phys_addr,
+				 skb_headlen(skb), DMA_TO_DEVICE);
+		dev_kfree_skb_any(skb);
+	}
+}
+
+/* Handle end of transmission */
+static void mv_pp2x_txq_done(struct mv_pp2x_port *port,
+				   struct mv_pp2x_tx_queue *txq,
+				   struct mv_pp2x_txq_pcpu *txq_pcpu)
+{
+	struct netdev_queue *nq = netdev_get_tx_queue(port->dev, txq->log_id);
+	int tx_done;
+
+	MVPP2_PRINT_LINE();
+
+#ifdef DEV_NETMAP
+	if (port->flags & MVPP2_F_IFCAP_NETMAP) {
+		if (netmap_tx_irq(port->dev, 0))
+			return; /* cleaned ok */
+	}
+#endif /* DEV_NETMAP */
+
+	if (txq_pcpu->cpu != smp_processor_id())
+		netdev_err(port->dev, "wrong cpu on the end of Tx processing\n");
+	MVPP2_PRINT_LINE();
+
+	tx_done = mv_pp2x_txq_sent_desc_proc(port, txq);
+	if (!tx_done)
+		return;
+	MVPP2_PRINT_LINE();
+
+	mv_pp2x_txq_bufs_free(port, txq, txq_pcpu, tx_done);
+
+	txq_pcpu->count -= tx_done;
+
+	if (netif_tx_queue_stopped(nq))
+		if (txq_pcpu->size - txq_pcpu->count >= MAX_SKB_FRAGS + 1)
+			netif_tx_wake_queue(nq);
+}
+
+static unsigned int mv_pp2x_tx_done(struct mv_pp2x_port *port, u32 cause)
+{
+	struct mv_pp2x_tx_queue *txq;
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	unsigned int tx_todo = 0;
+
+	while (cause) {
+		txq = mv_pp2x_get_tx_queue(port, cause);
+		if (!txq)
+			break;
+
+		txq_pcpu = this_cpu_ptr(txq->pcpu);
+
+		if (txq_pcpu->count) {
+			mv_pp2x_txq_done(port, txq, txq_pcpu);
+			tx_todo += txq_pcpu->count;
+		}
+
+		cause &= ~(1 << txq->log_id);
+	}
+	return tx_todo;
+}
+
+
+/* Rx/Tx queue initialization/cleanup methods */
+
+/* Allocate and initialize descriptors for aggr TXQ */
+static int mv_pp2x_aggr_txq_init(struct platform_device *pdev,
+				      struct mv_pp2x_aggr_tx_queue *aggr_txq,
+				      int desc_num, int cpu,
+				      struct mv_pp2x *priv)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+	dma_addr_t first_desc_phy;
+
+	/* Allocate memory for TX descriptors, ensure it can be 512B aligned. */
+	aggr_txq->desc_mem = dma_alloc_coherent(&pdev->dev,
+		MVPP2_DESCQ_MEM_SIZE(desc_num),
+		&aggr_txq->descs_phys, GFP_KERNEL);
+	if (!aggr_txq->desc_mem)
+		return -ENOMEM;
+
+	aggr_txq->first_desc = (struct mv_pp2x_tx_desc *)
+			MVPP2_DESCQ_MEM_ALIGN((uintptr_t)aggr_txq->desc_mem);
+	first_desc_phy = MVPP2_DESCQ_MEM_ALIGN(aggr_txq->descs_phys);
+
+	DBG_MSG("first_desc=%p, desc_mem=%p\n",
+		aggr_txq->desc_mem, aggr_txq->first_desc);
+
+	aggr_txq->last_desc = aggr_txq->size - 1;
+
+	/* Aggr TXQ no reset WA */
+	aggr_txq->next_desc_to_proc = mv_pp2x_read(hw,
+						MVPP2_AGGR_TXQ_INDEX_REG(cpu));
+
+	/* Set Tx descriptors queue starting address
+	 * indirect access
+	 */
+	mv_pp2x_write(hw, MVPP2_AGGR_TXQ_DESC_ADDR_REG(cpu),
+		      first_desc_phy >>
+		      priv->pp2xdata->hw.desc_queue_addr_shift);
+	mv_pp2x_write(hw, MVPP2_AGGR_TXQ_DESC_SIZE_REG(cpu), desc_num);
+
+	return 0;
+}
+
+/* Create a specified Rx queue */
+static int mv_pp2x_rxq_init(struct mv_pp2x_port *port,
+			       struct mv_pp2x_rx_queue *rxq)
+{
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	dma_addr_t first_desc_phy;
+
+	rxq->size = port->rx_ring_size;
+
+	/* Allocate memory for RX descriptors */
+	rxq->desc_mem = dma_alloc_coherent(port->dev->dev.parent,
+					   MVPP2_DESCQ_MEM_SIZE(rxq->size),
+					   &rxq->descs_phys, GFP_KERNEL);
+	if (!rxq->desc_mem)
+		return -ENOMEM;
+
+	rxq->first_desc = (struct mv_pp2x_rx_desc *)
+		MVPP2_DESCQ_MEM_ALIGN((uintptr_t)rxq->desc_mem);
+	first_desc_phy  = MVPP2_DESCQ_MEM_ALIGN(rxq->descs_phys);
+
+	rxq->last_desc = rxq->size - 1;
+
+	/* Zero occupied and non-occupied counters - direct access */
+	mv_pp2x_write(hw, MVPP2_RXQ_STATUS_REG(rxq->id), 0);
+
+	/* Set Rx descriptors queue starting address - indirect access */
+	mv_pp2x_write(hw, MVPP2_RXQ_NUM_REG, rxq->id);
+
+	mv_pp2x_write(hw, MVPP2_RXQ_DESC_ADDR_REG, (first_desc_phy >>
+		      port->priv->pp2xdata->hw.desc_queue_addr_shift));
+	mv_pp2x_write(hw, MVPP2_RXQ_DESC_SIZE_REG, rxq->size);
+	mv_pp2x_write(hw, MVPP2_RXQ_INDEX_REG, 0);
+
+	/* Set Offset */
+	mv_pp2x_rxq_offset_set(port, rxq->id, NET_SKB_PAD);
+
+	/* Set coalescing pkts and time */
+	mv_pp2x_rx_pkts_coal_set(port, rxq, rxq->pkts_coal);
+	mv_pp2x_rx_time_coal_set(port, rxq, rxq->time_coal);
+
+	/* Add number of descriptors ready for receiving packets */
+	mv_pp2x_rxq_status_update(port, rxq->id, 0, rxq->size);
+
+	return 0;
+}
+
+/* Push packets received by the RXQ to BM pool */
+static void mv_pp2x_rxq_drop_pkts(struct mv_pp2x_port *port,
+					struct mv_pp2x_rx_queue *rxq)
+{
+	int rx_received, i;
+	struct sk_buff *buf_cookie;
+	dma_addr_t buf_phys_addr;
+
+	rx_received = mv_pp2x_rxq_received(port, rxq->id);
+	if (!rx_received)
+		return;
+
+	for (i = 0; i < rx_received; i++) {
+		struct mv_pp2x_rx_desc *rx_desc =
+			mv_pp2x_rxq_next_desc_get(rxq);
+
+		if (port->priv->pp2_version == PPV21) {
+			buf_cookie = mv_pp21_rxdesc_cookie_get(rx_desc);
+			buf_phys_addr = mv_pp21_rxdesc_phys_addr_get(rx_desc);
+		} else {
+			buf_cookie = mv_pp22_rxdesc_cookie_get(rx_desc);
+			buf_phys_addr = mv_pp22_rxdesc_phys_addr_get(rx_desc);
+		}
+		mv_pp2x_pool_refill(port->priv, MVPP2_RX_DESC_POOL(rx_desc),
+			buf_phys_addr, buf_cookie);
+	}
+	mv_pp2x_rxq_status_update(port, rxq->id, rx_received, rx_received);
+}
+
+/* Cleanup Rx queue */
+static void mv_pp2x_rxq_deinit(struct mv_pp2x_port *port,
+				   struct mv_pp2x_rx_queue *rxq)
+{
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+
+	mv_pp2x_rxq_drop_pkts(port, rxq);
+
+	if (rxq->desc_mem)
+		dma_free_coherent(port->dev->dev.parent,
+				  MVPP2_DESCQ_MEM_SIZE(rxq->size),
+				  rxq->desc_mem,
+				  rxq->descs_phys);
+
+	rxq->first_desc		= NULL;
+	rxq->desc_mem		= NULL;
+	rxq->last_desc		= 0;
+	rxq->next_desc_to_proc  = 0;
+	rxq->descs_phys         = 0;
+
+	/* Clear Rx descriptors queue starting address and size;
+	 * free descriptor number
+	 */
+	mv_pp2x_write(hw, MVPP2_RXQ_STATUS_REG(rxq->id), 0);
+	mv_pp2x_write(hw, MVPP2_RXQ_NUM_REG, rxq->id);
+	mv_pp2x_write(hw, MVPP2_RXQ_DESC_ADDR_REG, 0);
+	mv_pp2x_write(hw, MVPP2_RXQ_DESC_SIZE_REG, 0);
+}
+
+/* Create and initialize a Tx queue */
+static int mv_pp2x_txq_init(struct mv_pp2x_port *port,
+			       struct mv_pp2x_tx_queue *txq)
+{
+	u32 val;
+	int cpu, desc, desc_per_txq, tx_port_num;
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	dma_addr_t first_desc_phy;
+
+	txq->size = port->tx_ring_size;
+
+	/* Allocate memory for Tx descriptors */
+	txq->desc_mem = dma_alloc_coherent(port->dev->dev.parent,
+					   MVPP2_DESCQ_MEM_SIZE(txq->size),
+					   &txq->descs_phys, GFP_KERNEL);
+	if (!txq->desc_mem)
+		return -ENOMEM;
+
+	txq->first_desc = (struct mv_pp2x_tx_desc *)
+		MVPP2_DESCQ_MEM_ALIGN((uintptr_t)txq->desc_mem);
+	first_desc_phy  = MVPP2_DESCQ_MEM_ALIGN(txq->descs_phys);
+
+
+	txq->last_desc = txq->size - 1;
+
+	/* Set Tx descriptors queue starting address - indirect access */
+	mv_pp2x_write(hw, MVPP2_TXQ_NUM_REG, txq->id);
+	mv_pp2x_write(hw, MVPP2_TXQ_DESC_ADDR_LOW_REG,
+		      first_desc_phy >> MVPP2_TXQ_DESC_ADDR_LOW_SHIFT);
+	mv_pp2x_write(hw, MVPP2_TXQ_DESC_SIZE_REG,
+		      txq->size & MVPP2_TXQ_DESC_SIZE_MASK);
+	mv_pp2x_write(hw, MVPP2_TXQ_INDEX_REG, 0);
+	mv_pp2x_write(hw, MVPP2_TXQ_RSVD_CLR_REG,
+		      txq->id << MVPP2_TXQ_RSVD_CLR_OFFSET);
+	val = mv_pp2x_read(hw, MVPP2_TXQ_PENDING_REG);
+	val &= ~MVPP2_TXQ_PENDING_MASK;
+	mv_pp2x_write(hw, MVPP2_TXQ_PENDING_REG, val);
+
+	/* Calculate base address in prefetch buffer. We reserve 16 descriptors
+	 * for each existing TXQ.
+	 * TCONTS for PON port must be continuous from 0 to MVPP2_MAX_TCONT
+	 * GBE ports assumed to be continious from 0 to MVPP2_MAX_PORTS
+	 */
+	desc_per_txq = 16;
+	desc = (port->id * MVPP2_MAX_TXQ * desc_per_txq) +
+	       (txq->log_id * desc_per_txq);
+
+	mv_pp2x_write(hw, MVPP2_TXQ_PREF_BUF_REG,
+		      MVPP2_PREF_BUF_PTR(desc) | MVPP2_PREF_BUF_SIZE_16 |
+		      MVPP2_PREF_BUF_THRESH(desc_per_txq/2));
+
+	/* WRR / EJP configuration - indirect access */
+	tx_port_num = mv_pp2x_egress_port(port);
+	mv_pp2x_write(hw, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);
+
+	val = mv_pp2x_read(hw, MVPP2_TXQ_SCHED_REFILL_REG(txq->log_id));
+	val &= ~MVPP2_TXQ_REFILL_PERIOD_ALL_MASK;
+	val |= MVPP2_TXQ_REFILL_PERIOD_MASK(1);
+	val |= MVPP2_TXQ_REFILL_TOKENS_ALL_MASK;
+	mv_pp2x_write(hw, MVPP2_TXQ_SCHED_REFILL_REG(txq->log_id), val);
+
+	val = MVPP2_TXQ_TOKEN_SIZE_MAX;
+	mv_pp2x_write(hw, MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq->log_id),
+		      val);
+
+	for_each_online_cpu(cpu) {
+		txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
+		txq_pcpu->size = txq->size;
+		txq_pcpu->tx_skb = kmalloc(txq_pcpu->size *
+					   sizeof(*txq_pcpu->tx_skb),
+					   GFP_KERNEL);
+		if (!txq_pcpu->tx_skb)
+			goto error;
+
+		txq_pcpu->tx_buffs = kmalloc(txq_pcpu->size *
+					     sizeof(dma_addr_t), GFP_KERNEL);
+		if (!txq_pcpu->tx_buffs)
+			goto error;
+
+		txq_pcpu->count = 0;
+		txq_pcpu->reserved_num = 0;
+		txq_pcpu->txq_put_index = 0;
+		txq_pcpu->txq_get_index = 0;
+	}
+
+	return 0;
+
+error:
+	for_each_online_cpu(cpu) {
+		txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
+		kfree(txq_pcpu->tx_skb);
+		kfree(txq_pcpu->tx_buffs);
+	}
+
+	dma_free_coherent(port->dev->dev.parent,
+			  txq->size * MVPP2_DESC_ALIGNED_SIZE,
+			  txq->first_desc, txq->descs_phys);
+
+	return -ENOMEM;
+}
+
+/* Free allocated TXQ resources */
+static void mv_pp2x_txq_deinit(struct mv_pp2x_port *port,
+				   struct mv_pp2x_tx_queue *txq)
+{
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
+		kfree(txq_pcpu->tx_skb);
+		kfree(txq_pcpu->tx_buffs);
+	}
+
+	if (txq->desc_mem)
+		dma_free_coherent(port->dev->dev.parent,
+				  MVPP2_DESCQ_MEM_SIZE(txq->size),
+				  txq->desc_mem, txq->descs_phys);
+
+	txq->desc_mem		= NULL;
+	txq->first_desc		= NULL;
+	txq->last_desc		= 0;
+	txq->next_desc_to_proc	= 0;
+	txq->descs_phys		= 0;
+
+	/* Set minimum bandwidth for disabled TXQs */
+	mv_pp2x_write(hw, MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(txq->id), 0);
+
+	/* Set Tx descriptors queue starting address and size */
+	mv_pp2x_write(hw, MVPP2_TXQ_NUM_REG, txq->id);
+	mv_pp2x_write(hw, MVPP2_TXQ_DESC_ADDR_LOW_REG, 0);
+	mv_pp2x_write(hw, MVPP2_TXQ_DESC_SIZE_REG, 0);
+}
+
+/* Cleanup Tx ports */
+static void mv_pp2x_txq_clean(struct mv_pp2x_port *port,
+				   struct mv_pp2x_tx_queue *txq)
+{
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	int delay, pending, cpu;
+	u32 val;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	mv_pp2x_write(hw, MVPP2_TXQ_NUM_REG, txq->id);
+	val = mv_pp2x_read(hw, MVPP2_TXQ_PREF_BUF_REG);
+	val |= MVPP2_TXQ_DRAIN_EN_MASK;
+	mv_pp2x_write(hw, MVPP2_TXQ_PREF_BUF_REG, val);
+
+	/* The napi queue has been stopped so wait for all packets
+	 * to be transmitted.
+	 */
+	delay = 0;
+	do {
+		if (delay >= MVPP2_TX_PENDING_TIMEOUT_MSEC) {
+			netdev_warn(port->dev,
+				    "port %d: cleaning queue %d timed out\n",
+				    port->id, txq->log_id);
+			break;
+		}
+		mdelay(1);
+		delay++;
+
+		pending = mv_pp2x_txq_pend_desc_num_get(port, txq);
+	} while (pending);
+
+	val &= ~MVPP2_TXQ_DRAIN_EN_MASK;
+	mv_pp2x_write(hw, MVPP2_TXQ_PREF_BUF_REG, val);
+
+	for_each_online_cpu(cpu) {
+		txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
+
+		/* Release all packets */
+		mv_pp2x_txq_bufs_free(port, txq, txq_pcpu, txq_pcpu->count);
+
+		/* Reset queue */
+		txq_pcpu->count = 0;
+		txq_pcpu->txq_put_index = 0;
+		txq_pcpu->txq_get_index = 0;
+	}
+}
+
+/* Cleanup all Tx queues */
+void mv_pp2x_cleanup_txqs(struct mv_pp2x_port *port)
+{
+	struct mv_pp2x_tx_queue *txq;
+	int queue;
+	u32 val;
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+
+	val = mv_pp2x_read(hw, MVPP2_TX_PORT_FLUSH_REG);
+
+	/* Reset Tx ports and delete Tx queues */
+	val |= MVPP2_TX_PORT_FLUSH_MASK(port->id);
+	mv_pp2x_write(hw, MVPP2_TX_PORT_FLUSH_REG, val);
+
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		txq = port->txqs[queue];
+		mv_pp2x_txq_clean(port, txq);
+		mv_pp2x_txq_deinit(port, txq);
+	}
+
+	on_each_cpu(mv_pp2x_txq_sent_counter_clear, port, 1);
+
+	val &= ~MVPP2_TX_PORT_FLUSH_MASK(port->id);
+	mv_pp2x_write(hw, MVPP2_TX_PORT_FLUSH_REG, val);
+}
+
+/* Cleanup all Rx queues */
+void mv_pp2x_cleanup_rxqs(struct mv_pp2x_port *port)
+{
+	int queue;
+
+	for (queue = 0; queue < port->num_rx_queues; queue++)
+		mv_pp2x_rxq_deinit(port, port->rxqs[queue]);
+}
+
+/* Init all Rx queues for port */
+int mv_pp2x_setup_rxqs(struct mv_pp2x_port *port)
+{
+	int queue, err;
+
+	for (queue = 0; queue < port->num_rx_queues; queue++) {
+		err = mv_pp2x_rxq_init(port, port->rxqs[queue]);
+		if (err)
+			goto err_cleanup;
+	}
+	return 0;
+
+err_cleanup:
+	mv_pp2x_cleanup_rxqs(port);
+	return err;
+}
+
+/* Init all tx queues for port */
+int mv_pp2x_setup_txqs(struct mv_pp2x_port *port)
+{
+	struct mv_pp2x_tx_queue *txq;
+	int queue, err;
+
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		txq = port->txqs[queue];
+		err = mv_pp2x_txq_init(port, txq);
+		if (err)
+			goto err_cleanup;
+	}
+	if (port->priv->pp2xdata->interrupt_tx_done == true) {
+		mv_pp2x_tx_done_time_coal_set(port, port->tx_time_coal);
+		on_each_cpu(mv_pp2x_tx_done_pkts_coal_set, port, 1);
+	}
+	on_each_cpu(mv_pp2x_txq_sent_counter_clear, port, 1);
+	return 0;
+
+err_cleanup:
+	mv_pp2x_cleanup_txqs(port);
+	return err;
+}
+
+void mv_pp2x_cleanup_irqs(struct mv_pp2x_port *port)
+{
+	int qvec;
+
+	/* YuvalC TODO: Check, according to free_irq(),
+	 *	it is safe to free a non-requested irq
+	 */
+	for (qvec = 0; qvec < port->num_qvector; qvec++)
+		free_irq(port->q_vector[qvec].irq, &port->q_vector[qvec]);
+}
+
+/* The callback for per-q_vector interrupt */
+static irqreturn_t mv_pp2x_isr(int irq, void *dev_id)
+{
+	struct queue_vector *q_vec = (struct queue_vector *)dev_id;
+
+	mv_pp2x_qvector_interrupt_disable(q_vec);
+	napi_schedule(&q_vec->napi);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t mv_pp2_link_change_isr(int irq, void *data)
+{
+	struct mv_pp2x_port *port = (struct mv_pp2x_port *)data;
+
+	/* mask all events from this mac */
+	mv_gop110_port_events_mask(&port->priv->hw.gop, &port->mac_data);
+	/* read cause register to clear event */
+	mv_gop110_port_events_clear(&port->priv->hw.gop, &port->mac_data);
+
+	tasklet_schedule(&port->link_change_tasklet);
+
+	return IRQ_HANDLED;
+
+}
+
+int mv_pp2x_setup_irqs(struct net_device *dev, struct mv_pp2x_port *port)
+{
+	int qvec, err;
+	char temp_buf[32];
+
+	/* Rx/TX irq's */
+	for (qvec = 0; qvec < port->num_qvector; qvec++) {
+		sprintf(temp_buf, "%s.q_vec[%d]", dev->name, qvec);
+		err = request_irq(port->q_vector[qvec].irq, mv_pp2x_isr, 0,
+				  temp_buf, &port->q_vector[qvec]);
+		if (err) {
+			netdev_err(dev, "cannot request IRQ %d\n",
+				   port->q_vector[qvec].irq);
+			goto err_cleanup;
+		}
+	}
+	/* Link irq */
+	if (port->mac_data.link_irq != MVPP2_NO_LINK_IRQ) {
+		sprintf(temp_buf, "%s link_change", dev->name);
+		err = request_irq(port->mac_data.link_irq,
+				  mv_pp2_link_change_isr, 0, temp_buf, dev);
+		if (err) {
+			netdev_err(dev, "cannot request IRQ %d\n",
+				   port->mac_data.link_irq);
+			goto err_cleanup;
+		}
+	}
+	return 0;
+err_cleanup:
+	mv_pp2x_cleanup_irqs(port);
+	return err;
+}
+
+/* Adjust link */
+
+/* Called from link_tasklet */
+static void mv_pp22_dev_link_event(struct net_device *dev)
+{
+	bool link_is_up;
+
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct gop_hw *gop = &port->priv->hw.gop;
+
+	/* Check Link status on ethernet port */
+	link_is_up = mv_gop110_port_is_link_up(gop, &port->mac_data);
+
+
+	if (link_is_up) {
+		if (netif_carrier_ok(dev))
+			return;
+
+		netif_carrier_on(dev);
+		netif_tx_wake_all_queues(dev);
+		pr_crit("%s: link up\n", dev->name);
+		port->mac_data.flags |= MV_EMAC_F_LINK_UP;
+	} else {
+		if (!netif_carrier_ok(dev))
+			return;
+		netif_carrier_off(dev);
+		netif_tx_stop_all_queues(dev);
+		pr_crit("%s: link down\n", dev->name);
+		port->mac_data.flags &= ~MV_EMAC_F_LINK_UP;
+	}
+}
+
+/* Called from phy_lib */
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+static void mv_pp21_link_event(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct phy_device *phydev = port->mac_data.phy_dev;
+	int status_change = 0;
+	u32 val;
+
+	if (!phydev)
+		return;
+
+	if (phydev->link) {
+		if ((port->mac_data.speed != phydev->speed) ||
+		    (port->mac_data.duplex != phydev->duplex)) {
+			u32 val;
+
+			val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+			val &= ~(MVPP2_GMAC_CONFIG_MII_SPEED |
+				 MVPP2_GMAC_CONFIG_GMII_SPEED |
+				 MVPP2_GMAC_CONFIG_FULL_DUPLEX |
+				 MVPP2_GMAC_AN_SPEED_EN |
+				 MVPP2_GMAC_AN_DUPLEX_EN);
+
+			if (phydev->duplex)
+				val |= MVPP2_GMAC_CONFIG_FULL_DUPLEX;
+
+			if (phydev->speed == SPEED_1000)
+				val |= MVPP2_GMAC_CONFIG_GMII_SPEED;
+			else if (phydev->speed == SPEED_100)
+				val |= MVPP2_GMAC_CONFIG_MII_SPEED;
+
+			writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+
+			port->mac_data.duplex = phydev->duplex;
+			port->mac_data.speed  = phydev->speed;
+		}
+	}
+
+	if (phydev->link != port->mac_data.link) {
+		if (!phydev->link) {
+			port->mac_data.duplex = -1;
+			port->mac_data.speed = 0;
+		}
+
+		port->mac_data.link = phydev->link;
+		status_change = 1;
+	}
+
+	if (status_change) {
+		if (phydev->link) {
+			val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+			val |= (MVPP2_GMAC_FORCE_LINK_PASS |
+				MVPP2_GMAC_FORCE_LINK_DOWN);
+			writel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);
+			mv_pp2x_egress_enable(port);
+			mv_pp2x_ingress_enable(port);
+		} else {
+			mv_pp2x_ingress_disable(port);
+			mv_pp2x_egress_disable(port);
+		}
+		phy_print_status(phydev);
+	}
+}
+#endif
+
+void mv_pp2_link_change_tasklet(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *)data;
+#if !defined(CONFIG_MV_PP2_POLLING)
+	struct mv_pp2x_port *port = netdev_priv(dev);
+#endif
+
+	mv_pp22_dev_link_event(dev);
+
+#if !defined(CONFIG_MV_PP2_POLLING)
+	/* Unmask interrupt */
+	mv_gop110_port_events_unmask(&port->priv->hw.gop, &port->mac_data);
+#endif
+}
+
+static void mv_pp2x_timer_set(struct mv_pp2x_port_pcpu *port_pcpu)
+{
+#if !defined(CONFIG_MV_PP2_PALLADIUM)
+	ktime_t interval;
+#endif
+
+	if (!port_pcpu->timer_scheduled) {
+		port_pcpu->timer_scheduled = true;
+#ifdef CONFIG_MV_PP2_PALLADIUM
+		 /*CONFIG_HZ=20*/
+		mod_timer(&port_pcpu->slow_tx_done_timer,
+			  jiffies + msecs_to_jiffies(MV_PP2_FPGA_PERODIC_TIME));
+#else
+		interval = ktime_set(0, MVPP2_TXDONE_HRTIMER_PERIOD_NS);
+		hrtimer_start(&port_pcpu->tx_done_timer, interval,
+			      HRTIMER_MODE_REL_PINNED);
+#endif
+	}
+}
+
+static void mv_pp2x_tx_proc_cb(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *)data;
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct mv_pp2x_port_pcpu *port_pcpu = this_cpu_ptr(port->pcpu);
+	unsigned int tx_todo, cause;
+
+	if (!netif_running(dev))
+		return;
+	port_pcpu->timer_scheduled = false;
+
+	/* Process all the Tx queues */
+	cause = (1 << mv_pp2x_txq_number) - 1;
+	tx_todo = mv_pp2x_tx_done(port, cause);
+
+	/* Set the timer in case not all the packets were processed */
+	if (tx_todo)
+		mv_pp2x_timer_set(port_pcpu);
+}
+
+#ifdef CONFIG_MV_PP2_PALLADIUM
+void  mv_pp2x_timer_cb(unsigned long data)
+{
+	struct mv_pp2x_port_pcpu *port_pcpu = (struct mv_pp2x_port_pcpu *) data;
+
+	tasklet_schedule(&port_pcpu->tx_done_tasklet);
+}
+#else
+static enum hrtimer_restart mv_pp2x_hr_timer_cb(struct hrtimer *timer)
+{
+	struct mv_pp2x_port_pcpu *port_pcpu = container_of(timer,
+			 struct mv_pp2x_port_pcpu, tx_done_timer);
+
+	tasklet_schedule(&port_pcpu->tx_done_tasklet);
+
+	return HRTIMER_NORESTART;
+}
+#endif
+
+
+
+
+/* The function get the number of cpu online */
+static inline int mv_pp2x_num_online_cpu_get(struct mv_pp2x *pp2)
+{
+	u8 num_online_cpus = 0;
+	u16 x = pp2->cpu_map;
+
+	while (x) {
+		x &= (x - 1);
+		num_online_cpus++;
+	}
+
+	return num_online_cpus;
+}
+
+/* The function calculate the width, such as cpu width, cos queue width */
+static inline void mv_pp2x_width_calc(struct mv_pp2x *pp2, u32 *cpu_width,
+				u32 *cos_width, u32 *port_rxq_width)
+{
+	if (pp2) {
+		/* Calculate CPU width */
+		if (cpu_width)
+			*cpu_width = ilog2(roundup_pow_of_two(
+				mv_pp2x_num_online_cpu_get(pp2)));
+		/* Calculate cos queue width */
+		if (cos_width)
+			*cos_width = ilog2(roundup_pow_of_two(
+				pp2->pp2_cfg.cos_cfg.num_cos_queues));
+		/* Calculate rx queue width on the port */
+		if (port_rxq_width)
+			*port_rxq_width = ilog2(roundup_pow_of_two(
+				pp2->pp2xdata->pp2x_max_port_rxqs));
+	}
+}
+
+
+/* CoS API */
+
+/* mv_pp2x_cos_classifier_set
+*  -- The API supplies interface to config cos classifier:
+*     0: cos based on vlan pri;
+*     1: cos based on dscp;
+*     2: cos based on vlan for tagged packets,
+*		and based on dscp for untagged IP packets;
+*     3: cos based on dscp for IP packets, and based on vlan for non-IP packets;
+*/
+int mv_pp2x_cos_classifier_set(struct mv_pp2x_port *port,
+					enum mv_pp2x_cos_classifier cos_mode)
+{
+	int index, flow_idx, lkpid;
+	int data[3];
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+	struct mv_pp2x_cls_flow_info *flow_info;
+
+	for (index = 0; index < (MVPP2_PRS_FL_LAST - MVPP2_PRS_FL_START);
+		index++) {
+		flow_info = &(hw->cls_shadow->flow_info[index]);
+		data[0] = MVPP2_FLOW_TBL_SIZE;
+		data[1] = MVPP2_FLOW_TBL_SIZE;
+		data[2] = MVPP2_FLOW_TBL_SIZE;
+		lkpid = index + MVPP2_PRS_FL_START;
+		/* Prepare a temp table for the lkpid */
+		mv_pp2x_cls_flow_tbl_temp_copy(hw, lkpid, &flow_idx);
+		/* Update lookup table to temp flow table */
+		mv_pp2x_cls_lkp_flow_set(hw, lkpid, 0, flow_idx);
+		mv_pp2x_cls_lkp_flow_set(hw, lkpid, 1, flow_idx);
+		/* Update original flow table */
+		/* First, remove the port from original table */
+		if (flow_info->flow_entry_dflt) {
+			mv_pp2x_cls_flow_port_del(hw,
+			flow_info->flow_entry_dflt,
+			port->id);
+			data[0] =
+			flow_info->flow_entry_dflt;
+		}
+		if (flow_info->flow_entry_vlan) {
+			mv_pp2x_cls_flow_port_del(hw,
+				flow_info->flow_entry_vlan, port->id);
+			data[1] = flow_info->flow_entry_vlan;
+		}
+		if (flow_info->flow_entry_dscp) {
+			mv_pp2x_cls_flow_port_del(hw,
+				flow_info->flow_entry_dscp, port->id);
+			data[2] = flow_info->flow_entry_dscp;
+		}
+
+		/* Second, update the port in original table */
+		if (mv_pp2x_prs_flow_id_attr_get(lkpid) &
+			MVPP2_PRS_FL_ATTR_VLAN_BIT) {
+			if (cos_mode == MVPP2_COS_CLS_VLAN ||
+			    cos_mode == MVPP2_COS_CLS_VLAN_DSCP ||
+			    (cos_mode == MVPP2_COS_CLS_DSCP_VLAN &&
+				lkpid == MVPP2_PRS_FL_NON_IP_TAG))
+				mv_pp2x_cls_flow_port_add(hw,
+				flow_info->flow_entry_vlan, port->id);
+			/* Hanlde NON-IP tagged packet */
+			else if (cos_mode == MVPP2_COS_CLS_DSCP &&
+					lkpid == MVPP2_PRS_FL_NON_IP_TAG)
+				mv_pp2x_cls_flow_port_add(hw,
+				flow_info->flow_entry_dflt, port->id);
+			else if (cos_mode == MVPP2_COS_CLS_DSCP ||
+					cos_mode == MVPP2_COS_CLS_DSCP_VLAN)
+				mv_pp2x_cls_flow_port_add(hw,
+				flow_info->flow_entry_dscp, port->id);
+		} else {
+			if (lkpid == MVPP2_PRS_FL_NON_IP_UNTAG ||
+					cos_mode == MVPP2_COS_CLS_VLAN)
+				mv_pp2x_cls_flow_port_add(hw,
+				flow_info->flow_entry_dflt, port->id);
+			else if (cos_mode == MVPP2_COS_CLS_DSCP ||
+				 cos_mode == MVPP2_COS_CLS_VLAN_DSCP ||
+				 cos_mode == MVPP2_COS_CLS_DSCP_VLAN)
+				mv_pp2x_cls_flow_port_add(hw,
+				flow_info->flow_entry_dscp, port->id);
+		}
+		/* Restore lookup table */
+		flow_idx = min(data[0], min(data[1], data[2]));
+			mv_pp2x_cls_lkp_flow_set(hw, lkpid, 0, flow_idx);
+		PALAD(MVPP2_PRINT_LINE());
+			mv_pp2x_cls_lkp_flow_set(hw, lkpid, 1, flow_idx);
+	}
+
+	/* Update it in priv */
+	port->priv->pp2_cfg.cos_cfg.cos_classifier = cos_mode;
+
+	return 0;
+}
+
+/* mv_pp2x_cos_classifier_get
+*  -- Get the cos classifier on the port.
+*/
+int mv_pp2x_cos_classifier_get(struct mv_pp2x_port *port)
+{
+	return port->priv->pp2_cfg.cos_cfg.cos_classifier;
+}
+
+/* mv_pp2x_cos_pri_map_set
+*  -- Set priority_map per port, nibble for each cos value(0~7).
+*/
+int mv_pp2x_cos_pri_map_set(struct mv_pp2x_port *port, int cos_pri_map)
+{
+	int ret, prev_pri_map;
+	u8 bound_cpu_first_rxq;
+
+
+	if (port->priv->pp2_cfg.cos_cfg.pri_map == cos_pri_map)
+		return 0;
+
+	prev_pri_map = port->priv->pp2_cfg.cos_cfg.pri_map;
+	port->priv->pp2_cfg.cos_cfg.pri_map = cos_pri_map;
+
+
+	/* Update C2 rules with nre pri_map */
+	bound_cpu_first_rxq  = mv_pp2x_bound_cpu_first_rxq_calc(port);
+	ret = mv_pp2x_cls_c2_rule_set(port, bound_cpu_first_rxq);
+	if (ret) {
+		port->priv->pp2_cfg.cos_cfg.pri_map = prev_pri_map;
+		return ret;
+	}
+
+	return 0;
+}
+
+/* mv_pp2x_cos_pri_map_get
+*  -- Get priority_map on the port.
+*/
+int mv_pp2x_cos_pri_map_get(struct mv_pp2x_port *port)
+{
+	return port->priv->pp2_cfg.cos_cfg.pri_map;
+}
+
+/* mv_pp2x_cos_default_value_set
+*  -- Set default cos value for untagged or non-IP packets per port.
+*/
+int mv_pp2x_cos_default_value_set(struct mv_pp2x_port *port, int cos_value)
+{
+	int ret, prev_cos_value;
+	u8 bound_cpu_first_rxq;
+
+	if (port->priv->pp2_cfg.cos_cfg.default_cos == cos_value)
+		return 0;
+
+	prev_cos_value = port->priv->pp2_cfg.cos_cfg.default_cos;
+	port->priv->pp2_cfg.cos_cfg.default_cos = cos_value;
+
+	/* Update C2 rules with the pri_map */
+	bound_cpu_first_rxq  = mv_pp2x_bound_cpu_first_rxq_calc(port);
+	ret = mv_pp2x_cls_c2_rule_set(port, bound_cpu_first_rxq);
+	if (ret) {
+		port->priv->pp2_cfg.cos_cfg.default_cos = prev_cos_value;
+		return ret;
+	}
+
+	return 0;
+}
+
+/* mv_pp2x_cos_default_value_get
+*  -- Get default cos value for untagged or non-IP packets on the port.
+*/
+int mv_pp2x_cos_default_value_get(struct mv_pp2x_port *port)
+{
+	return port->priv->pp2_cfg.cos_cfg.default_cos;
+}
+
+/* RSS API */
+
+/* Translate CPU sequence number to real CPU ID */
+static inline int mv_pp22_cpu_id_from_indir_tbl_get(struct mv_pp2x *pp2,
+					int cpu_seq, u32 *cpu_id)
+{
+	int i;
+	int seq = 0;
+
+	if (!pp2 || !cpu_id || cpu_seq >= 16)
+		return -EINVAL;
+
+	for (i = 0; i < 16; i++) {
+		if (pp2->cpu_map & (1 << i)) {
+			if (seq == cpu_seq) {
+				*cpu_id = i;
+				return 0;
+			}
+			seq++;
+		}
+	}
+
+	return -1;
+}
+
+/* mv_pp22_rss_rxfh_indir_set
+*  -- The API set the RSS table according to CPU weight from ethtool
+*/
+int mv_pp22_rss_rxfh_indir_set(struct mv_pp2x_port *port)
+{
+	struct mv_pp22_rss_entry rss_entry;
+	int rss_tbl, entry_idx;
+	u32 cos_width = 0, cpu_width = 0, cpu_id = 0;
+	int rss_tbl_needed = port->priv->pp2_cfg.cos_cfg.num_cos_queues;
+
+	if (port->priv->pp2_cfg.queue_mode == MVPP2_QDIST_SINGLE_MODE)
+		return -1;
+
+	memset(&rss_entry, 0, sizeof(struct mv_pp22_rss_entry));
+
+	if (!port->priv->cpu_map)
+		return -1;
+
+	/* Calculate cpu and cos width */
+	mv_pp2x_width_calc(port->priv, &cpu_width, &cos_width, NULL);
+
+	rss_entry.u.entry.width = cos_width + cpu_width;
+
+	rss_entry.sel = MVPP22_RSS_ACCESS_TBL;
+
+	for (rss_tbl = 0; rss_tbl < rss_tbl_needed; rss_tbl++) {
+		for (entry_idx = 0; entry_idx < MVPP22_RSS_TBL_LINE_NUM;
+			entry_idx++) {
+			rss_entry.u.entry.tbl_id = rss_tbl;
+			rss_entry.u.entry.tbl_line = entry_idx;
+			if (mv_pp22_cpu_id_from_indir_tbl_get(port->priv,
+			     port->priv->rx_indir_table[entry_idx],
+			     &cpu_id))
+				return -1;
+			/* Value of rss_tbl equals to cos queue */
+			rss_entry.u.entry.rxq = (cpu_id << cos_width) |
+				rss_tbl;
+			PALAD(MVPP2_PRINT_LINE());
+			if (mv_pp22_rss_tbl_entry_set(
+				&port->priv->hw, &rss_entry))
+				return -1;
+		}
+	}
+
+	return 0;
+}
+
+/* mv_pp22_rss_enable_set
+*  -- The API enable or disable RSS on the port
+*/
+void mv_pp22_rss_enable(struct mv_pp2x_port *port, bool en)
+{
+	u8 bound_cpu_first_rxq;
+
+
+	if (port->priv->pp2_cfg.rss_cfg.rss_en == en)
+		return;
+
+	bound_cpu_first_rxq  = mv_pp2x_bound_cpu_first_rxq_calc(port);
+
+	if (port->priv->pp2_cfg.queue_mode == MVPP2_QDIST_MULTI_MODE) {
+		mv_pp22_rss_c2_enable(port, en);
+		if (en) {
+			if (mv_pp22_rss_default_cpu_set(port,
+				port->priv->pp2_cfg.rss_cfg.dflt_cpu))
+				netdev_err(port->dev,
+				"cannot set rss default cpu on port(%d)\n",
+				port->id);
+			else
+				port->priv->pp2_cfg.rss_cfg.rss_en = 1;
+		} else {
+			if (mv_pp2x_cls_c2_rule_set(port, bound_cpu_first_rxq))
+				netdev_err(port->dev,
+				"cannot set c2 and qos table on port(%d)\n",
+				port->id);
+			else
+				port->priv->pp2_cfg.rss_cfg.rss_en = 0;
+		}
+	}
+}
+
+/* mv_pp2x_rss_mode_set
+*  -- The API to update RSS hash mode for non-fragemnt UDP packet per port.
+*/
+int mv_pp22_rss_mode_set(struct mv_pp2x_port *port, int rss_mode)
+{
+	int index, flow_idx, flow_idx_rss, lkpid, lkpid_attr;
+	int data[3];
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+	struct mv_pp2x_cls_flow_info *flow_info;
+
+	if (port->priv->pp2_cfg.queue_mode == MVPP2_QDIST_SINGLE_MODE)
+		return -1;
+
+	for (index = 0; index < (MVPP2_PRS_FL_LAST - MVPP2_PRS_FL_START);
+		index++) {
+		PALAD(MVPP2_PRINT_LINE());
+		flow_info = &(hw->cls_shadow->flow_info[index]);
+		data[0] = MVPP2_FLOW_TBL_SIZE;
+		data[1] = MVPP2_FLOW_TBL_SIZE;
+		data[2] = MVPP2_FLOW_TBL_SIZE;
+		lkpid = index + MVPP2_PRS_FL_START;
+		/* Get lookup ID attribute */
+		lkpid_attr = mv_pp2x_prs_flow_id_attr_get(lkpid);
+		/* Only non-frag UDP can set rss mode */
+		if ((lkpid_attr & MVPP2_PRS_FL_ATTR_UDP_BIT) &&
+		    !(lkpid_attr & MVPP2_PRS_FL_ATTR_FRAG_BIT)) {
+			/* Prepare a temp table for the lkpid */
+			mv_pp2x_cls_flow_tbl_temp_copy(hw, lkpid, &flow_idx);
+			/* Update lookup table to temp flow table */
+			mv_pp2x_cls_lkp_flow_set(hw, lkpid, 0, flow_idx);
+			mv_pp2x_cls_lkp_flow_set(hw, lkpid, 1, flow_idx);
+			/* Update original flow table */
+			/* First, remove the port from original table */
+			mv_pp2x_cls_flow_port_del(hw,
+				flow_info->flow_entry_rss1, port->id);
+			mv_pp2x_cls_flow_port_del(hw,
+				flow_info->flow_entry_rss2, port->id);
+			if (flow_info->flow_entry_dflt)
+				data[0] = flow_info->flow_entry_dflt;
+			if (flow_info->flow_entry_vlan)
+				data[1] = flow_info->flow_entry_vlan;
+			if (flow_info->flow_entry_dscp)
+				data[2] = flow_info->flow_entry_dscp;
+			/* Second, update port in original table with rss_mode*/
+			if (rss_mode == MVPP2_RSS_NF_UDP_2T)
+				flow_idx_rss = flow_info->flow_entry_rss1;
+			else
+				flow_idx_rss = flow_info->flow_entry_rss2;
+			mv_pp2x_cls_flow_port_add(hw, flow_idx_rss, port->id);
+
+			/*Find the ptr of flow table, the min flow index */
+			flow_idx_rss = min(flow_info->flow_entry_rss1,
+				flow_info->flow_entry_rss2);
+			flow_idx = min(min(data[0], data[1]), min(data[2],
+				flow_idx_rss));
+			/*Third, restore lookup table */
+			mv_pp2x_cls_lkp_flow_set(hw, lkpid, 0, flow_idx);
+			mv_pp2x_cls_lkp_flow_set(hw, lkpid, 1, flow_idx);
+		} else
+			if (flow_info->flow_entry_rss1) {
+				flow_idx_rss = flow_info->flow_entry_rss1;
+				mv_pp2x_cls_flow_port_add(hw, flow_idx_rss,
+					port->id);
+		}
+	}
+	/* Record it in priv */
+	port->priv->pp2_cfg.rss_cfg.rss_mode = rss_mode;
+
+	return 0;
+}
+
+/* mv_pp22_rss_default_cpu_set
+*  -- The API to update the default CPU to handle the non-IP packets.
+*/
+int mv_pp22_rss_default_cpu_set(struct mv_pp2x_port *port, int default_cpu)
+{
+	u8 index, queue, q_cpu_mask;
+	u32 cpu_width = 0, cos_width = 0;
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+
+	if (port->priv->pp2_cfg.queue_mode == MVPP2_QDIST_SINGLE_MODE)
+		return -1;
+
+	/* Calculate width */
+	mv_pp2x_width_calc(port->priv, &cpu_width, &cos_width, NULL);
+	q_cpu_mask = (1 << cpu_width) - 1;
+
+	/* Update LSB[cpu_width + cos_width - 1 : cos_width]
+	 * of queue (queue high and low) on c2 rule.
+	 */
+	index = hw->c2_shadow->rule_idx_info[port->id].default_rule_idx;
+	queue = mv_pp2x_cls_c2_rule_queue_get(hw, index);
+	queue &= ~(q_cpu_mask << cos_width);
+	queue |= (default_cpu << cos_width);
+	mv_pp2x_cls_c2_rule_queue_set(hw, index, queue);
+
+	/* Update LSB[cpu_width + cos_width - 1 : cos_width]
+	 * of queue on pbit table, table id equals to port id
+	 */
+	for (index = 0; index < MVPP2_QOS_TBL_LINE_NUM_PRI; index++) {
+		queue = mv_pp2x_cls_c2_pbit_tbl_queue_get(hw, port->id, index);
+		PALAD(MVPP2_PRINT_LINE());
+		queue &= ~(q_cpu_mask << cos_width);
+		queue |= (default_cpu << cos_width);
+		mv_pp2x_cls_c2_pbit_tbl_queue_set(hw, port->id, index, queue);
+	}
+
+	/* Update default cpu in cfg */
+	port->priv->pp2_cfg.rss_cfg.dflt_cpu = default_cpu;
+
+	return 0;
+}
+
+/* Main RX/TX processing routines */
+
+
+/* Reuse skb if possible, or allocate a new skb and add it to BM pool */
+static int mv_pp2x_rx_refill(struct mv_pp2x_port *port,
+			   struct mv_pp2x_bm_pool *bm_pool,
+			   u32 pool, int is_recycle)
+{
+	struct sk_buff *skb;
+	dma_addr_t phys_addr;
+
+	if (is_recycle &&
+	    (atomic_read(&bm_pool->in_use) < bm_pool->in_use_thresh))
+		return 0;
+
+	/* No recycle or too many buffers are in use, so allocate a new skb */
+	skb = mv_pp2x_skb_alloc(port, bm_pool, &phys_addr, GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	mv_pp2x_pool_refill(port->priv, pool, phys_addr, skb);
+	atomic_dec(&bm_pool->in_use);
+	return 0;
+}
+
+/* Handle tx checksum */
+static u32 mv_pp2x_skb_tx_csum(struct mv_pp2x_port *port, struct sk_buff *skb)
+{
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		int ip_hdr_len = 0;
+		u8 l4_proto;
+
+		if (skb->protocol == htons(ETH_P_IP)) {
+			struct iphdr *ip4h = ip_hdr(skb);
+
+			/* Calculate IPv4 checksum and L4 checksum */
+			ip_hdr_len = ip4h->ihl;
+			l4_proto = ip4h->protocol;
+		} else if (skb->protocol == htons(ETH_P_IPV6)) {
+			struct ipv6hdr *ip6h = ipv6_hdr(skb);
+
+			/* Read l4_protocol from one of IPv6 extra headers */
+			if (skb_network_header_len(skb) > 0)
+				ip_hdr_len = (skb_network_header_len(skb) >> 2);
+			l4_proto = ip6h->nexthdr;
+		} else {
+			return MVPP2_TXD_L4_CSUM_NOT;
+		}
+
+		return mv_pp2x_txq_desc_csum(skb_network_offset(skb),
+				skb->protocol, ip_hdr_len, l4_proto);
+	}
+
+	return MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE;
+}
+
+static void mv_pp2x_buff_hdr_rx(struct mv_pp2x_port *port,
+			      struct mv_pp2x_rx_desc *rx_desc)
+{
+	struct mv_pp2x_buff_hdr *buff_hdr;
+	struct sk_buff *skb;
+	u32 rx_status = rx_desc->status;
+	u32 buff_phys_addr;
+	u32 buff_virt_addr;
+	u32 buff_phys_addr_next;
+	u32 buff_virt_addr_next;
+	int mc_id;
+	int pool_id;
+
+	pool_id = (rx_status & MVPP2_RXD_BM_POOL_ID_MASK) >>
+		   MVPP2_RXD_BM_POOL_ID_OFFS;
+	/*TODO : YuvalC, this is just workaround to compile.
+	* Need to handle mv_pp2x_buff_hdr_rx().
+	*/
+	buff_phys_addr = rx_desc->u.pp21.buf_phys_addr;
+	buff_virt_addr = rx_desc->u.pp21.buf_cookie;
+
+	do {
+		skb = (struct sk_buff *)(u64)buff_virt_addr;
+		buff_hdr = (struct mv_pp2x_buff_hdr *)skb->head;
+
+		mc_id = MVPP2_B_HDR_INFO_MC_ID(buff_hdr->info);
+
+		buff_phys_addr_next = buff_hdr->next_buff_phys_addr;
+		buff_virt_addr_next = buff_hdr->next_buff_virt_addr;
+
+		/* Release buffer */
+		mv_pp2x_bm_pool_mc_put(port, pool_id, buff_phys_addr,
+				     buff_virt_addr, mc_id);
+
+		buff_phys_addr = buff_phys_addr_next;
+		buff_virt_addr = buff_virt_addr_next;
+
+	} while (!MVPP2_B_HDR_INFO_IS_LAST(buff_hdr->info));
+}
+
+/* Main rx processing */
+static int mv_pp2x_rx(struct mv_pp2x_port *port, struct napi_struct *napi,
+			int rx_todo, struct mv_pp2x_rx_queue *rxq)
+{
+	struct net_device *dev = port->dev;
+	int rx_received, rx_filled, i;
+	u32 rcvd_pkts = 0;
+	u32 rcvd_bytes = 0;
+
+#ifdef DEV_NETMAP
+		if (port->flags & MVPP2_F_IFCAP_NETMAP) {
+			int netmap_done = 0;
+
+			if (netmap_rx_irq(port->dev, 0, &netmap_done))
+				return netmap_done;
+		}
+#endif /* DEV_NETMAP */
+
+	/* Get number of received packets and clamp the to-do */
+	rx_received = mv_pp2x_rxq_received(port, rxq->id);
+
+	PALAD(MVPP2_PRINT_VAR(rx_received));
+
+	if (rx_todo > rx_received)
+		rx_todo = rx_received;
+
+	rx_filled = 0;
+	for (i = 0; i < rx_todo; i++) {
+		struct mv_pp2x_rx_desc *rx_desc =
+			mv_pp2x_rxq_next_desc_get(rxq);
+		struct mv_pp2x_bm_pool *bm_pool;
+		struct sk_buff *skb;
+		u32 rx_status, pool;
+		int rx_bytes, err;
+		dma_addr_t buf_phys_addr;
+
+#if defined(__BIG_ENDIAN)
+		if (port->priv->pp2_version == PPV21)
+			mv_pp21_rx_desc_swap(rx_desc);
+		else
+			mv_pp22_rx_desc_swap(rx_desc);
+#endif /* __BIG_ENDIAN */
+
+		rx_filled++;
+		rx_status = rx_desc->status;
+		rx_bytes = rx_desc->data_size - MVPP2_MH_SIZE;
+
+		PALAD(mv_pp2x_rx_desc_print(port->priv, rx_desc));
+
+
+		pool = MVPP2_RX_DESC_POOL(rx_desc);
+		bm_pool = &port->priv->bm_pools[pool];
+		/* Check if buffer header is used */
+		if (rx_status & MVPP2_RXD_BUF_HDR) {
+			mv_pp2x_buff_hdr_rx(port, rx_desc);
+			continue;
+		}
+
+		if (port->priv->pp2_version == PPV21) {
+			skb = mv_pp21_rxdesc_cookie_get(rx_desc);
+			buf_phys_addr = mv_pp21_rxdesc_phys_addr_get(rx_desc);
+		} else {
+			skb = mv_pp22_rxdesc_cookie_get(rx_desc);
+			buf_phys_addr = mv_pp22_rxdesc_phys_addr_get(rx_desc);
+		}
+
+#ifdef CONFIG_64BIT
+		skb = (struct sk_buff *)((uintptr_t)skb |
+			port->priv->pp2xdata->skb_base_addr);
+#endif
+#ifdef MVPP2_VERBOSE
+		mv_pp2x_skb_dump(skb, rx_desc->data_size, 4);
+#endif
+
+
+		/* In case of an error, release the requested buffer pointer
+		 * to the Buffer Manager. This request process is controlled
+		 * by the hardware, and the information about the buffer is
+		 * comprised by the RX descriptor.
+		 */
+		if (rx_status & MVPP2_RXD_ERR_SUMMARY) {
+			pr_err("MVPP2_RXD_ERR_SUMMARY\n");
+			dev->stats.rx_errors++;
+			mv_pp2x_rx_error(port, rx_desc);
+			mv_pp2x_pool_refill(port->priv, pool, buf_phys_addr,
+				skb);
+			continue;
+		}
+
+
+		rcvd_pkts++;
+		rcvd_bytes += rx_bytes;
+		atomic_inc(&bm_pool->in_use);
+
+		skb_reserve(skb, MVPP2_MH_SIZE);
+		skb_put(skb, rx_bytes);
+		skb->protocol = eth_type_trans(skb, dev);
+		mv_pp2x_rx_csum(port, rx_status, skb);
+
+		napi_gro_receive(napi, skb);
+
+		err = mv_pp2x_rx_refill(port, bm_pool, pool, 0);
+		if (err) {
+			netdev_err(port->dev, "failed to refill BM pools\n");
+			rx_filled--;
+		}
+	}
+
+	if (rcvd_pkts) {
+		struct mv_pp2x_pcpu_stats *stats = this_cpu_ptr(port->stats);
+
+		u64_stats_update_begin(&stats->syncp);
+		stats->rx_packets += rcvd_pkts;
+		stats->rx_bytes   += rcvd_bytes;
+		u64_stats_update_end(&stats->syncp);
+	}
+
+	/* Update Rx queue management counters */
+	wmb();
+	mv_pp2x_rxq_status_update(port, rxq->id, rx_todo, rx_filled);
+
+	return rx_todo;
+}
+
+static inline void tx_desc_unmap_put(struct device *dev,
+	struct mv_pp2x_tx_queue *txq, struct mv_pp2x_tx_desc *desc)
+{
+	dma_addr_t buf_phys_addr;
+
+	buf_phys_addr = mv_pp2x_txdesc_phys_addr_get(
+		((struct mv_pp2x *)(dev_get_drvdata(dev)))->pp2_version, desc);
+	dma_unmap_single(dev, buf_phys_addr,
+			 desc->data_size, DMA_TO_DEVICE);
+	mv_pp2x_txq_desc_put(txq);
+}
+
+/* Handle tx fragmentation processing */
+static int mv_pp2x_tx_frag_process(struct mv_pp2x_port *port,
+	struct sk_buff *skb, struct mv_pp2x_aggr_tx_queue *aggr_txq,
+	 struct mv_pp2x_tx_queue *txq)
+{
+	struct mv_pp2x_txq_pcpu *txq_pcpu = this_cpu_ptr(txq->pcpu);
+	struct mv_pp2x_tx_desc *tx_desc;
+	int i;
+	dma_addr_t buf_phys_addr;
+
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+		void *addr = page_address(frag->page.p) + frag->page_offset;
+
+		tx_desc = mv_pp2x_txq_next_desc_get(aggr_txq);
+		tx_desc->phys_txq = txq->id;
+		tx_desc->data_size = frag->size;
+
+		buf_phys_addr = dma_map_single(port->dev->dev.parent, addr,
+					       tx_desc->data_size,
+					       DMA_TO_DEVICE);
+		if (dma_mapping_error(port->dev->dev.parent, buf_phys_addr)) {
+			mv_pp2x_txq_desc_put(txq);
+			goto error;
+		}
+		tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;
+		mv_pp2x_txdesc_phys_addr_set(port->priv->pp2_version,
+			buf_phys_addr & ~MVPP2_TX_DESC_ALIGN, tx_desc);
+
+		if (i == (skb_shinfo(skb)->nr_frags - 1)) {
+			/* Last descriptor */
+			tx_desc->command = MVPP2_TXD_L_DESC;
+			mv_pp2x_txq_inc_put(port->priv->pp2_version,
+				txq_pcpu, skb, tx_desc);
+		} else {
+			/* Descriptor in the middle: Not First, Not Last */
+			tx_desc->command = 0;
+			mv_pp2x_txq_inc_put(port->priv->pp2_version,
+				txq_pcpu, NULL, tx_desc);
+		}
+	}
+
+	return 0;
+
+error:
+	/* Release all descriptors that were used to map fragments of
+	 * this packet, as well as the corresponding DMA mappings
+	 */
+	for (i = i - 1; i >= 0; i--) {
+		tx_desc = txq->first_desc + i;
+		tx_desc_unmap_put(port->dev->dev.parent, txq, tx_desc);
+	}
+
+	return -ENOMEM;
+}
+
+
+
+static inline void mv_pp2x_tx_done_post_proc(struct mv_pp2x_tx_queue *txq,
+	struct mv_pp2x_txq_pcpu *txq_pcpu, struct mv_pp2x_port *port, int frags)
+{
+
+	/* Finalize TX processing */
+	if (txq_pcpu->count >= txq->pkts_coal)
+		mv_pp2x_txq_done(port, txq, txq_pcpu);
+
+	/* Set the timer in case not all frags were processed */
+	if (txq_pcpu->count <= frags && txq_pcpu->count > 0) {
+		struct mv_pp2x_port_pcpu *port_pcpu = this_cpu_ptr(port->pcpu);
+
+		mv_pp2x_timer_set(port_pcpu);
+	}
+}
+
+/* Main tx processing */
+static int mv_pp2x_tx(struct sk_buff *skb, struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct mv_pp2x_tx_queue *txq;
+	struct mv_pp2x_aggr_tx_queue *aggr_txq;
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	struct netdev_queue *nq;
+	struct mv_pp2x_tx_desc *tx_desc;
+	dma_addr_t buf_phys_addr;
+	int frags = 0;
+	u16 txq_id;
+	u32 tx_cmd;
+
+	txq_id = skb_get_queue_mapping(skb);
+	txq = port->txqs[txq_id];
+	txq_pcpu = this_cpu_ptr(txq->pcpu);
+	aggr_txq = &port->priv->aggr_txqs[smp_processor_id()];
+
+	frags = skb_shinfo(skb)->nr_frags + 1;
+	PALAD(pr_crit("txq_id=%d, frags=%d\n", txq_id, frags));
+
+	/* Check number of available descriptors */
+	if (mv_pp2x_aggr_desc_num_check(port->priv, aggr_txq, frags) ||
+	    mv_pp2x_txq_reserved_desc_num_proc(port->priv, txq,
+					     txq_pcpu, frags)) {
+		frags = 0;
+		goto out;
+	}
+
+	/* Get a descriptor for the first part of the packet */
+	tx_desc = mv_pp2x_txq_next_desc_get(aggr_txq);
+	tx_desc->phys_txq = txq->id;
+	tx_desc->data_size = skb_headlen(skb);
+#ifdef MVPP2_VERBOSE
+	pr_debug(
+		"tx_desc=%p, cmd(0x%x), pkt_offset(%d), phys_txq=%d, data_size=%d\n"
+		"rsrvd_hw_cmd1(0x%llx)\n"
+		"buf_phys_addr_hw_cmd2(0x%llx)\n"
+		"buf_cookie_bm_qset_hw_cmd3(0x%llx)\n"
+		"skb_len=%d, skb_data_len=%d\n",
+		tx_desc, tx_desc->command, tx_desc->packet_offset,
+		tx_desc->phys_txq, tx_desc->data_size,
+		tx_desc->u.pp22.rsrvd_hw_cmd1,
+		tx_desc->u.pp22.buf_phys_addr_hw_cmd2,
+		tx_desc->u.pp22.buf_cookie_bm_qset_hw_cmd3,
+		skb->len, skb->data_len);
+#endif
+
+	buf_phys_addr = dma_map_single(dev->dev.parent, skb->data,
+				       tx_desc->data_size, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dev->dev.parent, buf_phys_addr))) {
+		mv_pp2x_txq_desc_put(txq);
+		frags = 0;
+		goto out;
+	}
+	PALAD(pr_crit("buf_phys_addr=%x\n", (unsigned int)buf_phys_addr));
+
+	tx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;
+	mv_pp2x_txdesc_phys_addr_set(port->priv->pp2_version,
+		buf_phys_addr & ~MVPP2_TX_DESC_ALIGN, tx_desc);
+
+	tx_cmd = mv_pp2x_skb_tx_csum(port, skb);
+
+	if (frags == 1) {
+		/* First and Last descriptor */
+
+		tx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_L_DESC;
+		tx_desc->command = tx_cmd;
+		mv_pp2x_txq_inc_put(port->priv->pp2_version,
+			txq_pcpu, skb, tx_desc);
+	} else {
+		/* First but not Last */
+		MVPP2_PRINT_LINE();
+		tx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_PADDING_DISABLE;
+		tx_desc->command = tx_cmd;
+		mv_pp2x_txq_inc_put(port->priv->pp2_version,
+			txq_pcpu, NULL, tx_desc);
+
+		/* Continue with other skb fragments */
+		if (mv_pp2x_tx_frag_process(port, skb, aggr_txq, txq)) {
+			MVPP2_PRINT_LINE();
+			tx_desc_unmap_put(port->dev->dev.parent, txq, tx_desc);
+			frags = 0;
+			goto out;
+		}
+	}
+	MVPP2_PRINT_2LINE();
+	txq_pcpu->reserved_num -= frags;
+	txq_pcpu->count += frags;
+	aggr_txq->count += frags;
+
+	/* Enable transmit */
+	wmb();
+	mv_pp2x_aggr_txq_pend_desc_add(port, frags);
+
+	if (txq_pcpu->size - txq_pcpu->count < MAX_SKB_FRAGS + 1) {
+		MVPP2_PRINT_LINE();
+		nq = netdev_get_tx_queue(dev, txq_id);
+		netif_tx_stop_queue(nq);
+	}
+out:
+	if (frags > 0) {
+		struct mv_pp2x_pcpu_stats *stats = this_cpu_ptr(port->stats);
+
+		MVPP2_PRINT_LINE();
+
+		u64_stats_update_begin(&stats->syncp);
+		stats->tx_packets++;
+		stats->tx_bytes += skb->len;
+		u64_stats_update_end(&stats->syncp);
+	} else {
+		MVPP2_PRINT_LINE();
+
+		dev->stats.tx_dropped++;
+		dev_kfree_skb_any(skb);
+	}
+	/* PPV21 TX Post-Processing */
+
+	if (port->priv->pp2xdata->interrupt_tx_done == false && frags > 0)
+		mv_pp2x_tx_done_post_proc(txq, txq_pcpu, port, frags);
+
+	MVPP2_PRINT_2LINE();
+	return NETDEV_TX_OK;
+}
+static inline void mv_pp2x_cause_misc_handle(struct mv_pp2x_port *port,
+	struct mv_pp2x_hw *hw, u32 cause_rx_tx)
+{
+	u32 cause_misc = cause_rx_tx & MVPP2_CAUSE_MISC_SUM_MASK;
+
+	if (cause_misc) {
+		mv_pp2x_cause_error(port->dev, cause_misc);
+
+		/* Clear the cause register */
+		mv_pp2x_write(hw, MVPP2_ISR_MISC_CAUSE_REG, 0);
+		mv_pp2x_write(hw, MVPP2_ISR_RX_TX_CAUSE_REG(port->id),
+			    cause_rx_tx & ~MVPP2_CAUSE_MISC_SUM_MASK);
+	}
+}
+
+
+static inline int mv_pp2x_cause_rx_handle(struct mv_pp2x_port *port,
+		struct queue_vector *q_vec, struct napi_struct *napi,
+		int budget, u32 cause_rx)
+{
+	int rx_done = 0, count = 0;
+	struct mv_pp2x_rx_queue *rxq;
+
+	while (cause_rx && budget > 0) {
+		rxq = mv_pp2x_get_rx_queue(port, cause_rx);
+		if (!rxq)
+			break;
+
+		count = mv_pp2x_rx(port, &q_vec->napi, budget, rxq);
+		rx_done += count;
+		budget -= count;
+		if (budget > 0) {
+			/* Clear the bit associated to this Rx queue
+			 * so that next iteration will continue from
+			 * the next Rx queue.
+			 */
+			cause_rx &= ~(1 << rxq->log_id);
+		}
+	}
+	if (budget > 0) {
+		cause_rx = 0;
+		napi_complete(napi);
+		mv_pp2x_qvector_interrupt_enable(q_vec);
+	}
+	q_vec->pending_cause_rx = cause_rx;
+
+	return rx_done;
+}
+
+
+
+static int mv_pp21_poll(struct napi_struct *napi, int budget)
+{
+	u32 cause_rx_tx, cause_rx;
+	int rx_done = 0;
+	struct mv_pp2x_port *port = netdev_priv(napi->dev);
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	struct queue_vector *q_vec = container_of(napi,
+			struct queue_vector, napi);
+
+	/* Rx/Tx cause register
+	 *
+	 * Bits 0-15: each bit indicates received packets on the Rx queue
+	 * (bit 0 is for Rx queue 0).
+	 *
+	 * Bits 16-23: each bit indicates transmitted packets on the Tx queue
+	 * (bit 16 is for Tx queue 0).
+	 *
+	 * Each CPU has its own Rx/Tx cause register
+	 */
+	cause_rx_tx = mv_pp2x_read(hw, MVPP2_ISR_RX_TX_CAUSE_REG(port->id));
+
+	cause_rx_tx &= ~MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;
+
+	/*Process misc errors */
+	mv_pp2x_cause_misc_handle(port, hw, cause_rx_tx);
+
+	/* Process RX packets */
+	cause_rx = cause_rx_tx & MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK;
+	cause_rx |= q_vec->pending_cause_rx;
+
+	rx_done = mv_pp2x_cause_rx_handle(port, q_vec, napi, budget, cause_rx);
+
+	return rx_done;
+}
+
+
+static int mv_pp22_poll(struct napi_struct *napi, int budget)
+{
+	u32 cause_rx_tx, cause_rx, cause_tx;
+	int rx_done = 0;
+	struct mv_pp2x_port *port = netdev_priv(napi->dev);
+	struct mv_pp2x_hw *hw = &port->priv->hw;
+	struct queue_vector *q_vec = container_of(napi,
+			struct queue_vector, napi);
+
+	/* Rx/Tx cause register
+	 * Each CPU has its own Tx cause register
+	 */
+
+	/*The read is in the q_vector's sw_thread_id  address_space */
+	cause_rx_tx = mv_pp22_thread_read(hw, q_vec->sw_thread_id,
+			MVPP2_ISR_RX_TX_CAUSE_REG(port->id));
+
+
+	/*Process misc errors */
+	mv_pp2x_cause_misc_handle(port, hw, cause_rx_tx);
+
+	/* Release TX descriptors */
+	cause_tx = (cause_rx_tx & MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK) >>
+			MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_OFFSET;
+	if (cause_tx)
+		mv_pp2x_tx_done(port, cause_tx);
+
+	/* Process RX packets */
+	cause_rx = cause_rx_tx & MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK;
+	/*Convert queues from subgroup-relative to port-relative */
+	cause_rx <<= q_vec->first_rx_queue;
+	cause_rx |= q_vec->pending_cause_rx;
+
+	rx_done = mv_pp2x_cause_rx_handle(port, q_vec, napi, budget, cause_rx);
+
+	return rx_done;
+}
+
+void mv_pp2x_port_napi_enable(struct mv_pp2x_port *port)
+{
+	int i;
+
+	for (i = 0; i < port->num_qvector; i++)
+		napi_enable(&port->q_vector[i].napi);
+}
+
+void mv_pp2x_port_napi_disable(struct mv_pp2x_port *port)
+{
+	int i;
+
+	for (i = 0; i < port->num_qvector; i++)
+		napi_disable(&port->q_vector[i].napi);
+}
+
+static inline void mv_pp2x_port_irqs_dispose_mapping(struct mv_pp2x_port *port)
+{
+	int i;
+
+	for (i = 0; i < port->num_irqs; i++)
+		irq_dispose_mapping(port->of_irqs[i]);
+}
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+static int mvcpn110_mac_hw_init(struct mv_pp2x_port *port)
+{
+
+	struct gop_hw *gop = &port->priv->hw.gop;
+	struct mv_mac_data *mac = &port->mac_data;
+	int gop_port = mac->gop_index;
+
+
+	/* configure port PHY address */
+	mv_gop110_smi_phy_addr_cfg(gop, gop_port, mac->phy_addr);
+
+	/* enable MAC PTP unit */
+	/*mv_pp3_ptp_enable(emac, true);*/
+	/*mv_pp3_ptp_tai_tod_uio_init(shared_pdev);*/
+
+	mv_gop110_port_init(gop, mac);
+
+	if (mac->force_link)
+		mv_gop110_fl_cfg(gop, mac);
+
+	mac->flags |= MV_EMAC_F_INIT;
+
+	return 0;
+}
+#endif
+
+/* Set hw internals when starting port */
+void mv_pp2x_start_dev(struct mv_pp2x_port *port)
+{
+	struct gop_hw *gop = &port->priv->hw.gop;
+	struct mv_mac_data *mac = &port->mac_data;
+	int mac_num = port->mac_data.gop_index;
+#ifdef DEV_NETMAP
+	if (port->flags & MVPP2_F_IFCAP_NETMAP) {
+		if (mv_pp2x_netmap_rxq_init_buffers(port))
+			pr_debug("%s: Netmap rxq_init_buffers done\n",
+				__func__);
+		if (mv_pp2x_netmap_txq_init_buffers(port))
+			pr_debug("%s: Netmap txq_init_buffers done\n",
+				__func__);
+	}
+#endif /* DEV_NETMAP */
+	if (FPGA || port->priv->pp2_version == PPV21)
+		mv_pp21_gmac_max_rx_size_set(port);
+	else
+		mv_gop110_gmac_max_rx_size_set(gop, mac_num, port->pkt_size);
+	mv_pp2x_txp_max_tx_size_set(port);
+
+	mv_pp2x_port_napi_enable(port);
+
+	/* Enable RX/TX interrupts on all CPUs */
+#if !defined(CONFIG_MV_PP2_POLLING)
+	mv_pp2x_port_interrupts_enable(port);
+#endif
+
+	if (FPGA || port->priv->pp2_version == PPV21) {
+		mv_pp21_port_enable(port);
+	} else {
+		mv_gop110_port_events_mask(gop, mac);
+		mv_gop110_port_enable(gop, mac);
+	}
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	if (port->mac_data.phy_dev)
+		phy_start(port->mac_data.phy_dev);
+	else
+		mv_pp22_dev_link_event(port->dev);
+#else
+	if (!netif_carrier_ok(port->dev))
+		netif_carrier_on(port->dev);
+	netif_tx_start_all_queues(port->dev);
+#endif
+
+	tasklet_init(&port->link_change_tasklet, mv_pp2_link_change_tasklet,
+		(unsigned long)(port->dev));
+	/* Unmask link_event */
+	if (!FPGA && port->priv->pp2_version == PPV22)
+		mv_gop110_port_events_unmask(gop, mac);
+
+	mv_pp2x_egress_enable(port);
+	mv_pp2x_ingress_enable(port);
+	MVPP2_PRINT_VAR(mac->phy_mode);
+}
+
+/* Set hw internals when stopping port */
+void mv_pp2x_stop_dev(struct mv_pp2x_port *port)
+{
+	struct gop_hw *gop = &port->priv->hw.gop;
+	struct mv_mac_data *mac = &port->mac_data;
+
+	/* Stop new packets from arriving to RXQs */
+	mv_pp2x_ingress_disable(port);
+
+	mdelay(10);
+
+	/* Disable interrupts on all CPUs */
+	mv_pp2x_port_interrupts_disable(port);
+
+	mv_pp2x_port_napi_disable(port);
+
+	netif_carrier_off(port->dev);
+	netif_tx_stop_all_queues(port->dev);
+
+	mv_pp2x_egress_disable(port);
+	if (FPGA || port->priv->pp2_version == PPV21) {
+		mv_pp21_port_disable(port);
+	} else {
+		mv_gop110_port_events_mask(gop, mac);
+		mv_gop110_port_disable(gop, mac);
+		port->mac_data.flags &= ~MV_EMAC_F_LINK_UP;
+	}
+
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	if (port->mac_data.phy_dev)
+		phy_stop(port->mac_data.phy_dev);
+#endif
+}
+
+/* Return positive if MTU is valid */
+static inline int mv_pp2x_check_mtu_valid(struct net_device *dev, int mtu)
+{
+	if (mtu < 68) {
+		netdev_err(dev, "cannot change mtu to less than 68\n");
+		return -EINVAL;
+	}
+	if (MVPP2_RX_PKT_SIZE(mtu) > MVPP2_BM_LONG_PKT_SIZE &&
+		jumbo_pool == false) {
+		netdev_err(dev, "jumbo packet not supported (%d)\n", mtu);
+		return -EINVAL;
+	}
+
+	/* 9676 == 9700 - 20 and rounding to 8 */
+	if (mtu > 9676) {
+		netdev_info(dev, "illegal MTU value %d, round to 9676\n", mtu);
+		mtu = 9676;
+	}
+
+/*TOO: Below code is incorrect. Check if rounding to 8 is still relevant. */
+	return mtu;
+}
+
+int mv_pp2x_check_ringparam_valid(struct net_device *dev,
+				       struct ethtool_ringparam *ring)
+{
+	u16 new_rx_pending = ring->rx_pending;
+	u16 new_tx_pending = ring->tx_pending;
+
+	if (ring->rx_pending == 0 || ring->tx_pending == 0)
+		return -EINVAL;
+
+	if (ring->rx_pending > MVPP2_MAX_RXD)
+		new_rx_pending = MVPP2_MAX_RXD;
+	else if (!IS_ALIGNED(ring->rx_pending, 16))
+		new_rx_pending = ALIGN(ring->rx_pending, 16);
+
+	if (ring->tx_pending > MVPP2_MAX_TXD)
+		new_tx_pending = MVPP2_MAX_TXD;
+	else if (!IS_ALIGNED(ring->tx_pending, 32))
+		new_tx_pending = ALIGN(ring->tx_pending, 32);
+
+	if (ring->rx_pending != new_rx_pending) {
+		netdev_info(dev, "illegal Rx ring size value %d, round to %d\n",
+			    ring->rx_pending, new_rx_pending);
+		ring->rx_pending = new_rx_pending;
+	}
+
+	if (ring->tx_pending != new_tx_pending) {
+		netdev_info(dev, "illegal Tx ring size value %d, round to %d\n",
+			    ring->tx_pending, new_tx_pending);
+		ring->tx_pending = new_tx_pending;
+	}
+
+	return 0;
+}
+
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+static int mv_pp2x_phy_connect(struct mv_pp2x_port *port)
+{
+	struct phy_device *phy_dev;
+
+	phy_dev = of_phy_connect(port->dev, port->mac_data.phy_node,
+		mv_pp21_link_event, 0, port->mac_data.phy_mode);
+	if (!phy_dev) {
+		netdev_err(port->dev, "cannot connect to phy\n");
+		return -ENODEV;
+	}
+	phy_dev->supported &= PHY_GBIT_FEATURES;
+	phy_dev->advertising = phy_dev->supported;
+
+	port->mac_data.phy_dev = phy_dev;
+	port->mac_data.link    = 0;
+	port->mac_data.duplex  = 0;
+	port->mac_data.speed   = 0;
+
+	return 0;
+}
+
+static void mv_pp2x_phy_disconnect(struct mv_pp2x_port *port)
+{
+	if (port->mac_data.phy_dev) {
+		phy_disconnect(port->mac_data.phy_dev);
+		port->mac_data.phy_dev = NULL;
+	}
+}
+#endif
+
+int mv_pp2x_open_cls(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	unsigned char mac_bcast[ETH_ALEN] = {
+			0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+	int err;
+	u32 cpu_width = 0, cos_width = 0, port_rxq_width = 0;
+	u8 bound_cpu_first_rxq;
+
+	/* Calculate width */
+	mv_pp2x_width_calc(port->priv, &cpu_width, &cos_width, &port_rxq_width);
+	if (cpu_width + cos_width > port_rxq_width) {
+		err = -1;
+		netdev_err(dev, "cpu or cos queue width invalid\n");
+		return err;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	err = mv_pp2x_prs_mac_da_accept(hw, port->id, mac_bcast, true);
+	if (err) {
+		netdev_err(dev, "mv_pp2x_prs_mac_da_accept BC failed\n");
+		return err;
+	}
+
+	PALAD(MVPP2_PRINT_LINE());
+	err = mv_pp2x_prs_mac_da_accept(hw, port->id,
+				      dev->dev_addr, true);
+	if (err) {
+		netdev_err(dev, "mv_pp2x_prs_mac_da_accept MC failed\n");
+		return err;
+	}
+	err = mv_pp2x_prs_tag_mode_set(hw, port->id, MVPP2_TAG_TYPE_MH);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err) {
+		netdev_err(dev, "mv_pp2x_prs_tag_mode_set failed\n");
+		return err;
+	}
+
+	err = mv_pp2x_prs_def_flow(port);
+	PALAD(MVPP2_PRINT_LINE());
+	if (err) {
+		netdev_err(dev, "mv_pp2x_prs_def_flow failed\n");
+		return err;
+	}
+
+	err = mv_pp2x_prs_flow_set(port);
+	PALAD(MVPP2_PRINT_LINE());
+	if (err) {
+		netdev_err(dev, "mv_pp2x_prs_flow_set failed\n");
+		return err;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Set CoS classifier */
+	err = mv_pp2x_cos_classifier_set(port, cos_classifer);
+	if (err) {
+		netdev_err(port->dev, "cannot set cos classifier\n");
+		return err;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Init C2 rules */
+	bound_cpu_first_rxq  = mv_pp2x_bound_cpu_first_rxq_calc(port);
+	err = mv_pp2x_cls_c2_rule_set(port, bound_cpu_first_rxq);
+	if (err) {
+		netdev_err(port->dev, "cannot init C2 rules\n");
+		return err;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+
+	/* Assign rss table for rxq belong to this port */
+	err = mv_pp22_rss_rxq_set(port, cos_width);
+	if (err) {
+		netdev_err(port->dev, "cannot allocate rss table for rxq\n");
+		return err;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* RSS related config */
+	if (port->priv->pp2_cfg.queue_mode == MVPP2_QDIST_MULTI_MODE) {
+		/* Set RSS mode */
+		err = mv_pp22_rss_mode_set(port,
+			port->priv->pp2_cfg.rss_cfg.rss_mode);
+		if (err) {
+			netdev_err(port->dev, "cannot set rss mode\n");
+			return err;
+		}
+
+		PALAD(MVPP2_PRINT_LINE());
+		/* Init RSS table */
+		err = mv_pp22_rss_rxfh_indir_set(port);
+		if (err) {
+			netdev_err(port->dev, "cannot init rss rxfh indir\n");
+			return err;
+		}
+
+		PALAD(MVPP2_PRINT_LINE());
+
+		/* Set rss default CPU only when rss enabled */
+		if (port->priv->pp2_cfg.rss_cfg.rss_en) {
+			err = mv_pp22_rss_default_cpu_set(port,
+					port->priv->pp2_cfg.rss_cfg.dflt_cpu);
+			if (err) {
+				netdev_err(port->dev, "cannot set rss default cpu\n");
+				return err;
+			}
+		}
+
+	}
+
+	return 0;
+
+
+}
+int mv_pp2x_open(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	int err;
+
+
+	err = mv_pp2x_open_cls(dev);
+	if (err)
+		return err;
+
+
+	/* Allocate the Rx/Tx queues */
+	err = mv_pp2x_setup_rxqs(port);
+	if (err) {
+		netdev_err(port->dev, "cannot allocate Rx queues\n");
+		return err;
+	}
+	err = mv_pp2x_setup_txqs(port);
+	if (err) {
+		netdev_err(port->dev, "cannot allocate Tx queues\n");
+		goto err_cleanup_rxqs;
+	}
+
+#if !defined(CONFIG_MV_PP2_POLLING)
+	err = mv_pp2x_setup_irqs(dev, port);
+	if (err) {
+		netdev_err(port->dev, "cannot allocate irq's\n");
+		goto err_cleanup_txqs;
+	}
+#endif
+	/* In default link is down */
+	netif_carrier_off(port->dev);
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	/*FIXME: Should check which gop_version
+	 * (better, gop_version_attr: support_phy_connect), not the pp_version
+	 */
+	if (port->priv->pp2_version == PPV21) {
+		err = mv_pp2x_phy_connect(port);
+		if (err < 0)
+			goto err_free_irq;
+	}
+#endif
+
+#if !defined(CONFIG_MV_PP2_POLLING)
+
+	/* Unmask interrupts on all CPUs */
+	on_each_cpu(mv_pp2x_interrupts_unmask, port, 1);
+	PALAD(MVPP2_PRINT_LINE());
+
+
+	/* Unmask shared interrupts */
+	mv_pp2x_shared_thread_interrupts_unmask(port);
+
+	PALAD(MVPP2_PRINT_LINE());
+#endif
+
+#if defined(CONFIG_MV_PP2_POLLING)
+	if (cpu_poll_timer_ref_cnt == 0) {
+		cpu_poll_timer.expires  =
+		jiffies + msecs_to_jiffies(MV_PP2_FPGA_PERODIC_TIME*1000);
+		add_timer(&cpu_poll_timer);
+		cpu_poll_timer_ref_cnt++;
+	}
+#endif
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	if (port->priv->pp2_version == PPV22)
+		mvcpn110_mac_hw_init(port);
+#endif
+	mv_pp2x_start_dev(port);
+	PALAD(MVPP2_PRINT_LINE());
+
+#if defined(CONFIG_MV_PP2_FPGA) || defined(CONFIG_MV_PP2_PALLADIUM)
+	netif_carrier_on(port->dev);
+	netif_tx_start_all_queues(port->dev);
+
+#endif
+	MVPP2_PRINT_2LINE();
+	return 0;
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+err_free_irq:
+	mv_pp2x_cleanup_irqs(port);
+	PALAD(MVPP2_PRINT_LINE());
+#endif
+#if !defined(CONFIG_MV_PP2_POLLING)
+err_cleanup_txqs:
+	mv_pp2x_cleanup_txqs(port);
+	MVPP2_PRINT_2LINE();
+#endif
+err_cleanup_rxqs:
+	mv_pp2x_cleanup_rxqs(port);
+	MVPP2_PRINT_2LINE();
+	return err;
+}
+
+static int mv_pp2x_stop(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct mv_pp2x_port_pcpu *port_pcpu;
+	int cpu;
+
+#if defined(CONFIG_MV_PP2_POLLING)
+	cpu_poll_timer_ref_cnt--;
+	if (cpu_poll_timer_ref_cnt == 0)
+		del_timer_sync(&cpu_poll_timer);
+#endif
+	mv_pp2x_stop_dev(port);
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	if (port->priv->pp2_version == PPV21)
+		mv_pp2x_phy_disconnect(port);
+#endif
+#if !defined(CONFIG_MV_PP2_POLLING)
+	/* Mask interrupts on all CPUs */
+	on_each_cpu(mv_pp2x_interrupts_mask, port, 1);
+
+	/* Mask shared interrupts */
+	mv_pp2x_shared_thread_interrupts_mask(port);
+	mv_pp2x_cleanup_irqs(port);
+#endif
+	if (port->priv->pp2xdata->interrupt_tx_done == false) {
+		for_each_online_cpu(cpu) {
+			port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+#ifdef CONFIG_MV_PP2_PALLADIUM
+			del_timer(&port_pcpu->slow_tx_done_timer);
+#else
+			hrtimer_cancel(&port_pcpu->tx_done_timer);
+#endif
+			port_pcpu->timer_scheduled = false;
+			tasklet_kill(&port_pcpu->tx_done_tasklet);
+		}
+	}
+
+	mv_pp2x_cleanup_rxqs(port);
+	mv_pp2x_cleanup_txqs(port);
+
+	return 0;
+}
+
+static void mv_pp2x_set_rx_mode(struct net_device *dev)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+	struct netdev_hw_addr *ha;
+	int id = port->id;
+	bool allmulti = dev->flags & IFF_ALLMULTI;
+
+	mv_pp2x_prs_mac_promisc_set(hw, id, dev->flags & IFF_PROMISC);
+	mv_pp2x_prs_mac_multi_set(hw, id, MVPP2_PE_MAC_MC_ALL, allmulti);
+	mv_pp2x_prs_mac_multi_set(hw, id, MVPP2_PE_MAC_MC_IP6, allmulti);
+
+	/* Remove all port->id's mcast enries */
+	mv_pp2x_prs_mcast_del_all(hw, id);
+
+	if (allmulti && !netdev_mc_empty(dev)) {
+		netdev_for_each_mc_addr(ha, dev)
+			mv_pp2x_prs_mac_da_accept(hw, id, ha->addr, true);
+	}
+}
+
+static int mv_pp2x_set_mac_address(struct net_device *dev, void *p)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	const struct sockaddr *addr = p;
+	int err;
+
+	if (!is_valid_ether_addr(addr->sa_data)) {
+		err = -EADDRNOTAVAIL;
+		goto error;
+	}
+
+	if (!netif_running(dev)) {
+		err = mv_pp2x_prs_update_mac_da(dev, addr->sa_data);
+		if (!err)
+			return 0;
+		/* Reconfigure parser to accept the original MAC address */
+		err = mv_pp2x_prs_update_mac_da(dev, dev->dev_addr);
+		goto error;
+	}
+
+	mv_pp2x_stop_dev(port);
+
+	err = mv_pp2x_prs_update_mac_da(dev, addr->sa_data);
+	if (!err)
+		goto out_start;
+
+	/* Reconfigure parser accept the original MAC address */
+	err = mv_pp2x_prs_update_mac_da(dev, dev->dev_addr);
+	if (err)
+		goto error;
+out_start:
+	mv_pp2x_start_dev(port);
+	return 0;
+
+error:
+	netdev_err(dev, "fail to change MAC address\n");
+	return err;
+}
+
+static int mv_pp2x_change_mtu(struct net_device *dev, int mtu)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	int err;
+
+	mtu = mv_pp2x_check_mtu_valid(dev, mtu);
+	if (mtu < 0) {
+		err = mtu;
+		goto error;
+	}
+
+	if (!netif_running(dev)) {
+		err = mv_pp2x_bm_update_mtu(dev, mtu);
+		if (!err) {
+			port->pkt_size =  MVPP2_RX_PKT_SIZE(mtu);
+			return 0;
+		}
+
+		/* Reconfigure BM to the original MTU */
+		err = mv_pp2x_bm_update_mtu(dev, dev->mtu);
+		goto error;
+	}
+
+	mv_pp2x_stop_dev(port);
+
+	err = mv_pp2x_bm_update_mtu(dev, mtu);
+	if (!err) {
+		port->pkt_size =  MVPP2_RX_PKT_SIZE(mtu);
+		goto out_start;
+	}
+
+	/* Reconfigure BM to the original MTU */
+	err = mv_pp2x_bm_update_mtu(dev, dev->mtu);
+	if (err)
+		goto error;
+
+out_start:
+	mv_pp2x_start_dev(port);
+	return 0;
+
+error:
+	netdev_err(dev, "fail to change MTU\n");
+	return err;
+}
+
+static struct rtnl_link_stats64 *
+mv_pp2x_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	unsigned int start;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		struct mv_pp2x_pcpu_stats *cpu_stats;
+		u64 rx_packets;
+		u64 rx_bytes;
+		u64 tx_packets;
+		u64 tx_bytes;
+
+		cpu_stats = per_cpu_ptr(port->stats, cpu);
+		do {
+			start = u64_stats_fetch_begin_irq(&cpu_stats->syncp);
+			rx_packets = cpu_stats->rx_packets;
+			rx_bytes   = cpu_stats->rx_bytes;
+			tx_packets = cpu_stats->tx_packets;
+			tx_bytes   = cpu_stats->tx_bytes;
+		} while (u64_stats_fetch_retry_irq(&cpu_stats->syncp, start));
+
+		stats->rx_packets += rx_packets;
+		stats->rx_bytes   += rx_bytes;
+		stats->tx_packets += tx_packets;
+		stats->tx_bytes   += tx_bytes;
+	}
+
+	stats->rx_errors	= dev->stats.rx_errors;
+	stats->rx_dropped	= dev->stats.rx_dropped;
+	stats->tx_dropped	= dev->stats.tx_dropped;
+
+	return stats;
+}
+
+static int mv_pp2x_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	struct mv_pp2x_port *port = netdev_priv(dev);
+	int ret = 0;
+
+	if (!port->mac_data.phy_dev)
+		return -ENOTSUPP;
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	ret = phy_mii_ioctl(port->mac_data.phy_dev, ifr, cmd);
+	if (!ret)
+		mv_pp21_link_event(dev);
+#endif
+
+	return ret;
+}
+
+/*of_irq_count is not exported */
+int mv_pp2x_of_irq_count(struct device_node *dev)
+{
+	struct of_phandle_args irq;
+	int nr = 0;
+
+	while (of_irq_parse_one(dev, nr, &irq) == 0)
+		nr++;
+
+	return nr;
+}
+
+/* Currently only support LK-3.18 and above, no back support */
+static int mv_pp2x_netdev_set_features(struct net_device *dev,
+	netdev_features_t features)
+{
+	u32 changed = dev->features ^ features;
+	struct mv_pp2x_port *port = netdev_priv(dev);
+
+	/* dev->features is not changed */
+	if (!changed)
+		return 0;
+
+	if (changed & NETIF_F_RXHASH) {
+		if (features & NETIF_F_RXHASH) {
+			/* Enable RSS */
+			mv_pp22_rss_enable(port, true);
+		} else {
+			/* Disable RSS */
+			mv_pp22_rss_enable(port, false);
+		}
+	}
+
+	dev->features = features;
+
+	return 0;
+}
+
+/* Device ops */
+
+static const struct net_device_ops mv_pp2x_netdev_ops = {
+	.ndo_open		= mv_pp2x_open,
+	.ndo_stop		= mv_pp2x_stop,
+	.ndo_start_xmit		= mv_pp2x_tx,
+	.ndo_set_rx_mode	= mv_pp2x_set_rx_mode,
+	.ndo_set_mac_address	= mv_pp2x_set_mac_address,
+	.ndo_change_mtu		= mv_pp2x_change_mtu,
+	.ndo_get_stats64	= mv_pp2x_get_stats64,
+	.ndo_do_ioctl		= mv_pp2x_ioctl,
+	.ndo_set_features	= mv_pp2x_netdev_set_features,
+};
+
+/* Driver initialization */
+
+static void mv_pp21_port_power_up(struct mv_pp2x_port *port)
+{
+
+	mv_pp21_port_mii_set(port);
+	mv_pp21_port_periodic_xon_disable(port);
+	mv_pp21_port_fc_adv_enable(port);
+	mv_pp21_port_reset(port);
+}
+
+static int  mv_pp2x_port_txqs_init(struct device *dev,
+		struct mv_pp2x_port *port)
+{
+	int queue, cpu;
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+
+	port->tx_time_coal = MVPP2_TXDONE_COAL_USEC;
+
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		int queue_phy_id = mv_pp2x_txq_phys(port->id, queue);
+		struct mv_pp2x_tx_queue *txq;
+
+		txq = devm_kzalloc(dev, sizeof(*txq), GFP_KERNEL);
+		if (!txq)
+			return -ENOMEM;
+
+		txq->pcpu = alloc_percpu(struct mv_pp2x_txq_pcpu);
+		if (!txq->pcpu)
+			return(-ENOMEM);
+
+		txq->id = queue_phy_id;
+		txq->log_id = queue;
+		txq->pkts_coal = MVPP2_TXDONE_COAL_PKTS;
+
+		for_each_online_cpu(cpu) {
+			txq_pcpu = per_cpu_ptr(txq->pcpu, cpu);
+			txq_pcpu->cpu = cpu;
+		}
+
+		port->txqs[queue] = txq;
+	}
+
+	return 0;
+}
+
+static int  mv_pp2x_port_rxqs_init(struct device *dev,
+		struct mv_pp2x_port *port)
+{
+	int queue;
+
+	/* Allocate and initialize Rx queue for this port */
+	for (queue = 0; queue < port->num_rx_queues; queue++) {
+		struct mv_pp2x_rx_queue *rxq;
+
+		/* Map physical Rx queue to port's logical Rx queue */
+		rxq = devm_kzalloc(dev, sizeof(*rxq), GFP_KERNEL);
+		if (!rxq)
+			return(-ENOMEM);
+		/* Map this Rx queue to a physical queue */
+		rxq->id = port->first_rxq + queue;
+		rxq->port = port->id;
+		rxq->log_id = queue;
+
+		port->rxqs[queue] = rxq;
+	}
+
+	return 0;
+}
+
+static void mv_pp21_port_queue_vectors_init(struct mv_pp2x_port *port)
+{
+	struct queue_vector *q_vec = &port->q_vector[0];
+
+	q_vec[0].first_rx_queue = 0;
+	q_vec[0].num_rx_queues = port->num_rx_queues;
+	q_vec[0].parent = port;
+	q_vec[0].pending_cause_rx = 0;
+	q_vec[0].qv_type = MVPP2_SHARED;
+	q_vec[0].sw_thread_id = 0;
+	q_vec[0].sw_thread_mask = port->priv->cpu_map;
+	q_vec[0].irq = port->of_irqs[0];
+	netif_napi_add(port->dev, &q_vec[0].napi, mv_pp21_poll,
+		NAPI_POLL_WEIGHT);
+
+	port->num_qvector = 1;
+}
+
+static void mv_pp22_port_queue_vectors_init(struct mv_pp2x_port *port)
+{
+	int cpu;
+	struct queue_vector *q_vec = &port->q_vector[0];
+
+	/* Each cpu has zero private rx_queues */
+	for (cpu = 0; cpu < num_active_cpus(); cpu++) {
+		q_vec[cpu].parent = port;
+		q_vec[cpu].qv_type = MVPP2_PRIVATE;
+		q_vec[cpu].sw_thread_id = first_addr_space+cpu;
+		q_vec[cpu].sw_thread_mask = (1<<q_vec[cpu].sw_thread_id);
+		q_vec[cpu].pending_cause_rx = 0;
+#if !defined(CONFIG_MV_PP2_POLLING)
+		q_vec[cpu].irq = port->of_irqs[first_addr_space+cpu];
+#endif
+		netif_napi_add(port->dev, &q_vec[cpu].napi, mv_pp22_poll,
+			NAPI_POLL_WEIGHT);
+		if (mv_pp2x_queue_mode == MVPP2_QDIST_MULTI_MODE) {
+			q_vec[cpu].num_rx_queues = mv_pp2x_num_cos_queues;
+			q_vec[cpu].first_rx_queue = cpu*mv_pp2x_num_cos_queues;
+		} else {
+			q_vec[cpu].first_rx_queue = 0;
+			q_vec[cpu].num_rx_queues = 0;
+		}
+		port->num_qvector++;
+	}
+	/*Additional queue_vector for Shared RX */
+	if (mv_pp2x_queue_mode == MVPP2_QDIST_SINGLE_MODE) {
+		q_vec[cpu].parent = port;
+		q_vec[cpu].qv_type = MVPP2_SHARED;
+		q_vec[cpu].sw_thread_id = first_addr_space+cpu;
+		q_vec[cpu].sw_thread_mask = (1<<q_vec[cpu].sw_thread_id);
+		q_vec[cpu].pending_cause_rx = 0;
+#if !defined(CONFIG_MV_PP2_POLLING)
+		q_vec[cpu].irq = port->of_irqs[first_addr_space+cpu];
+#endif
+		netif_napi_add(port->dev, &q_vec[cpu].napi, mv_pp22_poll,
+			NAPI_POLL_WEIGHT);
+		q_vec[cpu].first_rx_queue = 0;
+		q_vec[cpu].num_rx_queues = port->num_rx_queues;
+
+		port->num_qvector++;
+	}
+}
+
+static void mv_pp21x_port_isr_rx_group_cfg(struct mv_pp2x_port *port)
+{
+	mv_pp21_isr_rx_group_write(&port->priv->hw, port->id,
+		port->num_rx_queues);
+}
+
+static void mv_pp22_port_isr_rx_group_cfg(struct mv_pp2x_port *port)
+{
+	int i;
+/*	u8 cur_rx_queue; */
+	struct mv_pp2x_hw *hw = &(port->priv->hw);
+
+	for (i = 0; i < port->num_qvector; i++) {
+		MVPP2_PRINT_LINE();
+		if (port->q_vector[i].num_rx_queues != 0) {
+			MVPP2_PRINT_LINE();
+			mv_pp22_isr_rx_group_write(hw, port->id,
+				port->q_vector[i].sw_thread_id,
+				port->q_vector[i].first_rx_queue,
+				port->q_vector[i].num_rx_queues);
+		}
+
+		MVPP2_PRINT_LINE();
+	}
+}
+
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+static int mv_pp2_init_emac_data(struct mv_pp2x_port *port,
+		struct device_node *emac_node)
+{
+	struct device_node *fixed_link_node, *phy_node;
+	int phy_mode;
+	u32 speed, id;
+
+	if (of_property_read_u32(emac_node, "port-id", &id))
+		return -EINVAL;
+
+	port->mac_data.gop_index = id;
+#if !defined(CONFIG_MV_PP2_POLLING)
+	port->mac_data.link_irq = irq_of_parse_and_map(emac_node, 0);
+#endif
+
+	if (of_phy_is_fixed_link(emac_node)) {
+		port->mac_data.force_link = true;
+		port->mac_data.link = true;
+		fixed_link_node = of_get_child_by_name(emac_node, "fixed-link");
+		port->mac_data.duplex = of_property_read_bool(fixed_link_node,
+				"full-duplex");
+		if (of_property_read_u32(fixed_link_node, "speed",
+				&port->mac_data.speed))
+			return -EINVAL;
+	} else {
+		port->mac_data.force_link = false;
+
+		phy_mode = of_get_phy_mode(emac_node);
+
+		pr_debug("gop_mac(%d), phy_mode(%d) (%s)\n", id,  phy_mode,
+			phy_modes(phy_mode));
+
+
+		switch (phy_mode) {
+		case PHY_INTERFACE_MODE_SGMII:
+			speed = 0;
+			/* check phy speed */
+			of_property_read_u32(emac_node, "phy-speed", &speed);
+			switch (speed) {
+			case 1000:
+				port->mac_data.speed = 1000; /* sgmii */
+				break;
+			case 2500:
+				port->mac_data.speed = 2500; /* sgmii */
+				port->mac_data.flags |= MV_EMAC_F_SGMII2_5;
+				break;
+			default:
+				port->mac_data.speed = 1000; /* sgmii */
+			}
+			break;
+		case PHY_INTERFACE_MODE_RXAUI:
+			break;
+		case PHY_INTERFACE_MODE_QSGMII:
+			break;
+		case PHY_INTERFACE_MODE_RGMII:
+			break;
+		case PHY_INTERFACE_MODE_KR:
+			break;
+
+		default:
+			pr_err("%s: incorrect phy-mode\n", __func__);
+			return -1;
+		}
+		port->mac_data.phy_mode = phy_mode;
+	}
+	pr_debug("gop_mac(%d), phy_speed(%d)\n", id,  port->mac_data.speed);
+
+
+	phy_node = of_parse_phandle(emac_node, "phy", 0);
+	if (phy_node) {
+		port->mac_data.phy_node = phy_node;
+		if (of_property_read_u32(phy_node, "reg",
+		    &port->mac_data.phy_addr))
+			pr_err("%s: NO PHY address on emac %d\n", __func__,
+			       port->mac_data.gop_index);
+
+		pr_debug("gop_mac(%d), phy_reg(%d)\n", id,
+			     port->mac_data.phy_addr);
+	} else {
+		pr_debug("No PHY NODE on emac %d\n", id);
+	}
+return 0;
+}
+#endif
+
+static u32 mvp_pp2x_gop110_netc_cfg_create(struct mv_pp2x *priv)
+{
+	u32 val = 0;
+	int i;
+	struct mv_pp2x_port *port;
+	struct mv_mac_data *mac;
+
+	for (i = 0; i < priv->num_ports; i++) {
+		port = priv->port_list[i];
+		mac = &port->mac_data;
+		if (mac->gop_index == 0) {
+			if (mac->phy_mode == PHY_INTERFACE_MODE_XAUI)
+				val |= MV_NETC_GE_MAC0_XAUI;
+			else if (mac->phy_mode == PHY_INTERFACE_MODE_RXAUI)
+				val |= MV_NETC_GE_MAC0_RXAUI_L23;
+		}
+		if (mac->gop_index == 2) {
+			if (mac->phy_mode == PHY_INTERFACE_MODE_SGMII)
+				val |= MV_NETC_GE_MAC2_SGMII;
+		}
+		if (mac->gop_index == 3) {
+			if (mac->phy_mode == PHY_INTERFACE_MODE_SGMII)
+				val |= MV_NETC_GE_MAC3_SGMII;
+			else if (mac->phy_mode == PHY_INTERFACE_MODE_RGMII)
+				val |= MV_NETC_GE_MAC3_RGMII;
+		}
+
+	}
+	return val;
+}
+
+
+/* Initialize port HW */
+static int mv_pp2x_port_init(struct mv_pp2x_port *port)
+{
+	struct device *dev = port->dev->dev.parent;
+	struct mv_pp2x *priv = port->priv;
+	struct gop_hw *gop = &port->priv->hw.gop;
+	struct mv_mac_data *mac = &port->mac_data;
+	int queue, err;
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Disable port */
+	mv_pp2x_egress_disable(port);
+
+	if (FPGA || port->priv->pp2_version == PPV21)
+		mv_pp21_port_disable(port);
+	else
+		mv_gop110_port_disable(gop, mac);
+
+	/* Allocate queues */
+	port->txqs = devm_kcalloc(dev, port->num_tx_queues, sizeof(*port->txqs),
+				  GFP_KERNEL);
+	if (!port->txqs)
+		return -ENOMEM;
+	port->rxqs = devm_kcalloc(dev, port->num_rx_queues, sizeof(*port->rxqs),
+				  GFP_KERNEL);
+	if (!port->rxqs)
+		return -ENOMEM;
+
+	/* Associate physical Tx queues to port and initialize.  */
+	err = mv_pp2x_port_txqs_init(dev, port);
+
+	if (err)
+		goto err_free_percpu;
+
+	/* Associate physical Rx queues to port and initialize.  */
+	err = mv_pp2x_port_rxqs_init(dev, port);
+
+	if (err)
+		goto err_free_percpu;
+
+	/* Configure queue_vectors */
+	priv->pp2xdata->mv_pp2x_port_queue_vectors_init(port);
+
+	/* Configure Rx queue group interrupt for this port */
+	priv->pp2xdata->mv_pp2x_port_isr_rx_group_cfg(port);
+
+	/* Create Rx descriptor rings */
+	for (queue = 0; queue < port->num_rx_queues; queue++) {
+		struct mv_pp2x_rx_queue *rxq = port->rxqs[queue];
+
+		rxq->size = port->rx_ring_size;
+		rxq->pkts_coal = MVPP2_RX_COAL_PKTS;
+		rxq->time_coal = MVPP2_RX_COAL_USEC;
+	}
+
+	mv_pp2x_ingress_disable(port);
+
+	/* Port default configuration */
+	mv_pp2x_defaults_set(port);
+
+	/* Port's classifier configuration */
+	mv_pp2x_cls_oversize_rxq_set(port);
+	mv_pp2x_cls_port_config(port);
+
+	/* Provide an initial Rx packet size */
+	port->pkt_size = MVPP2_RX_PKT_SIZE(port->dev->mtu);
+
+	/* Initialize pools for swf */
+	err = mv_pp2x_swf_bm_pool_init(port);
+	if (err)
+		goto err_free_percpu;
+	return 0;
+
+err_free_percpu:
+	for (queue = 0; queue < port->num_tx_queues; queue++) {
+		if (!port->txqs[queue])
+			continue;
+		free_percpu(port->txqs[queue]->pcpu);
+	}
+	return err;
+}
+
+
+/* Ports initialization */
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+static int mv_pp2x_port_probe(struct platform_device *pdev,
+			    struct device_node *port_node,
+			    struct mv_pp2x *priv)
+{
+	struct device_node *emac_node;
+	struct mv_pp2x_port *port;
+	struct mv_pp2x_port_pcpu *port_pcpu;
+	struct net_device *dev;
+	struct resource *res;
+	const char *dt_mac_addr;
+	const char *mac_from;
+	char hw_mac_addr[ETH_ALEN];
+	u32 id;
+	int features, err = 0, i, cpu;
+	int priv_common_regs_num = 2;
+#if !defined(CONFIG_MV_PP2_POLLING)
+	unsigned int *port_irqs;
+	int port_num_irq;
+#endif
+
+	dev = alloc_etherdev_mqs(sizeof(struct mv_pp2x_port),
+		mv_pp2x_txq_number, mv_pp2x_rxq_number);
+	if (!dev)
+		return -ENOMEM;
+
+
+
+	/*Connect entities */
+	port = netdev_priv(dev);
+	port->dev = dev;
+	SET_NETDEV_DEV(dev, &pdev->dev);
+	port->priv = priv;
+
+	if (of_property_read_u32(port_node, "port-id", &id)) {
+		err = -EINVAL;
+		dev_err(&pdev->dev, "missing port-id value\n");
+		goto err_free_netdev;
+	}
+	port->id = id;
+
+	emac_node = of_parse_phandle(port_node, "emac-data", 0);
+	if (!emac_node) {
+		dev_err(&pdev->dev, "missing emac-data\n");
+		err = -EINVAL;
+		goto err_free_netdev;
+
+	}
+	/* Init emac_data, includes link interrupt */
+	if (mv_pp2_init_emac_data(port, emac_node))
+		goto err_free_netdev;
+
+
+	/* get MAC address */
+	dt_mac_addr = of_get_mac_address(emac_node);
+	if (dt_mac_addr && is_valid_ether_addr(dt_mac_addr)) {
+		mac_from = "device tree";
+		ether_addr_copy(dev->dev_addr, dt_mac_addr);
+		pr_debug("gop_index(%d), mac_addr %x:%x:%x:%x:%x:%x",
+			port->mac_data.gop_index, dev->dev_addr[0],
+			dev->dev_addr[1], dev->dev_addr[2], dev->dev_addr[3],
+			dev->dev_addr[4], dev->dev_addr[5]);
+	} else {
+		if (priv->pp2_version == PPV21)
+			mv_pp21_get_mac_address(port, hw_mac_addr);
+		if (is_valid_ether_addr(hw_mac_addr)) {
+			mac_from = "hardware";
+			ether_addr_copy(dev->dev_addr, hw_mac_addr);
+		} else {
+			mac_from = "random";
+			eth_hw_addr_random(dev);
+		}
+	}
+
+	/* Tx/Rx Interrupt */
+#if !defined(CONFIG_MV_PP2_POLLING)
+	port_num_irq = mv_pp2x_of_irq_count(port_node);
+	if (port_num_irq != priv->pp2xdata->num_port_irq) {
+		dev_err(&pdev->dev,
+			"port(%d)-number of irq's doesn't match hw\n", id);
+		goto err_free_netdev;
+	}
+	port_irqs = devm_kcalloc(&pdev->dev, port_num_irq,
+			sizeof(u32), GFP_KERNEL);
+	port->of_irqs = port_irqs;
+	port->num_irqs = 0;
+
+	for (i = 0; i < port_num_irq; i++) {
+		port_irqs[i] = irq_of_parse_and_map(port_node, i);
+		if (port_irqs[i] == 0) {
+			dev_err(&pdev->dev,
+				"Fail to parse port(%d), irq(%d)\n", id, i);
+			err = -EINVAL;
+			goto err_free_irq;
+		}
+		port->num_irqs++;
+	}
+#endif
+
+	/*FIXME, full handling loopback */
+	if (of_property_read_bool(port_node, "marvell,loopback"))
+		port->flags |= MVPP2_F_LOOPBACK;
+
+	port->num_tx_queues = mv_pp2x_txq_number;
+	port->num_rx_queues = mv_pp2x_rxq_number;
+
+
+	dev->tx_queue_len = MVPP2_MAX_TXD;
+	dev->watchdog_timeo = 5 * HZ;
+	dev->netdev_ops = &mv_pp2x_netdev_ops;
+	mv_pp2x_set_ethtool_ops(dev);
+
+	/*YuvalC: Port first_rxq relative to port->id, not dependent on board
+	 * topology, i.e. not dynamically allocated
+	 */
+	port->first_rxq = (port->id)*(priv->pp2xdata->pp2x_max_port_rxqs) +
+		first_log_rxq_queue;
+
+	if (priv->pp2_version == PPV21) {
+		res = platform_get_resource(pdev, IORESOURCE_MEM,
+					    priv_common_regs_num + id);
+		port->base = devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(port->base)) {
+			err = PTR_ERR(port->base);
+			goto err_free_irq;
+		}
+	}
+
+	/* Alloc per-cpu stats */
+	port->stats = netdev_alloc_pcpu_stats(struct mv_pp2x_pcpu_stats);
+	if (!port->stats) {
+		err = -ENOMEM;
+		goto err_free_irq;
+	}
+
+
+	port->tx_ring_size = tx_queue_size;
+	port->rx_ring_size = rx_queue_size;
+
+	err = mv_pp2x_port_init(port);
+	if (err < 0) {
+		dev_err(&pdev->dev, "failed to init port %d\n", id);
+		goto err_free_stats;
+	}
+	MVPP2_PRINT_LINE();
+	if (port->priv->pp2_version == PPV21)
+		mv_pp21_port_power_up(port);
+
+	port->pcpu = alloc_percpu(struct mv_pp2x_port_pcpu);
+	if (!port->pcpu) {
+		err = -ENOMEM;
+		goto err_free_txq_pcpu;
+	}
+	MVPP2_PRINT_LINE();
+	if (port->priv->pp2xdata->interrupt_tx_done == false) {
+		for_each_online_cpu(cpu) {
+			port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+
+			hrtimer_init(&port_pcpu->tx_done_timer, CLOCK_MONOTONIC,
+				     HRTIMER_MODE_REL_PINNED);
+			port_pcpu->tx_done_timer.function = mv_pp2x_hr_timer_cb;
+			port_pcpu->timer_scheduled = false;
+
+			tasklet_init(&port_pcpu->tx_done_tasklet,
+				mv_pp2x_tx_proc_cb, (unsigned long)dev);
+		}
+	}
+	MVPP2_PRINT_LINE();
+	features = NETIF_F_SG | NETIF_F_IP_CSUM;
+	dev->features = features | NETIF_F_RXCSUM;
+	dev->hw_features |= features | NETIF_F_RXCSUM | NETIF_F_GRO;
+	/* Only when multi queue mode, rxhash is supported */
+	if (mv_pp2x_queue_mode)
+		dev->hw_features |= NETIF_F_RXHASH;
+	dev->vlan_features |= features;
+	MVPP2_PRINT_LINE();
+	err = register_netdev(dev);
+	if (err < 0) {
+		dev_err(&pdev->dev, "failed to register netdev\n");
+		goto err_free_port_pcpu;
+	}
+	netdev_info(dev, "Using %s mac address %pM\n", mac_from, dev->dev_addr);
+
+	MVPP2_PRINT_LINE();
+	priv->port_list[priv->num_ports] = port;
+	priv->num_ports++;
+	return 0;
+	dev_err(&pdev->dev, "%s failed for port_id(%d)\n", __func__, id);
+#ifdef DEV_NETMAP
+	mv_pp2x_netmap_attach(port);
+#endif /* DEV_NETMAP */
+
+err_free_port_pcpu:
+	free_percpu(port->pcpu);
+err_free_txq_pcpu:
+	for (i = 0; i < mv_pp2x_txq_number; i++)
+		free_percpu(port->txqs[i]->pcpu);
+err_free_stats:
+	free_percpu(port->stats);
+err_free_irq:
+#if !defined(CONFIG_MV_PP2_POLLING)
+	mv_pp2x_port_irqs_dispose_mapping(port);
+#endif
+err_free_netdev:
+	free_netdev(dev);
+	return err;
+}
+
+#else
+
+static int mv_pp2x_port_probe_fpga(struct platform_device *pdev,
+				int port_i,
+			    struct mv_pp2x *priv)
+{
+	struct device_node *phy_node = NULL;
+	struct mv_pp2x_port *port;
+	struct mv_pp2x_port_pcpu *port_pcpu;
+	struct net_device *dev;
+	const char *mac_from;
+	char hw_mac_addr[ETH_ALEN];
+	u32 id;
+	int features, phy_mode = 0, err = 0, i, cpu;
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	dev = alloc_etherdev_mqs(sizeof(struct mv_pp2x_port),
+		mv_pp2x_txq_number, mv_pp2x_rxq_number);
+	if (!dev)
+		return -ENOMEM;
+	MVPP2_PRINT_LINE();
+
+	dev->tx_queue_len = MVPP2_MAX_TXD;
+	dev->watchdog_timeo = 5 * HZ;
+	dev->netdev_ops = &mv_pp2x_netdev_ops;
+	mv_pp2x_set_ethtool_ops(dev);
+	PALAD(MVPP2_PRINT_LINE());
+
+	port = netdev_priv(dev);
+
+	port->priv = priv;
+	port->id = port_i;
+	port->num_tx_queues = mv_pp2x_txq_number;
+	port->num_rx_queues = mv_pp2x_rxq_number;
+
+	/*YuvalC: Port first_rxq relative to port->id, not dependent on
+	 * board topology, i.e. not dynamically allocated
+	 */
+	port->first_rxq = (port->id)*(priv->pp2xdata->pp2x_max_port_rxqs) +
+		first_log_rxq_queue;
+	port->mac_data.phy_node = phy_node;
+	port->mac_data.phy_mode = phy_mode;
+	port->base = ((mv_pp2_vfpga_address + MAC_PORT0_OFFSET) +
+		((port->id) * 0x1000));
+	DBG_MSG("mvpp2(%d): port_probe: id-%d vfpga_add=0x%p base=0x%p\n",
+		__LINE__, port->id, mv_pp2_vfpga_address, port->base);
+	MVPP2_PRINT_LINE();
+
+	if (IS_ERR(port->base)) {
+		PALAD(MVPP2_PRINT_LINE());
+		err = PTR_ERR(port->base);
+		goto err_free_irq;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Alloc per-cpu stats */
+	port->stats = netdev_alloc_pcpu_stats(struct mv_pp2x_pcpu_stats);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (!port->stats) {
+		MVPP2_PRINT_LINE();
+
+		err = -ENOMEM;
+		goto err_free_irq;
+	}
+
+
+	mac_from = "hardware";
+	hw_mac_addr[0] = 0x02;
+	hw_mac_addr[1] = 0x68;
+	hw_mac_addr[2] = 0xb3;
+	hw_mac_addr[3] = 0x29;
+	hw_mac_addr[4] = 0xda;
+	hw_mac_addr[5] = 0x98 | port_i;
+
+	ether_addr_copy(dev->dev_addr, hw_mac_addr);
+
+	port->tx_ring_size = tx_queue_size;
+	port->rx_ring_size = rx_queue_size;
+	port->dev = dev;
+	SET_NETDEV_DEV(dev, &pdev->dev);
+	err = mv_pp2x_port_init(port);
+
+	PALAD(MVPP2_PRINT_LINE());
+	if (err < 0) {
+		dev_err(&pdev->dev, "failed to init port %d\n", port->id);
+		goto err_free_stats;
+	}
+	/* FPGA uses ppv21 GOP */
+	mv_pp21_port_power_up(port);
+	MVPP2_PRINT_LINE();
+
+	port->pcpu = alloc_percpu(struct mv_pp2x_port_pcpu);
+	if (!port->pcpu) {
+		err = -ENOMEM;
+		goto err_free_txq_pcpu;
+	}
+	MVPP2_PRINT_LINE();
+
+	if (port->priv->pp2xdata->interrupt_tx_done == false) {
+		for_each_online_cpu(cpu) {
+			port_pcpu = per_cpu_ptr(port->pcpu, cpu);
+
+#ifdef CONFIG_MV_PP2_PALLADIUM
+			init_timer(&port_pcpu->slow_tx_done_timer);
+			port_pcpu->slow_tx_done_timer.function =
+				mv_pp2x_timer_cb;
+			port_pcpu->slow_tx_done_timer.data =
+				(unsigned long) (port_pcpu);
+#else
+			hrtimer_init(&port_pcpu->tx_done_timer,
+				CLOCK_MONOTONIC,
+				HRTIMER_MODE_REL_PINNED);
+			port_pcpu->tx_done_timer.function =
+				mv_pp2x_hr_timer_cb;
+#endif
+			port_pcpu->timer_scheduled = false;
+
+			tasklet_init(&port_pcpu->tx_done_tasklet,
+				mv_pp2x_tx_proc_cb, (unsigned long)dev);
+
+		}
+	}
+	MVPP2_PRINT_LINE();
+
+	features = NETIF_F_SG | NETIF_F_IP_CSUM;
+	dev->features = features | NETIF_F_RXCSUM;
+	dev->hw_features |= features | NETIF_F_RXCSUM | NETIF_F_GRO;
+	/* Only when multi queue mode, rxhash is supported */
+	if (mv_pp2x_queue_mode)
+		dev->hw_features |= NETIF_F_RXHASH;
+	dev->vlan_features |= features;
+
+	err = register_netdev(dev);
+	PALAD(MVPP2_PRINT_LINE());
+
+	if (err < 0) {
+		MVPP2_PRINT_LINE();
+
+		dev_err(&pdev->dev, "failed to register netdev\n");
+		goto err_free_port_pcpu;
+	}
+
+#ifdef DEV_NETMAP
+	mv_pp2x_netmap_attach(port);
+#endif /* DEV_NETMAP */
+	netdev_info(dev, "Using %s mac address %pM\n", mac_from, dev->dev_addr);
+	priv->port_list[priv->num_ports] = port;
+	priv->num_ports++;
+	return 0;
+	dev_err(&pdev->dev, "%s failed for port_id(%d)\n", __func__, id);
+
+err_free_port_pcpu:
+	free_percpu(port->pcpu);
+err_free_txq_pcpu:
+	for (i = 0; i < mv_pp2x_txq_number; i++)
+		free_percpu(port->txqs[i]->pcpu);
+err_free_stats:
+	free_percpu(port->stats);
+err_free_irq:
+#if !defined(CONFIG_MV_PP2_POLLING)
+	mv_pp2x_port_irqs_dispose_mapping(port);
+#endif
+	free_netdev(dev);
+	return err;
+}
+
+
+#endif
+
+/* Ports removal routine */
+static void mv_pp2x_port_remove(struct mv_pp2x_port *port)
+{
+	int i;
+#ifdef DEV_NETMAP
+	netmap_detach(port->dev);
+#endif /* DEV_NETMAP */
+
+	unregister_netdev(port->dev);
+	free_percpu(port->pcpu);
+	free_percpu(port->stats);
+	for (i = 0; i < port->num_tx_queues; i++)
+		free_percpu(port->txqs[i]->pcpu);
+#if !defined(CONFIG_MV_PP2_POLLING)
+	mv_pp2x_port_irqs_dispose_mapping(port);
+#endif
+	free_netdev(port->dev);
+}
+
+
+/* Initialize decoding windows */
+static void mv_pp2x_conf_mbus_windows(const struct mbus_dram_target_info *dram,
+				    struct mv_pp2x_hw *hw)
+{
+	u32 win_enable;
+	int i;
+
+	for (i = 0; i < 6; i++) {
+		mv_pp2x_write(hw, MVPP2_WIN_BASE(i), 0);
+		mv_pp2x_write(hw, MVPP2_WIN_SIZE(i), 0);
+
+		if (i < 4)
+			mv_pp2x_write(hw, MVPP2_WIN_REMAP(i), 0);
+	}
+
+	win_enable = 0;
+
+	for (i = 0; i < dram->num_cs; i++) {
+		const struct mbus_dram_window *cs = dram->cs + i;
+
+		mv_pp2x_write(hw, MVPP2_WIN_BASE(i),
+			    (cs->base & 0xffff0000) | (cs->mbus_attr << 8) |
+			    dram->mbus_dram_target_id);
+
+		mv_pp2x_write(hw, MVPP2_WIN_SIZE(i),
+			    (cs->size - 1) & 0xffff0000);
+
+		win_enable |= (1 << i);
+	}
+
+	mv_pp2x_write(hw, MVPP2_BASE_ADDR_ENABLE, win_enable);
+}
+
+/* Initialize Rx FIFO's */
+static void mv_pp2x_rx_fifo_init(struct mv_pp2x_hw *hw)
+{
+	int port;
+
+	for (port = 0; port < MVPP2_MAX_PORTS; port++) {
+		mv_pp2x_write(hw, MVPP2_RX_DATA_FIFO_SIZE_REG(port),
+			    MVPP2_RX_FIFO_PORT_DATA_SIZE);
+		mv_pp2x_write(hw, MVPP2_RX_ATTR_FIFO_SIZE_REG(port),
+			    MVPP2_RX_FIFO_PORT_ATTR_SIZE);
+	}
+
+	mv_pp2x_write(hw, MVPP2_RX_MIN_PKT_SIZE_REG,
+		    MVPP2_RX_FIFO_PORT_MIN_PKT);
+	mv_pp2x_write(hw, MVPP2_RX_FIFO_INIT_REG, 0x1);
+}
+
+/* Initialize network controller common part HW */
+static int mv_pp2x_init(struct platform_device *pdev, struct mv_pp2x *priv)
+{
+	int err, i, cpu;
+	int last_log_rx_queue;
+	u32 val;
+	const struct mbus_dram_target_info *dram_target_info;
+	u8 pp2_ver = priv->pp2xdata->pp2x_ver;
+	struct mv_pp2x_hw *hw = &priv->hw;
+
+	/* Checks for hardware constraints */
+	last_log_rx_queue = first_log_rxq_queue + mv_pp2x_rxq_number;
+	if (last_log_rx_queue > priv->pp2xdata->pp2x_max_port_rxqs) {
+		dev_err(&pdev->dev, "too high num_cos_queue parameter\n");
+		return -EINVAL;
+	}
+
+	PALAD(MVPP2_PRINT_LINE());
+	/*TODO: YuvalC, replace this with a per-pp2x validation function. */
+	if ((pp2_ver == PPV21) && (mv_pp2x_rxq_number % 4)) {
+		dev_err(&pdev->dev, "invalid num_cos_queue parameter\n");
+		return -EINVAL;
+	}
+	PALAD(MVPP2_PRINT_LINE());
+
+	if (mv_pp2x_txq_number > MVPP2_MAX_TXQ) {
+		dev_err(&pdev->dev, "invalid num_cos_queue parameter\n");
+		return -EINVAL;
+	}
+	MVPP2_PRINT_LINE();
+
+	/* MBUS windows configuration */
+	dram_target_info = mv_mbus_dram_info();
+	if (dram_target_info)
+		mv_pp2x_conf_mbus_windows(dram_target_info, hw);
+	MVPP2_PRINT_LINE();
+
+#if !defined(CONFIG_MV_PP2_FPGA)
+	/*AXI Bridge Configuration */
+	mv_pp2x_write(hw, MVPP22_AXI_BM_WR_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_BM_RD_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_AGGRQ_DESCR_RD_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_TXQ_DESCR_WR_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_TXQ_DESCR_RD_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_RXQ_DESCR_WR_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_RX_DATA_WR_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	mv_pp2x_write(hw, MVPP22_AXI_TX_DATA_RD_ATTR_REG,
+		MVPP22_AXI_ATTR_SNOOP_CNTRL_BIT);
+	MVPP2_PRINT_LINE();
+#endif
+
+	/* Disable HW PHY polling */
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	if (priv->pp2_version == PPV21) {
+		val = readl(hw->lms_base + MVPP2_PHY_AN_CFG0_REG);
+		val |= MVPP2_PHY_AN_STOP_SMI0_MASK;
+		writel(val, hw->lms_base + MVPP2_PHY_AN_CFG0_REG);
+		writel(MVPP2_EXT_GLOBAL_CTRL_DEFAULT,
+			hw->lms_base + MVPP2_MNG_EXTENDED_GLOBAL_CTRL_REG);
+	}
+#endif
+	MVPP2_PRINT_LINE();
+
+	/* Allocate and initialize aggregated TXQs */
+	priv->aggr_txqs = devm_kcalloc(&pdev->dev, num_active_cpus(),
+				       sizeof(struct mv_pp2x_aggr_tx_queue),
+				       GFP_KERNEL);
+	PALAD(MVPP2_PRINT_LINE());
+
+	if (!priv->aggr_txqs)
+		return -ENOMEM;
+	priv->num_aggr_qs = num_active_cpus();
+	PALAD(MVPP2_PRINT_LINE());
+
+	i = 0;
+	for_each_online_cpu(cpu) {
+		PALAD(MVPP2_PRINT_LINE());
+		priv->aggr_txqs[i].id = cpu;
+		priv->aggr_txqs[i].size = MVPP2_AGGR_TXQ_SIZE;
+
+		err = mv_pp2x_aggr_txq_init(pdev, &priv->aggr_txqs[i],
+					  MVPP2_AGGR_TXQ_SIZE, i, priv);
+		if (err < 0)
+			return err;
+		i++;
+	}
+	MVPP2_PRINT_LINE();
+
+	/* Rx Fifo Init */
+	mv_pp2x_rx_fifo_init(hw);
+
+	MVPP2_PRINT_LINE();
+
+
+	/* Allow cache snoop when transmiting packets */
+	mv_pp2x_write(hw, MVPP2_TX_SNOOP_REG, 0x1);
+	MVPP2_PRINT_LINE();
+
+
+	/* Buffer Manager initialization */
+	err = mv_pp2x_bm_init(pdev, priv);
+	if (err < 0)
+		return err;
+	MVPP2_PRINT_LINE();
+
+	/* Parser flow id attribute tbl init */
+	mv_pp2x_prs_flow_id_attr_init();
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Parser default initialization */
+	err = mv_pp2x_prs_default_init(pdev, hw);
+	if (err < 0)
+		return err;
+	MVPP2_PRINT_LINE();
+
+	/* Classifier default initialization */
+	err = mv_pp2x_cls_init(pdev, hw);
+	if (err < 0)
+		return err;
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Classifier engine2 initialization */
+	err = mv_pp2x_c2_init(pdev, hw);
+	if (err < 0)
+		return err;
+	MVPP2_PRINT_LINE();
+
+
+	if (pp2_ver == PPV22) {
+		for (i = 0; i < 128; i++) {
+			val = mv_pp2x_read(hw, MVPP2_RXQ_CONFIG_REG(i));
+			val |= MVPP2_RXQ_DISABLE_MASK;
+			mv_pp2x_write(hw, MVPP2_RXQ_CONFIG_REG(i), val);
+		}
+	}
+	MVPP2_PRINT_LINE();
+
+	return 0;
+}
+
+
+static struct mv_pp2x_platform_data pp21_pdata = {
+	.pp2x_ver = PPV21,
+	.pp2x_max_port_rxqs = 8,
+	.mv_pp2x_rxq_short_pool_set = mv_pp21_rxq_short_pool_set,
+	.mv_pp2x_rxq_long_pool_set = mv_pp21_rxq_long_pool_set,
+	.multi_addr_space = false,
+	.interrupt_tx_done = false,
+	.multi_hw_instance = false,
+	.mv_pp2x_port_queue_vectors_init = mv_pp21_port_queue_vectors_init,
+	.mv_pp2x_port_isr_rx_group_cfg = mv_pp21x_port_isr_rx_group_cfg,
+	.num_port_irq = 1,
+	.hw.desc_queue_addr_shift = MVPP21_DESC_ADDR_SHIFT,
+#ifdef CONFIG_64BIT
+	.skb_base_addr = 0,
+	.skb_base_mask = DMA_BIT_MASK(32),
+#endif
+};
+
+static struct mv_pp2x_platform_data pp22_pdata = {
+	.pp2x_ver = PPV22,
+	.pp2x_max_port_rxqs = 32,
+	.mv_pp2x_rxq_short_pool_set = mv_pp22_rxq_short_pool_set,
+	.mv_pp2x_rxq_long_pool_set = mv_pp22_rxq_long_pool_set,
+	.multi_addr_space = true,
+#ifdef CONFIG_MV_PP2_POLLING
+	.interrupt_tx_done = false,
+#else
+	.interrupt_tx_done = true,
+#endif
+	.multi_hw_instance = true,
+	.mv_pp2x_port_queue_vectors_init = mv_pp22_port_queue_vectors_init,
+	.mv_pp2x_port_isr_rx_group_cfg = mv_pp22_port_isr_rx_group_cfg,
+	.num_port_irq = 1,
+	.hw.desc_queue_addr_shift = MVPP22_DESC_ADDR_SHIFT,
+	.skb_base_addr = 0,
+#ifdef CONFIG_64BIT
+	.skb_base_mask = DMA_BIT_MASK(40),
+#else
+	.skb_base_mask = DMA_BIT_MASK(32),
+#endif
+};
+
+
+
+static const struct of_device_id mv_pp2x_match_tbl[] = {
+		{
+			.compatible = "marvell,armada-375-pp2",
+			.data = &pp21_pdata,
+		},
+		{
+			.compatible = "marvell,mv-pp22",
+			.data = &pp22_pdata,
+		},
+	{ }
+};
+
+static void mv_pp2x_init_config(struct mv_pp2x_param_config *pp2_cfg,
+	u32 cell_index)
+{
+	pp2_cfg->cell_index = cell_index;
+	pp2_cfg->first_bm_pool = first_bm_pool;
+	pp2_cfg->first_sw_thread = first_addr_space;
+	pp2_cfg->first_log_rxq = first_log_rxq_queue;
+	pp2_cfg->jumbo_pool = jumbo_pool;
+	pp2_cfg->queue_mode = mv_pp2x_queue_mode;
+
+	pp2_cfg->cos_cfg.cos_classifier = cos_classifer;
+	pp2_cfg->cos_cfg.default_cos = default_cos;
+	pp2_cfg->cos_cfg.num_cos_queues = mv_pp2x_num_cos_queues;
+	pp2_cfg->cos_cfg.pri_map = pri_map;
+
+	pp2_cfg->rss_cfg.dflt_cpu = default_cpu;
+	/* RSS is disabled as default, which can be update in running time */
+	pp2_cfg->rss_cfg.rss_en = 0;
+	pp2_cfg->rss_cfg.rss_mode = rss_mode;
+
+	pp2_cfg->rx_cpu_map = port_cpu_bind_map;
+}
+
+static void mv_pp2x_init_rxfhindir(struct mv_pp2x *pp2)
+{
+	int i;
+	int online_cpus = mv_pp2x_num_online_cpu_get(pp2);
+
+	if (!online_cpus)
+		return;
+
+	for (i = 0; i < MVPP22_RSS_TBL_LINE_NUM; i++)
+		pp2->rx_indir_table[i] = i%online_cpus;
+}
+
+void mv_pp2x_pp2_basic_print(struct platform_device *pdev, struct mv_pp2x *priv)
+{
+
+	DBG_MSG("%s\n", __func__);
+
+	DBG_MSG("num_present_cpus(%d) num_act_cpus(%d) num_online_cpus(%d)\n",
+		num_present_cpus(), num_active_cpus(), num_online_cpus());
+	DBG_MSG("cpu_map(%x)\n", priv->cpu_map);
+
+	DBG_MSG("pdev->name(%s) pdev->id(%d)\n", pdev->name, pdev->id);
+	DBG_MSG("dev.init_name(%s) dev.id(%d)\n",
+		pdev->dev.init_name, pdev->dev.id);
+	DBG_MSG("dev.kobj.name(%s)\n", pdev->dev.kobj.name);
+	DBG_MSG("dev->bus.name(%s) pdev.dev->bus.dev_name(%s)\n",
+		pdev->dev.bus->name, pdev->dev.bus->dev_name);
+
+	DBG_MSG("Device dma_coherent(%d)\n", pdev->dev.archdata.dma_coherent);
+
+
+	DBG_MSG("pp2_ver(%d)\n", priv->pp2_version);
+	DBG_MSG("queue_mode(%d)\n", priv->pp2_cfg.queue_mode);
+	DBG_MSG("first_bm_pool(%d) jumbo_pool(%d)\n",
+		priv->pp2_cfg.first_bm_pool, priv->pp2_cfg.jumbo_pool);
+	DBG_MSG("cell_index(%d) num_ports(%d)\n",
+		priv->pp2_cfg.cell_index, priv->num_ports);
+#ifdef CONFIG_64BIT
+	DBG_MSG("skb_base_addr(%p)\n", (void *)priv->pp2xdata->skb_base_addr);
+#endif
+	DBG_MSG("hw->base(%p)\n", priv->hw.base);
+	if (priv->pp2_version == PPV22) {
+		DBG_MSG("gop_addr: gmac(%p) xlg(%p) serdes(%p)\n",
+			priv->hw.gop.gop_110.gmac.base,
+			priv->hw.gop.gop_110.xlg_mac.base,
+			priv->hw.gop.gop_110.serdes.base);
+		DBG_MSG("gop_addr: xmib(%p) smi(%p) xsmi(%p)\n",
+			priv->hw.gop.gop_110.xmib.base,
+			priv->hw.gop.gop_110.smi_base,
+			priv->hw.gop.gop_110.xsmi_base);
+		DBG_MSG("gop_addr: mspg(%p) xpcs(%p) ptp(%p)\n",
+			priv->hw.gop.gop_110.mspg_base,
+			priv->hw.gop.gop_110.xpcs_base,
+			priv->hw.gop.gop_110.ptp.base);
+		DBG_MSG("gop_addr: rfu1(%p)\n",
+			priv->hw.gop.gop_110.rfu1_base);
+	}
+
+}
+EXPORT_SYMBOL(mv_pp2x_pp2_basic_print);
+
+void mv_pp2x_pp2_port_print(struct mv_pp2x_port *port)
+{
+	int i;
+
+	DBG_MSG("%s port_id(%d)\n", __func__, port->id);
+	DBG_MSG("\t ifname(%s)\n", port->dev->name);
+	DBG_MSG("\t first_rxq(%d)\n", port->first_rxq);
+	DBG_MSG("\t num_irqs(%d)\n", port->num_irqs);
+	DBG_MSG("\t pkt_size(%d)\n", port->pkt_size);
+	DBG_MSG("\t flags(%lx)\n", port->flags);
+	DBG_MSG("\t tx_ring_size(%d)\n", port->tx_ring_size);
+	DBG_MSG("\t rx_ring_size(%d)\n", port->rx_ring_size);
+	DBG_MSG("\t time_coal(%d)\n", port->tx_time_coal);
+	DBG_MSG("\t pool_long(%p)\n", port->pool_long);
+	DBG_MSG("\t pool_short(%p)\n", port->pool_short);
+	DBG_MSG("\t first_rxq(%d)\n", port->first_rxq);
+	DBG_MSG("\t num_rx_queues(%d)\n", port->num_rx_queues);
+	DBG_MSG("\t num_tx_queues(%d)\n", port->num_tx_queues);
+	DBG_MSG("\t num_qvector(%d)\n", port->num_qvector);
+
+	for (i = 0; i < port->num_qvector; i++) {
+		DBG_MSG("\t qvector_index(%d)\n", i);
+#if !defined(CONFIG_MV_PP2_POLLING)
+		DBG_MSG("\t\t irq(%d)\n",
+			port->q_vector[i].irq);
+#endif
+		DBG_MSG("\t\t qv_type(%d)\n",
+			port->q_vector[i].qv_type);
+		DBG_MSG("\t\t sw_thread_id	(%d)\n",
+			port->q_vector[i].sw_thread_id);
+		DBG_MSG("\t\t sw_thread_mask(%d)\n",
+			port->q_vector[i].sw_thread_mask);
+		DBG_MSG("\t\t first_rx_queue(%d)\n",
+			port->q_vector[i].first_rx_queue);
+		DBG_MSG("\t\t num_rx_queues(%d)\n",
+			port->q_vector[i].num_rx_queues);
+		DBG_MSG("\t\t pending_cause_rx(%d)\n",
+			port->q_vector[i].pending_cause_rx);
+
+	}
+	DBG_MSG("\t GOP ind(%d) phy_mode(%d) phy_addr(%d)\n",
+		port->mac_data.gop_index, port->mac_data.phy_mode,
+		port->mac_data.phy_addr);
+	DBG_MSG("\t GOP force_link(%d) autoneg(%d) duplex(%d) speed(%d)\n",
+		port->mac_data.force_link, port->mac_data.autoneg,
+		port->mac_data.duplex, port->mac_data.speed);
+#if !defined(CONFIG_MV_PP2_POLLING)
+	DBG_MSG("\t GOP link_irq(%d)\n", port->mac_data.link_irq);
+#endif
+	DBG_MSG("\t GOP phy_dev(%p) phy_node(%p)\n", port->mac_data.phy_dev,
+		port->mac_data.phy_node);
+
+}
+EXPORT_SYMBOL(mv_pp2x_pp2_port_print);
+
+static void mv_pp2x_pp2_ports_print(struct mv_pp2x *priv)
+{
+	int i;
+	struct mv_pp2x_port *port;
+
+	for (i = 0; i < priv->num_ports; i++) {
+		if (priv->port_list[i] == NULL) {
+			pr_emerg("\t port_list[%d]= NULL!\n", i);
+			continue;
+		}
+		port = priv->port_list[i];
+		mv_pp2x_pp2_port_print(port);
+	}
+}
+EXPORT_SYMBOL(mv_pp2x_pp2_ports_print);
+
+static int mv_pp2x_platform_data_get(struct platform_device *pdev,
+		struct mv_pp2x *priv,	u32 *cell_index, int *port_count)
+{
+	struct mv_pp2x_hw *hw = &priv->hw;
+	const struct of_device_id *match;
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	struct device_node *dn = pdev->dev.of_node;
+	struct resource *res;
+	resource_size_t mspg_base, mspg_end;
+	u32	err;
+#endif
+
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	match = of_match_node(mv_pp2x_match_tbl, dn);
+#else
+	match = &mv_pp2x_match_tbl[1];
+	PALAD(MVPP2_PRINT_LINE());
+#endif
+	if (!match)
+		return -ENODEV;
+
+	priv->pp2xdata = (struct mv_pp2x_platform_data *) match->data;
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	if (of_property_read_u32(dn, "cell-index", cell_index))
+		*cell_index = 0;
+
+	MVPP2_PRINT_VAR(*cell_index);
+
+	/* PPV2 Address Space */
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "pp");
+	hw->base = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(hw->base))
+		return PTR_ERR(hw->base);
+	MVPP2_PRINT_2LINE();
+
+	if (priv->pp2xdata->pp2x_ver == PPV21) {
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "lms");
+		hw->lms_base = devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->lms_base))
+			return PTR_ERR(hw->lms_base);
+	} else {
+		/* serdes */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "serdes");
+		hw->gop.gop_110.serdes.base =
+			devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->gop.gop_110.serdes.base))
+			return PTR_ERR(hw->gop.gop_110.serdes.base);
+		hw->gop.gop_110.serdes.obj_size = 0x1000;
+
+		MVPP2_PRINT_2LINE();
+
+		/* xmib */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "xmib");
+		hw->gop.gop_110.xmib.base =
+			devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->gop.gop_110.xmib.base))
+			return PTR_ERR(hw->gop.gop_110.xmib.base);
+		hw->gop.gop_110.xmib.obj_size = 0x0100;
+
+
+		MVPP2_PRINT_2LINE();
+
+		/* skipped led */
+
+		/* smi */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "smi");
+		hw->gop.gop_110.smi_base =
+			devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->gop.gop_110.smi_base))
+			return PTR_ERR(hw->gop.gop_110.smi_base);
+
+		/* rfu1 */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "rfu1");
+		hw->gop.gop_110.rfu1_base =
+			devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->gop.gop_110.rfu1_base))
+			return PTR_ERR(hw->gop.gop_110.rfu1_base);
+
+		MVPP2_PRINT_2LINE();
+
+		/* skipped tai */
+
+		/* xsmi  */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "xsmi");
+		hw->gop.gop_110.xsmi_base =
+			devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->gop.gop_110.xsmi_base))
+			return PTR_ERR(hw->gop.gop_110.xsmi_base);
+
+
+		MVPP2_PRINT_2LINE();
+
+		/* MSPG - base register */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "mspg");
+		hw->gop.gop_110.mspg_base =
+			devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(hw->gop.gop_110.mspg_base))
+			return PTR_ERR(hw->gop.gop_110.mspg_base);
+		mspg_base = res->start;
+		mspg_end  = res->end;
+
+
+		MVPP2_PRINT_2LINE();
+
+
+		/* xpcs */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "xpcs");
+		if ((res->start <= mspg_base) || (res->end >= mspg_end))
+			return -ENXIO;
+		hw->gop.gop_110.xpcs_base =
+			(void *)(hw->gop.gop_110.mspg_base +
+				(res->start-mspg_base));
+
+
+		MVPP2_PRINT_2LINE();
+
+		hw->gop.gop_110.ptp.base =
+			(void *)(hw->gop.gop_110.mspg_base + 0x0800);
+		hw->gop.gop_110.ptp.obj_size = 0x1000;
+		/* MSPG - gmac */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "gmac");
+		if ((res->start <= mspg_base) || (res->end >= mspg_end))
+			return -ENXIO;
+		hw->gop.gop_110.gmac.base =
+			(void *)(hw->gop.gop_110.mspg_base +
+			(res->start-mspg_base));
+		hw->gop.gop_110.gmac.obj_size = 0x1000;
+
+
+		MVPP2_PRINT_2LINE();
+
+
+		/* MSPG - xlg */
+		res = platform_get_resource_byname(pdev,
+			IORESOURCE_MEM, "xlg");
+		if ((res->start <= mspg_base) || (res->end >= mspg_end))
+			return -ENXIO;
+		hw->gop.gop_110.xlg_mac.base =
+			(void *)(hw->gop.gop_110.mspg_base +
+			(res->start-mspg_base));
+		hw->gop.gop_110.xlg_mac.obj_size = 0x1000;
+
+
+		MVPP2_PRINT_2LINE();
+
+	}
+#else /*CONFIG_MV_PP2_FPGA*/
+	MVPP2_PRINT_VAR(hw->base);
+	hw->base = mv_pp2_vfpga_address;
+	pr_debug("mvpp2(%d): mv_pp2x_probe:mv_pp2_vfpga_address=0x%p\n",
+		__LINE__, mv_pp2_vfpga_address);
+#endif
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+
+	hw->gop_core_clk = devm_clk_get(&pdev->dev, "gop_core_clk");
+	if (IS_ERR(hw->gop_core_clk))
+		return PTR_ERR(hw->gop_core_clk);
+	err = clk_prepare_enable(hw->gop_core_clk);
+	if (err < 0)
+		return err;
+
+	hw->gop_clk = devm_clk_get(&pdev->dev, "gop_clk");
+	if (IS_ERR(hw->gop_clk))
+		return PTR_ERR(hw->gop_clk);
+	err = clk_prepare_enable(hw->gop_clk);
+	if (err < 0)
+		return err;
+
+	hw->mg_core_clk = devm_clk_get(&pdev->dev, "mg_core_clk");
+	if (IS_ERR(hw->mg_clk))
+		return PTR_ERR(hw->mg_core_clk);
+	err = clk_prepare_enable(hw->mg_core_clk);
+	if (err < 0)
+		return err;
+
+	hw->mg_clk = devm_clk_get(&pdev->dev, "mg_clk");
+	if (IS_ERR(hw->mg_clk))
+		return PTR_ERR(hw->mg_clk);
+	err = clk_prepare_enable(hw->mg_clk);
+	if (err < 0)
+		return err;
+	hw->pp_clk = devm_clk_get(&pdev->dev, "pp_clk");
+	if (IS_ERR(hw->pp_clk))
+		return PTR_ERR(hw->pp_clk);
+	err = clk_prepare_enable(hw->pp_clk);
+	if (err < 0)
+		return err;
+
+	/* Get system's tclk rate */
+	hw->tclk = clk_get_rate(hw->pp_clk);
+	MVPP2_PRINT_VAR(hw->tclk);
+#else
+	hw->tclk = 25000000;
+#endif
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	*port_count = of_get_available_child_count(dn);
+	MVPP2_PRINT_VAR(*port_count);
+	if (*port_count == 0) {
+		dev_err(&pdev->dev, "no ports enabled\n");
+		err = -ENODEV;
+	}
+#else
+	*port_count = 2;
+	PALAD(MVPP2_PRINT_LINE());
+
+#endif
+	return 0;
+}
+static int mv_pp2x_probe(struct platform_device *pdev)
+{
+	struct mv_pp2x *priv;
+	struct mv_pp2x_hw *hw;
+	int port_count = 0, cpu;
+	int i, err;
+	u16 cpu_map;
+	u32 cell_index = 0;
+	u32 net_comp_config;
+
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	struct device_node *dn = pdev->dev.of_node;
+	struct device_node *port_node;
+#endif
+#ifdef CONFIG_MV_PP2_FPGA
+	int start_port = 0;
+#endif
+#ifdef CONFIG_MV_PP2_PALLADIUM
+	int start_port = 1;
+#endif
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(struct mv_pp2x), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+	hw = &priv->hw;
+
+	err = mv_pp2x_platform_data_get(pdev, priv, &cell_index, &port_count);
+	if (err) {
+		pr_crit("mvpp2: platform_data get failed\n");
+		goto err_clk;
+	}
+	MVPP2_PRINT_2LINE();
+
+	priv->pp2_version = priv->pp2xdata->pp2x_ver;
+
+	/* DMA Configruation */
+	err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	if (err) {
+		pr_crit("mvpp2: cannot set dma_mask\n");
+		goto err_clk;
+	}
+	/*Set dma_coherency to 0 */
+#ifdef CONFIG_MV_PP2_FPGA
+	pdev->dev.archdata.dma_coherent = 0;
+#else
+	pdev->dev.archdata.dma_coherent = 0; /*PKAK not coherent*/
+#endif
+MVPP2_PRINT_LINE();
+
+#ifdef CONFIG_64BIT
+{
+	/* Set skb_base_address (MSB bits) */
+	struct sk_buff *skb;
+
+	if (priv->pp2xdata->skb_base_addr == 0) {
+		skb = alloc_skb(MVPP2_SKB_TEST_SIZE, GFP_KERNEL);
+		MVPP2_PRINT_VAR(skb);
+		if (!skb) {
+			err = ENOMEM;
+			goto err_clk;
+		}
+		priv->pp2xdata->skb_base_addr =
+			(uintptr_t)skb & ~(priv->pp2xdata->skb_base_mask);
+		kfree_skb(skb);
+	}
+}
+#endif
+
+	MVPP2_PRINT_LINE();
+
+	/* Save cpu_present_mask + populate the per_cpu address space */
+	cpu_map = 0;
+	i = 0;
+
+	for_each_online_cpu(cpu) {
+		PALAD(MVPP2_PRINT_LINE());
+		cpu_map |= (1<<cpu);
+		hw->cpu_base[cpu] = hw->base;
+		if (priv->pp2xdata->multi_addr_space) {
+			hw->cpu_base[cpu] +=
+				(first_addr_space + i)*MVPP2_ADDR_SPACE_SIZE;
+			i++;
+		}
+	}
+	priv->cpu_map = cpu_map;
+
+	/* Initialize network controller */
+	err = mv_pp2x_init(pdev, priv);
+	if (err < 0) {
+		dev_err(&pdev->dev, "failed to initialize controller\n");
+		goto err_clk;
+	}
+
+	/*Init GOP */
+
+#ifdef MV_PP22_GOP_DEBUG
+	for (i = 0; i < MVCPN110_GOP_MAC_NUM; i++)
+		hw->gop.gop_110.gop_port_debug[i].flags = (1 << NOT_CREATED);
+#endif
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	/* smi init */
+	if (priv->pp2_version == PPV21)
+		mv_gop110_smi_init(&hw->gop);
+#endif
+
+	priv->port_list = devm_kcalloc(&pdev->dev, port_count,
+				      sizeof(struct mv_pp2x_port *),
+				      GFP_KERNEL);
+	if (!priv->port_list) {
+		err = -ENOMEM;
+		goto err_clk;
+	}
+
+	/*Init PP2 Configuration */
+	mv_pp2x_init_config(&priv->pp2_cfg, cell_index);
+	mv_pp2x_pp2_basic_print(pdev, priv);
+
+
+	/* Init PP22 rxfhindir table evenly in probe */
+	mv_pp2x_init_rxfhindir(priv);
+
+	/* Initialize ports */
+#if !defined(CONFIG_MV_PP2_FPGA) && !defined(CONFIG_MV_PP2_PALLADIUM)
+	for_each_available_child_of_node(dn, port_node) {
+		err = mv_pp2x_port_probe(pdev, port_node, priv);
+		if (err < 0)
+			goto err_clk;
+	}
+#else
+
+	for (i = start_port ; i < (start_port+port_count) ; i++) {
+		err = mv_pp2x_port_probe_fpga(pdev, i, priv);
+		if (err < 0)
+			goto err_clk;
+	}
+#endif
+
+	net_comp_config = mvp_pp2x_gop110_netc_cfg_create(priv);
+	mv_gop110_netc_init(&priv->hw.gop, net_comp_config,
+				MV_NETC_FIRST_PHASE);
+	mv_gop110_netc_init(&priv->hw.gop, net_comp_config,
+				MV_NETC_SECOND_PHASE);
+
+	mv_pp2x_pp2_ports_print(priv);
+#if defined(CONFIG_MV_PP2_POLLING)
+	init_timer(&cpu_poll_timer);
+	cpu_poll_timer.function = mv_pp22_cpu_timer_callback;
+	cpu_poll_timer.data     = (unsigned long)pdev;
+#endif
+
+	platform_set_drvdata(pdev, priv);
+	pr_debug("Platform Device Name : %s\n", kobject_name(&pdev->dev.kobj));
+	return 0;
+
+err_clk:
+	clk_disable_unprepare(hw->gop_clk);
+	clk_disable_unprepare(hw->pp_clk);
+	clk_disable_unprepare(hw->gop_clk);
+	clk_disable_unprepare(hw->gop_core_clk);
+	clk_disable_unprepare(hw->mg_clk);
+	clk_disable_unprepare(hw->mg_core_clk);
+	return err;
+}
+
+static int mv_pp2x_remove(struct platform_device *pdev)
+{
+	struct mv_pp2x *priv = platform_get_drvdata(pdev);
+	struct mv_pp2x_hw *hw = &priv->hw;
+	int i;
+
+	for (i = 0; i < priv->num_ports; i++) {
+		if (priv->port_list[i])
+			mv_pp2x_port_remove(priv->port_list[i]);
+		priv->num_ports--;
+	}
+
+	for (i = 0; i < priv->num_pools; i++) {
+		struct mv_pp2x_bm_pool *bm_pool = &priv->bm_pools[i];
+
+		mv_pp2x_bm_pool_destroy(pdev, priv, bm_pool);
+	}
+
+	for_each_online_cpu(i) {
+		struct mv_pp2x_aggr_tx_queue *aggr_txq = &priv->aggr_txqs[i];
+
+		dma_free_coherent(&pdev->dev,
+				  MVPP2_DESCQ_MEM_SIZE(aggr_txq->size),
+				  aggr_txq->desc_mem,
+				  aggr_txq->descs_phys);
+	}
+
+	clk_disable_unprepare(hw->pp_clk);
+	clk_disable_unprepare(hw->gop_clk);
+
+	return 0;
+}
+
+
+
+
+MODULE_DEVICE_TABLE(of, mv_pp2x_match_tbl);
+
+static struct platform_driver mv_pp2x_driver = {
+	.probe = mv_pp2x_probe,
+	.remove = mv_pp2x_remove,
+	.driver = {
+		.name = MVPP2_DRIVER_NAME,
+		.of_match_table = mv_pp2x_match_tbl,
+	},
+};
+
+static int mv_pp2x_rxq_number_get(void)
+{
+	int rx_queue_num;
+
+	if (mv_pp2x_queue_mode == MVPP2_QDIST_SINGLE_MODE)
+		rx_queue_num = mv_pp2x_num_cos_queues;
+	else
+		rx_queue_num = mv_pp2x_num_cos_queues * num_active_cpus();
+
+	return rx_queue_num;
+}
+
+
+#ifdef CONFIG_MV_PP2_FPGA
+
+static int mv_pp2_pci_probe(struct pci_dev *pdev,
+	const struct pci_device_id *ent)
+{
+	unsigned long flags;
+
+	/* code below relevant for FPGA only */
+	if (pci_enable_device(pdev)) {
+		pr_err("mvpp2: can not enable PCI device\n");
+		return -1;
+	}
+	flags = pci_resource_flags(pdev, 0);
+
+	if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {
+		pr_err("mvpp2: can not find proper PCI device base address\n");
+		return -ENODEV;
+	}
+	pr_crit("pci flags:0x%lx\n", flags);
+
+	if (pci_request_regions(pdev, "mv_pp2_pci")) {
+		pr_err("mvpp2: can not obtain PCI resources\n");
+		return -ENODEV;
+	}
+
+	if (pci_set_dma_mask(pdev, DMA_BIT_MASK(32))) {
+		pr_err("mvpp2: no usable DMA configuration\n");
+		return -ENODEV;
+	}
+
+	mv_pp2_vfpga_address = pci_iomap(pdev, 0, 16 * 1024 * 1024);
+
+	if (!mv_pp2_vfpga_address)
+		pr_err("mvpp2: can not map device registers\n");
+
+	pr_debug("mvpp2: fpga base: VIRT=0x%p, size=%d KBytes\n",
+		mv_pp2_vfpga_address, 16 * 1024);
+	return 0;
+}
+
+
+static void mv_pp2_pci_remove(struct pci_dev *pdev)
+{
+	if (mv_pp2_vfpga_address)
+		pci_iounmap(pdev, mv_pp2_vfpga_address);
+	pci_release_regions(pdev);
+	pr_info("mvpp2: PCI device removed\n");
+}
+
+static const struct pci_device_id fpga_id_table[] = {
+	{ 0x1234, 0x1234, PCI_ANY_ID, PCI_ANY_ID, 2, 0, 0}, {0}
+};
+
+MODULE_DEVICE_TABLE(pci, fpga_id_table);
+
+
+static struct pci_driver mv_pp2_pci_driver = {
+	.name	= "mv_pp2_pci",
+	.id_table = fpga_id_table,
+	.probe		= mv_pp2_pci_probe,
+	.remove		= mv_pp2_pci_remove,
+};
+
+#endif
+
+#if defined(CONFIG_MV_PP2_FPGA) || defined(CONFIG_MV_PP2_PALLADIUM)
+
+static struct resource mv_pp2x_resources[] = {
+#ifdef CONFIG_MV_PP2_PALLADIUM
+	{
+		.name = MVPP2_DRIVER_NAME,
+		.start = MVPP2_ADDRESS,
+		.end   = MVPP2_ADDRESS + (CPN110_ADDRESS_SPACE_SIZE - 1),
+		.flags = IORESOURCE_MEM,
+	},
+#endif
+};
+
+static struct platform_device mv_pp2x_device = {
+	.name           = MVPP2_DRIVER_NAME,
+	.id             = 0,
+	.num_resources	= ARRAY_SIZE(mv_pp2x_resources),
+	.resource       = mv_pp2x_resources,
+	.dev            = {
+		.platform_data = 0,
+		.init_name = "f2000000.ppv22",
+	},
+};
+
+#endif
+
+static int __init mpp2_module_init(void)
+{
+	int ret = 0;
+
+
+#if defined(CONFIG_MV_PP2_FPGA) || defined(CONFIG_MV_PP2_PALLADIUM)
+
+	if (platform_device_register(&mv_pp2x_device)) {
+		pr_crit("mvpp2(%d): platform_device_register failed\n",
+			__LINE__);
+		return -1;
+	}
+
+#ifdef CONFIG_MV_PP2_FPGA
+	MVPP2_PRINT_LINE();
+	ret = pci_register_driver(&mv_pp2_pci_driver);
+	if (ret < 0) {
+		pr_err(
+		"mvpp2: PCI card not found, driver not installed. rc=%d\n",
+		ret);
+		return ret;
+	}
+#endif
+
+	/*pr_crit("mv_pp2x_device list_empty=%d line=%d\n",
+	 *	list_empty(&mv_pp2x_device.dev.devres_head), __LINE__);
+	*/
+
+	mv_pp2x_device.dev.dma_mask = &(mv_pp2x_device.dev.coherent_dma_mask);
+
+	/*pr_crit("mv_pp2x_device list_empty=%d line=%d\n",
+	 *	list_empty(&mv_pp2x_device.dev.devres_head), __LINE__);
+	 */
+
+	mv_pp2x_num_cos_queues = 4;
+
+#endif
+	mv_pp2x_rxq_number = mv_pp2x_rxq_number_get();
+	mv_pp2x_txq_number = mv_pp2x_num_cos_queues;
+	PALAD(MVPP2_PRINT_LINE());
+
+	/* Compiler does not allow below Init in structure definition */
+	mv_pp2x_pools[MVPP2_BM_SWF_SHORT_POOL].pkt_size =
+		MVPP2_BM_SHORT_PKT_SIZE;
+	mv_pp2x_pools[MVPP2_BM_SWF_LONG_POOL].pkt_size =
+		MVPP2_BM_LONG_PKT_SIZE;
+	mv_pp2x_pools[MVPP2_BM_SWF_JUMBO_POOL].pkt_size =
+		MVPP2_BM_JUMBO_PKT_SIZE;
+
+	/*pr_crit("mv_pp2x_device list_empty=%d line=%d\n",
+	 * list_empty(&mv_pp2x_device.dev.devres_head), __LINE__);
+	 */
+
+	PALAD(MVPP2_PRINT_LINE());
+
+	ret = platform_driver_register(&mv_pp2x_driver);
+
+	/*pr_crit("mv_pp2x_device list_empty=%d line=%d\n",
+	 *	list_empty(&mv_pp2x_device.dev.devres_head), __LINE__);
+	 */
+	PALAD(MVPP2_PRINT_LINE());
+
+	return ret;
+}
+
+static void __exit mpp2_module_exit(void)
+{
+	platform_driver_unregister(&mv_pp2x_driver);
+#if defined(CONFIG_MV_PP2_FPGA) || defined(CONFIG_MV_PP2_PALLADIUM)
+	/* [AW] kfree(NULL) is safe, so no need for check */
+	/* if (mv_pp2x_device.dev.dma_mask) */
+		kfree(mv_pp2x_device.dev.dma_mask);
+#ifdef CONFIG_MV_PP2_FPGA
+	pci_unregister_driver(&mv_pp2_pci_driver);
+#endif
+	platform_device_unregister(&mv_pp2x_device);
+#endif
+}
+
+
+#if defined(CONFIG_MV_PP2_POLLING)
+static void mv_pp22_cpu_timer_callback(unsigned long data)
+{
+	struct platform_device *pdev = (struct platform_device *)data;
+	struct mv_pp2x *priv = platform_get_drvdata(pdev);
+	int i = 0, j, err;
+	struct mv_pp2x_port *port;
+	u32 timeout;
+
+
+	/* Check link_change for initialized ports */
+	for (i = 0 ; i < priv->num_ports; i++) {
+		port = priv->port_list[i];
+		if (port && port->link_change_tasklet.func)
+			tasklet_schedule(&port->link_change_tasklet);
+
+	}
+
+	/* Schedule napi for ports with link_up. */
+	for (i = 0 ; i < priv->num_ports; i++) {
+		port = priv->port_list[i];
+		if (port && netif_carrier_ok(port->dev)) {
+			for (j = 0 ; j < num_active_cpus(); j++) {
+				if (j == smp_processor_id()) {
+					napi_schedule(&port->q_vector[j].napi);
+				} else {
+					err = smp_call_function_single(j,
+						(smp_call_func_t)napi_schedule,
+						&port->q_vector[j].napi, 1);
+					if (err)
+						pr_crit("napi_schedule error: %s\n",
+							__func__);
+				}
+			}
+		} else if (port) {
+			pr_debug("mvpp2(%d): port=%p netif_carrier_ok=%d\n",
+				__LINE__, port,
+				netif_carrier_ok(port->dev));
+		} else
+			pr_debug("mvpp2(%d): mv_pp22_cpu_timer_callback. PORT NULL !!!!\n",
+				__LINE__);
+	}
+
+	PALAD(MVPP2_PRINT_2LINE());
+
+	if (debug_param)
+		timeout = debug_param;
+	else
+		timeout = MV_PP2_FPGA_PERODIC_TIME;
+#ifdef CONFIG_MV_PP2_PALLADIUM
+	timeout = timeout*1000;
+#endif
+	mod_timer(&cpu_poll_timer, jiffies + msecs_to_jiffies(timeout));
+
+}
+
+#endif
+module_init(mpp2_module_init);
+module_exit(mpp2_module_exit);
+
+
+MODULE_DESCRIPTION("Marvell PPv2x Ethernet Driver - www.marvell.com");
+MODULE_AUTHOR("Marvell");
+MODULE_LICENSE("GPL v2");
-- 
1.7.9.5

