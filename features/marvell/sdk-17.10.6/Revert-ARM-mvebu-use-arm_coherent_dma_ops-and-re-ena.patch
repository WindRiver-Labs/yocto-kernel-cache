From 56b0be6c149f34f94b2ee93f3f1227d8435c44d0 Mon Sep 17 00:00:00 2001
From: Ofer Heifetz <oferh@marvell.com>
Date: Thu, 10 Mar 2016 12:59:34 +0200
Subject: [PATCH 0128/1345] Revert "ARM: mvebu: use arm_coherent_dma_ops and
 re-enable hardware I/O coherency"

commit  ee5c671c2481a0b4a5a85315093886869e5fa3fa from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

Revert commit 1bd4d8a6de5c ("ARM: mvebu: use arm_coherent_dma_ops and
re-enable hardware I/O coherency")

Now that we have enabled automatic I/O synchronization barriers, we no
longer need any explicit barriers. We can therefore simplify
arch/arm/mach-mvebu/coherency.c by using the existing
arm_coherent_dma_ops instead of our custom mvebu_hwcc_dma_ops, and
re-enable hardware I/O coherency support.

Change-Id: I3e827c6c2608b2570f22541f29fce5fb9b3f3ef9
Signed-off-by: Ofer Heifetz <oferh@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/28260
Tested-by: Star_Automation <star@marvell.com>
Reviewed-by: Lior Amsalem <alior@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 arch/arm/mach-mvebu/coherency.c |   54 ++++++++++++++++++++++++++++++++++++---
 1 file changed, 51 insertions(+), 3 deletions(-)

diff --git a/arch/arm/mach-mvebu/coherency.c b/arch/arm/mach-mvebu/coherency.c
index 35b2194..4ab8158 100644
--- a/arch/arm/mach-mvebu/coherency.c
+++ b/arch/arm/mach-mvebu/coherency.c
@@ -34,7 +34,6 @@
 #include <asm/smp_scu.h>
 #include <asm/cacheflush.h>
 #include <asm/mach/map.h>
-#include <asm/dma-mapping.h>
 #include "common.h"
 #include "coherency.h"
 #include "mvebu-soc-id.h"
@@ -93,6 +92,55 @@ static void armada_xp_clear_shared_l2(void)
 	writel(reg, cpu_config_base);
 }
 
+static inline void mvebu_hwcc_sync_io_barrier(void)
+{
+	writel(0x1, coherency_cpu_base + IO_SYNC_BARRIER_CTL_OFFSET);
+	while (readl(coherency_cpu_base + IO_SYNC_BARRIER_CTL_OFFSET) & 0x1)
+		;
+}
+
+static dma_addr_t mvebu_hwcc_dma_map_page(struct device *dev, struct page *page,
+					  unsigned long offset, size_t size,
+					  enum dma_data_direction dir,
+					  struct dma_attrs *attrs)
+{
+	if (dir != DMA_TO_DEVICE)
+		mvebu_hwcc_sync_io_barrier();
+	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
+}
+
+
+static void mvebu_hwcc_dma_unmap_page(struct device *dev, dma_addr_t dma_handle,
+				      size_t size, enum dma_data_direction dir,
+				      struct dma_attrs *attrs)
+{
+	if (dir != DMA_TO_DEVICE)
+		mvebu_hwcc_sync_io_barrier();
+}
+
+static void mvebu_hwcc_dma_sync(struct device *dev, dma_addr_t dma_handle,
+				size_t size, enum dma_data_direction dir)
+{
+	if (dir != DMA_TO_DEVICE)
+		mvebu_hwcc_sync_io_barrier();
+}
+
+static const struct dma_map_ops mvebu_hwcc_dma_ops = {
+	.alloc			= arm_dma_alloc,
+	.free			= arm_dma_free,
+	.mmap			= arm_dma_mmap,
+	.map_page		= mvebu_hwcc_dma_map_page,
+	.unmap_page		= mvebu_hwcc_dma_unmap_page,
+	.get_sgtable		= arm_dma_get_sgtable,
+	.map_sg			= arm_dma_map_sg,
+	.unmap_sg		= arm_dma_unmap_sg,
+	.sync_single_for_cpu	= mvebu_hwcc_dma_sync,
+	.sync_single_for_device	= mvebu_hwcc_dma_sync,
+	.sync_sg_for_cpu	= arm_dma_sync_sg_for_cpu,
+	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
+	.set_dma_mask	= arm_dma_set_mask,
+};
+
 static int mvebu_hwcc_notifier(struct notifier_block *nb,
 			       unsigned long event, void *__dev)
 {
@@ -100,7 +148,7 @@ static int mvebu_hwcc_notifier(struct notifier_block *nb,
 
 	if (event != BUS_NOTIFY_ADD_DEVICE)
 		return NOTIFY_DONE;
-	set_dma_ops(dev, &arm_coherent_dma_ops);
+	set_dma_ops(dev, &mvebu_hwcc_dma_ops);
 
 	return NOTIFY_OK;
 }
@@ -265,7 +313,7 @@ int set_cpu_coherent(void)
 
 int coherency_available(void)
 {
-	return coherency_type() != COHERENCY_FABRIC_TYPE_NONE;
+	return false;
 }
 
 int __init coherency_init(void)
-- 
1.7.9.5

